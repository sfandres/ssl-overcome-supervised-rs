IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 04:08:58,959	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 04:08:58,959	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 04:09:01,387	SUCC scripts.py:747 -- --------------------
2024-01-07 04:09:01,387	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 04:09:01,387	SUCC scripts.py:749 -- --------------------
2024-01-07 04:09:01,388	INFO scripts.py:751 -- Next steps
2024-01-07 04:09:01,388	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 04:09:01,388	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 04:09:01,388	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 04:09:01,388	INFO scripts.py:773 -- import ray
2024-01-07 04:09:01,388	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 04:09:01,388	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 04:09:01,388	INFO scripts.py:791 --   ray status
2024-01-07 04:09:01,388	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 04:09:01,388	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 04:09:01,388	INFO scripts.py:810 --   ray stop
2024-01-07 04:09:01,389	INFO scripts.py:891 -- --block
2024-01-07 04:09:01,389	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 04:09:01,389	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              9605903621821404172
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f6082ba7100>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [5 5 5 5 5 5 5 5 5 5]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.94
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [5 5 5 5 5 5 5 5 5 5]
Done!

Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 04:09:44,535	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 04:09:44,549	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 04:10:07,568	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 04:10:07 (running for 00:00:22.14)
Memory usage on this node: 13.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |
| train_35a0b_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_35a0b_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_35a0b_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=87918)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=87918)[0m Configuration completed!
[2m[36m(func pid=87918)[0m New optimizer parameters:
[2m[36m(func pid=87918)[0m SGD (
[2m[36m(func pid=87918)[0m Parameter Group 0
[2m[36m(func pid=87918)[0m     dampening: 0
[2m[36m(func pid=87918)[0m     differentiable: False
[2m[36m(func pid=87918)[0m     foreach: None
[2m[36m(func pid=87918)[0m     lr: 0.0001
[2m[36m(func pid=87918)[0m     maximize: False
[2m[36m(func pid=87918)[0m     momentum: 0.99
[2m[36m(func pid=87918)[0m     nesterov: False
[2m[36m(func pid=87918)[0m     weight_decay: 0
[2m[36m(func pid=87918)[0m )
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9817 | Steps: 2 | Val loss: 2.3224 | Batch size: 32 | lr: 0.0001 | Duration: 4.82s
[2m[36m(func pid=87918)[0m top1: 0.17350746268656717
[2m[36m(func pid=87918)[0m top5: 0.5284514925373134
[2m[36m(func pid=87918)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=87918)[0m f1_macro: 0.11842080407966486
[2m[36m(func pid=87918)[0m f1_weighted: 0.12294604610704965
[2m[36m(func pid=87918)[0m f1_per_class: [0.308, 0.344, 0.0, 0.092, 0.0, 0.212, 0.015, 0.014, 0.0, 0.2]
== Status ==
Current time: 2024-01-07 04:10:17 (running for 00:00:31.40)
Memory usage on this node: 15.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |
| train_35a0b_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_35a0b_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88300)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=88300)[0m Configuration completed!
[2m[36m(func pid=88300)[0m New optimizer parameters:
[2m[36m(func pid=88300)[0m SGD (
[2m[36m(func pid=88300)[0m Parameter Group 0
[2m[36m(func pid=88300)[0m     dampening: 0
[2m[36m(func pid=88300)[0m     differentiable: False
[2m[36m(func pid=88300)[0m     foreach: None
[2m[36m(func pid=88300)[0m     lr: 0.001
[2m[36m(func pid=88300)[0m     maximize: False
[2m[36m(func pid=88300)[0m     momentum: 0.99
[2m[36m(func pid=88300)[0m     nesterov: False
[2m[36m(func pid=88300)[0m     weight_decay: 0
[2m[36m(func pid=88300)[0m )
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9714 | Steps: 2 | Val loss: 2.3201 | Batch size: 32 | lr: 0.001 | Duration: 4.53s
[2m[36m(func pid=88300)[0m top1: 0.18050373134328357
[2m[36m(func pid=88300)[0m top5: 0.5317164179104478
[2m[36m(func pid=88300)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=88300)[0m f1_macro: 0.12562765530224132
[2m[36m(func pid=88300)[0m f1_weighted: 0.12831002998958962
[2m[36m(func pid=88300)[0m f1_per_class: [0.341, 0.343, 0.0, 0.093, 0.0, 0.237, 0.018, 0.027, 0.0, 0.197]
== Status ==
Current time: 2024-01-07 04:10:25 (running for 00:00:40.12)
Memory usage on this node: 17.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |
| train_35a0b_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=88716)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=88716)[0m Configuration completed!
[2m[36m(func pid=88716)[0m New optimizer parameters:
[2m[36m(func pid=88716)[0m SGD (
[2m[36m(func pid=88716)[0m Parameter Group 0
[2m[36m(func pid=88716)[0m     dampening: 0
[2m[36m(func pid=88716)[0m     differentiable: False
[2m[36m(func pid=88716)[0m     foreach: None
[2m[36m(func pid=88716)[0m     lr: 0.01
[2m[36m(func pid=88716)[0m     maximize: False
[2m[36m(func pid=88716)[0m     momentum: 0.99
[2m[36m(func pid=88716)[0m     nesterov: False
[2m[36m(func pid=88716)[0m     weight_decay: 0
[2m[36m(func pid=88716)[0m )
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9528 | Steps: 2 | Val loss: 2.3026 | Batch size: 32 | lr: 0.01 | Duration: 4.53s
[2m[36m(func pid=88716)[0m top1: 0.14039179104477612
[2m[36m(func pid=88716)[0m top5: 0.558768656716418
[2m[36m(func pid=88716)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=88716)[0m f1_macro: 0.08837503165677207
[2m[36m(func pid=88716)[0m f1_weighted: 0.09762646287960756
[2m[36m(func pid=88716)[0m f1_per_class: [0.144, 0.258, 0.0, 0.053, 0.0, 0.182, 0.036, 0.033, 0.0, 0.178]
== Status ==
Current time: 2024-01-07 04:10:34 (running for 00:00:48.78)
Memory usage on this node: 20.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=89134)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=89134)[0m Configuration completed!
[2m[36m(func pid=89134)[0m New optimizer parameters:
[2m[36m(func pid=89134)[0m SGD (
[2m[36m(func pid=89134)[0m Parameter Group 0
[2m[36m(func pid=89134)[0m     dampening: 0
[2m[36m(func pid=89134)[0m     differentiable: False
[2m[36m(func pid=89134)[0m     foreach: None
[2m[36m(func pid=89134)[0m     lr: 0.1
[2m[36m(func pid=89134)[0m     maximize: False
[2m[36m(func pid=89134)[0m     momentum: 0.99
[2m[36m(func pid=89134)[0m     nesterov: False
[2m[36m(func pid=89134)[0m     weight_decay: 0
[2m[36m(func pid=89134)[0m )
[2m[36m(func pid=89134)[0m 
== Status ==
Current time: 2024-01-07 04:10:43 (running for 00:00:57.37)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |        |            |                      |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  2.953 |      0.088 |                    1 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |        |            |                      |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9610 | Steps: 2 | Val loss: 2.3286 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9989 | Steps: 2 | Val loss: 2.3266 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7284 | Steps: 2 | Val loss: 2.3167 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9275 | Steps: 2 | Val loss: 2.3339 | Batch size: 32 | lr: 0.1 | Duration: 4.90s
== Status ==
Current time: 2024-01-07 04:10:48 (running for 00:01:02.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.982 |      0.118 |                    1 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.971 |      0.126 |                    1 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  2.953 |      0.088 |                    1 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |        |            |                      |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.18097014925373134
[2m[36m(func pid=88300)[0m top5: 0.519589552238806
[2m[36m(func pid=88300)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=88300)[0m f1_macro: 0.10940104891952293
[2m[36m(func pid=88300)[0m f1_weighted: 0.12578861757190055
[2m[36m(func pid=88300)[0m f1_per_class: [0.247, 0.325, 0.0, 0.09, 0.012, 0.295, 0.009, 0.037, 0.0, 0.078]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.10727611940298508
[2m[36m(func pid=88716)[0m top5: 0.5713619402985075
[2m[36m(func pid=88716)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=88716)[0m f1_macro: 0.09852204444631273
[2m[36m(func pid=88716)[0m f1_weighted: 0.1222406033072988
[2m[36m(func pid=88716)[0m f1_per_class: [0.069, 0.184, 0.0, 0.095, 0.0, 0.096, 0.128, 0.179, 0.015, 0.218]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.18236940298507462
[2m[36m(func pid=87918)[0m top5: 0.5284514925373134
[2m[36m(func pid=87918)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=87918)[0m f1_macro: 0.10874393631616641
[2m[36m(func pid=87918)[0m f1_weighted: 0.126799053430445
[2m[36m(func pid=87918)[0m f1_per_class: [0.216, 0.336, 0.0, 0.094, 0.011, 0.279, 0.012, 0.024, 0.0, 0.115]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.05083955223880597
[2m[36m(func pid=89134)[0m top5: 0.48367537313432835
[2m[36m(func pid=89134)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=89134)[0m f1_macro: 0.03467628530734626
[2m[36m(func pid=89134)[0m f1_weighted: 0.05604837822042564
[2m[36m(func pid=89134)[0m f1_per_class: [0.073, 0.023, 0.025, 0.169, 0.022, 0.0, 0.009, 0.0, 0.0, 0.026]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3864 | Steps: 2 | Val loss: 2.2575 | Batch size: 32 | lr: 0.01 | Duration: 2.57s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9352 | Steps: 2 | Val loss: 2.3270 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9621 | Steps: 2 | Val loss: 2.3323 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.0958 | Steps: 2 | Val loss: 2.2850 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=88300)[0m top1: 0.1865671641791045
[2m[36m(func pid=88300)[0m top5: 0.5317164179104478
[2m[36m(func pid=88300)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=88300)[0m f1_macro: 0.1295942361350841
[2m[36m(func pid=88300)[0m f1_weighted: 0.13364532234165538
[2m[36m(func pid=88300)[0m f1_per_class: [0.27, 0.309, 0.108, 0.085, 0.009, 0.331, 0.024, 0.078, 0.0, 0.082]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:10:53 (running for 00:01:07.71)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.999 |      0.109 |                    2 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.935 |      0.13  |                    3 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  2.728 |      0.099 |                    2 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.928 |      0.035 |                    1 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.1525186567164179
[2m[36m(func pid=88716)[0m top5: 0.6226679104477612
[2m[36m(func pid=88716)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=88716)[0m f1_macro: 0.11002121027417018
[2m[36m(func pid=88716)[0m f1_weighted: 0.17728150928468855
[2m[36m(func pid=88716)[0m f1_per_class: [0.086, 0.052, 0.051, 0.242, 0.018, 0.055, 0.274, 0.151, 0.0, 0.172]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.1791044776119403
[2m[36m(func pid=87918)[0m top5: 0.519589552238806
[2m[36m(func pid=87918)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=87918)[0m f1_macro: 0.10580189803369366
[2m[36m(func pid=87918)[0m f1_weighted: 0.12676831582052708
[2m[36m(func pid=87918)[0m f1_per_class: [0.203, 0.311, 0.0, 0.102, 0.01, 0.312, 0.009, 0.021, 0.0, 0.091]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.15345149253731344
[2m[36m(func pid=89134)[0m top5: 0.6791044776119403
[2m[36m(func pid=89134)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=89134)[0m f1_macro: 0.10941245023854003
[2m[36m(func pid=89134)[0m f1_weighted: 0.12153777320724969
[2m[36m(func pid=89134)[0m f1_per_class: [0.242, 0.0, 0.061, 0.362, 0.02, 0.0, 0.0, 0.221, 0.0, 0.188]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8358 | Steps: 2 | Val loss: 2.3058 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0530 | Steps: 2 | Val loss: 2.1706 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9890 | Steps: 2 | Val loss: 2.3345 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.6221 | Steps: 2 | Val loss: 2.8184 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:10:58 (running for 00:01:12.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.962 |      0.106 |                    3 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.836 |      0.14  |                    4 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  2.386 |      0.11  |                    3 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.096 |      0.109 |                    2 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.18050373134328357
[2m[36m(func pid=88300)[0m top5: 0.5559701492537313
[2m[36m(func pid=88300)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=88300)[0m f1_macro: 0.1395898754280193
[2m[36m(func pid=88300)[0m f1_weighted: 0.1586437883561532
[2m[36m(func pid=88300)[0m f1_per_class: [0.159, 0.293, 0.237, 0.104, 0.013, 0.289, 0.106, 0.16, 0.0, 0.034]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.23973880597014927
[2m[36m(func pid=88716)[0m top5: 0.6632462686567164
[2m[36m(func pid=88716)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=88716)[0m f1_macro: 0.17007872644496455
[2m[36m(func pid=88716)[0m f1_weighted: 0.26650059792535313
[2m[36m(func pid=88716)[0m f1_per_class: [0.129, 0.072, 0.218, 0.39, 0.056, 0.187, 0.375, 0.069, 0.067, 0.138]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.1730410447761194
[2m[36m(func pid=87918)[0m top5: 0.5139925373134329
[2m[36m(func pid=87918)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=87918)[0m f1_macro: 0.09909670075801667
[2m[36m(func pid=87918)[0m f1_weighted: 0.12209629260303079
[2m[36m(func pid=87918)[0m f1_per_class: [0.156, 0.293, 0.0, 0.098, 0.009, 0.318, 0.009, 0.02, 0.0, 0.089]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.197294776119403
[2m[36m(func pid=89134)[0m top5: 0.5405783582089553
[2m[36m(func pid=89134)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=89134)[0m f1_macro: 0.08856468865872173
[2m[36m(func pid=89134)[0m f1_weighted: 0.1542554477165519
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.067, 0.0, 0.489, 0.087, 0.008, 0.0, 0.017, 0.053, 0.164]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7363 | Steps: 2 | Val loss: 2.2964 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.6814 | Steps: 2 | Val loss: 2.0663 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9568 | Steps: 2 | Val loss: 2.3328 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=88300)[0m top1: 0.15578358208955223
[2m[36m(func pid=88300)[0m top5: 0.5634328358208955
[2m[36m(func pid=88300)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=88300)[0m f1_macro: 0.11465513839243324
[2m[36m(func pid=88300)[0m f1_weighted: 0.1562201849600191
[2m[36m(func pid=88300)[0m f1_per_class: [0.1, 0.281, 0.194, 0.104, 0.024, 0.114, 0.19, 0.091, 0.0, 0.049]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.1107 | Steps: 2 | Val loss: 3.7688 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:11:03 (running for 00:01:17.94)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.989 |      0.099 |                    4 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.736 |      0.115 |                    5 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  2.053 |      0.17  |                    4 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.622 |      0.089 |                    3 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.31669776119402987
[2m[36m(func pid=88716)[0m top5: 0.7061567164179104
[2m[36m(func pid=88716)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=88716)[0m f1_macro: 0.2482965150632777
[2m[36m(func pid=88716)[0m f1_weighted: 0.2940896557967399
[2m[36m(func pid=88716)[0m f1_per_class: [0.337, 0.044, 0.338, 0.533, 0.058, 0.349, 0.218, 0.313, 0.096, 0.197]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17164179104477612
[2m[36m(func pid=87918)[0m top5: 0.5177238805970149
[2m[36m(func pid=87918)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=87918)[0m f1_macro: 0.08896402877053981
[2m[36m(func pid=87918)[0m f1_weighted: 0.12297737407344059
[2m[36m(func pid=87918)[0m f1_per_class: [0.083, 0.28, 0.0, 0.104, 0.0, 0.328, 0.012, 0.045, 0.0, 0.038]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.04011194029850746
[2m[36m(func pid=89134)[0m top5: 0.47388059701492535
[2m[36m(func pid=89134)[0m f1_micro: 0.04011194029850746
[2m[36m(func pid=89134)[0m f1_macro: 0.03744165966067735
[2m[36m(func pid=89134)[0m f1_weighted: 0.014696838081972741
[2m[36m(func pid=89134)[0m f1_per_class: [0.092, 0.055, 0.182, 0.0, 0.0, 0.0, 0.003, 0.0, 0.043, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6265 | Steps: 2 | Val loss: 2.2669 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3577 | Steps: 2 | Val loss: 2.0488 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9307 | Steps: 2 | Val loss: 2.3241 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.2795 | Steps: 2 | Val loss: 11.2684 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 04:11:08 (running for 00:01:23.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.957 |      0.089 |                    5 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.627 |      0.139 |                    6 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.681 |      0.248 |                    5 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.111 |      0.037 |                    4 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.18097014925373134
[2m[36m(func pid=88300)[0m top5: 0.6114738805970149
[2m[36m(func pid=88300)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=88300)[0m f1_macro: 0.13885494294130474
[2m[36m(func pid=88300)[0m f1_weighted: 0.18972408637583546
[2m[36m(func pid=88300)[0m f1_per_class: [0.131, 0.293, 0.253, 0.119, 0.011, 0.121, 0.268, 0.125, 0.0, 0.067]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.30736940298507465
[2m[36m(func pid=88716)[0m top5: 0.707089552238806
[2m[36m(func pid=88716)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=88716)[0m f1_macro: 0.20330463546729013
[2m[36m(func pid=88716)[0m f1_weighted: 0.24519050405756662
[2m[36m(func pid=88716)[0m f1_per_class: [0.327, 0.005, 0.273, 0.568, 0.053, 0.304, 0.108, 0.096, 0.044, 0.254]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17164179104477612
[2m[36m(func pid=87918)[0m top5: 0.5247201492537313
[2m[36m(func pid=87918)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=87918)[0m f1_macro: 0.09681763461361123
[2m[36m(func pid=87918)[0m f1_weighted: 0.1282177896077643
[2m[36m(func pid=87918)[0m f1_per_class: [0.095, 0.286, 0.048, 0.095, 0.0, 0.318, 0.035, 0.055, 0.0, 0.037]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.08768656716417911
[2m[36m(func pid=89134)[0m top5: 0.4193097014925373
[2m[36m(func pid=89134)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=89134)[0m f1_macro: 0.047338567903015084
[2m[36m(func pid=89134)[0m f1_weighted: 0.045560405072241544
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.252, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.111]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5284 | Steps: 2 | Val loss: 2.2350 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9879 | Steps: 2 | Val loss: 1.8103 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9477 | Steps: 2 | Val loss: 2.3135 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.0126 | Steps: 2 | Val loss: 369.3205 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:11:14 (running for 00:01:28.45)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.931 |      0.097 |                    6 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.528 |      0.16  |                    7 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.358 |      0.203 |                    6 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.28  |      0.047 |                    5 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.20522388059701493
[2m[36m(func pid=88300)[0m top5: 0.6422574626865671
[2m[36m(func pid=88300)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=88300)[0m f1_macro: 0.16008783478012298
[2m[36m(func pid=88300)[0m f1_weighted: 0.21730665193469156
[2m[36m(func pid=88300)[0m f1_per_class: [0.164, 0.285, 0.229, 0.142, 0.016, 0.141, 0.326, 0.153, 0.0, 0.145]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.4006529850746269
[2m[36m(func pid=88716)[0m top5: 0.8003731343283582
[2m[36m(func pid=88716)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=88716)[0m f1_macro: 0.30309671905454805
[2m[36m(func pid=88716)[0m f1_weighted: 0.3494770889039107
[2m[36m(func pid=88716)[0m f1_per_class: [0.482, 0.034, 0.381, 0.584, 0.114, 0.396, 0.315, 0.421, 0.042, 0.263]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17723880597014927
[2m[36m(func pid=87918)[0m top5: 0.5307835820895522
[2m[36m(func pid=87918)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=87918)[0m f1_macro: 0.10147924316259545
[2m[36m(func pid=87918)[0m f1_weighted: 0.14129203840369498
[2m[36m(func pid=87918)[0m f1_per_class: [0.067, 0.284, 0.044, 0.098, 0.0, 0.335, 0.068, 0.075, 0.0, 0.043]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17210820895522388
[2m[36m(func pid=89134)[0m top5: 0.29384328358208955
[2m[36m(func pid=89134)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=89134)[0m f1_macro: 0.049289704708699124
[2m[36m(func pid=89134)[0m f1_weighted: 0.05162267274958011
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.293, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4521 | Steps: 2 | Val loss: 2.2098 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7014 | Steps: 2 | Val loss: 1.8425 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8838 | Steps: 2 | Val loss: 2.3111 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9144 | Steps: 2 | Val loss: 1803.0839 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:11:19 (running for 00:01:33.67)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.948 |      0.101 |                    7 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.452 |      0.168 |                    8 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.988 |      0.303 |                    7 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.013 |      0.049 |                    6 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.2140858208955224
[2m[36m(func pid=88300)[0m top5: 0.6805037313432836
[2m[36m(func pid=88300)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=88300)[0m f1_macro: 0.1678600579865101
[2m[36m(func pid=88300)[0m f1_weighted: 0.22703472548169412
[2m[36m(func pid=88300)[0m f1_per_class: [0.207, 0.265, 0.178, 0.165, 0.022, 0.153, 0.341, 0.148, 0.0, 0.2]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.3903917910447761
[2m[36m(func pid=88716)[0m top5: 0.8073694029850746
[2m[36m(func pid=88716)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=88716)[0m f1_macro: 0.31259821205466026
[2m[36m(func pid=88716)[0m f1_weighted: 0.35287223137659895
[2m[36m(func pid=88716)[0m f1_per_class: [0.473, 0.118, 0.444, 0.572, 0.114, 0.347, 0.3, 0.415, 0.138, 0.205]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17817164179104478
[2m[36m(func pid=87918)[0m top5: 0.5340485074626866
[2m[36m(func pid=87918)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=87918)[0m f1_macro: 0.0998598570511093
[2m[36m(func pid=87918)[0m f1_weighted: 0.1423259977930681
[2m[36m(func pid=87918)[0m f1_per_class: [0.061, 0.287, 0.047, 0.088, 0.019, 0.357, 0.074, 0.066, 0.0, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17210820895522388
[2m[36m(func pid=89134)[0m top5: 0.5727611940298507
[2m[36m(func pid=89134)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=89134)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=89134)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3165 | Steps: 2 | Val loss: 2.1743 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5373 | Steps: 2 | Val loss: 2.1110 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8621 | Steps: 2 | Val loss: 2.3098 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.7497 | Steps: 2 | Val loss: 10017.3984 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 04:11:24 (running for 00:01:38.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.884 |      0.1   |                    8 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.316 |      0.185 |                    9 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.701 |      0.313 |                    8 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.914 |      0.029 |                    7 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.23647388059701493
[2m[36m(func pid=88300)[0m top5: 0.7182835820895522
[2m[36m(func pid=88300)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=88300)[0m f1_macro: 0.1854621356569273
[2m[36m(func pid=88300)[0m f1_weighted: 0.25622222802465905
[2m[36m(func pid=88300)[0m f1_per_class: [0.246, 0.259, 0.132, 0.214, 0.035, 0.199, 0.361, 0.229, 0.0, 0.179]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.332089552238806
[2m[36m(func pid=88716)[0m top5: 0.7835820895522388
[2m[36m(func pid=88716)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=88716)[0m f1_macro: 0.26527445903015223
[2m[36m(func pid=88716)[0m f1_weighted: 0.2999084935303899
[2m[36m(func pid=88716)[0m f1_per_class: [0.442, 0.137, 0.429, 0.547, 0.121, 0.241, 0.212, 0.249, 0.14, 0.136]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.18003731343283583
[2m[36m(func pid=87918)[0m top5: 0.5433768656716418
[2m[36m(func pid=87918)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=87918)[0m f1_macro: 0.11127541486950347
[2m[36m(func pid=87918)[0m f1_weighted: 0.14686666598290202
[2m[36m(func pid=87918)[0m f1_per_class: [0.058, 0.282, 0.115, 0.084, 0.03, 0.353, 0.091, 0.084, 0.017, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17210820895522388
[2m[36m(func pid=89134)[0m top5: 0.5727611940298507
[2m[36m(func pid=89134)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=89134)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=89134)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2680 | Steps: 2 | Val loss: 2.1341 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3154 | Steps: 2 | Val loss: 2.2406 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8569 | Steps: 2 | Val loss: 2.3075 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.5393 | Steps: 2 | Val loss: 18582.9453 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=88300)[0m top1: 0.25513059701492535== Status ==
Current time: 2024-01-07 04:11:29 (running for 00:01:44.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.862 |      0.111 |                    9 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.268 |      0.198 |                   10 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.537 |      0.265 |                    9 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  3.75  |      0.029 |                    8 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=88300)[0m top5: 0.7513992537313433
[2m[36m(func pid=88300)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=88300)[0m f1_macro: 0.19828373979129177
[2m[36m(func pid=88300)[0m f1_weighted: 0.27898463825376624
[2m[36m(func pid=88300)[0m f1_per_class: [0.292, 0.252, 0.108, 0.253, 0.039, 0.217, 0.389, 0.264, 0.0, 0.168]
[2m[36m(func pid=88716)[0m top1: 0.3041044776119403
[2m[36m(func pid=88716)[0m top5: 0.784981343283582
[2m[36m(func pid=88716)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=88716)[0m f1_macro: 0.23542351103150322
[2m[36m(func pid=88716)[0m f1_weighted: 0.2854371386840674
[2m[36m(func pid=88716)[0m f1_per_class: [0.368, 0.164, 0.312, 0.503, 0.108, 0.168, 0.228, 0.22, 0.161, 0.122]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17817164179104478
[2m[36m(func pid=87918)[0m top5: 0.5494402985074627
[2m[36m(func pid=87918)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=87918)[0m f1_macro: 0.1228081042232279
[2m[36m(func pid=87918)[0m f1_weighted: 0.14710144317228954
[2m[36m(func pid=87918)[0m f1_per_class: [0.073, 0.287, 0.207, 0.079, 0.022, 0.348, 0.088, 0.109, 0.016, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17210820895522388
[2m[36m(func pid=89134)[0m top5: 0.5727611940298507
[2m[36m(func pid=89134)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=89134)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=89134)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1109 | Steps: 2 | Val loss: 2.0964 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3125 | Steps: 2 | Val loss: 2.2247 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8343 | Steps: 2 | Val loss: 2.3059 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7440 | Steps: 2 | Val loss: 11447.1680 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=88716)[0m top1: 0.31529850746268656
[2m[36m(func pid=88716)[0m top5: 0.8185634328358209
[2m[36m(func pid=88716)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=88716)[0m f1_macro: 0.21974566860591044
[2m[36m(func pid=88716)[0m f1_weighted: 0.301000226016367
[2m[36m(func pid=88716)[0m f1_per_class: [0.162, 0.176, 0.154, 0.525, 0.123, 0.131, 0.256, 0.351, 0.168, 0.151]
[2m[36m(func pid=88716)[0m 
== Status ==
Current time: 2024-01-07 04:11:34 (running for 00:01:49.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.857 |      0.123 |                   10 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.268 |      0.198 |                   10 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.313 |      0.22  |                   11 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  3.539 |      0.029 |                    9 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.27611940298507465
[2m[36m(func pid=88300)[0m top5: 0.7737873134328358
[2m[36m(func pid=88300)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=88300)[0m f1_macro: 0.21674714705057982
[2m[36m(func pid=88300)[0m f1_weighted: 0.30004181225118026
[2m[36m(func pid=88300)[0m f1_per_class: [0.318, 0.23, 0.122, 0.31, 0.044, 0.177, 0.419, 0.311, 0.021, 0.216]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17257462686567165
[2m[36m(func pid=87918)[0m top5: 0.5615671641791045
[2m[36m(func pid=87918)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=87918)[0m f1_macro: 0.12573274710571564
[2m[36m(func pid=87918)[0m f1_weighted: 0.14543270073672168
[2m[36m(func pid=87918)[0m f1_per_class: [0.096, 0.278, 0.233, 0.077, 0.015, 0.338, 0.09, 0.113, 0.017, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.16977611940298507
[2m[36m(func pid=89134)[0m top5: 0.5708955223880597
[2m[36m(func pid=89134)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=89134)[0m f1_macro: 0.029119999999999997
[2m[36m(func pid=89134)[0m f1_weighted: 0.050117910447761184
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1674 | Steps: 2 | Val loss: 2.2555 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0059 | Steps: 2 | Val loss: 2.0594 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7787 | Steps: 2 | Val loss: 2.3002 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6069 | Steps: 2 | Val loss: 5548.6060 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 04:11:40 (running for 00:01:54.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.834 |      0.126 |                   11 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  2.111 |      0.217 |                   11 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.167 |      0.23  |                   12 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.744 |      0.029 |                   10 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3031716417910448
[2m[36m(func pid=88300)[0m top5: 0.7905783582089553
[2m[36m(func pid=88300)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=88300)[0m f1_macro: 0.23812894284378044
[2m[36m(func pid=88300)[0m f1_weighted: 0.3296883298461768
[2m[36m(func pid=88300)[0m f1_per_class: [0.374, 0.225, 0.146, 0.39, 0.042, 0.168, 0.44, 0.316, 0.062, 0.217]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.34888059701492535
[2m[36m(func pid=88716)[0m top5: 0.8568097014925373
[2m[36m(func pid=88716)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=88716)[0m f1_macro: 0.23032117804677704
[2m[36m(func pid=88716)[0m f1_weighted: 0.35610186357629164
[2m[36m(func pid=88716)[0m f1_per_class: [0.081, 0.195, 0.136, 0.519, 0.149, 0.109, 0.453, 0.342, 0.152, 0.167]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.1791044776119403
[2m[36m(func pid=87918)[0m top5: 0.5690298507462687
[2m[36m(func pid=87918)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=87918)[0m f1_macro: 0.13421297342416977
[2m[36m(func pid=87918)[0m f1_weighted: 0.15430043883515052
[2m[36m(func pid=87918)[0m f1_per_class: [0.133, 0.283, 0.23, 0.086, 0.019, 0.36, 0.097, 0.118, 0.016, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.007462686567164179
[2m[36m(func pid=89134)[0m top5: 0.48367537313432835
[2m[36m(func pid=89134)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=89134)[0m f1_macro: 0.002102496714848883
[2m[36m(func pid=89134)[0m f1_weighted: 0.00015690273991409573
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.021, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2837 | Steps: 2 | Val loss: 2.3640 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9217 | Steps: 2 | Val loss: 2.0329 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8231 | Steps: 2 | Val loss: 2.2937 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0379 | Steps: 2 | Val loss: 4244.9023 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 04:11:45 (running for 00:01:59.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.779 |      0.134 |                   12 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.922 |      0.247 |                   13 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.167 |      0.23  |                   12 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.607 |      0.002 |                   11 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.30830223880597013
[2m[36m(func pid=88300)[0m top5: 0.7989738805970149
[2m[36m(func pid=88300)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=88300)[0m f1_macro: 0.24679054084948407
[2m[36m(func pid=88300)[0m f1_weighted: 0.3401064562114839
[2m[36m(func pid=88300)[0m f1_per_class: [0.412, 0.2, 0.139, 0.429, 0.04, 0.184, 0.442, 0.309, 0.092, 0.221]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.3358208955223881
[2m[36m(func pid=88716)[0m top5: 0.8708022388059702
[2m[36m(func pid=88716)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=88716)[0m f1_macro: 0.2246661089496861
[2m[36m(func pid=88716)[0m f1_weighted: 0.3442506234631473
[2m[36m(func pid=88716)[0m f1_per_class: [0.059, 0.196, 0.183, 0.485, 0.14, 0.117, 0.451, 0.297, 0.152, 0.167]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17863805970149255
[2m[36m(func pid=87918)[0m top5: 0.5746268656716418
[2m[36m(func pid=87918)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=87918)[0m f1_macro: 0.13832682099480656
[2m[36m(func pid=87918)[0m f1_weighted: 0.15951660942137688
[2m[36m(func pid=87918)[0m f1_per_class: [0.144, 0.275, 0.23, 0.091, 0.018, 0.364, 0.109, 0.138, 0.016, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.006063432835820896
[2m[36m(func pid=89134)[0m top5: 0.5102611940298507
[2m[36m(func pid=89134)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=89134)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=89134)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.7071 | Steps: 2 | Val loss: 2.0054 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1171 | Steps: 2 | Val loss: 2.6126 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7747 | Steps: 2 | Val loss: 2.2818 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.1451 | Steps: 2 | Val loss: 3018.6997 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=88300)[0m top1: 0.324160447761194
[2m[36m(func pid=88300)[0m top5: 0.8083022388059702
[2m[36m(func pid=88300)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=88300)[0m f1_macro: 0.2562962199315707
[2m[36m(func pid=88300)[0m f1_weighted: 0.3556981112014149
[2m[36m(func pid=88300)[0m f1_per_class: [0.417, 0.197, 0.15, 0.449, 0.052, 0.2, 0.47, 0.31, 0.103, 0.216]
== Status ==
Current time: 2024-01-07 04:11:50 (running for 00:02:04.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.823 |      0.138 |                   13 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.707 |      0.256 |                   14 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.284 |      0.225 |                   13 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.038 |      0.001 |                   12 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.269589552238806
[2m[36m(func pid=88716)[0m top5: 0.8442164179104478
[2m[36m(func pid=88716)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=88716)[0m f1_macro: 0.1991434090216904
[2m[36m(func pid=88716)[0m f1_weighted: 0.29087983018124414
[2m[36m(func pid=88716)[0m f1_per_class: [0.062, 0.148, 0.218, 0.405, 0.105, 0.136, 0.375, 0.266, 0.145, 0.132]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.1791044776119403
[2m[36m(func pid=87918)[0m top5: 0.5881529850746269
[2m[36m(func pid=87918)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=87918)[0m f1_macro: 0.14654046662654543
[2m[36m(func pid=87918)[0m f1_weighted: 0.1650315503389444
[2m[36m(func pid=87918)[0m f1_per_class: [0.158, 0.269, 0.276, 0.091, 0.013, 0.358, 0.127, 0.158, 0.015, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.006063432835820896
[2m[36m(func pid=89134)[0m top5: 0.5139925373134329
[2m[36m(func pid=89134)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=89134)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=89134)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6277 | Steps: 2 | Val loss: 1.9652 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0483 | Steps: 2 | Val loss: 2.8173 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7576 | Steps: 2 | Val loss: 2.2756 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9221 | Steps: 2 | Val loss: 2704.4412 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=88300)[0m top1: 0.3451492537313433
[2m[36m(func pid=88300)[0m top5: 0.8157649253731343
[2m[36m(func pid=88300)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=88300)[0m f1_macro: 0.27407087599261254
[2m[36m(func pid=88300)[0m f1_weighted: 0.3759661490696721
[2m[36m(func pid=88300)[0m f1_per_class: [0.439, 0.187, 0.189, 0.494, 0.052, 0.258, 0.474, 0.318, 0.107, 0.221]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:11:55 (running for 00:02:10.09)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.775 |      0.147 |                   14 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.628 |      0.274 |                   15 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.117 |      0.199 |                   14 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.145 |      0.001 |                   13 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.2555970149253731
[2m[36m(func pid=88716)[0m top5: 0.8278917910447762
[2m[36m(func pid=88716)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=88716)[0m f1_macro: 0.21343022229623498
[2m[36m(func pid=88716)[0m f1_weighted: 0.2906103020326913
[2m[36m(func pid=88716)[0m f1_per_class: [0.089, 0.172, 0.272, 0.334, 0.08, 0.238, 0.383, 0.261, 0.184, 0.122]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17583955223880596
[2m[36m(func pid=87918)[0m top5: 0.5960820895522388
[2m[36m(func pid=87918)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=87918)[0m f1_macro: 0.15012260938384425
[2m[36m(func pid=87918)[0m f1_weighted: 0.16289293063433416
[2m[36m(func pid=87918)[0m f1_per_class: [0.186, 0.271, 0.3, 0.088, 0.013, 0.352, 0.122, 0.154, 0.015, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.006063432835820896
[2m[36m(func pid=89134)[0m top5: 0.519589552238806
[2m[36m(func pid=89134)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=89134)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=89134)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5716 | Steps: 2 | Val loss: 1.9262 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0952 | Steps: 2 | Val loss: 3.0094 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7136 | Steps: 2 | Val loss: 2.2669 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.2167 | Steps: 2 | Val loss: 843.5598 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:12:00 (running for 00:02:15.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.758 |      0.15  |                   15 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.572 |      0.283 |                   16 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.048 |      0.213 |                   15 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.922 |      0.001 |                   14 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3619402985074627
[2m[36m(func pid=88300)[0m top5: 0.8199626865671642
[2m[36m(func pid=88300)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=88300)[0m f1_macro: 0.2826490890081955
[2m[36m(func pid=88300)[0m f1_weighted: 0.38810074187322813
[2m[36m(func pid=88300)[0m f1_per_class: [0.464, 0.191, 0.202, 0.524, 0.05, 0.239, 0.484, 0.357, 0.096, 0.22]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.27238805970149255
[2m[36m(func pid=88716)[0m top5: 0.8227611940298507
[2m[36m(func pid=88716)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=88716)[0m f1_macro: 0.2151569321042503
[2m[36m(func pid=88716)[0m f1_weighted: 0.3025821812700148
[2m[36m(func pid=88716)[0m f1_per_class: [0.184, 0.176, 0.233, 0.375, 0.087, 0.104, 0.427, 0.259, 0.194, 0.114]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.12733208955223882
[2m[36m(func pid=89134)[0m top5: 0.458955223880597
[2m[36m(func pid=89134)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=89134)[0m f1_macro: 0.03625498007968127
[2m[36m(func pid=89134)[0m f1_weighted: 0.062397796872212634
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=87918)[0m top1: 0.17444029850746268
[2m[36m(func pid=87918)[0m top5: 0.6068097014925373
[2m[36m(func pid=87918)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=87918)[0m f1_macro: 0.1526055206950844
[2m[36m(func pid=87918)[0m f1_weighted: 0.16576388853997845
[2m[36m(func pid=87918)[0m f1_per_class: [0.176, 0.264, 0.321, 0.096, 0.017, 0.348, 0.129, 0.161, 0.015, 0.0]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.3967 | Steps: 2 | Val loss: 1.8833 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0258 | Steps: 2 | Val loss: 3.2689 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7288 | Steps: 2 | Val loss: 666.9908 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6839 | Steps: 2 | Val loss: 2.2612 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 04:12:05 (running for 00:02:20.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.714 |      0.153 |                   16 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.397 |      0.288 |                   17 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.095 |      0.215 |                   16 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  3.217 |      0.036 |                   15 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.37593283582089554
[2m[36m(func pid=88300)[0m top5: 0.8250932835820896
[2m[36m(func pid=88300)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=88300)[0m f1_macro: 0.28827571276989644
[2m[36m(func pid=88300)[0m f1_weighted: 0.39778649272922545
[2m[36m(func pid=88300)[0m f1_per_class: [0.472, 0.187, 0.224, 0.548, 0.056, 0.249, 0.491, 0.362, 0.094, 0.2]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.34468283582089554
[2m[36m(func pid=88716)[0m top5: 0.8120335820895522
[2m[36m(func pid=88716)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=88716)[0m f1_macro: 0.23519081460456426
[2m[36m(func pid=88716)[0m f1_weighted: 0.3525171694973044
[2m[36m(func pid=88716)[0m f1_per_class: [0.228, 0.195, 0.364, 0.461, 0.11, 0.016, 0.564, 0.08, 0.194, 0.14]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.15205223880597016
[2m[36m(func pid=89134)[0m top5: 0.5256529850746269
[2m[36m(func pid=89134)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=89134)[0m f1_macro: 0.027009113504556752
[2m[36m(func pid=89134)[0m f1_weighted: 0.04648490150737613
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.17957089552238806
[2m[36m(func pid=87918)[0m top5: 0.6189365671641791
[2m[36m(func pid=87918)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=87918)[0m f1_macro: 0.15860691561483858
[2m[36m(func pid=87918)[0m f1_weighted: 0.17729234432444407
[2m[36m(func pid=87918)[0m f1_per_class: [0.183, 0.271, 0.305, 0.113, 0.012, 0.344, 0.146, 0.168, 0.015, 0.028]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4197 | Steps: 2 | Val loss: 1.8436 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0436 | Steps: 2 | Val loss: 3.6646 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 7.2408 | Steps: 2 | Val loss: 532.5478 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6809 | Steps: 2 | Val loss: 2.2518 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 04:12:10 (running for 00:02:25.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.684 |      0.159 |                   17 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.42  |      0.289 |                   18 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.026 |      0.235 |                   17 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.729 |      0.027 |                   16 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.37406716417910446
[2m[36m(func pid=88300)[0m top5: 0.8311567164179104
[2m[36m(func pid=88300)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=88300)[0m f1_macro: 0.28923130104359096
[2m[36m(func pid=88300)[0m f1_weighted: 0.39344149727865513
[2m[36m(func pid=88300)[0m f1_per_class: [0.467, 0.187, 0.247, 0.55, 0.063, 0.254, 0.473, 0.354, 0.11, 0.186]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.384794776119403
[2m[36m(func pid=88716)[0m top5: 0.8194962686567164
[2m[36m(func pid=88716)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=88716)[0m f1_macro: 0.2105623152855806
[2m[36m(func pid=88716)[0m f1_weighted: 0.3735235933487749
[2m[36m(func pid=88716)[0m f1_per_class: [0.25, 0.205, 0.0, 0.519, 0.143, 0.0, 0.587, 0.086, 0.171, 0.145]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.15858208955223882
[2m[36m(func pid=89134)[0m top5: 0.5513059701492538
[2m[36m(func pid=89134)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=89134)[0m f1_macro: 0.027586206896551724
[2m[36m(func pid=89134)[0m f1_weighted: 0.04747812660833762
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.1828358208955224
[2m[36m(func pid=87918)[0m top5: 0.6324626865671642
[2m[36m(func pid=87918)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=87918)[0m f1_macro: 0.16825870253517267
[2m[36m(func pid=87918)[0m f1_weighted: 0.184102628000377
[2m[36m(func pid=87918)[0m f1_per_class: [0.203, 0.272, 0.349, 0.127, 0.016, 0.322, 0.156, 0.195, 0.014, 0.028]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1909 | Steps: 2 | Val loss: 1.8102 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0618 | Steps: 2 | Val loss: 4.0151 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 4.6695 | Steps: 2 | Val loss: 432.3478 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6428 | Steps: 2 | Val loss: 2.2431 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:12:16 (running for 00:02:30.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.681 |      0.168 |                   18 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.191 |      0.302 |                   19 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.044 |      0.211 |                   18 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  7.241 |      0.028 |                   17 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.37826492537313433
[2m[36m(func pid=88300)[0m top5: 0.8423507462686567
[2m[36m(func pid=88300)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=88300)[0m f1_macro: 0.3016349925501035
[2m[36m(func pid=88300)[0m f1_weighted: 0.394848470850159
[2m[36m(func pid=88300)[0m f1_per_class: [0.485, 0.197, 0.27, 0.555, 0.075, 0.311, 0.442, 0.348, 0.139, 0.196]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.38759328358208955
[2m[36m(func pid=88716)[0m top5: 0.8185634328358209
[2m[36m(func pid=88716)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=88716)[0m f1_macro: 0.25265650259246286
[2m[36m(func pid=88716)[0m f1_weighted: 0.38181483555645457
[2m[36m(func pid=88716)[0m f1_per_class: [0.265, 0.196, 0.333, 0.516, 0.14, 0.0, 0.596, 0.191, 0.163, 0.126]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.13386194029850745
[2m[36m(func pid=89134)[0m top5: 0.40671641791044777
[2m[36m(func pid=89134)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=89134)[0m f1_macro: 0.03671576945415618
[2m[36m(func pid=89134)[0m f1_weighted: 0.04294507892851905
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.18796641791044777
[2m[36m(func pid=87918)[0m top5: 0.6427238805970149
[2m[36m(func pid=87918)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=87918)[0m f1_macro: 0.17271300816421678
[2m[36m(func pid=87918)[0m f1_weighted: 0.1934619677929661
[2m[36m(func pid=87918)[0m f1_per_class: [0.2, 0.27, 0.324, 0.135, 0.021, 0.329, 0.18, 0.19, 0.0, 0.079]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.0798 | Steps: 2 | Val loss: 1.7860 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2156 | Steps: 2 | Val loss: 6.3079 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 7.0279 | Steps: 2 | Val loss: 486.5436 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6529 | Steps: 2 | Val loss: 2.2375 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:12:21 (running for 00:02:35.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.643 |      0.173 |                   19 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  1.08  |      0.308 |                   20 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.062 |      0.253 |                   19 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  4.67  |      0.037 |                   18 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3694029850746269
[2m[36m(func pid=88300)[0m top5: 0.8498134328358209
[2m[36m(func pid=88300)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=88300)[0m f1_macro: 0.30784350241256536
[2m[36m(func pid=88300)[0m f1_weighted: 0.382226892130539
[2m[36m(func pid=88300)[0m f1_per_class: [0.495, 0.21, 0.312, 0.535, 0.084, 0.316, 0.4, 0.383, 0.136, 0.209]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.1515858208955224
[2m[36m(func pid=88716)[0m top5: 0.8367537313432836
[2m[36m(func pid=88716)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=88716)[0m f1_macro: 0.2015108638607
[2m[36m(func pid=88716)[0m f1_weighted: 0.17506636911811718
[2m[36m(func pid=88716)[0m f1_per_class: [0.349, 0.193, 0.522, 0.152, 0.225, 0.039, 0.244, 0.062, 0.192, 0.038]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.013526119402985074
[2m[36m(func pid=89134)[0m top5: 0.300839552238806
[2m[36m(func pid=89134)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=89134)[0m f1_macro: 0.020029041760360307
[2m[36m(func pid=89134)[0m f1_weighted: 0.009469210284803502
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.042, 0.001, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.149]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.19169776119402984
[2m[36m(func pid=87918)[0m top5: 0.644589552238806
[2m[36m(func pid=87918)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=87918)[0m f1_macro: 0.17630070556490146
[2m[36m(func pid=87918)[0m f1_weighted: 0.19752205833619285
[2m[36m(func pid=87918)[0m f1_per_class: [0.248, 0.273, 0.282, 0.138, 0.024, 0.342, 0.18, 0.195, 0.0, 0.08]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9466 | Steps: 2 | Val loss: 1.7651 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0917 | Steps: 2 | Val loss: 6.3620 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.9719 | Steps: 2 | Val loss: 722.4560 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5564 | Steps: 2 | Val loss: 2.2275 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:12:26 (running for 00:02:40.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.653 |      0.176 |                   20 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.947 |      0.32  |                   21 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.216 |      0.202 |                   20 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  7.028 |      0.02  |                   19 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.373134328358209
[2m[36m(func pid=88300)[0m top5: 0.8521455223880597
[2m[36m(func pid=88300)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=88300)[0m f1_macro: 0.31960666020770956
[2m[36m(func pid=88300)[0m f1_weighted: 0.3848415445135376
[2m[36m(func pid=88300)[0m f1_per_class: [0.468, 0.238, 0.353, 0.53, 0.086, 0.373, 0.369, 0.405, 0.159, 0.216]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.125
[2m[36m(func pid=88716)[0m top5: 0.7742537313432836
[2m[36m(func pid=88716)[0m f1_micro: 0.125
[2m[36m(func pid=88716)[0m f1_macro: 0.11092145807382754
[2m[36m(func pid=88716)[0m f1_weighted: 0.10728112249433669
[2m[36m(func pid=88716)[0m f1_per_class: [0.135, 0.242, 0.229, 0.054, 0.192, 0.008, 0.134, 0.043, 0.027, 0.044]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.011194029850746268
[2m[36m(func pid=89134)[0m top5: 0.4197761194029851
[2m[36m(func pid=89134)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=89134)[0m f1_macro: 0.006242833202468634
[2m[36m(func pid=89134)[0m f1_weighted: 0.004090219625840232
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.011, 0.015, 0.0, 0.0, 0.0, 0.0, 0.037, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.19636194029850745
[2m[36m(func pid=87918)[0m top5: 0.6576492537313433
[2m[36m(func pid=87918)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=87918)[0m f1_macro: 0.1819554468764352
[2m[36m(func pid=87918)[0m f1_weighted: 0.20870393109811042
[2m[36m(func pid=87918)[0m f1_per_class: [0.256, 0.261, 0.296, 0.159, 0.02, 0.349, 0.2, 0.203, 0.0, 0.074]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.9087 | Steps: 2 | Val loss: 1.7522 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2535 | Steps: 2 | Val loss: 5.7976 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 10.8406 | Steps: 2 | Val loss: 832.2607 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5322 | Steps: 2 | Val loss: 2.2167 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 04:12:31 (running for 00:02:45.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.556 |      0.182 |                   21 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.909 |      0.327 |                   22 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.092 |      0.111 |                   21 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.972 |      0.006 |                   20 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.373134328358209
[2m[36m(func pid=88300)[0m top5: 0.8535447761194029
[2m[36m(func pid=88300)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=88300)[0m f1_macro: 0.3269551822026955
[2m[36m(func pid=88300)[0m f1_weighted: 0.38091894273239957
[2m[36m(func pid=88300)[0m f1_per_class: [0.466, 0.264, 0.375, 0.524, 0.099, 0.367, 0.343, 0.407, 0.2, 0.226]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.18889925373134328
[2m[36m(func pid=88716)[0m top5: 0.7350746268656716
[2m[36m(func pid=88716)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=88716)[0m f1_macro: 0.09614643363985384
[2m[36m(func pid=88716)[0m f1_weighted: 0.1811580965960576
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.255, 0.079, 0.017, 0.045, 0.016, 0.419, 0.08, 0.0, 0.05]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.01958955223880597
[2m[36m(func pid=89134)[0m top5: 0.44822761194029853
[2m[36m(func pid=89134)[0m f1_micro: 0.01958955223880597
[2m[36m(func pid=89134)[0m f1_macro: 0.010562152543956356
[2m[36m(func pid=89134)[0m f1_weighted: 0.00767151609706215
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.021, 0.017, 0.0, 0.0, 0.0, 0.0, 0.067, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.20149253731343283
[2m[36m(func pid=87918)[0m top5: 0.6707089552238806
[2m[36m(func pid=87918)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=87918)[0m f1_macro: 0.18901607418176378
[2m[36m(func pid=87918)[0m f1_weighted: 0.21467063176110027
[2m[36m(func pid=87918)[0m f1_per_class: [0.235, 0.264, 0.293, 0.17, 0.021, 0.355, 0.2, 0.227, 0.015, 0.111]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.9042 | Steps: 2 | Val loss: 1.7391 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4610 | Steps: 2 | Val loss: 7.4870 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 10.8849 | Steps: 2 | Val loss: 852.8683 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4725 | Steps: 2 | Val loss: 2.2065 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:12:36 (running for 00:02:50.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.532 |      0.189 |                   22 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.904 |      0.338 |                   23 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.253 |      0.096 |                   22 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      | 10.841 |      0.011 |                   21 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.37546641791044777
[2m[36m(func pid=88300)[0m top5: 0.8614738805970149
[2m[36m(func pid=88300)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=88300)[0m f1_macro: 0.3381019017546868
[2m[36m(func pid=88300)[0m f1_weighted: 0.38381588617407447
[2m[36m(func pid=88300)[0m f1_per_class: [0.505, 0.264, 0.421, 0.525, 0.096, 0.376, 0.344, 0.404, 0.21, 0.237]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.23041044776119404
[2m[36m(func pid=88716)[0m top5: 0.6721082089552238
[2m[36m(func pid=88716)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=88716)[0m f1_macro: 0.09626120494358228
[2m[36m(func pid=88716)[0m f1_weighted: 0.1910089521423509
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.163, 0.042, 0.0, 0.0, 0.0, 0.519, 0.113, 0.0, 0.126]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.057369402985074626
[2m[36m(func pid=89134)[0m top5: 0.5121268656716418
[2m[36m(func pid=89134)[0m f1_micro: 0.057369402985074626
[2m[36m(func pid=89134)[0m f1_macro: 0.029958390850341604
[2m[36m(func pid=89134)[0m f1_weighted: 0.027745896329315196
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.101, 0.021, 0.0, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.20522388059701493
[2m[36m(func pid=87918)[0m top5: 0.6907649253731343
[2m[36m(func pid=87918)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=87918)[0m f1_macro: 0.19470775425960157
[2m[36m(func pid=87918)[0m f1_weighted: 0.21962811644557914
[2m[36m(func pid=87918)[0m f1_per_class: [0.246, 0.264, 0.289, 0.175, 0.02, 0.352, 0.208, 0.249, 0.015, 0.13]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7235 | Steps: 2 | Val loss: 1.7265 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5496 | Steps: 2 | Val loss: 8.9397 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.7266 | Steps: 2 | Val loss: 485.1497 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4961 | Steps: 2 | Val loss: 2.1963 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 04:12:41 (running for 00:02:55.86)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.473 |      0.195 |                   23 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.724 |      0.342 |                   24 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.461 |      0.096 |                   23 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      | 10.885 |      0.03  |                   22 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3773320895522388
[2m[36m(func pid=88300)[0m top5: 0.8703358208955224
[2m[36m(func pid=88300)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=88300)[0m f1_macro: 0.3424371709744705
[2m[36m(func pid=88300)[0m f1_weighted: 0.3855056768257298
[2m[36m(func pid=88300)[0m f1_per_class: [0.5, 0.29, 0.471, 0.525, 0.088, 0.371, 0.337, 0.402, 0.199, 0.242]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.18003731343283583
[2m[36m(func pid=88716)[0m top5: 0.6152052238805971
[2m[36m(func pid=88716)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=88716)[0m f1_macro: 0.08713448083812114
[2m[36m(func pid=88716)[0m f1_weighted: 0.1636469824450631
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.132, 0.04, 0.0, 0.0, 0.0, 0.437, 0.162, 0.0, 0.101]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.08815298507462686
[2m[36m(func pid=89134)[0m top5: 0.5760261194029851
[2m[36m(func pid=89134)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=89134)[0m f1_macro: 0.05544565004767595
[2m[36m(func pid=89134)[0m f1_weighted: 0.056201789063541424
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.254, 0.026, 0.0, 0.0, 0.0, 0.0, 0.197, 0.0, 0.077]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.20988805970149255
[2m[36m(func pid=87918)[0m top5: 0.7154850746268657
[2m[36m(func pid=87918)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=87918)[0m f1_macro: 0.20297716419838846
[2m[36m(func pid=87918)[0m f1_weighted: 0.22444456548599673
[2m[36m(func pid=87918)[0m f1_per_class: [0.269, 0.264, 0.286, 0.196, 0.024, 0.348, 0.2, 0.262, 0.016, 0.165]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7280 | Steps: 2 | Val loss: 1.7233 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5069 | Steps: 2 | Val loss: 9.0623 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.5032 | Steps: 2 | Val loss: 312.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:12:46 (running for 00:03:01.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.496 |      0.203 |                   24 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.728 |      0.352 |                   25 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.55  |      0.087 |                   24 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.727 |      0.055 |                   23 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3824626865671642
[2m[36m(func pid=88300)[0m top5: 0.8801305970149254
[2m[36m(func pid=88300)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=88300)[0m f1_macro: 0.35234708234762424
[2m[36m(func pid=88300)[0m f1_weighted: 0.3921035280796035
[2m[36m(func pid=88300)[0m f1_per_class: [0.514, 0.294, 0.511, 0.521, 0.089, 0.38, 0.354, 0.404, 0.201, 0.255]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4863 | Steps: 2 | Val loss: 2.1904 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=88716)[0m top1: 0.18610074626865672
[2m[36m(func pid=88716)[0m top5: 0.5629664179104478
[2m[36m(func pid=88716)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=88716)[0m f1_macro: 0.12097955213726426
[2m[36m(func pid=88716)[0m f1_weighted: 0.16656526868643862
[2m[36m(func pid=88716)[0m f1_per_class: [0.073, 0.132, 0.071, 0.0, 0.167, 0.0, 0.417, 0.267, 0.0, 0.083]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.09981343283582089
[2m[36m(func pid=89134)[0m top5: 0.5904850746268657
[2m[36m(func pid=89134)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=89134)[0m f1_macro: 0.0648585672811505
[2m[36m(func pid=89134)[0m f1_weighted: 0.0647046484260288
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.286, 0.031, 0.01, 0.0, 0.0, 0.0, 0.191, 0.0, 0.131]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.2103544776119403
[2m[36m(func pid=87918)[0m top5: 0.7220149253731343
[2m[36m(func pid=87918)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=87918)[0m f1_macro: 0.20405741193070207
[2m[36m(func pid=87918)[0m f1_weighted: 0.22701930005402707
[2m[36m(func pid=87918)[0m f1_per_class: [0.286, 0.254, 0.255, 0.203, 0.028, 0.342, 0.207, 0.27, 0.016, 0.18]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5879 | Steps: 2 | Val loss: 1.7208 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3145 | Steps: 2 | Val loss: 10.3266 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.6371 | Steps: 2 | Val loss: 165.5699 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 04:12:51 (running for 00:03:06.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.486 |      0.204 |                   25 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.588 |      0.357 |                   26 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.507 |      0.121 |                   25 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.503 |      0.065 |                   24 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3917910447761194
[2m[36m(func pid=88300)[0m top5: 0.8782649253731343
[2m[36m(func pid=88300)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=88300)[0m f1_macro: 0.3566407284670391
[2m[36m(func pid=88300)[0m f1_weighted: 0.4003134636725532
[2m[36m(func pid=88300)[0m f1_per_class: [0.514, 0.292, 0.522, 0.54, 0.083, 0.376, 0.363, 0.418, 0.204, 0.253]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4217 | Steps: 2 | Val loss: 2.1802 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m top1: 0.19496268656716417
[2m[36m(func pid=88716)[0m top5: 0.5410447761194029
[2m[36m(func pid=88716)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=88716)[0m f1_macro: 0.15504958322490633
[2m[36m(func pid=88716)[0m f1_weighted: 0.17483654459456358
[2m[36m(func pid=88716)[0m f1_per_class: [0.075, 0.056, 0.16, 0.0, 0.257, 0.0, 0.463, 0.323, 0.066, 0.149]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.12733208955223882
[2m[36m(func pid=89134)[0m top5: 0.6231343283582089
[2m[36m(func pid=89134)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=89134)[0m f1_macro: 0.0827700488589136
[2m[36m(func pid=89134)[0m f1_weighted: 0.08032971023723971
[2m[36m(func pid=89134)[0m f1_per_class: [0.106, 0.332, 0.0, 0.029, 0.0, 0.0, 0.0, 0.19, 0.0, 0.171]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.21735074626865672
[2m[36m(func pid=87918)[0m top5: 0.7336753731343284
[2m[36m(func pid=87918)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=87918)[0m f1_macro: 0.20929746149333703
[2m[36m(func pid=87918)[0m f1_weighted: 0.23571961418068846
[2m[36m(func pid=87918)[0m f1_per_class: [0.297, 0.257, 0.24, 0.217, 0.028, 0.351, 0.214, 0.291, 0.015, 0.183]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5967 | Steps: 2 | Val loss: 1.7240 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4939 | Steps: 2 | Val loss: 12.4661 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4945 | Steps: 2 | Val loss: 120.5394 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:12:57 (running for 00:03:11.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.422 |      0.209 |                   26 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.597 |      0.353 |                   27 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.315 |      0.155 |                   26 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.637 |      0.083 |                   25 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3917910447761194
[2m[36m(func pid=88300)[0m top5: 0.8782649253731343
[2m[36m(func pid=88300)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=88300)[0m f1_macro: 0.3533559756984322
[2m[36m(func pid=88300)[0m f1_weighted: 0.39825239614483937
[2m[36m(func pid=88300)[0m f1_per_class: [0.504, 0.282, 0.522, 0.547, 0.075, 0.356, 0.362, 0.434, 0.203, 0.25]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3785 | Steps: 2 | Val loss: 2.1693 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=88716)[0m top1: 0.11147388059701492
[2m[36m(func pid=88716)[0m top5: 0.5251865671641791
[2m[36m(func pid=88716)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=88716)[0m f1_macro: 0.15617601570678605
[2m[36m(func pid=88716)[0m f1_weighted: 0.05982736641119487
[2m[36m(func pid=88716)[0m f1_per_class: [0.084, 0.088, 0.512, 0.003, 0.176, 0.0, 0.052, 0.267, 0.103, 0.276]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.14505597014925373
[2m[36m(func pid=89134)[0m top5: 0.6557835820895522
[2m[36m(func pid=89134)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=89134)[0m f1_macro: 0.09012522712251686
[2m[36m(func pid=89134)[0m f1_weighted: 0.10759509694164285
[2m[36m(func pid=89134)[0m f1_per_class: [0.112, 0.343, 0.0, 0.118, 0.0, 0.0, 0.0, 0.204, 0.0, 0.124]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.22014925373134328
[2m[36m(func pid=87918)[0m top5: 0.7434701492537313
[2m[36m(func pid=87918)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=87918)[0m f1_macro: 0.21205017704696819
[2m[36m(func pid=87918)[0m f1_weighted: 0.2396887419938645
[2m[36m(func pid=87918)[0m f1_per_class: [0.309, 0.253, 0.245, 0.231, 0.032, 0.341, 0.218, 0.286, 0.031, 0.174]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4577 | Steps: 2 | Val loss: 1.7250 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6980 | Steps: 2 | Val loss: 11.6374 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6819 | Steps: 2 | Val loss: 92.8526 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 04:13:02 (running for 00:03:16.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.378 |      0.212 |                   27 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.458 |      0.355 |                   28 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.494 |      0.156 |                   27 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.495 |      0.09  |                   26 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.39552238805970147
[2m[36m(func pid=88300)[0m top5: 0.8805970149253731
[2m[36m(func pid=88300)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=88300)[0m f1_macro: 0.3553939656243933
[2m[36m(func pid=88300)[0m f1_weighted: 0.40306262052749725
[2m[36m(func pid=88300)[0m f1_per_class: [0.492, 0.285, 0.545, 0.554, 0.075, 0.339, 0.377, 0.432, 0.196, 0.259]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3219 | Steps: 2 | Val loss: 2.1587 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=88716)[0m top1: 0.14972014925373134
[2m[36m(func pid=88716)[0m top5: 0.7028917910447762
[2m[36m(func pid=88716)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=88716)[0m f1_macro: 0.1287772374189477
[2m[36m(func pid=88716)[0m f1_weighted: 0.10336055165314492
[2m[36m(func pid=88716)[0m f1_per_class: [0.057, 0.304, 0.0, 0.069, 0.192, 0.0, 0.015, 0.317, 0.111, 0.222]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.10727611940298508
[2m[36m(func pid=89134)[0m top5: 0.6875
[2m[36m(func pid=89134)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=89134)[0m f1_macro: 0.06860666270339136
[2m[36m(func pid=89134)[0m f1_weighted: 0.10772266816917757
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.334, 0.049, 0.155, 0.0, 0.0, 0.0, 0.107, 0.0, 0.041]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.23227611940298507
[2m[36m(func pid=87918)[0m top5: 0.7565298507462687
[2m[36m(func pid=87918)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=87918)[0m f1_macro: 0.22285974970433756
[2m[36m(func pid=87918)[0m f1_weighted: 0.2511903178320188
[2m[36m(func pid=87918)[0m f1_per_class: [0.314, 0.258, 0.261, 0.247, 0.034, 0.343, 0.229, 0.32, 0.048, 0.175]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4636 | Steps: 2 | Val loss: 1.7457 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3931 | Steps: 2 | Val loss: 8.7334 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2557 | Steps: 2 | Val loss: 64.0742 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=88300)[0m top1: 0.38992537313432835
[2m[36m(func pid=88300)[0m top5: 0.8847947761194029
[2m[36m(func pid=88300)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=88300)[0m f1_macro: 0.3532008146467277
[2m[36m(func pid=88300)[0m f1_weighted: 0.3999328905356027
[2m[36m(func pid=88300)[0m f1_per_class: [0.451, 0.298, 0.585, 0.541, 0.074, 0.311, 0.383, 0.441, 0.194, 0.253]
== Status ==
Current time: 2024-01-07 04:13:07 (running for 00:03:21.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.322 |      0.223 |                   28 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.464 |      0.353 |                   29 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.698 |      0.129 |                   28 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.682 |      0.069 |                   27 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3193 | Steps: 2 | Val loss: 2.1527 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=88716)[0m top1: 0.25699626865671643
[2m[36m(func pid=88716)[0m top5: 0.7159514925373134
[2m[36m(func pid=88716)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=88716)[0m f1_macro: 0.1582168346670112
[2m[36m(func pid=88716)[0m f1_weighted: 0.22933225292442286
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.185, 0.0, 0.476, 0.057, 0.0, 0.103, 0.448, 0.189, 0.124]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.11800373134328358
[2m[36m(func pid=89134)[0m top5: 0.7131529850746269
[2m[36m(func pid=89134)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=89134)[0m f1_macro: 0.06971207155654885
[2m[36m(func pid=89134)[0m f1_weighted: 0.09841250495267735
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.376, 0.06, 0.094, 0.0, 0.0, 0.0, 0.115, 0.0, 0.053]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.23227611940298507
[2m[36m(func pid=87918)[0m top5: 0.7602611940298507
[2m[36m(func pid=87918)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=87918)[0m f1_macro: 0.22119264911003939
[2m[36m(func pid=87918)[0m f1_weighted: 0.25093114033781455
[2m[36m(func pid=87918)[0m f1_per_class: [0.307, 0.257, 0.245, 0.255, 0.035, 0.341, 0.226, 0.308, 0.032, 0.206]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4953 | Steps: 2 | Val loss: 1.7563 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.7192 | Steps: 2 | Val loss: 12.1816 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 6.4177 | Steps: 2 | Val loss: 33.6717 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:13:12 (running for 00:03:26.88)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.319 |      0.221 |                   29 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.495 |      0.362 |                   30 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.393 |      0.158 |                   29 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.256 |      0.07  |                   28 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3931902985074627
[2m[36m(func pid=88300)[0m top5: 0.8885261194029851
[2m[36m(func pid=88300)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=88300)[0m f1_macro: 0.3624095962834808
[2m[36m(func pid=88300)[0m f1_weighted: 0.40462386534248745
[2m[36m(func pid=88300)[0m f1_per_class: [0.477, 0.306, 0.632, 0.54, 0.071, 0.319, 0.389, 0.446, 0.188, 0.257]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3189 | Steps: 2 | Val loss: 2.1415 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=88716)[0m top1: 0.2555970149253731
[2m[36m(func pid=88716)[0m top5: 0.7751865671641791
[2m[36m(func pid=88716)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=88716)[0m f1_macro: 0.11268638821091206
[2m[36m(func pid=88716)[0m f1_weighted: 0.22182438911057367
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.222, 0.0, 0.54, 0.0, 0.0, 0.07, 0.16, 0.063, 0.072]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.16744402985074627
[2m[36m(func pid=89134)[0m top5: 0.7607276119402985
[2m[36m(func pid=89134)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=89134)[0m f1_macro: 0.09612931704770797
[2m[36m(func pid=89134)[0m f1_weighted: 0.08161405070126777
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.376, 0.146, 0.007, 0.0, 0.0, 0.0, 0.197, 0.0, 0.235]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.23973880597014927
[2m[36m(func pid=87918)[0m top5: 0.7723880597014925
[2m[36m(func pid=87918)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=87918)[0m f1_macro: 0.23030354672861178
[2m[36m(func pid=87918)[0m f1_weighted: 0.25887029736602635
[2m[36m(func pid=87918)[0m f1_per_class: [0.324, 0.246, 0.247, 0.263, 0.039, 0.346, 0.241, 0.339, 0.031, 0.226]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3348 | Steps: 2 | Val loss: 1.7837 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.3351 | Steps: 2 | Val loss: 13.7034 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.8174 | Steps: 2 | Val loss: 33.2567 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:13:17 (running for 00:03:31.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.319 |      0.23  |                   30 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.335 |      0.366 |                   31 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.719 |      0.113 |                   30 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  6.418 |      0.096 |                   29 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3983208955223881
[2m[36m(func pid=88300)[0m top5: 0.8871268656716418
[2m[36m(func pid=88300)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=88300)[0m f1_macro: 0.36597288234179565
[2m[36m(func pid=88300)[0m f1_weighted: 0.41320014723927734
[2m[36m(func pid=88300)[0m f1_per_class: [0.453, 0.313, 0.649, 0.538, 0.069, 0.311, 0.416, 0.462, 0.183, 0.267]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2856 | Steps: 2 | Val loss: 2.1331 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=88716)[0m top1: 0.11054104477611941
[2m[36m(func pid=88716)[0m top5: 0.7178171641791045
[2m[36m(func pid=88716)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=88716)[0m f1_macro: 0.05352900818340807
[2m[36m(func pid=88716)[0m f1_weighted: 0.11221477040838844
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.072, 0.0, 0.348, 0.0, 0.0, 0.0, 0.0, 0.063, 0.052]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18516791044776118
[2m[36m(func pid=89134)[0m top5: 0.7416044776119403
[2m[36m(func pid=89134)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=89134)[0m f1_macro: 0.16419465447704054
[2m[36m(func pid=89134)[0m f1_weighted: 0.08572537573561934
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.386, 0.75, 0.0, 0.0, 0.0, 0.0, 0.192, 0.0, 0.314]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.23880597014925373
[2m[36m(func pid=87918)[0m top5: 0.7742537313432836
[2m[36m(func pid=87918)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=87918)[0m f1_macro: 0.22453464652940802
[2m[36m(func pid=87918)[0m f1_weighted: 0.2596508263774939
[2m[36m(func pid=87918)[0m f1_per_class: [0.309, 0.252, 0.231, 0.265, 0.04, 0.342, 0.246, 0.318, 0.031, 0.211]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2704 | Steps: 2 | Val loss: 1.8142 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4966 | Steps: 2 | Val loss: 13.9407 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.8690 | Steps: 2 | Val loss: 29.8449 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 04:13:22 (running for 00:03:36.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.286 |      0.225 |                   31 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.27  |      0.364 |                   32 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.335 |      0.054 |                   31 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.817 |      0.164 |                   30 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3917910447761194
[2m[36m(func pid=88300)[0m top5: 0.8880597014925373
[2m[36m(func pid=88300)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=88300)[0m f1_macro: 0.3635403440588946
[2m[36m(func pid=88300)[0m f1_weighted: 0.41156016820879054
[2m[36m(func pid=88300)[0m f1_per_class: [0.44, 0.314, 0.649, 0.524, 0.063, 0.308, 0.428, 0.449, 0.185, 0.277]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2559 | Steps: 2 | Val loss: 2.1228 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=88716)[0m top1: 0.134794776119403
[2m[36m(func pid=88716)[0m top5: 0.7145522388059702
[2m[36m(func pid=88716)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=88716)[0m f1_macro: 0.06754747634872725
[2m[36m(func pid=88716)[0m f1_weighted: 0.10993216268311871
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.233, 0.006, 0.162, 0.0, 0.008, 0.041, 0.191, 0.0, 0.034]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17723880597014927
[2m[36m(func pid=89134)[0m top5: 0.7406716417910447
[2m[36m(func pid=89134)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=89134)[0m f1_macro: 0.0831000101999184
[2m[36m(func pid=89134)[0m f1_weighted: 0.07874346520153214
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.192, 0.0, 0.264]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3046 | Steps: 2 | Val loss: 1.8367 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=87918)[0m top1: 0.24580223880597016
[2m[36m(func pid=87918)[0m top5: 0.7817164179104478
[2m[36m(func pid=87918)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=87918)[0m f1_macro: 0.2296596476302702
[2m[36m(func pid=87918)[0m f1_weighted: 0.2679150185437541
[2m[36m(func pid=87918)[0m f1_per_class: [0.322, 0.257, 0.242, 0.284, 0.04, 0.321, 0.258, 0.332, 0.031, 0.21]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8529 | Steps: 2 | Val loss: 26.2973 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 04:13:27 (running for 00:03:42.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.256 |      0.23  |                   32 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.305 |      0.362 |                   33 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.497 |      0.068 |                   32 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.869 |      0.083 |                   31 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3880597014925373
[2m[36m(func pid=88300)[0m top5: 0.8889925373134329
[2m[36m(func pid=88300)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=88300)[0m f1_macro: 0.3617560325827518
[2m[36m(func pid=88300)[0m f1_weighted: 0.4098679089678145
[2m[36m(func pid=88300)[0m f1_per_class: [0.44, 0.315, 0.686, 0.521, 0.063, 0.284, 0.44, 0.415, 0.169, 0.283]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.6568 | Steps: 2 | Val loss: 24.6008 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2016 | Steps: 2 | Val loss: 2.1168 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=88716)[0m top1: 0.11147388059701492
[2m[36m(func pid=88716)[0m top5: 0.7005597014925373
[2m[36m(func pid=88716)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=88716)[0m f1_macro: 0.041934670589112724
[2m[36m(func pid=88716)[0m f1_weighted: 0.05759883796562658
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.273, 0.0, 0.0, 0.0, 0.0, 0.009, 0.138, 0.0, 0.0]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17537313432835822
[2m[36m(func pid=89134)[0m top5: 0.7588619402985075
[2m[36m(func pid=89134)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=89134)[0m f1_macro: 0.08331394178485034
[2m[36m(func pid=89134)[0m f1_weighted: 0.08251549801993319
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.371, 0.0, 0.003, 0.0, 0.0, 0.011, 0.2, 0.0, 0.248]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2313 | Steps: 2 | Val loss: 1.8705 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=87918)[0m top1: 0.2462686567164179
[2m[36m(func pid=87918)[0m top5: 0.7803171641791045
[2m[36m(func pid=87918)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=87918)[0m f1_macro: 0.23280843312729874
[2m[36m(func pid=87918)[0m f1_weighted: 0.2692201278991451
[2m[36m(func pid=87918)[0m f1_per_class: [0.34, 0.243, 0.238, 0.29, 0.036, 0.31, 0.267, 0.323, 0.048, 0.234]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6882 | Steps: 2 | Val loss: 16.9719 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:13:32 (running for 00:03:47.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.202 |      0.233 |                   33 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.231 |      0.365 |                   34 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.853 |      0.042 |                   33 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.657 |      0.083 |                   32 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3885261194029851
[2m[36m(func pid=88300)[0m top5: 0.8894589552238806
[2m[36m(func pid=88300)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=88300)[0m f1_macro: 0.36488133928489636
[2m[36m(func pid=88300)[0m f1_weighted: 0.4134058223154548
[2m[36m(func pid=88300)[0m f1_per_class: [0.434, 0.313, 0.686, 0.507, 0.068, 0.275, 0.466, 0.429, 0.184, 0.288]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.7749 | Steps: 2 | Val loss: 21.5091 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1756 | Steps: 2 | Val loss: 2.1107 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=88716)[0m top1: 0.14272388059701493
[2m[36m(func pid=88716)[0m top5: 0.7723880597014925
[2m[36m(func pid=88716)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=88716)[0m f1_macro: 0.05606012516354808
[2m[36m(func pid=88716)[0m f1_weighted: 0.10498359143795814
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.078, 0.0, 0.0, 0.0, 0.0, 0.277, 0.146, 0.0, 0.061]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.16791044776119404
[2m[36m(func pid=89134)[0m top5: 0.7714552238805971
[2m[36m(func pid=89134)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=89134)[0m f1_macro: 0.0897686007227845
[2m[36m(func pid=89134)[0m f1_weighted: 0.08548855609430252
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.356, 0.0, 0.019, 0.0, 0.0, 0.014, 0.187, 0.0, 0.321]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.24953358208955223
[2m[36m(func pid=87918)[0m top5: 0.7831156716417911
[2m[36m(func pid=87918)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=87918)[0m f1_macro: 0.2333093819591686
[2m[36m(func pid=87918)[0m f1_weighted: 0.27303032356727985
[2m[36m(func pid=87918)[0m f1_per_class: [0.333, 0.244, 0.242, 0.304, 0.041, 0.316, 0.266, 0.318, 0.048, 0.222]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2222 | Steps: 2 | Val loss: 1.9069 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0957 | Steps: 2 | Val loss: 20.4712 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.6008 | Steps: 2 | Val loss: 19.1893 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 04:13:37 (running for 00:03:52.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.176 |      0.233 |                   34 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.222 |      0.359 |                   35 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  2.688 |      0.056 |                   34 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.775 |      0.09  |                   33 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.384794776119403
[2m[36m(func pid=88300)[0m top5: 0.8927238805970149
[2m[36m(func pid=88300)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=88300)[0m f1_macro: 0.3588383509018601
[2m[36m(func pid=88300)[0m f1_weighted: 0.4157799144890013
[2m[36m(func pid=88300)[0m f1_per_class: [0.414, 0.317, 0.686, 0.497, 0.063, 0.289, 0.488, 0.386, 0.166, 0.283]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.1459 | Steps: 2 | Val loss: 2.1085 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=88716)[0m top1: 0.27098880597014924
[2m[36m(func pid=88716)[0m top5: 0.7859141791044776
[2m[36m(func pid=88716)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=88716)[0m f1_macro: 0.08736500071360642
[2m[36m(func pid=88716)[0m f1_weighted: 0.1811835559645217
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.119, 0.0, 0.0, 0.154, 0.0, 0.532, 0.005, 0.0, 0.063]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.15438432835820895
[2m[36m(func pid=89134)[0m top5: 0.7877798507462687
[2m[36m(func pid=89134)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=89134)[0m f1_macro: 0.10671523651421169
[2m[36m(func pid=89134)[0m f1_weighted: 0.09138916709005193
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.316, 0.0, 0.075, 0.143, 0.0, 0.003, 0.171, 0.0, 0.36]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.2490671641791045
[2m[36m(func pid=87918)[0m top5: 0.7840485074626866
[2m[36m(func pid=87918)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=87918)[0m f1_macro: 0.23429957015603167
[2m[36m(func pid=87918)[0m f1_weighted: 0.2736233617572309
[2m[36m(func pid=87918)[0m f1_per_class: [0.343, 0.238, 0.231, 0.304, 0.04, 0.308, 0.269, 0.327, 0.063, 0.22]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3268 | Steps: 2 | Val loss: 1.9680 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0867 | Steps: 2 | Val loss: 24.1043 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:13:43 (running for 00:03:57.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.146 |      0.234 |                   35 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.327 |      0.361 |                   36 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.096 |      0.087 |                   35 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.601 |      0.107 |                   34 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3833955223880597
[2m[36m(func pid=88300)[0m top5: 0.8903917910447762
[2m[36m(func pid=88300)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=88300)[0m f1_macro: 0.3611211105085698
[2m[36m(func pid=88300)[0m f1_weighted: 0.4149514290481725
[2m[36m(func pid=88300)[0m f1_per_class: [0.39, 0.325, 0.686, 0.484, 0.063, 0.303, 0.484, 0.397, 0.182, 0.298]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.6219 | Steps: 2 | Val loss: 18.1826 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1179 | Steps: 2 | Val loss: 2.1007 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=88716)[0m top1: 0.28777985074626866
[2m[36m(func pid=88716)[0m top5: 0.8512126865671642
[2m[36m(func pid=88716)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=88716)[0m f1_macro: 0.08942618952201802
[2m[36m(func pid=88716)[0m f1_weighted: 0.21023876170172381
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.302, 0.0, 0.0, 0.0, 0.0, 0.528, 0.003, 0.0, 0.062]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.14925373134328357
[2m[36m(func pid=89134)[0m top5: 0.7831156716417911
[2m[36m(func pid=89134)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=89134)[0m f1_macro: 0.1069504674166573
[2m[36m(func pid=89134)[0m f1_weighted: 0.09811467690416363
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.288, 0.0, 0.119, 0.111, 0.0, 0.0, 0.166, 0.027, 0.359]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.25279850746268656
[2m[36m(func pid=87918)[0m top5: 0.7877798507462687
[2m[36m(func pid=87918)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=87918)[0m f1_macro: 0.23389802169516968
[2m[36m(func pid=87918)[0m f1_weighted: 0.27638155763418015
[2m[36m(func pid=87918)[0m f1_per_class: [0.353, 0.255, 0.233, 0.312, 0.041, 0.286, 0.272, 0.323, 0.048, 0.216]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.1896 | Steps: 2 | Val loss: 2.0179 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7422 | Steps: 2 | Val loss: 29.7497 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=88300)[0m top1: 0.37220149253731344
[2m[36m(func pid=88300)[0m top5: 0.8871268656716418
[2m[36m(func pid=88300)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=88300)[0m f1_macro: 0.35079505483742957
[2m[36m(func pid=88300)[0m f1_weighted: 0.406301190453568
[2m[36m(func pid=88300)[0m f1_per_class: [0.356, 0.318, 0.667, 0.464, 0.061, 0.304, 0.483, 0.383, 0.183, 0.288]
== Status ==
Current time: 2024-01-07 04:13:48 (running for 00:04:02.73)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.118 |      0.234 |                   36 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.19  |      0.351 |                   37 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.087 |      0.089 |                   36 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.622 |      0.107 |                   35 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.5852 | Steps: 2 | Val loss: 17.0027 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.0523 | Steps: 2 | Val loss: 2.0967 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=88716)[0m top1: 0.2150186567164179
[2m[36m(func pid=88716)[0m top5: 0.8353544776119403
[2m[36m(func pid=88716)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=88716)[0m f1_macro: 0.07494507182050987
[2m[36m(func pid=88716)[0m f1_weighted: 0.16170063732596443
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.182, 0.0, 0.0, 0.029, 0.0, 0.427, 0.036, 0.0, 0.075]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.13992537313432835
[2m[36m(func pid=89134)[0m top5: 0.7784514925373134
[2m[36m(func pid=89134)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=89134)[0m f1_macro: 0.0999388109174
[2m[36m(func pid=89134)[0m f1_weighted: 0.088921276173432
[2m[36m(func pid=89134)[0m f1_per_class: [0.044, 0.294, 0.0, 0.075, 0.057, 0.0, 0.006, 0.166, 0.024, 0.333]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m top1: 0.2513992537313433
[2m[36m(func pid=87918)[0m top5: 0.7882462686567164
[2m[36m(func pid=87918)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=87918)[0m f1_macro: 0.23317819194930406
[2m[36m(func pid=87918)[0m f1_weighted: 0.2747366869537219
[2m[36m(func pid=87918)[0m f1_per_class: [0.347, 0.254, 0.235, 0.309, 0.041, 0.283, 0.273, 0.316, 0.048, 0.226]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.1493 | Steps: 2 | Val loss: 2.0466 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7209 | Steps: 2 | Val loss: 20.7303 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6690 | Steps: 2 | Val loss: 16.1045 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=88300)[0m top1: 0.37453358208955223
[2m[36m(func pid=88300)[0m top5: 0.8931902985074627
[2m[36m(func pid=88300)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=88300)[0m f1_macro: 0.34942581528511035
[2m[36m(func pid=88300)[0m f1_weighted: 0.41103842259668044
[2m[36m(func pid=88300)[0m f1_per_class: [0.314, 0.31, 0.667, 0.458, 0.064, 0.332, 0.5, 0.391, 0.183, 0.275]
== Status ==
Current time: 2024-01-07 04:13:53 (running for 00:04:07.98)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.052 |      0.233 |                   37 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.149 |      0.349 |                   38 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.742 |      0.075 |                   37 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.585 |      0.1   |                   36 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.9934 | Steps: 2 | Val loss: 2.0923 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=88716)[0m top1: 0.18983208955223882
[2m[36m(func pid=88716)[0m top5: 0.7495335820895522
[2m[36m(func pid=88716)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=88716)[0m f1_macro: 0.07833965210384765
[2m[36m(func pid=88716)[0m f1_weighted: 0.13546614611966118
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.143, 0.0, 0.0, 0.046, 0.0, 0.33, 0.196, 0.0, 0.069]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.13059701492537312
[2m[36m(func pid=89134)[0m top5: 0.7630597014925373
[2m[36m(func pid=89134)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=89134)[0m f1_macro: 0.08543955248383225
[2m[36m(func pid=89134)[0m f1_weighted: 0.07269295531800431
[2m[36m(func pid=89134)[0m f1_per_class: [0.022, 0.309, 0.0, 0.007, 0.0, 0.0, 0.009, 0.168, 0.04, 0.3]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1661 | Steps: 2 | Val loss: 2.1034 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=87918)[0m top1: 0.25699626865671643
[2m[36m(func pid=87918)[0m top5: 0.7859141791044776
[2m[36m(func pid=87918)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=87918)[0m f1_macro: 0.23689417738127086
[2m[36m(func pid=87918)[0m f1_weighted: 0.2806997515075046
[2m[36m(func pid=87918)[0m f1_per_class: [0.351, 0.259, 0.24, 0.319, 0.046, 0.285, 0.278, 0.322, 0.048, 0.221]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.1457 | Steps: 2 | Val loss: 14.4006 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.4664 | Steps: 2 | Val loss: 15.4381 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=88300)[0m top1: 0.36986940298507465
[2m[36m(func pid=88300)[0m top5: 0.8908582089552238
[2m[36m(func pid=88300)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=88300)[0m f1_macro: 0.3449315317218812
[2m[36m(func pid=88300)[0m f1_weighted: 0.408975525005994
[2m[36m(func pid=88300)[0m f1_per_class: [0.281, 0.309, 0.632, 0.449, 0.062, 0.352, 0.492, 0.413, 0.195, 0.265]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:13:58 (running for 00:04:13.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.993 |      0.237 |                   38 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.166 |      0.345 |                   39 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.721 |      0.078 |                   38 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.669 |      0.085 |                   37 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.13899253731343283
[2m[36m(func pid=88716)[0m top5: 0.6273320895522388
[2m[36m(func pid=88716)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=88716)[0m f1_macro: 0.0799615526039329
[2m[36m(func pid=88716)[0m f1_weighted: 0.12514346798009637
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.209, 0.0, 0.0, 0.061, 0.0, 0.249, 0.244, 0.0, 0.037]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.0713 | Steps: 2 | Val loss: 2.0888 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=89134)[0m top1: 0.13759328358208955
[2m[36m(func pid=89134)[0m top5: 0.7569962686567164
[2m[36m(func pid=89134)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=89134)[0m f1_macro: 0.08113667720694001
[2m[36m(func pid=89134)[0m f1_weighted: 0.07668556097355539
[2m[36m(func pid=89134)[0m f1_per_class: [0.02, 0.324, 0.0, 0.003, 0.0, 0.0, 0.02, 0.174, 0.02, 0.25]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1044 | Steps: 2 | Val loss: 2.1510 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=87918)[0m top1: 0.2579291044776119
[2m[36m(func pid=87918)[0m top5: 0.789179104477612
[2m[36m(func pid=87918)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=87918)[0m f1_macro: 0.23670448396199237
[2m[36m(func pid=87918)[0m f1_weighted: 0.2812446879442663
[2m[36m(func pid=87918)[0m f1_per_class: [0.359, 0.256, 0.218, 0.313, 0.046, 0.266, 0.29, 0.342, 0.049, 0.229]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8441 | Steps: 2 | Val loss: 16.6750 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8659 | Steps: 2 | Val loss: 14.8426 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
== Status ==
Current time: 2024-01-07 04:14:03 (running for 00:04:18.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  2.071 |      0.237 |                   39 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.104 |      0.336 |                   40 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.146 |      0.08  |                   39 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.466 |      0.081 |                   38 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3596082089552239
[2m[36m(func pid=88300)[0m top5: 0.886660447761194
[2m[36m(func pid=88300)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=88300)[0m f1_macro: 0.33607887376797063
[2m[36m(func pid=88300)[0m f1_weighted: 0.39947695670586
[2m[36m(func pid=88300)[0m f1_per_class: [0.256, 0.303, 0.6, 0.429, 0.062, 0.353, 0.484, 0.42, 0.198, 0.255]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.05690298507462686
[2m[36m(func pid=88716)[0m top5: 0.5
[2m[36m(func pid=88716)[0m f1_micro: 0.05690298507462686
[2m[36m(func pid=88716)[0m f1_macro: 0.03733726863585404
[2m[36m(func pid=88716)[0m f1_weighted: 0.04238530856923335
[2m[36m(func pid=88716)[0m f1_per_class: [0.0, 0.219, 0.0, 0.0, 0.075, 0.0, 0.003, 0.051, 0.0, 0.025]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.9604 | Steps: 2 | Val loss: 2.0829 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=89134)[0m top1: 0.14925373134328357
[2m[36m(func pid=89134)[0m top5: 0.7639925373134329
[2m[36m(func pid=89134)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=89134)[0m f1_macro: 0.08641475527150058
[2m[36m(func pid=89134)[0m f1_weighted: 0.08566844986498402
[2m[36m(func pid=89134)[0m f1_per_class: [0.038, 0.332, 0.0, 0.0, 0.0, 0.0, 0.048, 0.182, 0.0, 0.264]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1066 | Steps: 2 | Val loss: 2.2265 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7861 | Steps: 2 | Val loss: 26.0356 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=87918)[0m top1: 0.2579291044776119
[2m[36m(func pid=87918)[0m top5: 0.789179104477612
[2m[36m(func pid=87918)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=87918)[0m f1_macro: 0.2394100875945782
[2m[36m(func pid=87918)[0m f1_weighted: 0.2822164282179199
[2m[36m(func pid=87918)[0m f1_per_class: [0.347, 0.247, 0.231, 0.319, 0.041, 0.273, 0.287, 0.347, 0.083, 0.221]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7904 | Steps: 2 | Val loss: 14.7220 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:14:09 (running for 00:04:23.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.96  |      0.239 |                   40 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.107 |      0.335 |                   41 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.844 |      0.037 |                   40 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.866 |      0.086 |                   39 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.35074626865671643
[2m[36m(func pid=88300)[0m top5: 0.8843283582089553
[2m[36m(func pid=88300)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=88300)[0m f1_macro: 0.3351020671941646
[2m[36m(func pid=88300)[0m f1_weighted: 0.3920112753548108
[2m[36m(func pid=88300)[0m f1_per_class: [0.241, 0.296, 0.649, 0.407, 0.068, 0.343, 0.488, 0.422, 0.188, 0.25]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.06576492537313433
[2m[36m(func pid=88716)[0m top5: 0.31296641791044777
[2m[36m(func pid=88716)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=88716)[0m f1_macro: 0.04128422616305109
[2m[36m(func pid=88716)[0m f1_weighted: 0.04467752501107188
[2m[36m(func pid=88716)[0m f1_per_class: [0.067, 0.247, 0.0, 0.0, 0.068, 0.0, 0.0, 0.0, 0.0, 0.032]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.8902 | Steps: 2 | Val loss: 2.0799 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89134)[0m top1: 0.16044776119402984
[2m[36m(func pid=89134)[0m top5: 0.7854477611940298
[2m[36m(func pid=89134)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=89134)[0m f1_macro: 0.09372219236190008
[2m[36m(func pid=89134)[0m f1_weighted: 0.09054751209160201
[2m[36m(func pid=89134)[0m f1_per_class: [0.019, 0.356, 0.0, 0.003, 0.0, 0.0, 0.045, 0.183, 0.0, 0.33]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0796 | Steps: 2 | Val loss: 2.2606 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9956 | Steps: 2 | Val loss: 29.3472 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=87918)[0m top1: 0.26026119402985076
[2m[36m(func pid=87918)[0m top5: 0.789179104477612
[2m[36m(func pid=87918)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=87918)[0m f1_macro: 0.2418453164645435
[2m[36m(func pid=87918)[0m f1_weighted: 0.284261112931998
[2m[36m(func pid=87918)[0m f1_per_class: [0.338, 0.246, 0.229, 0.318, 0.042, 0.275, 0.29, 0.36, 0.096, 0.225]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4821 | Steps: 2 | Val loss: 15.3172 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 04:14:14 (running for 00:04:28.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.89  |      0.242 |                   41 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.08  |      0.333 |                   42 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.786 |      0.041 |                   41 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.79  |      0.094 |                   40 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.35074626865671643
[2m[36m(func pid=88300)[0m top5: 0.8852611940298507
[2m[36m(func pid=88300)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=88300)[0m f1_macro: 0.332511047832061
[2m[36m(func pid=88300)[0m f1_weighted: 0.3921272546283753
[2m[36m(func pid=88300)[0m f1_per_class: [0.241, 0.293, 0.615, 0.406, 0.069, 0.356, 0.485, 0.436, 0.187, 0.237]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.08488805970149253
[2m[36m(func pid=88716)[0m top5: 0.28218283582089554
[2m[36m(func pid=88716)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=88716)[0m f1_macro: 0.04649956023435408
[2m[36m(func pid=88716)[0m f1_weighted: 0.05383075853974172
[2m[36m(func pid=88716)[0m f1_per_class: [0.044, 0.265, 0.0, 0.0, 0.087, 0.0, 0.021, 0.0, 0.0, 0.048]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.9456 | Steps: 2 | Val loss: 2.0764 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=89134)[0m top1: 0.16884328358208955
[2m[36m(func pid=89134)[0m top5: 0.7905783582089553
[2m[36m(func pid=89134)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=89134)[0m f1_macro: 0.0990218028112044
[2m[36m(func pid=89134)[0m f1_weighted: 0.09359346459168792
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.376, 0.0, 0.026, 0.0, 0.0, 0.023, 0.18, 0.0, 0.386]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1192 | Steps: 2 | Val loss: 2.3163 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5638 | Steps: 2 | Val loss: 28.7675 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=87918)[0m top1: 0.26026119402985076
[2m[36m(func pid=87918)[0m top5: 0.7877798507462687
[2m[36m(func pid=87918)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=87918)[0m f1_macro: 0.24133037225598467
[2m[36m(func pid=87918)[0m f1_weighted: 0.2844795998219993
[2m[36m(func pid=87918)[0m f1_per_class: [0.342, 0.244, 0.22, 0.319, 0.041, 0.268, 0.294, 0.362, 0.096, 0.227]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3296 | Steps: 2 | Val loss: 14.9825 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=88300)[0m top1: 0.34421641791044777
[2m[36m(func pid=88300)[0m top5: 0.8843283582089553
[2m[36m(func pid=88300)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=88300)[0m f1_macro: 0.3289858592609873
[2m[36m(func pid=88300)[0m f1_weighted: 0.3836333881678246
[2m[36m(func pid=88300)[0m f1_per_class: [0.242, 0.298, 0.632, 0.386, 0.077, 0.327, 0.485, 0.428, 0.183, 0.233]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:19 (running for 00:04:33.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.946 |      0.241 |                   42 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.119 |      0.329 |                   43 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.996 |      0.046 |                   42 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.482 |      0.099 |                   41 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.10494402985074627
[2m[36m(func pid=88716)[0m top5: 0.3292910447761194
[2m[36m(func pid=88716)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=88716)[0m f1_macro: 0.051604997799228024
[2m[36m(func pid=88716)[0m f1_weighted: 0.07197030467533623
[2m[36m(func pid=88716)[0m f1_per_class: [0.044, 0.246, 0.0, 0.0, 0.056, 0.0, 0.092, 0.0, 0.0, 0.078]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9104 | Steps: 2 | Val loss: 2.0738 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=89134)[0m top1: 0.18423507462686567
[2m[36m(func pid=89134)[0m top5: 0.8111007462686567
[2m[36m(func pid=89134)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=89134)[0m f1_macro: 0.11124939998413086
[2m[36m(func pid=89134)[0m f1_weighted: 0.10425562395148516
[2m[36m(func pid=89134)[0m f1_per_class: [0.029, 0.395, 0.0, 0.062, 0.0, 0.0, 0.009, 0.185, 0.0, 0.432]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0765 | Steps: 2 | Val loss: 2.3485 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4142 | Steps: 2 | Val loss: 27.5616 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=87918)[0m top1: 0.26399253731343286
[2m[36m(func pid=87918)[0m top5: 0.789179104477612
[2m[36m(func pid=87918)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=87918)[0m f1_macro: 0.24485611449751712
[2m[36m(func pid=87918)[0m f1_weighted: 0.28854136729294955
[2m[36m(func pid=87918)[0m f1_per_class: [0.331, 0.244, 0.229, 0.326, 0.041, 0.267, 0.296, 0.38, 0.115, 0.219]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4440 | Steps: 2 | Val loss: 15.4036 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=88300)[0m top1: 0.34654850746268656
[2m[36m(func pid=88300)[0m top5: 0.8843283582089553
[2m[36m(func pid=88300)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=88300)[0m f1_macro: 0.33152633549989596
[2m[36m(func pid=88300)[0m f1_weighted: 0.38463936198789533
[2m[36m(func pid=88300)[0m f1_per_class: [0.256, 0.308, 0.649, 0.391, 0.077, 0.323, 0.478, 0.422, 0.197, 0.214]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:24 (running for 00:04:39.18)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.91  |      0.245 |                   43 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.077 |      0.332 |                   44 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.414 |      0.067 |                   44 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.33  |      0.111 |                   42 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.11753731343283583
[2m[36m(func pid=88716)[0m top5: 0.3833955223880597
[2m[36m(func pid=88716)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=88716)[0m f1_macro: 0.06660016998676259
[2m[36m(func pid=88716)[0m f1_weighted: 0.09129222396963423
[2m[36m(func pid=88716)[0m f1_per_class: [0.034, 0.2, 0.0, 0.007, 0.175, 0.0, 0.175, 0.0, 0.0, 0.075]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.8581 | Steps: 2 | Val loss: 2.0687 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=89134)[0m top1: 0.1865671641791045
[2m[36m(func pid=89134)[0m top5: 0.8255597014925373
[2m[36m(func pid=89134)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=89134)[0m f1_macro: 0.13650268541930957
[2m[36m(func pid=89134)[0m f1_weighted: 0.10441809060213826
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.389, 0.133, 0.07, 0.1, 0.0, 0.0, 0.189, 0.0, 0.485]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1041 | Steps: 2 | Val loss: 2.3751 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6813 | Steps: 2 | Val loss: 25.4707 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=87918)[0m top1: 0.2644589552238806
[2m[36m(func pid=87918)[0m top5: 0.7859141791044776
[2m[36m(func pid=87918)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=87918)[0m f1_macro: 0.24124542367093688
[2m[36m(func pid=87918)[0m f1_weighted: 0.28962413278672655
[2m[36m(func pid=87918)[0m f1_per_class: [0.314, 0.245, 0.209, 0.331, 0.042, 0.262, 0.3, 0.372, 0.114, 0.225]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.3904 | Steps: 2 | Val loss: 15.5729 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=88300)[0m top1: 0.3474813432835821
[2m[36m(func pid=88300)[0m top5: 0.886660447761194
[2m[36m(func pid=88300)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=88300)[0m f1_macro: 0.33017452386657736
[2m[36m(func pid=88300)[0m f1_weighted: 0.3846011372895679
[2m[36m(func pid=88300)[0m f1_per_class: [0.28, 0.304, 0.649, 0.409, 0.089, 0.333, 0.469, 0.36, 0.203, 0.206]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:29 (running for 00:04:44.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.858 |      0.241 |                   44 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.104 |      0.33  |                   45 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.681 |      0.074 |                   45 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.444 |      0.137 |                   43 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.1357276119402985
[2m[36m(func pid=88716)[0m top5: 0.43097014925373134
[2m[36m(func pid=88716)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=88716)[0m f1_macro: 0.07379641294488397
[2m[36m(func pid=88716)[0m f1_weighted: 0.11240859071545035
[2m[36m(func pid=88716)[0m f1_per_class: [0.031, 0.204, 0.0, 0.007, 0.133, 0.0, 0.238, 0.029, 0.0, 0.096]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18796641791044777
[2m[36m(func pid=89134)[0m top5: 0.8255597014925373
[2m[36m(func pid=89134)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=89134)[0m f1_macro: 0.15470982355853935
[2m[36m(func pid=89134)[0m f1_weighted: 0.10594978439826515
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.387, 0.421, 0.073, 0.0, 0.0, 0.0, 0.19, 0.0, 0.476]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.8126 | Steps: 2 | Val loss: 2.0619 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0381 | Steps: 2 | Val loss: 2.3847 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8815 | Steps: 2 | Val loss: 21.1166 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=87918)[0m top1: 0.271455223880597
[2m[36m(func pid=87918)[0m top5: 0.7840485074626866
[2m[36m(func pid=87918)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=87918)[0m f1_macro: 0.24486021466442623
[2m[36m(func pid=87918)[0m f1_weighted: 0.29717747141447964
[2m[36m(func pid=87918)[0m f1_per_class: [0.308, 0.249, 0.222, 0.354, 0.042, 0.259, 0.3, 0.382, 0.113, 0.218]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.4989 | Steps: 2 | Val loss: 15.9689 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=88300)[0m top1: 0.35494402985074625
[2m[36m(func pid=88300)[0m top5: 0.8922574626865671
[2m[36m(func pid=88300)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=88300)[0m f1_macro: 0.33176786783056994
[2m[36m(func pid=88300)[0m f1_weighted: 0.38958463340038213
[2m[36m(func pid=88300)[0m f1_per_class: [0.282, 0.316, 0.649, 0.411, 0.094, 0.333, 0.482, 0.335, 0.2, 0.217]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:35 (running for 00:04:49.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.813 |      0.245 |                   45 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.038 |      0.332 |                   46 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.881 |      0.092 |                   46 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.39  |      0.155 |                   44 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.166044776119403
[2m[36m(func pid=88716)[0m top5: 0.5097947761194029
[2m[36m(func pid=88716)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=88716)[0m f1_macro: 0.09225448334887035
[2m[36m(func pid=88716)[0m f1_weighted: 0.14487599399555004
[2m[36m(func pid=88716)[0m f1_per_class: [0.05, 0.221, 0.0, 0.022, 0.098, 0.0, 0.305, 0.115, 0.0, 0.112]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1875
[2m[36m(func pid=89134)[0m top5: 0.8255597014925373
[2m[36m(func pid=89134)[0m f1_micro: 0.1875
[2m[36m(func pid=89134)[0m f1_macro: 0.17042848063029353
[2m[36m(func pid=89134)[0m f1_weighted: 0.1063414596575164
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.384, 0.545, 0.07, 0.0, 0.0, 0.0, 0.189, 0.024, 0.492]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7524 | Steps: 2 | Val loss: 2.0555 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0387 | Steps: 2 | Val loss: 2.4285 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.9748 | Steps: 2 | Val loss: 17.1295 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=87918)[0m top1: 0.269589552238806
[2m[36m(func pid=87918)[0m top5: 0.784981343283582
[2m[36m(func pid=87918)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=87918)[0m f1_macro: 0.24615490661302464
[2m[36m(func pid=87918)[0m f1_weighted: 0.294324345890246
[2m[36m(func pid=87918)[0m f1_per_class: [0.306, 0.245, 0.24, 0.355, 0.042, 0.259, 0.292, 0.381, 0.113, 0.229]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.4397 | Steps: 2 | Val loss: 15.7382 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=88300)[0m top1: 0.3614738805970149
[2m[36m(func pid=88300)[0m top5: 0.8936567164179104
[2m[36m(func pid=88300)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=88300)[0m f1_macro: 0.3362301999850865
[2m[36m(func pid=88300)[0m f1_weighted: 0.39547226798791413
[2m[36m(func pid=88300)[0m f1_per_class: [0.284, 0.331, 0.649, 0.412, 0.097, 0.339, 0.488, 0.332, 0.218, 0.212]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:40 (running for 00:04:54.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.752 |      0.246 |                   46 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.039 |      0.336 |                   47 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.975 |      0.127 |                   47 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.499 |      0.17  |                   45 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.2178171641791045
[2m[36m(func pid=88716)[0m top5: 0.5881529850746269
[2m[36m(func pid=88716)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=88716)[0m f1_macro: 0.12653436403024623
[2m[36m(func pid=88716)[0m f1_weighted: 0.18704901480458036
[2m[36m(func pid=88716)[0m f1_per_class: [0.091, 0.272, 0.0, 0.025, 0.069, 0.008, 0.378, 0.27, 0.0, 0.152]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18236940298507462
[2m[36m(func pid=89134)[0m top5: 0.8250932835820896
[2m[36m(func pid=89134)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=89134)[0m f1_macro: 0.14992380168850755
[2m[36m(func pid=89134)[0m f1_weighted: 0.1028028679202955
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.376, 0.5, 0.07, 0.0, 0.0, 0.0, 0.189, 0.022, 0.343]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7136 | Steps: 2 | Val loss: 2.0508 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1867 | Steps: 2 | Val loss: 2.5160 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2767 | Steps: 2 | Val loss: 14.7198 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=87918)[0m top1: 0.2751865671641791
[2m[36m(func pid=87918)[0m top5: 0.7896455223880597
[2m[36m(func pid=87918)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=87918)[0m f1_macro: 0.24699260889754004
[2m[36m(func pid=87918)[0m f1_weighted: 0.30151101838503236
[2m[36m(func pid=87918)[0m f1_per_class: [0.29, 0.247, 0.233, 0.368, 0.043, 0.269, 0.299, 0.391, 0.116, 0.215]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.2635 | Steps: 2 | Val loss: 14.7991 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=88300)[0m top1: 0.35867537313432835
[2m[36m(func pid=88300)[0m top5: 0.8959888059701493
[2m[36m(func pid=88300)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=88300)[0m f1_macro: 0.33973383653311096
[2m[36m(func pid=88300)[0m f1_weighted: 0.39207936841139396
[2m[36m(func pid=88300)[0m f1_per_class: [0.281, 0.34, 0.686, 0.397, 0.088, 0.33, 0.486, 0.337, 0.226, 0.227]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:45 (running for 00:04:59.57)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.714 |      0.247 |                   47 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.187 |      0.34  |                   48 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.277 |      0.157 |                   48 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.44  |      0.15  |                   46 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.23740671641791045
[2m[36m(func pid=88716)[0m top5: 0.6478544776119403
[2m[36m(func pid=88716)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=88716)[0m f1_macro: 0.15704779487757933
[2m[36m(func pid=88716)[0m f1_weighted: 0.21294472825103092
[2m[36m(func pid=88716)[0m f1_per_class: [0.113, 0.281, 0.015, 0.037, 0.094, 0.042, 0.406, 0.38, 0.05, 0.153]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1828358208955224
[2m[36m(func pid=89134)[0m top5: 0.8274253731343284
[2m[36m(func pid=89134)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=89134)[0m f1_macro: 0.13872627261026899
[2m[36m(func pid=89134)[0m f1_weighted: 0.10674939283806681
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.366, 0.4, 0.089, 0.0, 0.008, 0.0, 0.193, 0.023, 0.308]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.6900 | Steps: 2 | Val loss: 2.0434 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0768 | Steps: 2 | Val loss: 2.6004 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0926 | Steps: 2 | Val loss: 13.5182 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=87918)[0m top1: 0.2775186567164179
[2m[36m(func pid=87918)[0m top5: 0.7905783582089553
[2m[36m(func pid=87918)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=87918)[0m f1_macro: 0.2444111390866499
[2m[36m(func pid=87918)[0m f1_weighted: 0.3038680532637761
[2m[36m(func pid=87918)[0m f1_per_class: [0.269, 0.255, 0.216, 0.365, 0.045, 0.264, 0.309, 0.39, 0.113, 0.218]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2493 | Steps: 2 | Val loss: 14.0146 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=88300)[0m top1: 0.35447761194029853
[2m[36m(func pid=88300)[0m top5: 0.8936567164179104
[2m[36m(func pid=88300)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=88300)[0m f1_macro: 0.3429765132417756
[2m[36m(func pid=88300)[0m f1_weighted: 0.3867412550554944
[2m[36m(func pid=88300)[0m f1_per_class: [0.278, 0.347, 0.727, 0.394, 0.095, 0.325, 0.468, 0.34, 0.221, 0.234]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:50 (running for 00:05:04.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.69  |      0.244 |                   48 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.077 |      0.343 |                   49 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.093 |      0.2   |                   49 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.264 |      0.139 |                   47 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.27052238805970147
[2m[36m(func pid=88716)[0m top5: 0.6996268656716418
[2m[36m(func pid=88716)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=88716)[0m f1_macro: 0.19987586846895758
[2m[36m(func pid=88716)[0m f1_weighted: 0.26099425575129703
[2m[36m(func pid=88716)[0m f1_per_class: [0.109, 0.316, 0.038, 0.051, 0.107, 0.163, 0.461, 0.473, 0.127, 0.155]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18423507462686567
[2m[36m(func pid=89134)[0m top5: 0.8283582089552238
[2m[36m(func pid=89134)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=89134)[0m f1_macro: 0.13435340118720715
[2m[36m(func pid=89134)[0m f1_weighted: 0.10920251685376794
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.362, 0.353, 0.1, 0.0, 0.008, 0.0, 0.2, 0.024, 0.295]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6527 | Steps: 2 | Val loss: 2.0350 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0496 | Steps: 2 | Val loss: 2.6662 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4530 | Steps: 2 | Val loss: 13.7210 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1760 | Steps: 2 | Val loss: 14.4173 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=87918)[0m top1: 0.28171641791044777
[2m[36m(func pid=87918)[0m top5: 0.7924440298507462
[2m[36m(func pid=87918)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=87918)[0m f1_macro: 0.2467302712812959
[2m[36m(func pid=87918)[0m f1_weighted: 0.3077311993158433
[2m[36m(func pid=87918)[0m f1_per_class: [0.267, 0.254, 0.231, 0.376, 0.047, 0.264, 0.312, 0.393, 0.111, 0.213]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.35261194029850745
[2m[36m(func pid=88300)[0m top5: 0.8964552238805971
[2m[36m(func pid=88300)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=88300)[0m f1_macro: 0.34823432516270947
[2m[36m(func pid=88300)[0m f1_weighted: 0.38143588445618926
[2m[36m(func pid=88300)[0m f1_per_class: [0.297, 0.345, 0.75, 0.393, 0.097, 0.331, 0.445, 0.346, 0.238, 0.241]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:14:55 (running for 00:05:09.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.653 |      0.247 |                   49 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.05  |      0.348 |                   50 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.453 |      0.198 |                   50 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.249 |      0.134 |                   48 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.28544776119402987
[2m[36m(func pid=88716)[0m top5: 0.7355410447761194
[2m[36m(func pid=88716)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=88716)[0m f1_macro: 0.1983690143387732
[2m[36m(func pid=88716)[0m f1_weighted: 0.27132341781972197
[2m[36m(func pid=88716)[0m f1_per_class: [0.083, 0.313, 0.054, 0.028, 0.041, 0.236, 0.491, 0.497, 0.111, 0.13]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18097014925373134
[2m[36m(func pid=89134)[0m top5: 0.8260261194029851
[2m[36m(func pid=89134)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=89134)[0m f1_macro: 0.1255731885920432
[2m[36m(func pid=89134)[0m f1_weighted: 0.10222967163114284
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.356, 0.308, 0.083, 0.0, 0.0, 0.0, 0.202, 0.027, 0.28]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6317 | Steps: 2 | Val loss: 2.0273 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0272 | Steps: 2 | Val loss: 2.6553 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4832 | Steps: 2 | Val loss: 14.3380 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2356 | Steps: 2 | Val loss: 14.5005 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=87918)[0m top1: 0.28824626865671643
[2m[36m(func pid=87918)[0m top5: 0.7896455223880597
[2m[36m(func pid=87918)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=87918)[0m f1_macro: 0.2501565248669416
[2m[36m(func pid=87918)[0m f1_weighted: 0.31365873908639197
[2m[36m(func pid=87918)[0m f1_per_class: [0.27, 0.259, 0.229, 0.4, 0.049, 0.279, 0.3, 0.392, 0.113, 0.211]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.3596082089552239
[2m[36m(func pid=88300)[0m top5: 0.8973880597014925
[2m[36m(func pid=88300)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=88300)[0m f1_macro: 0.35811812733110127
[2m[36m(func pid=88300)[0m f1_weighted: 0.3885492536731239
[2m[36m(func pid=88300)[0m f1_per_class: [0.32, 0.342, 0.8, 0.409, 0.098, 0.326, 0.453, 0.363, 0.22, 0.252]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:15:00 (running for 00:05:15.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.632 |      0.25  |                   50 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.027 |      0.358 |                   51 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.483 |      0.207 |                   51 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.176 |      0.126 |                   49 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.2905783582089552
[2m[36m(func pid=88716)[0m top5: 0.7532649253731343
[2m[36m(func pid=88716)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=88716)[0m f1_macro: 0.20727713832353517
[2m[36m(func pid=88716)[0m f1_weighted: 0.2787077621221095
[2m[36m(func pid=88716)[0m f1_per_class: [0.095, 0.316, 0.1, 0.026, 0.064, 0.292, 0.494, 0.508, 0.089, 0.089]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18330223880597016
[2m[36m(func pid=89134)[0m top5: 0.8246268656716418
[2m[36m(func pid=89134)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=89134)[0m f1_macro: 0.1268221843488822
[2m[36m(func pid=89134)[0m f1_weighted: 0.1046153941938411
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.356, 0.275, 0.093, 0.0, 0.0, 0.0, 0.204, 0.0, 0.341]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6003 | Steps: 2 | Val loss: 2.0161 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0362 | Steps: 2 | Val loss: 2.6783 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3298 | Steps: 2 | Val loss: 14.6616 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0570 | Steps: 2 | Val loss: 14.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=87918)[0m top1: 0.2971082089552239
[2m[36m(func pid=87918)[0m top5: 0.7943097014925373
[2m[36m(func pid=87918)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=87918)[0m f1_macro: 0.25644044771389274
[2m[36m(func pid=87918)[0m f1_weighted: 0.321795256786839
[2m[36m(func pid=87918)[0m f1_per_class: [0.259, 0.265, 0.242, 0.428, 0.05, 0.283, 0.294, 0.398, 0.128, 0.218]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.3670708955223881
[2m[36m(func pid=88300)[0m top5: 0.8992537313432836
[2m[36m(func pid=88300)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=88300)[0m f1_macro: 0.37120692045546494
[2m[36m(func pid=88300)[0m f1_weighted: 0.39760634115915056
[2m[36m(func pid=88300)[0m f1_per_class: [0.331, 0.339, 0.889, 0.417, 0.091, 0.326, 0.474, 0.374, 0.221, 0.252]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.27611940298507465
[2m[36m(func pid=88716)[0m top5: 0.7551305970149254
[2m[36m(func pid=88716)[0m f1_micro: 0.27611940298507465
== Status ==
Current time: 2024-01-07 04:15:05 (running for 00:05:20.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.6   |      0.256 |                   51 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.036 |      0.371 |                   52 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.33  |      0.181 |                   52 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.236 |      0.127 |                   50 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=88716)[0m f1_macro: 0.18073740528734708

[2m[36m(func pid=88716)[0m f1_weighted: 0.2693438522237682
[2m[36m(func pid=88716)[0m f1_per_class: [0.063, 0.311, 0.0, 0.022, 0.029, 0.299, 0.492, 0.4, 0.094, 0.098]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1912313432835821
[2m[36m(func pid=89134)[0m top5: 0.8292910447761194
[2m[36m(func pid=89134)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=89134)[0m f1_macro: 0.13011696830398417
[2m[36m(func pid=89134)[0m f1_weighted: 0.11835983585309588
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.356, 0.219, 0.141, 0.0, 0.0, 0.0, 0.205, 0.0, 0.38]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5413 | Steps: 2 | Val loss: 2.0098 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0499 | Steps: 2 | Val loss: 2.6933 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4860 | Steps: 2 | Val loss: 14.5757 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1259 | Steps: 2 | Val loss: 13.5921 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=87918)[0m top1: 0.3045708955223881
[2m[36m(func pid=87918)[0m top5: 0.7952425373134329
[2m[36m(func pid=87918)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=87918)[0m f1_macro: 0.2628070220177183
[2m[36m(func pid=87918)[0m f1_weighted: 0.3294356616783796
[2m[36m(func pid=87918)[0m f1_per_class: [0.267, 0.266, 0.25, 0.44, 0.051, 0.299, 0.299, 0.409, 0.13, 0.218]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.37779850746268656
[2m[36m(func pid=88300)[0m top5: 0.9020522388059702
[2m[36m(func pid=88300)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=88300)[0m f1_macro: 0.378591929140656
[2m[36m(func pid=88300)[0m f1_weighted: 0.4080975948099001
[2m[36m(func pid=88300)[0m f1_per_class: [0.352, 0.339, 0.889, 0.438, 0.093, 0.312, 0.49, 0.386, 0.218, 0.271]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:15:11 (running for 00:05:25.56)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.541 |      0.263 |                   52 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.05  |      0.379 |                   53 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.486 |      0.163 |                   53 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.057 |      0.13  |                   51 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.26725746268656714
[2m[36m(func pid=88716)[0m top5: 0.7639925373134329
[2m[36m(func pid=88716)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=88716)[0m f1_macro: 0.16303282692235
[2m[36m(func pid=88716)[0m f1_weighted: 0.26576694717535265
[2m[36m(func pid=88716)[0m f1_per_class: [0.062, 0.291, 0.0, 0.028, 0.014, 0.342, 0.499, 0.271, 0.08, 0.044]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.19682835820895522
[2m[36m(func pid=89134)[0m top5: 0.8306902985074627
[2m[36m(func pid=89134)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=89134)[0m f1_macro: 0.1358349261893282
[2m[36m(func pid=89134)[0m f1_weighted: 0.13071110511047604
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.354, 0.182, 0.185, 0.0, 0.0, 0.0, 0.211, 0.0, 0.427]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5803 | Steps: 2 | Val loss: 1.9978 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0190 | Steps: 2 | Val loss: 2.6896 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8393 | Steps: 2 | Val loss: 14.5129 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=87918)[0m top1: 0.31203358208955223
[2m[36m(func pid=87918)[0m top5: 0.7999067164179104
[2m[36m(func pid=87918)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=87918)[0m f1_macro: 0.26697848027826854
[2m[36m(func pid=87918)[0m f1_weighted: 0.33723486678818626
[2m[36m(func pid=87918)[0m f1_per_class: [0.257, 0.267, 0.253, 0.451, 0.064, 0.318, 0.307, 0.41, 0.13, 0.213]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.38526119402985076
[2m[36m(func pid=88300)[0m top5: 0.9029850746268657
[2m[36m(func pid=88300)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=88300)[0m f1_macro: 0.38614753321188816
[2m[36m(func pid=88300)[0m f1_weighted: 0.41580703207091824
[2m[36m(func pid=88300)[0m f1_per_class: [0.358, 0.338, 0.889, 0.452, 0.093, 0.31, 0.494, 0.423, 0.227, 0.277]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5739 | Steps: 2 | Val loss: 13.1040 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:15:16 (running for 00:05:30.59)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.58  |      0.267 |                   53 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.019 |      0.386 |                   54 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.839 |      0.151 |                   54 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.126 |      0.136 |                   52 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.2513992537313433
[2m[36m(func pid=88716)[0m top5: 0.7644589552238806
[2m[36m(func pid=88716)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=88716)[0m f1_macro: 0.15112043126360522
[2m[36m(func pid=88716)[0m f1_weighted: 0.25538598954117253
[2m[36m(func pid=88716)[0m f1_per_class: [0.068, 0.285, 0.0, 0.05, 0.0, 0.352, 0.464, 0.157, 0.086, 0.05]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1958955223880597
[2m[36m(func pid=89134)[0m top5: 0.8339552238805971
[2m[36m(func pid=89134)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=89134)[0m f1_macro: 0.1357648479149825
[2m[36m(func pid=89134)[0m f1_weighted: 0.1404021752749335
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.337, 0.139, 0.226, 0.0, 0.0, 0.003, 0.218, 0.0, 0.435]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5746 | Steps: 2 | Val loss: 1.9880 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0208 | Steps: 2 | Val loss: 2.7195 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2034 | Steps: 2 | Val loss: 14.7283 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9781 | Steps: 2 | Val loss: 13.0564 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=87918)[0m top1: 0.31296641791044777
[2m[36m(func pid=87918)[0m top5: 0.800839552238806
[2m[36m(func pid=87918)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=87918)[0m f1_macro: 0.2658944280724045
[2m[36m(func pid=87918)[0m f1_weighted: 0.33833471650296415
[2m[36m(func pid=87918)[0m f1_per_class: [0.261, 0.273, 0.255, 0.448, 0.059, 0.31, 0.314, 0.41, 0.114, 0.215]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.38526119402985076
[2m[36m(func pid=88300)[0m top5: 0.9029850746268657
[2m[36m(func pid=88300)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=88300)[0m f1_macro: 0.388022837389652
[2m[36m(func pid=88300)[0m f1_weighted: 0.4151131314700002
[2m[36m(func pid=88300)[0m f1_per_class: [0.395, 0.325, 0.889, 0.466, 0.091, 0.289, 0.49, 0.437, 0.226, 0.273]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.22807835820895522
[2m[36m(func pid=88716)[0m top5: 0.7546641791044776
[2m[36m(func pid=88716)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=88716)[0m f1_macro: 0.14366086021973193
[2m[36m(func pid=88716)[0m f1_weighted: 0.23879697318800472
[2m[36m(func pid=88716)[0m f1_per_class: [0.058, 0.257, 0.0, 0.061, 0.045, 0.372, 0.42, 0.085, 0.088, 0.05]
[2m[36m(func pid=88716)[0m 
== Status ==
Current time: 2024-01-07 04:15:21 (running for 00:05:35.95)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.575 |      0.266 |                   54 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.021 |      0.388 |                   55 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.203 |      0.144 |                   55 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.574 |      0.136 |                   53 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=89134)[0m top1: 0.19076492537313433
[2m[36m(func pid=89134)[0m top5: 0.8325559701492538
[2m[36m(func pid=89134)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=89134)[0m f1_macro: 0.13635561707165136
[2m[36m(func pid=89134)[0m f1_weighted: 0.1483131580352757
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.315, 0.121, 0.255, 0.0, 0.0, 0.014, 0.216, 0.025, 0.418]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4834 | Steps: 2 | Val loss: 1.9760 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0201 | Steps: 2 | Val loss: 2.7176 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4699 | Steps: 2 | Val loss: 14.5308 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.1772 | Steps: 2 | Val loss: 12.8419 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=87918)[0m top1: 0.31949626865671643
[2m[36m(func pid=87918)[0m top5: 0.8073694029850746
[2m[36m(func pid=87918)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=87918)[0m f1_macro: 0.27378258254964444
[2m[36m(func pid=87918)[0m f1_weighted: 0.34487976482890487
[2m[36m(func pid=87918)[0m f1_per_class: [0.266, 0.273, 0.276, 0.449, 0.061, 0.326, 0.326, 0.411, 0.13, 0.22]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.38619402985074625
[2m[36m(func pid=88300)[0m top5: 0.902518656716418
[2m[36m(func pid=88300)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=88300)[0m f1_macro: 0.38584381190700007
[2m[36m(func pid=88300)[0m f1_weighted: 0.41666544518513493
[2m[36m(func pid=88300)[0m f1_per_class: [0.405, 0.309, 0.889, 0.484, 0.098, 0.289, 0.49, 0.432, 0.215, 0.247]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:15:26 (running for 00:05:41.06)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.483 |      0.274 |                   55 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.02  |      0.386 |                   56 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.47  |      0.137 |                   56 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  0.978 |      0.136 |                   54 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.19542910447761194
[2m[36m(func pid=88716)[0m top5: 0.7388059701492538
[2m[36m(func pid=88716)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=88716)[0m f1_macro: 0.13728398640229153
[2m[36m(func pid=88716)[0m f1_weighted: 0.20947064278683575
[2m[36m(func pid=88716)[0m f1_per_class: [0.054, 0.23, 0.043, 0.07, 0.083, 0.364, 0.339, 0.03, 0.1, 0.061]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18796641791044777
[2m[36m(func pid=89134)[0m top5: 0.8316231343283582
[2m[36m(func pid=89134)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=89134)[0m f1_macro: 0.13465704313144097
[2m[36m(func pid=89134)[0m f1_weighted: 0.1467421741268681
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.31, 0.132, 0.264, 0.0, 0.0, 0.006, 0.206, 0.024, 0.406]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4373 | Steps: 2 | Val loss: 1.9653 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0087 | Steps: 2 | Val loss: 2.7527 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7440 | Steps: 2 | Val loss: 13.9218 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4992 | Steps: 2 | Val loss: 12.5544 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=87918)[0m top1: 0.3199626865671642
[2m[36m(func pid=87918)[0m top5: 0.8106343283582089
[2m[36m(func pid=87918)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=87918)[0m f1_macro: 0.2747775058107904
[2m[36m(func pid=87918)[0m f1_weighted: 0.34568845517336566
[2m[36m(func pid=87918)[0m f1_per_class: [0.262, 0.263, 0.296, 0.454, 0.061, 0.321, 0.332, 0.408, 0.132, 0.218]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.3908582089552239
[2m[36m(func pid=88300)[0m top5: 0.898320895522388
[2m[36m(func pid=88300)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=88300)[0m f1_macro: 0.3840323798034715
[2m[36m(func pid=88300)[0m f1_weighted: 0.4216322598523241
[2m[36m(func pid=88300)[0m f1_per_class: [0.405, 0.305, 0.857, 0.495, 0.094, 0.271, 0.502, 0.452, 0.226, 0.234]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.19029850746268656
[2m[36m(func pid=88716)[0m top5: 0.7513992537313433
[2m[36m(func pid=88716)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=88716)[0m f1_macro: 0.14890627517748334
[2m[36m(func pid=88716)[0m f1_weighted: 0.21354396755815316
[2m[36m(func pid=88716)[0m f1_per_class: [0.055, 0.204, 0.071, 0.096, 0.158, 0.324, 0.356, 0.029, 0.078, 0.118]
== Status ==
Current time: 2024-01-07 04:15:31 (running for 00:05:46.28)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.437 |      0.275 |                   56 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.009 |      0.384 |                   57 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.744 |      0.149 |                   57 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.177 |      0.135 |                   55 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1935634328358209
[2m[36m(func pid=89134)[0m top5: 0.8269589552238806
[2m[36m(func pid=89134)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=89134)[0m f1_macro: 0.14041972299059607
[2m[36m(func pid=89134)[0m f1_weighted: 0.15095237064366654
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.323, 0.169, 0.27, 0.0, 0.0, 0.006, 0.199, 0.025, 0.411]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0221 | Steps: 2 | Val loss: 2.7844 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.4076 | Steps: 2 | Val loss: 1.9502 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5964 | Steps: 2 | Val loss: 13.5526 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.4237 | Steps: 2 | Val loss: 12.1126 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=88300)[0m top1: 0.3927238805970149
[2m[36m(func pid=88300)[0m top5: 0.8950559701492538
[2m[36m(func pid=88300)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=88300)[0m f1_macro: 0.3852644488538086
[2m[36m(func pid=88300)[0m f1_weighted: 0.4243906393312887
[2m[36m(func pid=88300)[0m f1_per_class: [0.408, 0.297, 0.857, 0.502, 0.094, 0.264, 0.508, 0.477, 0.222, 0.224]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3278917910447761
[2m[36m(func pid=87918)[0m top5: 0.8185634328358209
[2m[36m(func pid=87918)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=87918)[0m f1_macro: 0.2820161449441908
[2m[36m(func pid=87918)[0m f1_weighted: 0.3526931307164388
[2m[36m(func pid=87918)[0m f1_per_class: [0.287, 0.267, 0.304, 0.468, 0.061, 0.337, 0.331, 0.414, 0.133, 0.218]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m top1: 0.18796641791044777
[2m[36m(func pid=88716)[0m top5: 0.7658582089552238
[2m[36m(func pid=88716)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=88716)[0m f1_macro: 0.1703895510844572
[2m[36m(func pid=88716)[0m f1_weighted: 0.21385407995611597
[2m[36m(func pid=88716)[0m f1_per_class: [0.055, 0.192, 0.167, 0.139, 0.244, 0.227, 0.343, 0.091, 0.076, 0.17]
== Status ==
Current time: 2024-01-07 04:15:37 (running for 00:05:51.44)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.408 |      0.282 |                   57 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.022 |      0.385 |                   58 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.596 |      0.17  |                   58 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.499 |      0.14  |                   56 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1912313432835821
[2m[36m(func pid=89134)[0m top5: 0.824160447761194
[2m[36m(func pid=89134)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=89134)[0m f1_macro: 0.14411800365317684
[2m[36m(func pid=89134)[0m f1_weighted: 0.15241899998329603
[2m[36m(func pid=89134)[0m f1_per_class: [0.052, 0.3, 0.192, 0.294, 0.0, 0.0, 0.0, 0.193, 0.024, 0.385]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0123 | Steps: 2 | Val loss: 2.8137 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3709 | Steps: 2 | Val loss: 1.9400 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.2274 | Steps: 2 | Val loss: 12.8039 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.1195 | Steps: 2 | Val loss: 10.5573 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=88300)[0m top1: 0.3894589552238806
[2m[36m(func pid=88300)[0m top5: 0.8899253731343284
[2m[36m(func pid=88300)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=88300)[0m f1_macro: 0.377658306132201
[2m[36m(func pid=88300)[0m f1_weighted: 0.4204636559838809
[2m[36m(func pid=88300)[0m f1_per_class: [0.397, 0.294, 0.815, 0.505, 0.094, 0.247, 0.501, 0.485, 0.221, 0.218]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.33255597014925375
[2m[36m(func pid=87918)[0m top5: 0.820429104477612
[2m[36m(func pid=87918)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=87918)[0m f1_macro: 0.28765978781554746
[2m[36m(func pid=87918)[0m f1_weighted: 0.3583971930852579
[2m[36m(func pid=87918)[0m f1_per_class: [0.284, 0.275, 0.32, 0.467, 0.071, 0.338, 0.345, 0.418, 0.133, 0.225]
== Status ==
Current time: 2024-01-07 04:15:42 (running for 00:05:56.50)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.371 |      0.288 |                   58 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.012 |      0.378 |                   59 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.596 |      0.17  |                   58 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.424 |      0.144 |                   57 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m top1: 0.1875
[2m[36m(func pid=88716)[0m top5: 0.7868470149253731
[2m[36m(func pid=88716)[0m f1_micro: 0.1875
[2m[36m(func pid=88716)[0m f1_macro: 0.17482343748179688
[2m[36m(func pid=88716)[0m f1_weighted: 0.20542916956176904
[2m[36m(func pid=88716)[0m f1_per_class: [0.062, 0.177, 0.261, 0.141, 0.211, 0.053, 0.354, 0.256, 0.096, 0.137]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.21688432835820895
[2m[36m(func pid=89134)[0m top5: 0.8180970149253731
[2m[36m(func pid=89134)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=89134)[0m f1_macro: 0.15647406114811094
[2m[36m(func pid=89134)[0m f1_weighted: 0.17570864088097
[2m[36m(func pid=89134)[0m f1_per_class: [0.056, 0.284, 0.253, 0.385, 0.0, 0.0, 0.0, 0.207, 0.024, 0.357]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0309 | Steps: 2 | Val loss: 2.8732 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4753 | Steps: 2 | Val loss: 1.9291 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8578 | Steps: 2 | Val loss: 12.0258 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6132 | Steps: 2 | Val loss: 9.5377 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:15:47 (running for 00:06:01.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.371 |      0.288 |                   58 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.031 |      0.38  |                   60 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.227 |      0.175 |                   59 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.119 |      0.156 |                   58 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.38572761194029853
[2m[36m(func pid=88300)[0m top5: 0.8889925373134329
[2m[36m(func pid=88300)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=88300)[0m f1_macro: 0.3802718287757866
[2m[36m(func pid=88300)[0m f1_weighted: 0.41446105426496893
[2m[36m(func pid=88300)[0m f1_per_class: [0.392, 0.295, 0.857, 0.504, 0.095, 0.239, 0.48, 0.493, 0.224, 0.222]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.32975746268656714
[2m[36m(func pid=87918)[0m top5: 0.8260261194029851
[2m[36m(func pid=87918)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=87918)[0m f1_macro: 0.28280583351013927
[2m[36m(func pid=87918)[0m f1_weighted: 0.35587941415643803
[2m[36m(func pid=87918)[0m f1_per_class: [0.28, 0.262, 0.324, 0.466, 0.073, 0.322, 0.354, 0.418, 0.113, 0.217]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m top1: 0.21128731343283583
[2m[36m(func pid=88716)[0m top5: 0.7980410447761194
[2m[36m(func pid=88716)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=88716)[0m f1_macro: 0.20929499761360368
[2m[36m(func pid=88716)[0m f1_weighted: 0.22100655768701236
[2m[36m(func pid=88716)[0m f1_per_class: [0.058, 0.164, 0.595, 0.265, 0.174, 0.0, 0.299, 0.34, 0.088, 0.111]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.251865671641791
[2m[36m(func pid=89134)[0m top5: 0.8194962686567164
[2m[36m(func pid=89134)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=89134)[0m f1_macro: 0.17340144513229339
[2m[36m(func pid=89134)[0m f1_weighted: 0.19481578892256685
[2m[36m(func pid=89134)[0m f1_per_class: [0.052, 0.291, 0.328, 0.442, 0.0, 0.0, 0.0, 0.227, 0.027, 0.366]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0141 | Steps: 2 | Val loss: 2.8794 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.2973 | Steps: 2 | Val loss: 1.9217 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2015 | Steps: 2 | Val loss: 12.4175 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 17.3224 | Steps: 2 | Val loss: 9.7981 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 04:15:52 (running for 00:06:06.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.475 |      0.283 |                   59 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.014 |      0.373 |                   61 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.858 |      0.209 |                   60 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.613 |      0.173 |                   59 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.38619402985074625
[2m[36m(func pid=88300)[0m top5: 0.8880597014925373
[2m[36m(func pid=88300)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=88300)[0m f1_macro: 0.372951126291905
[2m[36m(func pid=88300)[0m f1_weighted: 0.4144117816400361
[2m[36m(func pid=88300)[0m f1_per_class: [0.388, 0.293, 0.8, 0.505, 0.098, 0.243, 0.482, 0.49, 0.219, 0.212]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.33348880597014924
[2m[36m(func pid=87918)[0m top5: 0.8288246268656716
[2m[36m(func pid=87918)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=87918)[0m f1_macro: 0.28485213606127047
[2m[36m(func pid=87918)[0m f1_weighted: 0.3600113578607267
[2m[36m(func pid=87918)[0m f1_per_class: [0.293, 0.28, 0.308, 0.463, 0.073, 0.321, 0.359, 0.418, 0.117, 0.216]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m top1: 0.21082089552238806
[2m[36m(func pid=88716)[0m top5: 0.7887126865671642
[2m[36m(func pid=88716)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=88716)[0m f1_macro: 0.18409228258426463
[2m[36m(func pid=88716)[0m f1_weighted: 0.21206392363205787
[2m[36m(func pid=88716)[0m f1_per_class: [0.079, 0.169, 0.329, 0.326, 0.186, 0.0, 0.211, 0.337, 0.097, 0.107]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.25699626865671643
[2m[36m(func pid=89134)[0m top5: 0.8264925373134329
[2m[36m(func pid=89134)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=89134)[0m f1_macro: 0.17739269903094537
[2m[36m(func pid=89134)[0m f1_weighted: 0.19506581649990978
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.327, 0.409, 0.416, 0.0, 0.008, 0.003, 0.242, 0.0, 0.368]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0114 | Steps: 2 | Val loss: 2.9023 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5519 | Steps: 2 | Val loss: 13.1963 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2786 | Steps: 2 | Val loss: 1.9127 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4417 | Steps: 2 | Val loss: 10.7955 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 04:15:57 (running for 00:06:11.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.297 |      0.285 |                   60 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.011 |      0.366 |                   62 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.202 |      0.184 |                   61 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      | 17.322 |      0.177 |                   60 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3824626865671642
[2m[36m(func pid=88300)[0m top5: 0.8875932835820896
[2m[36m(func pid=88300)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=88300)[0m f1_macro: 0.3658942829591174
[2m[36m(func pid=88300)[0m f1_weighted: 0.40806279414047736
[2m[36m(func pid=88300)[0m f1_per_class: [0.372, 0.29, 0.774, 0.498, 0.102, 0.22, 0.479, 0.494, 0.219, 0.212]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3358208955223881
[2m[36m(func pid=87918)[0m top5: 0.832089552238806
[2m[36m(func pid=87918)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=87918)[0m f1_macro: 0.2907465991753871
[2m[36m(func pid=87918)[0m f1_weighted: 0.3641846530035067
[2m[36m(func pid=87918)[0m f1_per_class: [0.296, 0.273, 0.324, 0.454, 0.072, 0.355, 0.369, 0.431, 0.116, 0.217]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m top1: 0.17863805970149255
[2m[36m(func pid=88716)[0m top5: 0.7789179104477612
[2m[36m(func pid=88716)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=88716)[0m f1_macro: 0.15479449262429765
[2m[36m(func pid=88716)[0m f1_weighted: 0.17079647452596816
[2m[36m(func pid=88716)[0m f1_per_class: [0.068, 0.17, 0.167, 0.289, 0.209, 0.0, 0.112, 0.327, 0.103, 0.103]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.23787313432835822
[2m[36m(func pid=89134)[0m top5: 0.8236940298507462
[2m[36m(func pid=89134)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=89134)[0m f1_macro: 0.16644527371104934
[2m[36m(func pid=89134)[0m f1_weighted: 0.17948198061693063
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.37, 0.316, 0.33, 0.0, 0.024, 0.003, 0.235, 0.0, 0.386]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0137 | Steps: 2 | Val loss: 2.9388 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8108 | Steps: 2 | Val loss: 14.1511 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2606 | Steps: 2 | Val loss: 1.9093 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.6619 | Steps: 2 | Val loss: 13.0910 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=88300)[0m top1: 0.3787313432835821
[2m[36m(func pid=88300)[0m top5: 0.8899253731343284
[2m[36m(func pid=88300)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=88300)[0m f1_macro: 0.3638366043023338
[2m[36m(func pid=88300)[0m f1_weighted: 0.40429687381335777
[2m[36m(func pid=88300)[0m f1_per_class: [0.369, 0.293, 0.8, 0.492, 0.1, 0.229, 0.471, 0.489, 0.186, 0.209]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:16:02 (running for 00:06:17.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.279 |      0.291 |                   61 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.014 |      0.364 |                   63 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.552 |      0.155 |                   62 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.442 |      0.166 |                   61 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.16884328358208955
[2m[36m(func pid=88716)[0m top5: 0.7681902985074627
[2m[36m(func pid=88716)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=88716)[0m f1_macro: 0.150685660131145
[2m[36m(func pid=88716)[0m f1_weighted: 0.15513739461834755
[2m[36m(func pid=88716)[0m f1_per_class: [0.072, 0.1, 0.137, 0.298, 0.286, 0.0, 0.09, 0.319, 0.123, 0.082]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.32649253731343286
[2m[36m(func pid=87918)[0m top5: 0.8297574626865671
[2m[36m(func pid=87918)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=87918)[0m f1_macro: 0.28777544446923103
[2m[36m(func pid=87918)[0m f1_weighted: 0.3553746134196486
[2m[36m(func pid=87918)[0m f1_per_class: [0.303, 0.25, 0.329, 0.444, 0.068, 0.348, 0.365, 0.421, 0.129, 0.221]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.20242537313432835
[2m[36m(func pid=89134)[0m top5: 0.8222947761194029
[2m[36m(func pid=89134)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=89134)[0m f1_macro: 0.13370415823248225
[2m[36m(func pid=89134)[0m f1_weighted: 0.13624077000564666
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.375, 0.0, 0.178, 0.086, 0.032, 0.0, 0.205, 0.026, 0.436]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0085 | Steps: 2 | Val loss: 2.9483 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4362 | Steps: 2 | Val loss: 14.9855 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2222 | Steps: 2 | Val loss: 1.9024 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0092 | Steps: 2 | Val loss: 14.0942 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 04:16:07 (running for 00:06:22.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.261 |      0.288 |                   62 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.008 |      0.364 |                   64 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.811 |      0.151 |                   63 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.662 |      0.134 |                   62 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.37966417910447764
[2m[36m(func pid=88300)[0m top5: 0.8899253731343284
[2m[36m(func pid=88300)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=88300)[0m f1_macro: 0.3640004774717933
[2m[36m(func pid=88300)[0m f1_weighted: 0.4047180474516479
[2m[36m(func pid=88300)[0m f1_per_class: [0.361, 0.297, 0.8, 0.492, 0.103, 0.238, 0.468, 0.485, 0.188, 0.209]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.17444029850746268
[2m[36m(func pid=88716)[0m top5: 0.7705223880597015
[2m[36m(func pid=88716)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=88716)[0m f1_macro: 0.15039449509860486
[2m[36m(func pid=88716)[0m f1_weighted: 0.16096378461892877
[2m[36m(func pid=88716)[0m f1_per_class: [0.067, 0.084, 0.156, 0.329, 0.222, 0.063, 0.068, 0.317, 0.117, 0.081]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.32742537313432835
[2m[36m(func pid=87918)[0m top5: 0.8325559701492538
[2m[36m(func pid=87918)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=87918)[0m f1_macro: 0.289223724215996
[2m[36m(func pid=87918)[0m f1_weighted: 0.3567321077203045
[2m[36m(func pid=87918)[0m f1_per_class: [0.306, 0.262, 0.358, 0.441, 0.07, 0.332, 0.372, 0.415, 0.13, 0.206]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.17817164179104478
[2m[36m(func pid=89134)[0m top5: 0.8138992537313433
[2m[36m(func pid=89134)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=89134)[0m f1_macro: 0.11571529274614711
[2m[36m(func pid=89134)[0m f1_weighted: 0.09879057090551135
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.367, 0.0, 0.057, 0.06, 0.015, 0.0, 0.196, 0.042, 0.421]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0049 | Steps: 2 | Val loss: 2.9707 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3966 | Steps: 2 | Val loss: 15.5480 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.1460 | Steps: 2 | Val loss: 1.8917 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.4789 | Steps: 2 | Val loss: 14.3489 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:16:13 (running for 00:06:27.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.222 |      0.289 |                   63 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.005 |      0.364 |                   65 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.436 |      0.15  |                   64 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.009 |      0.116 |                   63 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3787313432835821
[2m[36m(func pid=88300)[0m top5: 0.8903917910447762
[2m[36m(func pid=88300)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=88300)[0m f1_macro: 0.36401863333220447
[2m[36m(func pid=88300)[0m f1_weighted: 0.4038250483269537
[2m[36m(func pid=88300)[0m f1_per_class: [0.357, 0.289, 0.8, 0.49, 0.103, 0.237, 0.47, 0.497, 0.189, 0.209]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3353544776119403
[2m[36m(func pid=87918)[0m top5: 0.835820895522388
[2m[36m(func pid=87918)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=87918)[0m f1_macro: 0.296961792646382
[2m[36m(func pid=87918)[0m f1_weighted: 0.3650863946972071
[2m[36m(func pid=87918)[0m f1_per_class: [0.325, 0.277, 0.381, 0.446, 0.071, 0.33, 0.385, 0.422, 0.132, 0.201]
[2m[36m(func pid=88716)[0m top1: 0.17257462686567165
[2m[36m(func pid=88716)[0m top5: 0.7677238805970149
[2m[36m(func pid=88716)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=88716)[0m f1_macro: 0.14899455588225244
[2m[36m(func pid=88716)[0m f1_weighted: 0.16474025581586507
[2m[36m(func pid=88716)[0m f1_per_class: [0.048, 0.055, 0.212, 0.355, 0.214, 0.055, 0.084, 0.288, 0.108, 0.071]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.1814365671641791
[2m[36m(func pid=89134)[0m top5: 0.7999067164179104
[2m[36m(func pid=89134)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=89134)[0m f1_macro: 0.10873710795850648
[2m[36m(func pid=89134)[0m f1_weighted: 0.09440505509020007
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.384, 0.0, 0.02, 0.0, 0.0, 0.012, 0.196, 0.117, 0.36]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0077 | Steps: 2 | Val loss: 3.0143 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3133 | Steps: 2 | Val loss: 16.1470 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.1206 | Steps: 2 | Val loss: 1.8835 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.8736 | Steps: 2 | Val loss: 13.6739 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:16:18 (running for 00:06:32.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.146 |      0.297 |                   64 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.008 |      0.365 |                   66 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.397 |      0.149 |                   65 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.479 |      0.109 |                   64 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.37826492537313433
[2m[36m(func pid=88300)[0m top5: 0.8894589552238806
[2m[36m(func pid=88300)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=88300)[0m f1_macro: 0.36513255582926946
[2m[36m(func pid=88300)[0m f1_weighted: 0.4014773193285537
[2m[36m(func pid=88300)[0m f1_per_class: [0.353, 0.293, 0.8, 0.496, 0.106, 0.232, 0.453, 0.497, 0.217, 0.205]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.18470149253731344
[2m[36m(func pid=88716)[0m top5: 0.7635261194029851
[2m[36m(func pid=88716)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=88716)[0m f1_macro: 0.1601676048109978
[2m[36m(func pid=88716)[0m f1_weighted: 0.19824692733274368
[2m[36m(func pid=88716)[0m f1_per_class: [0.046, 0.101, 0.338, 0.367, 0.167, 0.07, 0.17, 0.224, 0.057, 0.062]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.332089552238806
[2m[36m(func pid=87918)[0m top5: 0.8381529850746269
[2m[36m(func pid=87918)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=87918)[0m f1_macro: 0.2951632357169185
[2m[36m(func pid=87918)[0m f1_weighted: 0.3611129908223233
[2m[36m(func pid=87918)[0m f1_per_class: [0.324, 0.283, 0.4, 0.435, 0.067, 0.325, 0.384, 0.395, 0.13, 0.207]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.18796641791044777
[2m[36m(func pid=89134)[0m top5: 0.7989738805970149
[2m[36m(func pid=89134)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=89134)[0m f1_macro: 0.09942194562555287
[2m[36m(func pid=89134)[0m f1_weighted: 0.10328782229216064
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.397, 0.0, 0.007, 0.0, 0.0, 0.058, 0.199, 0.024, 0.31]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0028 | Steps: 2 | Val loss: 3.0351 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3810 | Steps: 2 | Val loss: 16.7297 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1544 | Steps: 2 | Val loss: 1.8737 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9449 | Steps: 2 | Val loss: 10.1791 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=88300)[0m top1: 0.3736007462686567
[2m[36m(func pid=88300)[0m top5: 0.8903917910447762
[2m[36m(func pid=88300)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=88300)[0m f1_macro: 0.35698603629591125
[2m[36m(func pid=88300)[0m f1_weighted: 0.3961829458179714
[2m[36m(func pid=88300)[0m f1_per_class: [0.342, 0.289, 0.759, 0.491, 0.108, 0.23, 0.446, 0.498, 0.205, 0.202]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:16:23 (running for 00:06:37.72)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.121 |      0.295 |                   65 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.003 |      0.357 |                   67 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.313 |      0.16  |                   66 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.874 |      0.099 |                   65 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.3381529850746269
[2m[36m(func pid=87918)[0m top5: 0.8428171641791045
[2m[36m(func pid=87918)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=87918)[0m f1_macro: 0.30450263387335075
[2m[36m(func pid=87918)[0m f1_weighted: 0.367284306608058
[2m[36m(func pid=87918)[0m f1_per_class: [0.337, 0.29, 0.444, 0.442, 0.069, 0.326, 0.392, 0.395, 0.145, 0.206]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88716)[0m top1: 0.2042910447761194
[2m[36m(func pid=88716)[0m top5: 0.7532649253731343
[2m[36m(func pid=88716)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=88716)[0m f1_macro: 0.17747924155795328
[2m[36m(func pid=88716)[0m f1_weighted: 0.23287891675614383
[2m[36m(func pid=88716)[0m f1_per_class: [0.058, 0.131, 0.478, 0.375, 0.148, 0.084, 0.275, 0.118, 0.049, 0.06]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=89134)[0m top1: 0.20942164179104478
[2m[36m(func pid=89134)[0m top5: 0.789179104477612
[2m[36m(func pid=89134)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=89134)[0m f1_macro: 0.12116468138403422
[2m[36m(func pid=89134)[0m f1_weighted: 0.13587133197805193
[2m[36m(func pid=89134)[0m f1_per_class: [0.034, 0.414, 0.0, 0.013, 0.0, 0.0, 0.145, 0.213, 0.0, 0.392]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0191 | Steps: 2 | Val loss: 3.0632 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7248 | Steps: 2 | Val loss: 17.2436 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.1948 | Steps: 2 | Val loss: 1.8712 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6668 | Steps: 2 | Val loss: 7.2506 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:16:28 (running for 00:06:42.88)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.154 |      0.305 |                   66 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.019 |      0.356 |                   68 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.381 |      0.177 |                   67 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.945 |      0.121 |                   66 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3736007462686567
[2m[36m(func pid=88300)[0m top5: 0.8903917910447762
[2m[36m(func pid=88300)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=88300)[0m f1_macro: 0.35611812410970195
[2m[36m(func pid=88300)[0m f1_weighted: 0.3948693729748467
[2m[36m(func pid=88300)[0m f1_per_class: [0.344, 0.293, 0.759, 0.492, 0.113, 0.241, 0.436, 0.495, 0.189, 0.2]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.21361940298507462
[2m[36m(func pid=88716)[0m top5: 0.7565298507462687
[2m[36m(func pid=88716)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=88716)[0m f1_macro: 0.18598464524703082
[2m[36m(func pid=88716)[0m f1_weighted: 0.24666409402694037
[2m[36m(func pid=88716)[0m f1_per_class: [0.071, 0.156, 0.55, 0.371, 0.188, 0.12, 0.318, 0.0, 0.025, 0.061]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.34095149253731344
[2m[36m(func pid=87918)[0m top5: 0.8418843283582089
[2m[36m(func pid=87918)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=87918)[0m f1_macro: 0.3043009637859161
[2m[36m(func pid=87918)[0m f1_weighted: 0.3702598099755726
[2m[36m(func pid=87918)[0m f1_per_class: [0.329, 0.297, 0.421, 0.446, 0.071, 0.328, 0.392, 0.404, 0.156, 0.201]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.2332089552238806
[2m[36m(func pid=89134)[0m top5: 0.7602611940298507
[2m[36m(func pid=89134)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=89134)[0m f1_macro: 0.1317758836818063
[2m[36m(func pid=89134)[0m f1_weighted: 0.17142816280696746
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.436, 0.0, 0.053, 0.0, 0.015, 0.209, 0.225, 0.0, 0.379]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0032 | Steps: 2 | Val loss: 3.1137 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.9576 | Steps: 2 | Val loss: 17.6686 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0550 | Steps: 2 | Val loss: 1.8683 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.1063 | Steps: 2 | Val loss: 5.1564 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 04:16:33 (running for 00:06:48.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.195 |      0.304 |                   67 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.003 |      0.357 |                   69 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  1.725 |      0.186 |                   68 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.667 |      0.132 |                   67 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3675373134328358
[2m[36m(func pid=88300)[0m top5: 0.8885261194029851
[2m[36m(func pid=88300)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=88300)[0m f1_macro: 0.3566062533997503
[2m[36m(func pid=88300)[0m f1_weighted: 0.3880856776530427
[2m[36m(func pid=88300)[0m f1_per_class: [0.317, 0.294, 0.786, 0.484, 0.115, 0.24, 0.419, 0.497, 0.217, 0.198]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.20242537313432835
[2m[36m(func pid=88716)[0m top5: 0.7747201492537313
[2m[36m(func pid=88716)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=88716)[0m f1_macro: 0.21484378621008945
[2m[36m(func pid=88716)[0m f1_weighted: 0.244827151721014
[2m[36m(func pid=88716)[0m f1_per_class: [0.074, 0.217, 0.8, 0.302, 0.195, 0.152, 0.326, 0.0, 0.0, 0.083]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.341884328358209
[2m[36m(func pid=87918)[0m top5: 0.847481343283582
[2m[36m(func pid=87918)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=87918)[0m f1_macro: 0.3115150948112865
[2m[36m(func pid=87918)[0m f1_weighted: 0.369760392163514
[2m[36m(func pid=87918)[0m f1_per_class: [0.335, 0.305, 0.471, 0.445, 0.072, 0.339, 0.38, 0.396, 0.168, 0.205]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.24720149253731344
[2m[36m(func pid=89134)[0m top5: 0.7756529850746269
[2m[36m(func pid=89134)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=89134)[0m f1_macro: 0.2181423398912449
[2m[36m(func pid=89134)[0m f1_weighted: 0.20510190965260794
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.458, 0.609, 0.146, 0.109, 0.073, 0.186, 0.222, 0.0, 0.379]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0041 | Steps: 2 | Val loss: 3.1517 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4611 | Steps: 2 | Val loss: 20.0548 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0484 | Steps: 2 | Val loss: 1.8593 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.4411 | Steps: 2 | Val loss: 4.4727 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:16:38 (running for 00:06:53.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.055 |      0.312 |                   68 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.004 |      0.356 |                   70 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.958 |      0.215 |                   69 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.106 |      0.218 |                   68 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.36427238805970147
[2m[36m(func pid=88300)[0m top5: 0.8885261194029851
[2m[36m(func pid=88300)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=88300)[0m f1_macro: 0.35635172651544256
[2m[36m(func pid=88300)[0m f1_weighted: 0.3872335101905863
[2m[36m(func pid=88300)[0m f1_per_class: [0.306, 0.291, 0.786, 0.476, 0.114, 0.256, 0.418, 0.495, 0.233, 0.189]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.14225746268656717
[2m[36m(func pid=88716)[0m top5: 0.7840485074626866
[2m[36m(func pid=88716)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=88716)[0m f1_macro: 0.17869723772230287
[2m[36m(func pid=88716)[0m f1_weighted: 0.1711017610748743
[2m[36m(func pid=88716)[0m f1_per_class: [0.065, 0.243, 0.818, 0.06, 0.108, 0.115, 0.307, 0.0, 0.0, 0.07]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.34841417910447764
[2m[36m(func pid=87918)[0m top5: 0.8488805970149254
[2m[36m(func pid=87918)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=87918)[0m f1_macro: 0.3204228830224297
[2m[36m(func pid=87918)[0m f1_weighted: 0.37484782970427577
[2m[36m(func pid=87918)[0m f1_per_class: [0.329, 0.307, 0.5, 0.451, 0.08, 0.381, 0.372, 0.405, 0.165, 0.214]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.26026119402985076
[2m[36m(func pid=89134)[0m top5: 0.7835820895522388
[2m[36m(func pid=89134)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=89134)[0m f1_macro: 0.19696464960101046
[2m[36m(func pid=89134)[0m f1_weighted: 0.25065247672332686
[2m[36m(func pid=89134)[0m f1_per_class: [0.058, 0.501, 0.113, 0.247, 0.075, 0.147, 0.196, 0.214, 0.027, 0.391]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0050 | Steps: 2 | Val loss: 3.1998 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3881 | Steps: 2 | Val loss: 21.5802 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.0457 | Steps: 2 | Val loss: 1.8500 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0535 | Steps: 2 | Val loss: 4.1735 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:16:44 (running for 00:06:58.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.048 |      0.32  |                   69 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.005 |      0.353 |                   71 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.461 |      0.179 |                   70 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.441 |      0.197 |                   69 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.36007462686567165
[2m[36m(func pid=88300)[0m top5: 0.886660447761194
[2m[36m(func pid=88300)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=88300)[0m f1_macro: 0.3532228362574815
[2m[36m(func pid=88300)[0m f1_weighted: 0.3820499004948414
[2m[36m(func pid=88300)[0m f1_per_class: [0.297, 0.297, 0.786, 0.47, 0.109, 0.26, 0.404, 0.483, 0.236, 0.189]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.13386194029850745
[2m[36m(func pid=88716)[0m top5: 0.7770522388059702
[2m[36m(func pid=88716)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=88716)[0m f1_macro: 0.15671670066880156
[2m[36m(func pid=88716)[0m f1_weighted: 0.15314690412723708
[2m[36m(func pid=88716)[0m f1_per_class: [0.066, 0.294, 0.7, 0.028, 0.067, 0.088, 0.262, 0.0, 0.0, 0.063]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.34841417910447764
[2m[36m(func pid=87918)[0m top5: 0.8535447761194029
[2m[36m(func pid=87918)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=87918)[0m f1_macro: 0.32008929786887375
[2m[36m(func pid=87918)[0m f1_weighted: 0.37360092017377017
[2m[36m(func pid=87918)[0m f1_per_class: [0.335, 0.31, 0.5, 0.45, 0.07, 0.384, 0.365, 0.417, 0.161, 0.211]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.24720149253731344
[2m[36m(func pid=89134)[0m top5: 0.7952425373134329
[2m[36m(func pid=89134)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=89134)[0m f1_macro: 0.17236566548253926
[2m[36m(func pid=89134)[0m f1_weighted: 0.26714977030155435
[2m[36m(func pid=89134)[0m f1_per_class: [0.073, 0.363, 0.051, 0.341, 0.041, 0.22, 0.22, 0.244, 0.026, 0.145]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0055 | Steps: 2 | Val loss: 3.2519 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5438 | Steps: 2 | Val loss: 22.1524 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1018 | Steps: 2 | Val loss: 1.8479 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.1290 | Steps: 2 | Val loss: 3.9537 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:16:49 (running for 00:07:03.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.046 |      0.32  |                   70 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.006 |      0.35  |                   72 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.388 |      0.157 |                   71 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  2.054 |      0.172 |                   70 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3530783582089552
[2m[36m(func pid=88300)[0m top5: 0.8852611940298507
[2m[36m(func pid=88300)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=88300)[0m f1_macro: 0.3503734632453027
[2m[36m(func pid=88300)[0m f1_weighted: 0.3734575075726024
[2m[36m(func pid=88300)[0m f1_per_class: [0.294, 0.304, 0.786, 0.452, 0.117, 0.256, 0.39, 0.487, 0.231, 0.188]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.13199626865671643
[2m[36m(func pid=88716)[0m top5: 0.7560634328358209
[2m[36m(func pid=88716)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=88716)[0m f1_macro: 0.14802524627705965
[2m[36m(func pid=88716)[0m f1_weighted: 0.14420442816479578
[2m[36m(func pid=88716)[0m f1_per_class: [0.068, 0.342, 0.7, 0.034, 0.0, 0.071, 0.206, 0.0, 0.0, 0.059]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3512126865671642
[2m[36m(func pid=87918)[0m top5: 0.8558768656716418
[2m[36m(func pid=87918)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=87918)[0m f1_macro: 0.32126832894090596
[2m[36m(func pid=87918)[0m f1_weighted: 0.3768577976244189
[2m[36m(func pid=87918)[0m f1_per_class: [0.329, 0.316, 0.48, 0.451, 0.07, 0.382, 0.368, 0.434, 0.175, 0.208]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.26399253731343286
[2m[36m(func pid=89134)[0m top5: 0.8050373134328358
[2m[36m(func pid=89134)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=89134)[0m f1_macro: 0.1832946466494351
[2m[36m(func pid=89134)[0m f1_weighted: 0.27636205363025873
[2m[36m(func pid=89134)[0m f1_per_class: [0.077, 0.264, 0.056, 0.397, 0.086, 0.249, 0.234, 0.279, 0.049, 0.14]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0015 | Steps: 2 | Val loss: 3.2933 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4017 | Steps: 2 | Val loss: 17.3285 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9954 | Steps: 2 | Val loss: 1.8392 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2985 | Steps: 2 | Val loss: 4.3966 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:16:54 (running for 00:07:08.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.102 |      0.321 |                   71 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.002 |      0.346 |                   73 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.544 |      0.148 |                   72 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  3.129 |      0.183 |                   71 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.34701492537313433
[2m[36m(func pid=88300)[0m top5: 0.8815298507462687
[2m[36m(func pid=88300)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=88300)[0m f1_macro: 0.34593939587832806
[2m[36m(func pid=88300)[0m f1_weighted: 0.36861895572686
[2m[36m(func pid=88300)[0m f1_per_class: [0.278, 0.304, 0.786, 0.437, 0.117, 0.258, 0.389, 0.483, 0.229, 0.179]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.208955223880597
[2m[36m(func pid=88716)[0m top5: 0.7709888059701493
[2m[36m(func pid=88716)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=88716)[0m f1_macro: 0.12818300563457852
[2m[36m(func pid=88716)[0m f1_weighted: 0.23067184054146073
[2m[36m(func pid=88716)[0m f1_per_class: [0.074, 0.395, 0.0, 0.123, 0.0, 0.082, 0.363, 0.126, 0.028, 0.091]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.35027985074626866
[2m[36m(func pid=87918)[0m top5: 0.8582089552238806
[2m[36m(func pid=87918)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=87918)[0m f1_macro: 0.32252619203608984
[2m[36m(func pid=87918)[0m f1_weighted: 0.37709003062316376
[2m[36m(func pid=87918)[0m f1_per_class: [0.315, 0.318, 0.522, 0.44, 0.066, 0.376, 0.381, 0.433, 0.174, 0.201]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.25886194029850745
[2m[36m(func pid=89134)[0m top5: 0.8092350746268657
[2m[36m(func pid=89134)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=89134)[0m f1_macro: 0.17127373466728704
[2m[36m(func pid=89134)[0m f1_weighted: 0.2650649492996899
[2m[36m(func pid=89134)[0m f1_per_class: [0.038, 0.205, 0.06, 0.413, 0.08, 0.226, 0.226, 0.288, 0.043, 0.134]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0028 | Steps: 2 | Val loss: 3.3194 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2440 | Steps: 2 | Val loss: 15.8592 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.0127 | Steps: 2 | Val loss: 1.8254 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.3247 | Steps: 2 | Val loss: 4.8890 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:16:59 (running for 00:07:14.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING  | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  0.995 |      0.323 |                   72 |
| train_35a0b_00001 | RUNNING  | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.003 |      0.341 |                   74 |
| train_35a0b_00002 | RUNNING  | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.402 |      0.128 |                   73 |
| train_35a0b_00003 | RUNNING  | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.299 |      0.171 |                   72 |
| train_35a0b_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.341884328358209
[2m[36m(func pid=88300)[0m top5: 0.8819962686567164
[2m[36m(func pid=88300)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=88300)[0m f1_macro: 0.341132267369129
[2m[36m(func pid=88300)[0m f1_weighted: 0.3647278429932016
[2m[36m(func pid=88300)[0m f1_per_class: [0.261, 0.303, 0.786, 0.422, 0.123, 0.239, 0.401, 0.476, 0.226, 0.175]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=88716)[0m top1: 0.25093283582089554
[2m[36m(func pid=88716)[0m top5: 0.7583955223880597
[2m[36m(func pid=88716)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=88716)[0m f1_macro: 0.16427147572839684
[2m[36m(func pid=88716)[0m f1_weighted: 0.2727994508670215
[2m[36m(func pid=88716)[0m f1_per_class: [0.073, 0.404, 0.0, 0.235, 0.0, 0.062, 0.357, 0.338, 0.055, 0.119]
[2m[36m(func pid=88716)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3516791044776119
[2m[36m(func pid=87918)[0m top5: 0.8638059701492538
[2m[36m(func pid=87918)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=87918)[0m f1_macro: 0.32630299901076876
[2m[36m(func pid=87918)[0m f1_weighted: 0.3792538129334958
[2m[36m(func pid=87918)[0m f1_per_class: [0.315, 0.319, 0.545, 0.436, 0.057, 0.379, 0.387, 0.441, 0.175, 0.208]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.24486940298507462
[2m[36m(func pid=89134)[0m top5: 0.8120335820895522
[2m[36m(func pid=89134)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=89134)[0m f1_macro: 0.1626701322049607
[2m[36m(func pid=89134)[0m f1_weighted: 0.24302194103078661
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.238, 0.064, 0.426, 0.08, 0.21, 0.133, 0.262, 0.052, 0.162]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0240 | Steps: 2 | Val loss: 3.3458 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=88716)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8354 | Steps: 2 | Val loss: 14.1742 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9255 | Steps: 2 | Val loss: 1.8214 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5491 | Steps: 2 | Val loss: 4.4340 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=88300)[0m top1: 0.33861940298507465
[2m[36m(func pid=88300)[0m top5: 0.8824626865671642
[2m[36m(func pid=88300)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=88300)[0m f1_macro: 0.3394823492740529
[2m[36m(func pid=88300)[0m f1_weighted: 0.3625232917409623
[2m[36m(func pid=88300)[0m f1_per_class: [0.254, 0.297, 0.786, 0.41, 0.115, 0.236, 0.408, 0.481, 0.237, 0.171]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:17:05 (running for 00:07:19.56)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.30100000000000005
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 3 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918 | 0.0001 |       0.99 |         0      |  1.013 |      0.326 |                   73 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300 | 0.001  |       0.99 |         0      |  0.024 |      0.339 |                   75 |
| train_35a0b_00003 | RUNNING    | 192.168.7.53:89134 | 0.1    |       0.99 |         0      |  1.325 |      0.163 |                   73 |
| train_35a0b_00004 | PENDING    |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716 | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88716)[0m top1: 0.29524253731343286
[2m[36m(func pid=88716)[0m top5: 0.7574626865671642
[2m[36m(func pid=88716)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=88716)[0m f1_macro: 0.18681422275568177
[2m[36m(func pid=88716)[0m f1_weighted: 0.302081850852149
[2m[36m(func pid=88716)[0m f1_per_class: [0.116, 0.391, 0.0, 0.416, 0.0, 0.085, 0.274, 0.361, 0.078, 0.148]
[2m[36m(func pid=87918)[0m top1: 0.34888059701492535
[2m[36m(func pid=87918)[0m top5: 0.8642723880597015
[2m[36m(func pid=87918)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=87918)[0m f1_macro: 0.3297531111478761
[2m[36m(func pid=87918)[0m f1_weighted: 0.3728016901748154
[2m[36m(func pid=87918)[0m f1_per_class: [0.306, 0.324, 0.6, 0.432, 0.067, 0.375, 0.368, 0.444, 0.168, 0.214]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.25046641791044777
[2m[36m(func pid=89134)[0m top5: 0.8227611940298507
[2m[36m(func pid=89134)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=89134)[0m f1_macro: 0.1668500103579736
[2m[36m(func pid=89134)[0m f1_weighted: 0.24650901154725266
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.283, 0.058, 0.441, 0.093, 0.161, 0.119, 0.261, 0.076, 0.176]
[2m[36m(func pid=89134)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0098 | Steps: 2 | Val loss: 3.3707 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9020 | Steps: 2 | Val loss: 1.8129 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=89134)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2375 | Steps: 2 | Val loss: 3.9260 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=88300)[0m top1: 0.3306902985074627
[2m[36m(func pid=88300)[0m top5: 0.8810634328358209
[2m[36m(func pid=88300)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=88300)[0m f1_macro: 0.3359242498820352
[2m[36m(func pid=88300)[0m f1_weighted: 0.35606685958659384
[2m[36m(func pid=88300)[0m f1_per_class: [0.243, 0.294, 0.815, 0.406, 0.116, 0.254, 0.393, 0.455, 0.219, 0.167]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3572761194029851
[2m[36m(func pid=87918)[0m top5: 0.8684701492537313
[2m[36m(func pid=87918)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=87918)[0m f1_macro: 0.33515808149360626
[2m[36m(func pid=87918)[0m f1_weighted: 0.3830152486826443
[2m[36m(func pid=87918)[0m f1_per_class: [0.283, 0.331, 0.615, 0.444, 0.069, 0.384, 0.381, 0.455, 0.166, 0.222]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=89134)[0m top1: 0.29384328358208955
[2m[36m(func pid=89134)[0m top5: 0.8311567164179104
[2m[36m(func pid=89134)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=89134)[0m f1_macro: 0.1909226404925972
[2m[36m(func pid=89134)[0m f1_weighted: 0.29572105081303696
[2m[36m(func pid=89134)[0m f1_per_class: [0.0, 0.299, 0.104, 0.465, 0.03, 0.163, 0.243, 0.299, 0.083, 0.224]
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0083 | Steps: 2 | Val loss: 3.4190 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.9152 | Steps: 2 | Val loss: 1.8085 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:17:10 (running for 00:07:25.33)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.337
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.902 |      0.335 |                   75 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.01  |      0.336 |                   76 |
| train_35a0b_00003 | RUNNING    | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.549 |      0.167 |                   74 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=105422)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=105422)[0m Configuration completed!
[2m[36m(func pid=105422)[0m New optimizer parameters:
[2m[36m(func pid=105422)[0m SGD (
[2m[36m(func pid=105422)[0m Parameter Group 0
[2m[36m(func pid=105422)[0m     dampening: 0
[2m[36m(func pid=105422)[0m     differentiable: False
[2m[36m(func pid=105422)[0m     foreach: None
[2m[36m(func pid=105422)[0m     lr: 0.0001
[2m[36m(func pid=105422)[0m     maximize: False
[2m[36m(func pid=105422)[0m     momentum: 0.9
[2m[36m(func pid=105422)[0m     nesterov: False
[2m[36m(func pid=105422)[0m     weight_decay: 0
[2m[36m(func pid=105422)[0m )
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=88300)[0m top1: 0.32975746268656714
[2m[36m(func pid=88300)[0m top5: 0.8773320895522388
[2m[36m(func pid=88300)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=88300)[0m f1_macro: 0.3371562207927347
[2m[36m(func pid=88300)[0m f1_weighted: 0.35436074362038456
[2m[36m(func pid=88300)[0m f1_per_class: [0.242, 0.296, 0.815, 0.396, 0.117, 0.256, 0.391, 0.471, 0.222, 0.166]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.36100746268656714
[2m[36m(func pid=87918)[0m top5: 0.871268656716418
[2m[36m(func pid=87918)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=87918)[0m f1_macro: 0.340314048094538
[2m[36m(func pid=87918)[0m f1_weighted: 0.38941650726248755
[2m[36m(func pid=87918)[0m f1_per_class: [0.262, 0.334, 0.632, 0.443, 0.07, 0.379, 0.401, 0.463, 0.196, 0.224]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0031 | Steps: 2 | Val loss: 3.4575 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9866 | Steps: 2 | Val loss: 2.3180 | Batch size: 32 | lr: 0.0001 | Duration: 4.66s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.8555 | Steps: 2 | Val loss: 1.8000 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=88300)[0m top1: 0.32322761194029853
[2m[36m(func pid=88300)[0m top5: 0.8768656716417911
[2m[36m(func pid=88300)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=88300)[0m f1_macro: 0.3347231258780926
[2m[36m(func pid=88300)[0m f1_weighted: 0.34919205643354817
[2m[36m(func pid=88300)[0m f1_per_class: [0.232, 0.298, 0.815, 0.381, 0.107, 0.273, 0.38, 0.471, 0.224, 0.167]
[2m[36m(func pid=105422)[0m top1: 0.17630597014925373
[2m[36m(func pid=105422)[0m top5: 0.5354477611940298
[2m[36m(func pid=105422)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=105422)[0m f1_macro: 0.11935798833851678
[2m[36m(func pid=105422)[0m f1_weighted: 0.12630118451245248
[2m[36m(func pid=105422)[0m f1_per_class: [0.3, 0.347, 0.0, 0.092, 0.0, 0.213, 0.024, 0.014, 0.0, 0.203]
[2m[36m(func pid=87918)[0m top1: 0.36427238805970147
[2m[36m(func pid=87918)[0m top5: 0.8726679104477612
[2m[36m(func pid=87918)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=87918)[0m f1_macro: 0.3412948039636414
[2m[36m(func pid=87918)[0m f1_weighted: 0.3926305200324899
[2m[36m(func pid=87918)[0m f1_per_class: [0.264, 0.334, 0.632, 0.446, 0.07, 0.376, 0.41, 0.466, 0.184, 0.23]
== Status ==
Current time: 2024-01-07 04:17:16 (running for 00:07:30.54)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.915 |      0.34  |                   76 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.008 |      0.337 |                   77 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 04:17:22 (running for 00:07:36.42)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.915 |      0.34  |                   76 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.008 |      0.337 |                   77 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.987 |      0.119 |                    1 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=106010)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=106010)[0m Configuration completed!
[2m[36m(func pid=106010)[0m New optimizer parameters:
[2m[36m(func pid=106010)[0m SGD (
[2m[36m(func pid=106010)[0m Parameter Group 0
[2m[36m(func pid=106010)[0m     dampening: 0
[2m[36m(func pid=106010)[0m     differentiable: False
[2m[36m(func pid=106010)[0m     foreach: None
[2m[36m(func pid=106010)[0m     lr: 0.001
[2m[36m(func pid=106010)[0m     maximize: False
[2m[36m(func pid=106010)[0m     momentum: 0.9
[2m[36m(func pid=106010)[0m     nesterov: False
[2m[36m(func pid=106010)[0m     weight_decay: 0
[2m[36m(func pid=106010)[0m )
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.8925 | Steps: 2 | Val loss: 1.7933 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0099 | Steps: 2 | Val loss: 3.5025 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9935 | Steps: 2 | Val loss: 2.3208 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9929 | Steps: 2 | Val loss: 2.3266 | Batch size: 32 | lr: 0.001 | Duration: 4.77s
== Status ==
Current time: 2024-01-07 04:17:27 (running for 00:07:41.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.855 |      0.341 |                   77 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.003 |      0.335 |                   78 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.987 |      0.119 |                    1 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |        |            |                      |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.32322761194029853
[2m[36m(func pid=88300)[0m top5: 0.8759328358208955
[2m[36m(func pid=88300)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=88300)[0m f1_macro: 0.33732564091963557
[2m[36m(func pid=88300)[0m f1_weighted: 0.34937804205106504
[2m[36m(func pid=88300)[0m f1_per_class: [0.229, 0.297, 0.815, 0.378, 0.11, 0.286, 0.373, 0.506, 0.215, 0.164]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.36427238805970147
[2m[36m(func pid=87918)[0m top5: 0.8736007462686567
[2m[36m(func pid=87918)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=87918)[0m f1_macro: 0.3450711416888781
[2m[36m(func pid=87918)[0m f1_weighted: 0.3918795616440791
[2m[36m(func pid=87918)[0m f1_per_class: [0.276, 0.336, 0.667, 0.452, 0.068, 0.373, 0.401, 0.46, 0.19, 0.228]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.18190298507462688
[2m[36m(func pid=105422)[0m top5: 0.5321828358208955
[2m[36m(func pid=105422)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=105422)[0m f1_macro: 0.10942012637757523
[2m[36m(func pid=105422)[0m f1_weighted: 0.12707161027857997
[2m[36m(func pid=105422)[0m f1_per_class: [0.213, 0.333, 0.0, 0.091, 0.01, 0.28, 0.015, 0.036, 0.0, 0.115]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.17210820895522388
[2m[36m(func pid=106010)[0m top5: 0.5223880597014925
[2m[36m(func pid=106010)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=106010)[0m f1_macro: 0.12181113579284157
[2m[36m(func pid=106010)[0m f1_weighted: 0.12104509087784254
[2m[36m(func pid=106010)[0m f1_per_class: [0.324, 0.334, 0.0, 0.085, 0.0, 0.225, 0.012, 0.026, 0.0, 0.212]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0012 | Steps: 2 | Val loss: 3.5527 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.8352 | Steps: 2 | Val loss: 1.7945 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9356 | Steps: 2 | Val loss: 2.3283 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9404 | Steps: 2 | Val loss: 2.3457 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=88300)[0m top1: 0.31902985074626866
[2m[36m(func pid=88300)[0m top5: 0.8722014925373134
[2m[36m(func pid=88300)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=88300)[0m f1_macro: 0.32833798806192843
[2m[36m(func pid=88300)[0m f1_weighted: 0.3463853483034916
[2m[36m(func pid=88300)[0m f1_per_class: [0.224, 0.3, 0.759, 0.372, 0.113, 0.288, 0.372, 0.492, 0.207, 0.157]
== Status ==
Current time: 2024-01-07 04:17:32 (running for 00:07:46.88)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.893 |      0.345 |                   78 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.001 |      0.328 |                   80 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.993 |      0.109 |                    2 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.993 |      0.122 |                    1 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3628731343283582
[2m[36m(func pid=87918)[0m top5: 0.8740671641791045
[2m[36m(func pid=87918)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=87918)[0m f1_macro: 0.34567103645224223
[2m[36m(func pid=87918)[0m f1_weighted: 0.3895482215694922
[2m[36m(func pid=87918)[0m f1_per_class: [0.266, 0.346, 0.667, 0.44, 0.069, 0.383, 0.393, 0.471, 0.185, 0.236]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.17583955223880596
[2m[36m(func pid=105422)[0m top5: 0.5219216417910447
[2m[36m(func pid=105422)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=105422)[0m f1_macro: 0.09915749620573597
[2m[36m(func pid=105422)[0m f1_weighted: 0.12431140139618982
[2m[36m(func pid=105422)[0m f1_per_class: [0.15, 0.301, 0.0, 0.103, 0.01, 0.312, 0.009, 0.021, 0.0, 0.085]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.16138059701492538
[2m[36m(func pid=106010)[0m top5: 0.5027985074626866
[2m[36m(func pid=106010)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=106010)[0m f1_macro: 0.09799912298528565
[2m[36m(func pid=106010)[0m f1_weighted: 0.11369605312587217
[2m[36m(func pid=106010)[0m f1_per_class: [0.2, 0.283, 0.0, 0.088, 0.013, 0.268, 0.012, 0.019, 0.0, 0.098]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0048 | Steps: 2 | Val loss: 3.5853 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.8230 | Steps: 2 | Val loss: 1.7936 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9754 | Steps: 2 | Val loss: 2.3397 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9267 | Steps: 2 | Val loss: 2.3519 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 04:17:37 (running for 00:07:51.93)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.835 |      0.346 |                   79 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.005 |      0.328 |                   81 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.936 |      0.099 |                    3 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.94  |      0.098 |                    2 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.31763059701492535
[2m[36m(func pid=88300)[0m top5: 0.8731343283582089
[2m[36m(func pid=88300)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=88300)[0m f1_macro: 0.3275855840231308
[2m[36m(func pid=88300)[0m f1_weighted: 0.34613588207499446
[2m[36m(func pid=88300)[0m f1_per_class: [0.221, 0.304, 0.759, 0.359, 0.111, 0.306, 0.378, 0.472, 0.207, 0.158]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3628731343283582
[2m[36m(func pid=87918)[0m top5: 0.8759328358208955
[2m[36m(func pid=87918)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=87918)[0m f1_macro: 0.34733131430484215
[2m[36m(func pid=87918)[0m f1_weighted: 0.38994701124046516
[2m[36m(func pid=87918)[0m f1_per_class: [0.27, 0.348, 0.667, 0.441, 0.066, 0.396, 0.387, 0.47, 0.184, 0.243]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16791044776119404
[2m[36m(func pid=105422)[0m top5: 0.5097947761194029
[2m[36m(func pid=105422)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=105422)[0m f1_macro: 0.09587020115395167
[2m[36m(func pid=105422)[0m f1_weighted: 0.11932595861964738
[2m[36m(func pid=105422)[0m f1_per_class: [0.16, 0.282, 0.0, 0.097, 0.01, 0.306, 0.012, 0.019, 0.0, 0.074]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.16511194029850745
[2m[36m(func pid=106010)[0m top5: 0.4920708955223881
[2m[36m(func pid=106010)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=106010)[0m f1_macro: 0.10497212676474003
[2m[36m(func pid=106010)[0m f1_weighted: 0.11861277315647083
[2m[36m(func pid=106010)[0m f1_per_class: [0.214, 0.271, 0.049, 0.083, 0.011, 0.303, 0.024, 0.024, 0.0, 0.071]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0016 | Steps: 2 | Val loss: 3.5862 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9553 | Steps: 2 | Val loss: 2.3397 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.8772 | Steps: 2 | Val loss: 1.7868 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8388 | Steps: 2 | Val loss: 2.3094 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 04:17:42 (running for 00:07:57.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.823 |      0.347 |                   80 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.002 |      0.323 |                   82 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.975 |      0.096 |                    4 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.927 |      0.105 |                    3 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.31669776119402987
[2m[36m(func pid=88300)[0m top5: 0.8759328358208955
[2m[36m(func pid=88300)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=88300)[0m f1_macro: 0.323020544549021
[2m[36m(func pid=88300)[0m f1_weighted: 0.34626512068734516
[2m[36m(func pid=88300)[0m f1_per_class: [0.216, 0.304, 0.733, 0.354, 0.107, 0.302, 0.389, 0.443, 0.225, 0.157]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.36100746268656714
[2m[36m(func pid=87918)[0m top5: 0.8722014925373134
[2m[36m(func pid=87918)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=87918)[0m f1_macro: 0.3442520653657436
[2m[36m(func pid=87918)[0m f1_weighted: 0.3872978178209585
[2m[36m(func pid=87918)[0m f1_per_class: [0.281, 0.34, 0.667, 0.449, 0.066, 0.394, 0.379, 0.47, 0.167, 0.231]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.1623134328358209
[2m[36m(func pid=105422)[0m top5: 0.5144589552238806
[2m[36m(func pid=105422)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=105422)[0m f1_macro: 0.08973357262186459
[2m[36m(func pid=105422)[0m f1_weighted: 0.11961588542454635
[2m[36m(func pid=105422)[0m f1_per_class: [0.104, 0.269, 0.0, 0.101, 0.009, 0.301, 0.021, 0.027, 0.0, 0.066]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.20102611940298507
[2m[36m(func pid=106010)[0m top5: 0.5457089552238806
[2m[36m(func pid=106010)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=106010)[0m f1_macro: 0.1429967393419625
[2m[36m(func pid=106010)[0m f1_weighted: 0.15231442632460487
[2m[36m(func pid=106010)[0m f1_per_class: [0.248, 0.305, 0.222, 0.094, 0.008, 0.361, 0.077, 0.036, 0.0, 0.08]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0091 | Steps: 2 | Val loss: 3.6126 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9632 | Steps: 2 | Val loss: 2.3346 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.7633 | Steps: 2 | Val loss: 1.7875 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7967 | Steps: 2 | Val loss: 2.2985 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:17:48 (running for 00:08:02.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.877 |      0.344 |                   81 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.009 |      0.326 |                   83 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.955 |      0.09  |                    5 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.839 |      0.143 |                    4 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3260261194029851
[2m[36m(func pid=88300)[0m top5: 0.8745335820895522
[2m[36m(func pid=88300)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=88300)[0m f1_macro: 0.3260673872988311
[2m[36m(func pid=88300)[0m f1_weighted: 0.35709955451643616
[2m[36m(func pid=88300)[0m f1_per_class: [0.215, 0.309, 0.71, 0.359, 0.112, 0.303, 0.417, 0.451, 0.228, 0.159]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16511194029850745
[2m[36m(func pid=105422)[0m top5: 0.5139925373134329
[2m[36m(func pid=105422)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=105422)[0m f1_macro: 0.09808475657264018
[2m[36m(func pid=105422)[0m f1_weighted: 0.12696310644746744
[2m[36m(func pid=105422)[0m f1_per_class: [0.115, 0.267, 0.054, 0.101, 0.008, 0.311, 0.037, 0.053, 0.0, 0.034]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3619402985074627
[2m[36m(func pid=87918)[0m top5: 0.8731343283582089
[2m[36m(func pid=87918)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=87918)[0m f1_macro: 0.3460949400676382
[2m[36m(func pid=87918)[0m f1_weighted: 0.3888969956832379
[2m[36m(func pid=87918)[0m f1_per_class: [0.278, 0.341, 0.667, 0.445, 0.066, 0.4, 0.384, 0.474, 0.169, 0.238]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=106010)[0m top1: 0.21455223880597016
[2m[36m(func pid=106010)[0m top5: 0.5629664179104478
[2m[36m(func pid=106010)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=106010)[0m f1_macro: 0.1661587096022014
[2m[36m(func pid=106010)[0m f1_weighted: 0.17868158201475554
[2m[36m(func pid=106010)[0m f1_per_class: [0.291, 0.312, 0.262, 0.119, 0.0, 0.395, 0.11, 0.09, 0.0, 0.082]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0077 | Steps: 2 | Val loss: 3.6857 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.8251 | Steps: 2 | Val loss: 1.7857 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9487 | Steps: 2 | Val loss: 2.3318 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7234 | Steps: 2 | Val loss: 2.2797 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 04:17:53 (running for 00:08:07.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.763 |      0.346 |                   82 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.008 |      0.321 |                   84 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.963 |      0.098 |                    6 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.797 |      0.166 |                    5 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.322294776119403
[2m[36m(func pid=88300)[0m top5: 0.8745335820895522
[2m[36m(func pid=88300)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=88300)[0m f1_macro: 0.3210106321937003
[2m[36m(func pid=88300)[0m f1_weighted: 0.3519739680960009
[2m[36m(func pid=88300)[0m f1_per_class: [0.221, 0.317, 0.71, 0.355, 0.096, 0.304, 0.405, 0.418, 0.22, 0.164]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3605410447761194
[2m[36m(func pid=87918)[0m top5: 0.8754664179104478
[2m[36m(func pid=87918)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=87918)[0m f1_macro: 0.34337265428813446
[2m[36m(func pid=87918)[0m f1_weighted: 0.38711279717694513
[2m[36m(func pid=87918)[0m f1_per_class: [0.286, 0.343, 0.649, 0.441, 0.068, 0.387, 0.388, 0.456, 0.179, 0.238]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.1609141791044776
[2m[36m(func pid=105422)[0m top5: 0.5191231343283582
[2m[36m(func pid=105422)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=105422)[0m f1_macro: 0.09333040000745778
[2m[36m(func pid=105422)[0m f1_weighted: 0.12986802709466713
[2m[36m(func pid=105422)[0m f1_per_class: [0.087, 0.245, 0.049, 0.103, 0.008, 0.315, 0.056, 0.071, 0.0, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.22014925373134328
[2m[36m(func pid=106010)[0m top5: 0.5946828358208955
[2m[36m(func pid=106010)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=106010)[0m f1_macro: 0.17898413463873455
[2m[36m(func pid=106010)[0m f1_weighted: 0.19751843966852323
[2m[36m(func pid=106010)[0m f1_per_class: [0.221, 0.313, 0.257, 0.124, 0.012, 0.407, 0.152, 0.168, 0.0, 0.136]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0010 | Steps: 2 | Val loss: 3.6868 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.9024 | Steps: 2 | Val loss: 1.7776 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9180 | Steps: 2 | Val loss: 2.3238 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6607 | Steps: 2 | Val loss: 2.2553 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=88300)[0m top1: 0.3246268656716418
[2m[36m(func pid=88300)[0m top5: 0.8759328358208955
[2m[36m(func pid=88300)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=88300)[0m f1_macro: 0.3220187549020076
[2m[36m(func pid=88300)[0m f1_weighted: 0.3547292007191665
[2m[36m(func pid=88300)[0m f1_per_class: [0.234, 0.313, 0.71, 0.355, 0.096, 0.31, 0.416, 0.412, 0.211, 0.164]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:17:58 (running for 00:08:13.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.825 |      0.343 |                   83 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.001 |      0.322 |                   85 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.949 |      0.093 |                    7 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.723 |      0.179 |                    6 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m top1: 0.16511194029850745
[2m[36m(func pid=105422)[0m top5: 0.5284514925373134
[2m[36m(func pid=105422)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=105422)[0m f1_macro: 0.0976934285118133
[2m[36m(func pid=105422)[0m f1_weighted: 0.13654020139198608
[2m[36m(func pid=105422)[0m f1_per_class: [0.076, 0.246, 0.045, 0.103, 0.015, 0.326, 0.071, 0.081, 0.014, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3614738805970149
[2m[36m(func pid=87918)[0m top5: 0.8759328358208955
[2m[36m(func pid=87918)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=87918)[0m f1_macro: 0.34501655702244516
[2m[36m(func pid=87918)[0m f1_weighted: 0.38581239793586813
[2m[36m(func pid=87918)[0m f1_per_class: [0.305, 0.339, 0.649, 0.446, 0.069, 0.388, 0.379, 0.453, 0.182, 0.241]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=106010)[0m top1: 0.2196828358208955
[2m[36m(func pid=106010)[0m top5: 0.6063432835820896
[2m[36m(func pid=106010)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=106010)[0m f1_macro: 0.18554555356623847
[2m[36m(func pid=106010)[0m f1_weighted: 0.2074572355523785
[2m[36m(func pid=106010)[0m f1_per_class: [0.192, 0.312, 0.289, 0.13, 0.039, 0.396, 0.186, 0.157, 0.0, 0.154]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0170 | Steps: 2 | Val loss: 3.7345 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.7278 | Steps: 2 | Val loss: 1.7818 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8935 | Steps: 2 | Val loss: 2.3167 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6141 | Steps: 2 | Val loss: 2.2270 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 04:18:03 (running for 00:08:18.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.902 |      0.345 |                   84 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.017 |      0.32  |                   86 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.918 |      0.098 |                    8 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.661 |      0.186 |                    7 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.32322761194029853
[2m[36m(func pid=88300)[0m top5: 0.8726679104477612
[2m[36m(func pid=88300)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=88300)[0m f1_macro: 0.3204673539921378
[2m[36m(func pid=88300)[0m f1_weighted: 0.35199565819111334
[2m[36m(func pid=88300)[0m f1_per_class: [0.233, 0.326, 0.733, 0.351, 0.097, 0.323, 0.407, 0.364, 0.207, 0.164]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3591417910447761
[2m[36m(func pid=87918)[0m top5: 0.8768656716417911
[2m[36m(func pid=87918)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=87918)[0m f1_macro: 0.34400589805699744
[2m[36m(func pid=87918)[0m f1_weighted: 0.3843266882547101
[2m[36m(func pid=87918)[0m f1_per_class: [0.306, 0.34, 0.649, 0.445, 0.067, 0.39, 0.374, 0.453, 0.178, 0.238]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16651119402985073
[2m[36m(func pid=105422)[0m top5: 0.5317164179104478
[2m[36m(func pid=105422)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=105422)[0m f1_macro: 0.09889390506712727
[2m[36m(func pid=105422)[0m f1_weighted: 0.14105913929419492
[2m[36m(func pid=105422)[0m f1_per_class: [0.055, 0.246, 0.049, 0.11, 0.02, 0.327, 0.079, 0.088, 0.014, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.2178171641791045
[2m[36m(func pid=106010)[0m top5: 0.644589552238806
[2m[36m(func pid=106010)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=106010)[0m f1_macro: 0.18582534463813016
[2m[36m(func pid=106010)[0m f1_weighted: 0.2147871334393045
[2m[36m(func pid=106010)[0m f1_per_class: [0.19, 0.315, 0.31, 0.136, 0.017, 0.376, 0.21, 0.162, 0.0, 0.143]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0070 | Steps: 2 | Val loss: 3.7494 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.6704 | Steps: 2 | Val loss: 1.7822 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9060 | Steps: 2 | Val loss: 2.3121 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5095 | Steps: 2 | Val loss: 2.1990 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 04:18:09 (running for 00:08:23.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.728 |      0.344 |                   85 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.007 |      0.323 |                   87 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.894 |      0.099 |                    9 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.614 |      0.186 |                    8 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.32276119402985076
[2m[36m(func pid=88300)[0m top5: 0.8731343283582089
[2m[36m(func pid=88300)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=88300)[0m f1_macro: 0.3226684953670417
[2m[36m(func pid=88300)[0m f1_weighted: 0.35202596585742246
[2m[36m(func pid=88300)[0m f1_per_class: [0.231, 0.319, 0.774, 0.345, 0.095, 0.325, 0.419, 0.341, 0.213, 0.165]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.36240671641791045
[2m[36m(func pid=87918)[0m top5: 0.8810634328358209
[2m[36m(func pid=87918)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=87918)[0m f1_macro: 0.34778457766152826
[2m[36m(func pid=87918)[0m f1_weighted: 0.3889912363331821
[2m[36m(func pid=87918)[0m f1_per_class: [0.315, 0.335, 0.649, 0.448, 0.065, 0.396, 0.386, 0.453, 0.181, 0.25]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16744402985074627
[2m[36m(func pid=105422)[0m top5: 0.5387126865671642
[2m[36m(func pid=105422)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=105422)[0m f1_macro: 0.09943572467671119
[2m[36m(func pid=105422)[0m f1_weighted: 0.14295331087354152
[2m[36m(func pid=105422)[0m f1_per_class: [0.053, 0.253, 0.044, 0.114, 0.019, 0.333, 0.075, 0.089, 0.014, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.22201492537313433
[2m[36m(func pid=106010)[0m top5: 0.7005597014925373
[2m[36m(func pid=106010)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=106010)[0m f1_macro: 0.19620190359281078
[2m[36m(func pid=106010)[0m f1_weighted: 0.2235494298279398
[2m[36m(func pid=106010)[0m f1_per_class: [0.185, 0.309, 0.343, 0.156, 0.018, 0.379, 0.217, 0.179, 0.017, 0.159]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0043 | Steps: 2 | Val loss: 3.7203 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.6204 | Steps: 2 | Val loss: 1.7810 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8624 | Steps: 2 | Val loss: 2.3069 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4521 | Steps: 2 | Val loss: 2.1737 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:18:14 (running for 00:08:29.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.67  |      0.348 |                   86 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.004 |      0.321 |                   88 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.906 |      0.099 |                   10 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.51  |      0.196 |                    9 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.32649253731343286
[2m[36m(func pid=88300)[0m top5: 0.8773320895522388
[2m[36m(func pid=88300)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=88300)[0m f1_macro: 0.32124133884518624
[2m[36m(func pid=88300)[0m f1_weighted: 0.3564674065144305
[2m[36m(func pid=88300)[0m f1_per_class: [0.23, 0.323, 0.774, 0.35, 0.085, 0.32, 0.433, 0.318, 0.217, 0.162]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3619402985074627
[2m[36m(func pid=87918)[0m top5: 0.8815298507462687
[2m[36m(func pid=87918)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=87918)[0m f1_macro: 0.34735217673083196
[2m[36m(func pid=87918)[0m f1_weighted: 0.3889034341733263
[2m[36m(func pid=87918)[0m f1_per_class: [0.308, 0.339, 0.667, 0.44, 0.066, 0.403, 0.39, 0.45, 0.179, 0.232]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16837686567164178
[2m[36m(func pid=105422)[0m top5: 0.5410447761194029
[2m[36m(func pid=105422)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=105422)[0m f1_macro: 0.10937289977566086
[2m[36m(func pid=105422)[0m f1_weighted: 0.14713489442656286
[2m[36m(func pid=105422)[0m f1_per_class: [0.055, 0.254, 0.128, 0.119, 0.023, 0.327, 0.085, 0.089, 0.015, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.23227611940298507
[2m[36m(func pid=106010)[0m top5: 0.7397388059701493
[2m[36m(func pid=106010)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=106010)[0m f1_macro: 0.21129872542410996
[2m[36m(func pid=106010)[0m f1_weighted: 0.23880013678384884
[2m[36m(func pid=106010)[0m f1_per_class: [0.211, 0.308, 0.387, 0.184, 0.019, 0.37, 0.24, 0.192, 0.016, 0.187]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0065 | Steps: 2 | Val loss: 3.7252 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.6898 | Steps: 2 | Val loss: 1.7832 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8984 | Steps: 2 | Val loss: 2.3022 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3657 | Steps: 2 | Val loss: 2.1552 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:18:20 (running for 00:08:34.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.62  |      0.347 |                   87 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.323 |                   89 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.862 |      0.109 |                   11 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.452 |      0.211 |                   10 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3260261194029851
[2m[36m(func pid=88300)[0m top5: 0.8796641791044776
[2m[36m(func pid=88300)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=88300)[0m f1_macro: 0.3234987857734438
[2m[36m(func pid=88300)[0m f1_weighted: 0.35614837035758995
[2m[36m(func pid=88300)[0m f1_per_class: [0.226, 0.322, 0.8, 0.346, 0.09, 0.312, 0.438, 0.331, 0.213, 0.157]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3614738805970149
[2m[36m(func pid=87918)[0m top5: 0.8810634328358209
[2m[36m(func pid=87918)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=87918)[0m f1_macro: 0.3437222298687681
[2m[36m(func pid=87918)[0m f1_weighted: 0.38707948916337226
[2m[36m(func pid=87918)[0m f1_per_class: [0.32, 0.339, 0.615, 0.445, 0.065, 0.393, 0.381, 0.457, 0.185, 0.235]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=106010)[0m top1: 0.23647388059701493
[2m[36m(func pid=106010)[0m top5: 0.7527985074626866
[2m[36m(func pid=106010)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=106010)[0m f1_macro: 0.22057895575024541
[2m[36m(func pid=106010)[0m f1_weighted: 0.2474410306826448
[2m[36m(func pid=106010)[0m f1_per_class: [0.214, 0.293, 0.369, 0.206, 0.025, 0.372, 0.249, 0.209, 0.029, 0.238]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16744402985074627
[2m[36m(func pid=105422)[0m top5: 0.5461753731343284
[2m[36m(func pid=105422)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=105422)[0m f1_macro: 0.11600463609793436
[2m[36m(func pid=105422)[0m f1_weighted: 0.14752081969371367
[2m[36m(func pid=105422)[0m f1_per_class: [0.054, 0.252, 0.196, 0.12, 0.022, 0.331, 0.084, 0.087, 0.015, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0422 | Steps: 2 | Val loss: 3.6453 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.9069 | Steps: 2 | Val loss: 1.7983 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2860 | Steps: 2 | Val loss: 2.1378 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8873 | Steps: 2 | Val loss: 2.2961 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 04:18:25 (running for 00:08:39.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.69  |      0.344 |                   88 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.042 |      0.334 |                   90 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.898 |      0.116 |                   12 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.366 |      0.221 |                   11 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.3381529850746269
[2m[36m(func pid=88300)[0m top5: 0.8843283582089553
[2m[36m(func pid=88300)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=88300)[0m f1_macro: 0.33396520516227424
[2m[36m(func pid=88300)[0m f1_weighted: 0.3700448806670492
[2m[36m(func pid=88300)[0m f1_per_class: [0.229, 0.318, 0.8, 0.35, 0.094, 0.307, 0.471, 0.394, 0.221, 0.156]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.3596082089552239
[2m[36m(func pid=87918)[0m top5: 0.8796641791044776
[2m[36m(func pid=87918)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=87918)[0m f1_macro: 0.3417650475479873
[2m[36m(func pid=87918)[0m f1_weighted: 0.3843466324041291
[2m[36m(func pid=87918)[0m f1_per_class: [0.317, 0.351, 0.6, 0.433, 0.066, 0.389, 0.377, 0.464, 0.188, 0.234]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=106010)[0m top1: 0.25279850746268656
[2m[36m(func pid=106010)[0m top5: 0.7672574626865671
[2m[36m(func pid=106010)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=106010)[0m f1_macro: 0.23241204054278333
[2m[36m(func pid=106010)[0m f1_weighted: 0.2674040167232809
[2m[36m(func pid=106010)[0m f1_per_class: [0.195, 0.303, 0.381, 0.251, 0.049, 0.384, 0.26, 0.233, 0.031, 0.238]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16744402985074627
[2m[36m(func pid=105422)[0m top5: 0.5527052238805971
[2m[36m(func pid=105422)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=105422)[0m f1_macro: 0.11639313032710523
[2m[36m(func pid=105422)[0m f1_weighted: 0.15079072931931067
[2m[36m(func pid=105422)[0m f1_per_class: [0.05, 0.251, 0.185, 0.124, 0.02, 0.336, 0.088, 0.094, 0.015, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0020 | Steps: 2 | Val loss: 3.5722 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.5800 | Steps: 2 | Val loss: 1.7939 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.1689 | Steps: 2 | Val loss: 2.1232 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8653 | Steps: 2 | Val loss: 2.2912 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=88300)[0m top1: 0.345615671641791
[2m[36m(func pid=88300)[0m top5: 0.8903917910447762
[2m[36m(func pid=88300)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=88300)[0m f1_macro: 0.34155074536261965
[2m[36m(func pid=88300)[0m f1_weighted: 0.37640066729217003
[2m[36m(func pid=88300)[0m f1_per_class: [0.245, 0.331, 0.828, 0.368, 0.089, 0.285, 0.469, 0.429, 0.209, 0.162]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:18:30 (running for 00:08:45.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.907 |      0.342 |                   89 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.002 |      0.342 |                   91 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.887 |      0.116 |                   13 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.286 |      0.232 |                   12 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.36007462686567165
[2m[36m(func pid=87918)[0m top5: 0.8810634328358209
[2m[36m(func pid=87918)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=87918)[0m f1_macro: 0.3420152341820335
[2m[36m(func pid=87918)[0m f1_weighted: 0.3848592137132128
[2m[36m(func pid=87918)[0m f1_per_class: [0.322, 0.348, 0.6, 0.436, 0.066, 0.389, 0.378, 0.46, 0.188, 0.234]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=106010)[0m top1: 0.2583955223880597
[2m[36m(func pid=106010)[0m top5: 0.7719216417910447
[2m[36m(func pid=106010)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=106010)[0m f1_macro: 0.2459086558523967
[2m[36m(func pid=106010)[0m f1_weighted: 0.27276893043395906
[2m[36m(func pid=106010)[0m f1_per_class: [0.205, 0.307, 0.393, 0.228, 0.046, 0.385, 0.283, 0.27, 0.067, 0.275]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.166044776119403
[2m[36m(func pid=105422)[0m top5: 0.5634328358208955
[2m[36m(func pid=105422)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.11576649868416897
[2m[36m(func pid=105422)[0m f1_weighted: 0.14873100073333415
[2m[36m(func pid=105422)[0m f1_per_class: [0.048, 0.245, 0.192, 0.121, 0.016, 0.344, 0.086, 0.093, 0.014, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.7074 | Steps: 2 | Val loss: 1.7978 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0007 | Steps: 2 | Val loss: 3.5314 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2260 | Steps: 2 | Val loss: 2.1172 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8401 | Steps: 2 | Val loss: 2.2897 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:18:36 (running for 00:08:50.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.707 |      0.344 |                   91 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.002 |      0.342 |                   91 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.865 |      0.116 |                   14 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.169 |      0.246 |                   13 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.3619402985074627
[2m[36m(func pid=87918)[0m top5: 0.8805970149253731
[2m[36m(func pid=87918)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=87918)[0m f1_macro: 0.34371136417063397
[2m[36m(func pid=87918)[0m f1_weighted: 0.3867641464511311
[2m[36m(func pid=87918)[0m f1_per_class: [0.323, 0.348, 0.6, 0.436, 0.066, 0.391, 0.381, 0.467, 0.188, 0.235]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.34701492537313433
[2m[36m(func pid=88300)[0m top5: 0.8903917910447762
[2m[36m(func pid=88300)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=88300)[0m f1_macro: 0.3399223020992056
[2m[36m(func pid=88300)[0m f1_weighted: 0.374564740878882
[2m[36m(func pid=88300)[0m f1_per_class: [0.244, 0.329, 0.828, 0.374, 0.091, 0.225, 0.475, 0.455, 0.214, 0.163]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16791044776119404
[2m[36m(func pid=105422)[0m top5: 0.5680970149253731
[2m[36m(func pid=105422)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=105422)[0m f1_macro: 0.11771601373294063
[2m[36m(func pid=105422)[0m f1_weighted: 0.15311076092149495
[2m[36m(func pid=105422)[0m f1_per_class: [0.047, 0.251, 0.214, 0.139, 0.01, 0.341, 0.083, 0.078, 0.014, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.25046641791044777
[2m[36m(func pid=106010)[0m top5: 0.7728544776119403
[2m[36m(func pid=106010)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=106010)[0m f1_macro: 0.23630667746337153
[2m[36m(func pid=106010)[0m f1_weighted: 0.26414304926764115
[2m[36m(func pid=106010)[0m f1_per_class: [0.214, 0.296, 0.358, 0.215, 0.037, 0.369, 0.28, 0.274, 0.059, 0.261]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.6015 | Steps: 2 | Val loss: 1.7989 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0005 | Steps: 2 | Val loss: 3.5413 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7933 | Steps: 2 | Val loss: 2.2851 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0964 | Steps: 2 | Val loss: 2.1177 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:18:41 (running for 00:08:55.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.601 |      0.342 |                   92 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.001 |      0.34  |                   92 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.84  |      0.118 |                   15 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.226 |      0.236 |                   14 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.36240671641791045
[2m[36m(func pid=87918)[0m top5: 0.8819962686567164
[2m[36m(func pid=87918)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=87918)[0m f1_macro: 0.3421613368805923
[2m[36m(func pid=87918)[0m f1_weighted: 0.38820566617066987
[2m[36m(func pid=87918)[0m f1_per_class: [0.315, 0.351, 0.585, 0.436, 0.066, 0.382, 0.388, 0.469, 0.199, 0.231]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.34281716417910446
[2m[36m(func pid=88300)[0m top5: 0.8894589552238806
[2m[36m(func pid=88300)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=88300)[0m f1_macro: 0.339115594862024
[2m[36m(func pid=88300)[0m f1_weighted: 0.36773660021521115
[2m[36m(func pid=88300)[0m f1_per_class: [0.25, 0.327, 0.857, 0.366, 0.092, 0.208, 0.47, 0.442, 0.22, 0.16]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.166044776119403
[2m[36m(func pid=105422)[0m top5: 0.5718283582089553
[2m[36m(func pid=105422)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.1204366595444516
[2m[36m(func pid=105422)[0m f1_weighted: 0.15324452929269583
[2m[36m(func pid=105422)[0m f1_per_class: [0.046, 0.24, 0.241, 0.143, 0.01, 0.34, 0.085, 0.085, 0.014, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.24207089552238806
[2m[36m(func pid=106010)[0m top5: 0.7723880597014925
[2m[36m(func pid=106010)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=106010)[0m f1_macro: 0.23894686601392934
[2m[36m(func pid=106010)[0m f1_weighted: 0.2567379271368812
[2m[36m(func pid=106010)[0m f1_per_class: [0.223, 0.281, 0.407, 0.194, 0.033, 0.351, 0.283, 0.301, 0.062, 0.254]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0026 | Steps: 2 | Val loss: 3.5922 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.5164 | Steps: 2 | Val loss: 1.7937 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8162 | Steps: 2 | Val loss: 2.2822 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.0396 | Steps: 2 | Val loss: 2.1083 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:18:46 (running for 00:09:00.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.601 |      0.342 |                   92 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.003 |      0.339 |                   94 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.793 |      0.12  |                   16 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.096 |      0.239 |                   15 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=88300)[0m top1: 0.34328358208955223
[2m[36m(func pid=88300)[0m top5: 0.8880597014925373
[2m[36m(func pid=88300)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=88300)[0m f1_macro: 0.3389928593330473
[2m[36m(func pid=88300)[0m f1_weighted: 0.3666181798704928
[2m[36m(func pid=88300)[0m f1_per_class: [0.251, 0.345, 0.857, 0.363, 0.092, 0.198, 0.463, 0.433, 0.226, 0.162]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=87918)[0m top1: 0.36380597014925375
[2m[36m(func pid=87918)[0m top5: 0.8838619402985075
[2m[36m(func pid=87918)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=87918)[0m f1_macro: 0.34469475227824387
[2m[36m(func pid=87918)[0m f1_weighted: 0.3904856866817354
[2m[36m(func pid=87918)[0m f1_per_class: [0.323, 0.35, 0.6, 0.435, 0.066, 0.377, 0.398, 0.472, 0.191, 0.235]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.166044776119403
[2m[36m(func pid=105422)[0m top5: 0.5802238805970149
[2m[36m(func pid=105422)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.12064893785745431
[2m[36m(func pid=105422)[0m f1_weighted: 0.15723364603805745
[2m[36m(func pid=105422)[0m f1_per_class: [0.044, 0.237, 0.233, 0.153, 0.01, 0.332, 0.093, 0.09, 0.013, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.2462686567164179
[2m[36m(func pid=106010)[0m top5: 0.7700559701492538
[2m[36m(func pid=106010)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=106010)[0m f1_macro: 0.24408969149901355
[2m[36m(func pid=106010)[0m f1_weighted: 0.2611615855181637
[2m[36m(func pid=106010)[0m f1_per_class: [0.238, 0.282, 0.393, 0.198, 0.033, 0.365, 0.28, 0.329, 0.08, 0.241]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0187 | Steps: 2 | Val loss: 3.6164 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.5583 | Steps: 2 | Val loss: 1.7926 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7987 | Steps: 2 | Val loss: 2.2795 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.0226 | Steps: 2 | Val loss: 2.0996 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=88300)[0m top1: 0.3423507462686567
[2m[36m(func pid=88300)[0m top5: 0.8880597014925373
[2m[36m(func pid=88300)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=88300)[0m f1_macro: 0.33996073089716716
[2m[36m(func pid=88300)[0m f1_weighted: 0.3642235242825786
[2m[36m(func pid=88300)[0m f1_per_class: [0.253, 0.347, 0.857, 0.359, 0.095, 0.181, 0.46, 0.44, 0.246, 0.162]
[2m[36m(func pid=88300)[0m 
== Status ==
Current time: 2024-01-07 04:18:51 (running for 00:09:05.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.516 |      0.345 |                   93 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.019 |      0.34  |                   95 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.816 |      0.121 |                   17 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.04  |      0.244 |                   16 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.3666044776119403
[2m[36m(func pid=87918)[0m top5: 0.8838619402985075
[2m[36m(func pid=87918)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=87918)[0m f1_macro: 0.3486612994537853
[2m[36m(func pid=87918)[0m f1_weighted: 0.39520551015200583
[2m[36m(func pid=87918)[0m f1_per_class: [0.317, 0.348, 0.632, 0.444, 0.065, 0.374, 0.408, 0.471, 0.194, 0.235]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=105422)[0m top1: 0.1669776119402985
[2m[36m(func pid=105422)[0m top5: 0.5872201492537313
[2m[36m(func pid=105422)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=105422)[0m f1_macro: 0.12183446596754717
[2m[36m(func pid=105422)[0m f1_weighted: 0.16065888056198244
[2m[36m(func pid=105422)[0m f1_per_class: [0.043, 0.233, 0.23, 0.165, 0.01, 0.336, 0.093, 0.096, 0.013, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.25513059701492535
[2m[36m(func pid=106010)[0m top5: 0.7719216417910447
[2m[36m(func pid=106010)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=106010)[0m f1_macro: 0.24819556775989327
[2m[36m(func pid=106010)[0m f1_weighted: 0.26815794976732993
[2m[36m(func pid=106010)[0m f1_per_class: [0.251, 0.295, 0.353, 0.214, 0.029, 0.394, 0.267, 0.348, 0.085, 0.246]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0005 | Steps: 2 | Val loss: 3.6624 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.5389 | Steps: 2 | Val loss: 1.7874 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8127 | Steps: 2 | Val loss: 2.2761 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8982 | Steps: 2 | Val loss: 2.0719 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:18:56 (running for 00:09:10.93)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.558 |      0.349 |                   94 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0     |      0.34  |                   96 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.799 |      0.122 |                   18 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  2.023 |      0.248 |                   17 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.36613805970149255
[2m[36m(func pid=87918)[0m top5: 0.8847947761194029
[2m[36m(func pid=87918)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=87918)[0m f1_macro: 0.3447743429651716
[2m[36m(func pid=87918)[0m f1_weighted: 0.39547034461636876
[2m[36m(func pid=87918)[0m f1_per_class: [0.31, 0.349, 0.6, 0.442, 0.063, 0.376, 0.411, 0.465, 0.192, 0.238]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.3423507462686567
[2m[36m(func pid=88300)[0m top5: 0.8880597014925373
[2m[36m(func pid=88300)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=88300)[0m f1_macro: 0.33968225596460255
[2m[36m(func pid=88300)[0m f1_weighted: 0.36293037009622997
[2m[36m(func pid=88300)[0m f1_per_class: [0.255, 0.348, 0.857, 0.342, 0.092, 0.166, 0.475, 0.444, 0.255, 0.164]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16557835820895522
[2m[36m(func pid=105422)[0m top5: 0.5923507462686567
[2m[36m(func pid=105422)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=105422)[0m f1_macro: 0.12353217242380357
[2m[36m(func pid=105422)[0m f1_weighted: 0.16013299481068208
[2m[36m(func pid=105422)[0m f1_per_class: [0.068, 0.233, 0.23, 0.166, 0.019, 0.324, 0.095, 0.088, 0.013, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.27052238805970147
[2m[36m(func pid=106010)[0m top5: 0.784981343283582
[2m[36m(func pid=106010)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=106010)[0m f1_macro: 0.25487251064278793
[2m[36m(func pid=106010)[0m f1_weighted: 0.2805035614578798
[2m[36m(func pid=106010)[0m f1_per_class: [0.224, 0.327, 0.387, 0.25, 0.036, 0.397, 0.254, 0.37, 0.061, 0.243]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4705 | Steps: 2 | Val loss: 1.7844 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0018 | Steps: 2 | Val loss: 3.6521 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8201 | Steps: 2 | Val loss: 2.2729 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.9818 | Steps: 2 | Val loss: 2.0481 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:19:01 (running for 00:09:16.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.471 |      0.345 |                   96 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0     |      0.34  |                   96 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.813 |      0.124 |                   19 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.898 |      0.255 |                   18 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.3689365671641791
[2m[36m(func pid=87918)[0m top5: 0.8875932835820896
[2m[36m(func pid=87918)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=87918)[0m f1_macro: 0.3451754718552845
[2m[36m(func pid=87918)[0m f1_weighted: 0.3988707030294687
[2m[36m(func pid=87918)[0m f1_per_class: [0.312, 0.348, 0.585, 0.451, 0.064, 0.372, 0.416, 0.467, 0.198, 0.24]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.34794776119402987
[2m[36m(func pid=88300)[0m top5: 0.8875932835820896
[2m[36m(func pid=88300)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=88300)[0m f1_macro: 0.3422385264467368
[2m[36m(func pid=88300)[0m f1_weighted: 0.36821938898427725
[2m[36m(func pid=88300)[0m f1_per_class: [0.256, 0.35, 0.857, 0.344, 0.099, 0.162, 0.491, 0.444, 0.255, 0.164]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.166044776119403
[2m[36m(func pid=105422)[0m top5: 0.5951492537313433
[2m[36m(func pid=105422)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.12456838832206758
[2m[36m(func pid=105422)[0m f1_weighted: 0.16272414270502175
[2m[36m(func pid=105422)[0m f1_per_class: [0.08, 0.233, 0.222, 0.171, 0.019, 0.317, 0.1, 0.09, 0.013, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.28404850746268656
[2m[36m(func pid=106010)[0m top5: 0.800839552238806
[2m[36m(func pid=106010)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=106010)[0m f1_macro: 0.2805309778616064
[2m[36m(func pid=106010)[0m f1_weighted: 0.29781281948362137
[2m[36m(func pid=106010)[0m f1_per_class: [0.196, 0.334, 0.558, 0.3, 0.04, 0.41, 0.249, 0.379, 0.096, 0.243]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.5050 | Steps: 2 | Val loss: 1.7882 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0154 | Steps: 2 | Val loss: 3.6804 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8036 | Steps: 2 | Val loss: 2.2689 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7406 | Steps: 2 | Val loss: 2.0369 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:19:07 (running for 00:09:21.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.505 |      0.348 |                   97 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.002 |      0.342 |                   97 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.82  |      0.125 |                   20 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.982 |      0.281 |                   19 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.36800373134328357
[2m[36m(func pid=87918)[0m top5: 0.8852611940298507
[2m[36m(func pid=87918)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=87918)[0m f1_macro: 0.3475900678418674
[2m[36m(func pid=87918)[0m f1_weighted: 0.3971127831133649
[2m[36m(func pid=87918)[0m f1_per_class: [0.308, 0.35, 0.615, 0.446, 0.066, 0.376, 0.411, 0.472, 0.19, 0.242]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.34701492537313433
[2m[36m(func pid=88300)[0m top5: 0.8894589552238806
[2m[36m(func pid=88300)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=88300)[0m f1_macro: 0.33834275377389644
[2m[36m(func pid=88300)[0m f1_weighted: 0.3690682637765673
[2m[36m(func pid=88300)[0m f1_per_class: [0.258, 0.348, 0.857, 0.347, 0.091, 0.157, 0.499, 0.438, 0.224, 0.165]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16324626865671643
[2m[36m(func pid=105422)[0m top5: 0.6086753731343284
[2m[36m(func pid=105422)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=105422)[0m f1_macro: 0.12386678571251568
[2m[36m(func pid=105422)[0m f1_weighted: 0.16281829618720303
[2m[36m(func pid=105422)[0m f1_per_class: [0.08, 0.221, 0.215, 0.17, 0.019, 0.319, 0.107, 0.096, 0.012, 0.0]
[2m[36m(func pid=106010)[0m top1: 0.2849813432835821
[2m[36m(func pid=106010)[0m top5: 0.8059701492537313
[2m[36m(func pid=106010)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=106010)[0m f1_macro: 0.278439793228644
[2m[36m(func pid=106010)[0m f1_weighted: 0.2988926675551549
[2m[36m(func pid=106010)[0m f1_per_class: [0.219, 0.33, 0.533, 0.303, 0.038, 0.396, 0.257, 0.383, 0.078, 0.247]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.5337 | Steps: 2 | Val loss: 1.7906 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0019 | Steps: 2 | Val loss: 3.7052 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7892 | Steps: 2 | Val loss: 2.2637 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6360 | Steps: 2 | Val loss: 2.0372 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:19:12 (running for 00:09:26.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.534 |      0.346 |                   98 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.015 |      0.338 |                   98 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.804 |      0.124 |                   21 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.741 |      0.278 |                   20 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.3694029850746269
[2m[36m(func pid=87918)[0m top5: 0.8861940298507462
[2m[36m(func pid=87918)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=87918)[0m f1_macro: 0.34613847828612004
[2m[36m(func pid=87918)[0m f1_weighted: 0.3972435298725876
[2m[36m(func pid=87918)[0m f1_per_class: [0.304, 0.353, 0.585, 0.45, 0.067, 0.38, 0.404, 0.471, 0.205, 0.242]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.34468283582089554
[2m[36m(func pid=88300)[0m top5: 0.8861940298507462
[2m[36m(func pid=88300)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=88300)[0m f1_macro: 0.33474216016156216
[2m[36m(func pid=88300)[0m f1_weighted: 0.36595487043238023
[2m[36m(func pid=88300)[0m f1_per_class: [0.262, 0.35, 0.857, 0.329, 0.081, 0.139, 0.512, 0.433, 0.23, 0.155]
[2m[36m(func pid=88300)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16651119402985073
[2m[36m(func pid=105422)[0m top5: 0.6142723880597015
[2m[36m(func pid=105422)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=105422)[0m f1_macro: 0.12718261018162463
[2m[36m(func pid=105422)[0m f1_weighted: 0.1671158541422622
[2m[36m(func pid=105422)[0m f1_per_class: [0.083, 0.22, 0.225, 0.179, 0.023, 0.322, 0.112, 0.095, 0.012, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.279384328358209
[2m[36m(func pid=106010)[0m top5: 0.8083022388059702
[2m[36m(func pid=106010)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=106010)[0m f1_macro: 0.2731731353534844
[2m[36m(func pid=106010)[0m f1_weighted: 0.2984777311209354
[2m[36m(func pid=106010)[0m f1_per_class: [0.252, 0.313, 0.471, 0.283, 0.033, 0.386, 0.289, 0.359, 0.111, 0.235]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.4737 | Steps: 2 | Val loss: 1.7875 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=88300)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0062 | Steps: 2 | Val loss: 3.6659 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7659 | Steps: 2 | Val loss: 2.2608 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.6235 | Steps: 2 | Val loss: 2.0377 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:19:17 (running for 00:09:31.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | RUNNING    | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.474 |      0.347 |                   99 |
| train_35a0b_00001 | RUNNING    | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.002 |      0.335 |                   99 |
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.789 |      0.127 |                   22 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.636 |      0.273 |                   21 |
| train_35a0b_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=87918)[0m top1: 0.37080223880597013
[2m[36m(func pid=87918)[0m top5: 0.8875932835820896
[2m[36m(func pid=87918)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=87918)[0m f1_macro: 0.34742632044135763
[2m[36m(func pid=87918)[0m f1_weighted: 0.3991709020804803
[2m[36m(func pid=87918)[0m f1_per_class: [0.301, 0.351, 0.585, 0.45, 0.068, 0.381, 0.409, 0.485, 0.205, 0.24]
[2m[36m(func pid=87918)[0m 
[2m[36m(func pid=88300)[0m top1: 0.345615671641791
[2m[36m(func pid=88300)[0m top5: 0.8885261194029851
[2m[36m(func pid=88300)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=88300)[0m f1_macro: 0.33237766001573055
[2m[36m(func pid=88300)[0m f1_weighted: 0.3684355951899943
[2m[36m(func pid=88300)[0m f1_per_class: [0.255, 0.343, 0.857, 0.335, 0.091, 0.145, 0.522, 0.405, 0.229, 0.142]
[2m[36m(func pid=105422)[0m top1: 0.16744402985074627
[2m[36m(func pid=105422)[0m top5: 0.6180037313432836
[2m[36m(func pid=105422)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=105422)[0m f1_macro: 0.12769521075582507
[2m[36m(func pid=105422)[0m f1_weighted: 0.16902810653191233
[2m[36m(func pid=105422)[0m f1_per_class: [0.083, 0.223, 0.216, 0.175, 0.023, 0.324, 0.119, 0.103, 0.011, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.2775186567164179
[2m[36m(func pid=106010)[0m top5: 0.8101679104477612
[2m[36m(func pid=106010)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=106010)[0m f1_macro: 0.26764749022510126
[2m[36m(func pid=106010)[0m f1_weighted: 0.3021756149594192
[2m[36m(func pid=106010)[0m f1_per_class: [0.279, 0.304, 0.453, 0.285, 0.031, 0.337, 0.327, 0.333, 0.106, 0.22]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=87918)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.5443 | Steps: 2 | Val loss: 1.7877 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7328 | Steps: 2 | Val loss: 2.2560 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.6462 | Steps: 2 | Val loss: 2.0223 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=87918)[0m top1: 0.3712686567164179
[2m[36m(func pid=87918)[0m top5: 0.8880597014925373
[2m[36m(func pid=87918)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=87918)[0m f1_macro: 0.3496236932173836
[2m[36m(func pid=87918)[0m f1_weighted: 0.40008719913618074
[2m[36m(func pid=87918)[0m f1_per_class: [0.301, 0.347, 0.6, 0.456, 0.066, 0.382, 0.409, 0.481, 0.207, 0.248]
[2m[36m(func pid=106010)[0m top1: 0.28591417910447764
[2m[36m(func pid=106010)[0m top5: 0.8176305970149254
[2m[36m(func pid=106010)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=106010)[0m f1_macro: 0.2775960267124384
[2m[36m(func pid=106010)[0m f1_weighted: 0.3121515290045327
[2m[36m(func pid=106010)[0m f1_per_class: [0.29, 0.305, 0.5, 0.309, 0.031, 0.357, 0.333, 0.313, 0.108, 0.23]
[2m[36m(func pid=105422)[0m top1: 0.16791044776119404
[2m[36m(func pid=105422)[0m top5: 0.6291977611940298
[2m[36m(func pid=105422)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=105422)[0m f1_macro: 0.12994246158880135
[2m[36m(func pid=105422)[0m f1_weighted: 0.17177999359419988
[2m[36m(func pid=105422)[0m f1_per_class: [0.081, 0.217, 0.219, 0.178, 0.023, 0.322, 0.125, 0.123, 0.011, 0.0]
== Status ==
Current time: 2024-01-07 04:19:22 (running for 00:09:37.17)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 3 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.766 |      0.128 |                   23 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.624 |      0.268 |                   22 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111263)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=111263)[0m Configuration completed!
[2m[36m(func pid=111263)[0m New optimizer parameters:
[2m[36m(func pid=111263)[0m SGD (
[2m[36m(func pid=111263)[0m Parameter Group 0
[2m[36m(func pid=111263)[0m     dampening: 0
[2m[36m(func pid=111263)[0m     differentiable: False
[2m[36m(func pid=111263)[0m     foreach: None
[2m[36m(func pid=111263)[0m     lr: 0.01
[2m[36m(func pid=111263)[0m     maximize: False
[2m[36m(func pid=111263)[0m     momentum: 0.9
[2m[36m(func pid=111263)[0m     nesterov: False
[2m[36m(func pid=111263)[0m     weight_decay: 0
[2m[36m(func pid=111263)[0m )
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7277 | Steps: 2 | Val loss: 2.2518 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4705 | Steps: 2 | Val loss: 1.9949 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9894 | Steps: 2 | Val loss: 2.3763 | Batch size: 32 | lr: 0.01 | Duration: 4.88s
[2m[36m(func pid=106010)[0m top1: 0.2989738805970149
[2m[36m(func pid=106010)[0m top5: 0.824160447761194
[2m[36m(func pid=106010)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=106010)[0m f1_macro: 0.2962172554997986
[2m[36m(func pid=106010)[0m f1_weighted: 0.3245246743158248
[2m[36m(func pid=106010)[0m f1_per_class: [0.279, 0.313, 0.545, 0.317, 0.033, 0.394, 0.333, 0.371, 0.137, 0.241]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.16977611940298507
[2m[36m(func pid=105422)[0m top5: 0.6338619402985075
[2m[36m(func pid=105422)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=105422)[0m f1_macro: 0.13315883823759883
[2m[36m(func pid=105422)[0m f1_weighted: 0.17273892740709146
[2m[36m(func pid=105422)[0m f1_per_class: [0.082, 0.222, 0.232, 0.176, 0.023, 0.326, 0.123, 0.136, 0.012, 0.0]
[2m[36m(func pid=111263)[0m top1: 0.14505597014925373
[2m[36m(func pid=111263)[0m top5: 0.47947761194029853
[2m[36m(func pid=111263)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=111263)[0m f1_macro: 0.08936918123837831
[2m[36m(func pid=111263)[0m f1_weighted: 0.09698188034208831
[2m[36m(func pid=111263)[0m f1_per_class: [0.162, 0.26, 0.025, 0.066, 0.0, 0.229, 0.0, 0.051, 0.0, 0.101]
== Status ==
Current time: 2024-01-07 04:19:31 (running for 00:09:45.36)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.733 |      0.13  |                   24 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.47  |      0.296 |                   24 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |        |            |                      |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111842)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=111842)[0m Configuration completed!
[2m[36m(func pid=111842)[0m New optimizer parameters:
[2m[36m(func pid=111842)[0m SGD (
[2m[36m(func pid=111842)[0m Parameter Group 0
[2m[36m(func pid=111842)[0m     dampening: 0
[2m[36m(func pid=111842)[0m     differentiable: False
[2m[36m(func pid=111842)[0m     foreach: None
[2m[36m(func pid=111842)[0m     lr: 0.1
[2m[36m(func pid=111842)[0m     maximize: False
[2m[36m(func pid=111842)[0m     momentum: 0.9
[2m[36m(func pid=111842)[0m     nesterov: False
[2m[36m(func pid=111842)[0m     weight_decay: 0
[2m[36m(func pid=111842)[0m )
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5383 | Steps: 2 | Val loss: 1.9779 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6902 | Steps: 2 | Val loss: 2.2475 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 04:19:36 (running for 00:09:51.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.728 |      0.133 |                   25 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.538 |      0.301 |                   25 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  2.989 |      0.089 |                    1 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |        |            |                      |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3064365671641791
[2m[36m(func pid=106010)[0m top5: 0.8274253731343284
[2m[36m(func pid=106010)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=106010)[0m f1_macro: 0.30091675443876287
[2m[36m(func pid=106010)[0m f1_weighted: 0.3304721721376235
[2m[36m(func pid=106010)[0m f1_per_class: [0.268, 0.323, 0.545, 0.326, 0.036, 0.405, 0.329, 0.402, 0.134, 0.241]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7586 | Steps: 2 | Val loss: 2.3448 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9090 | Steps: 2 | Val loss: 2.3926 | Batch size: 32 | lr: 0.1 | Duration: 4.79s
[2m[36m(func pid=105422)[0m top1: 0.17257462686567165
[2m[36m(func pid=105422)[0m top5: 0.6417910447761194
[2m[36m(func pid=105422)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=105422)[0m f1_macro: 0.13578359180842653
[2m[36m(func pid=105422)[0m f1_weighted: 0.17730967487070534
[2m[36m(func pid=105422)[0m f1_per_class: [0.084, 0.221, 0.235, 0.18, 0.023, 0.334, 0.132, 0.137, 0.011, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.15951492537313433
[2m[36m(func pid=111263)[0m top5: 0.47994402985074625
[2m[36m(func pid=111263)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=111263)[0m f1_macro: 0.11558948975131951
[2m[36m(func pid=111263)[0m f1_weighted: 0.1161629675056342
[2m[36m(func pid=111263)[0m f1_per_class: [0.134, 0.268, 0.122, 0.072, 0.0, 0.321, 0.0, 0.151, 0.0, 0.088]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.4126 | Steps: 2 | Val loss: 1.9630 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=111842)[0m top1: 0.025186567164179104
[2m[36m(func pid=111842)[0m top5: 0.5471082089552238
[2m[36m(func pid=111842)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=111842)[0m f1_macro: 0.022875450624417308
[2m[36m(func pid=111842)[0m f1_weighted: 0.012484997231888301
[2m[36m(func pid=111842)[0m f1_per_class: [0.059, 0.025, 0.015, 0.007, 0.0, 0.0, 0.012, 0.0, 0.0, 0.111]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6891 | Steps: 2 | Val loss: 2.2431 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 04:19:41 (running for 00:09:56.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.69  |      0.136 |                   26 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.413 |      0.307 |                   26 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  2.759 |      0.116 |                    2 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.909 |      0.023 |                    1 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.30736940298507465
[2m[36m(func pid=106010)[0m top5: 0.8302238805970149
[2m[36m(func pid=106010)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=106010)[0m f1_macro: 0.30697184235631475
[2m[36m(func pid=106010)[0m f1_weighted: 0.3308028703564817
[2m[36m(func pid=106010)[0m f1_per_class: [0.27, 0.308, 0.558, 0.336, 0.035, 0.412, 0.317, 0.437, 0.149, 0.247]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4648 | Steps: 2 | Val loss: 2.3163 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9414 | Steps: 2 | Val loss: 2.6864 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=105422)[0m top1: 0.1767723880597015
[2m[36m(func pid=105422)[0m top5: 0.6478544776119403
[2m[36m(func pid=105422)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=105422)[0m f1_macro: 0.13830889210096978
[2m[36m(func pid=105422)[0m f1_weighted: 0.18204505626964512
[2m[36m(func pid=105422)[0m f1_per_class: [0.084, 0.223, 0.229, 0.185, 0.024, 0.341, 0.136, 0.149, 0.012, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.15065298507462688
[2m[36m(func pid=111263)[0m top5: 0.5368470149253731
[2m[36m(func pid=111263)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=111263)[0m f1_macro: 0.11869745305613424
[2m[36m(func pid=111263)[0m f1_weighted: 0.1347005584017241
[2m[36m(func pid=111263)[0m f1_per_class: [0.13, 0.22, 0.063, 0.141, 0.07, 0.396, 0.015, 0.051, 0.0, 0.1]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3826 | Steps: 2 | Val loss: 1.9432 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=111842)[0m top1: 0.024253731343283583
[2m[36m(func pid=111842)[0m top5: 0.5690298507462687
[2m[36m(func pid=111842)[0m f1_micro: 0.024253731343283583
[2m[36m(func pid=111842)[0m f1_macro: 0.02506647006016577
[2m[36m(func pid=111842)[0m f1_weighted: 0.03473786782744641
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.0, 0.006, 0.0, 0.042, 0.103, 0.07, 0.03, 0.0, 0.0]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6872 | Steps: 2 | Val loss: 2.2348 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 04:19:47 (running for 00:10:01.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.689 |      0.138 |                   27 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.383 |      0.315 |                   27 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  2.465 |      0.119 |                    3 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.941 |      0.025 |                    2 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.31669776119402987
[2m[36m(func pid=106010)[0m top5: 0.8372201492537313
[2m[36m(func pid=106010)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=106010)[0m f1_macro: 0.3151904104035178
[2m[36m(func pid=106010)[0m f1_weighted: 0.34242556843625843
[2m[36m(func pid=106010)[0m f1_per_class: [0.28, 0.308, 0.6, 0.342, 0.036, 0.409, 0.351, 0.436, 0.151, 0.238]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.1101 | Steps: 2 | Val loss: 2.2419 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.5955 | Steps: 2 | Val loss: 2.5455 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=105422)[0m top1: 0.18563432835820895
[2m[36m(func pid=105422)[0m top5: 0.65625
[2m[36m(func pid=105422)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=105422)[0m f1_macro: 0.14654505223618391
[2m[36m(func pid=105422)[0m f1_weighted: 0.19057205516479767
[2m[36m(func pid=105422)[0m f1_per_class: [0.092, 0.235, 0.269, 0.204, 0.024, 0.352, 0.137, 0.141, 0.012, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.3926 | Steps: 2 | Val loss: 1.9204 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=111263)[0m top1: 0.17257462686567165
[2m[36m(func pid=111263)[0m top5: 0.6077425373134329
[2m[36m(func pid=111263)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=111263)[0m f1_macro: 0.13023596975540463
[2m[36m(func pid=111263)[0m f1_weighted: 0.144212006562532
[2m[36m(func pid=111263)[0m f1_per_class: [0.11, 0.194, 0.122, 0.197, 0.122, 0.384, 0.021, 0.0, 0.0, 0.152]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.10727611940298508
[2m[36m(func pid=111842)[0m top5: 0.5499067164179104
[2m[36m(func pid=111842)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=111842)[0m f1_macro: 0.07571214216959071
[2m[36m(func pid=111842)[0m f1_weighted: 0.12529788594536223
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.061, 0.0, 0.126, 0.1, 0.084, 0.205, 0.109, 0.048, 0.025]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:19:52 (running for 00:10:06.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.687 |      0.147 |                   28 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.393 |      0.325 |                   28 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  2.11  |      0.13  |                    4 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.596 |      0.076 |                    3 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6780 | Steps: 2 | Val loss: 2.2326 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=106010)[0m top1: 0.3316231343283582
[2m[36m(func pid=106010)[0m top5: 0.8442164179104478
[2m[36m(func pid=106010)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=106010)[0m f1_macro: 0.32524901855093274
[2m[36m(func pid=106010)[0m f1_weighted: 0.3578810681071484
[2m[36m(func pid=106010)[0m f1_per_class: [0.267, 0.319, 0.615, 0.375, 0.038, 0.431, 0.354, 0.444, 0.171, 0.238]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.7767 | Steps: 2 | Val loss: 2.0647 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.5630 | Steps: 2 | Val loss: 2.9138 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=105422)[0m top1: 0.18796641791044777
[2m[36m(func pid=105422)[0m top5: 0.6623134328358209
[2m[36m(func pid=105422)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=105422)[0m f1_macro: 0.15012726429298112
[2m[36m(func pid=105422)[0m f1_weighted: 0.1943085374376558
[2m[36m(func pid=105422)[0m f1_per_class: [0.102, 0.24, 0.273, 0.205, 0.025, 0.35, 0.143, 0.152, 0.011, 0.0]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.2880 | Steps: 2 | Val loss: 1.8994 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=111263)[0m top1: 0.2593283582089552
[2m[36m(func pid=111263)[0m top5: 0.7523320895522388
[2m[36m(func pid=111263)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=111263)[0m f1_macro: 0.21154654228934203
[2m[36m(func pid=111263)[0m f1_weighted: 0.2216081820243034
[2m[36m(func pid=111263)[0m f1_per_class: [0.176, 0.188, 0.393, 0.363, 0.112, 0.396, 0.097, 0.047, 0.026, 0.319]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.10074626865671642
[2m[36m(func pid=111842)[0m top5: 0.5097947761194029
[2m[36m(func pid=111842)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=111842)[0m f1_macro: 0.08004810387065156
[2m[36m(func pid=111842)[0m f1_weighted: 0.12263932154868247
[2m[36m(func pid=111842)[0m f1_per_class: [0.036, 0.041, 0.006, 0.128, 0.0, 0.008, 0.2, 0.264, 0.087, 0.031]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:19:57 (running for 00:10:12.07)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.678 |      0.15  |                   29 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.288 |      0.332 |                   29 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  1.777 |      0.212 |                    5 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.563 |      0.08  |                    4 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6614 | Steps: 2 | Val loss: 2.2293 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=106010)[0m top1: 0.33908582089552236
[2m[36m(func pid=106010)[0m top5: 0.851679104477612
[2m[36m(func pid=106010)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=106010)[0m f1_macro: 0.3316712223380912
[2m[36m(func pid=106010)[0m f1_weighted: 0.36586603082648916
[2m[36m(func pid=106010)[0m f1_per_class: [0.287, 0.321, 0.632, 0.397, 0.037, 0.434, 0.357, 0.447, 0.164, 0.242]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4442 | Steps: 2 | Val loss: 1.9013 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8116 | Steps: 2 | Val loss: 7.5079 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=105422)[0m top1: 0.18983208955223882
[2m[36m(func pid=105422)[0m top5: 0.6646455223880597
[2m[36m(func pid=105422)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=105422)[0m f1_macro: 0.15258140668009762
[2m[36m(func pid=105422)[0m f1_weighted: 0.19692360112362048
[2m[36m(func pid=105422)[0m f1_per_class: [0.107, 0.253, 0.254, 0.2, 0.024, 0.346, 0.149, 0.156, 0.013, 0.025]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3306902985074627
[2m[36m(func pid=111263)[0m top5: 0.8418843283582089
[2m[36m(func pid=111263)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=111263)[0m f1_macro: 0.2728264885454319
[2m[36m(func pid=111263)[0m f1_weighted: 0.3202366287120979
[2m[36m(func pid=111263)[0m f1_per_class: [0.192, 0.114, 0.533, 0.471, 0.163, 0.388, 0.315, 0.353, 0.0, 0.2]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.2238 | Steps: 2 | Val loss: 1.8838 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.08348880597014925
[2m[36m(func pid=111842)[0m top5: 0.4141791044776119
[2m[36m(func pid=111842)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=111842)[0m f1_macro: 0.06830344537869978
[2m[36m(func pid=111842)[0m f1_weighted: 0.05265189835851931
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.0, 0.017, 0.0, 0.035, 0.438, 0.0, 0.0, 0.0, 0.194]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:02 (running for 00:10:17.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.661 |      0.153 |                   30 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.224 |      0.335 |                   30 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  1.444 |      0.273 |                    6 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.812 |      0.068 |                    5 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.33955223880597013
[2m[36m(func pid=106010)[0m top5: 0.8572761194029851
[2m[36m(func pid=106010)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=106010)[0m f1_macro: 0.33506396720426423
[2m[36m(func pid=106010)[0m f1_weighted: 0.36649309822556736
[2m[36m(func pid=106010)[0m f1_per_class: [0.266, 0.331, 0.686, 0.4, 0.038, 0.432, 0.352, 0.445, 0.156, 0.245]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6777 | Steps: 2 | Val loss: 2.2261 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.2538 | Steps: 2 | Val loss: 1.8174 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.5964 | Steps: 2 | Val loss: 11.5335 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=105422)[0m top1: 0.1935634328358209
[2m[36m(func pid=105422)[0m top5: 0.6665111940298507
[2m[36m(func pid=105422)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=105422)[0m f1_macro: 0.15471690981109051
[2m[36m(func pid=105422)[0m f1_weighted: 0.20148605954906068
[2m[36m(func pid=105422)[0m f1_per_class: [0.107, 0.27, 0.269, 0.197, 0.024, 0.334, 0.164, 0.145, 0.013, 0.024]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.1524 | Steps: 2 | Val loss: 1.8689 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=111263)[0m top1: 0.3516791044776119
[2m[36m(func pid=111263)[0m top5: 0.8591417910447762
[2m[36m(func pid=111263)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=111263)[0m f1_macro: 0.31034636651089276
[2m[36m(func pid=111263)[0m f1_weighted: 0.3232195621938389
[2m[36m(func pid=111263)[0m f1_per_class: [0.336, 0.185, 0.649, 0.5, 0.126, 0.385, 0.233, 0.392, 0.024, 0.273]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.01166044776119403
[2m[36m(func pid=111842)[0m top5: 0.4137126865671642
[2m[36m(func pid=111842)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=111842)[0m f1_macro: 0.02051532238140361
[2m[36m(func pid=111842)[0m f1_weighted: 0.004022507866999732
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.037, 0.0, 0.154]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:07 (running for 00:10:22.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.678 |      0.155 |                   31 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.152 |      0.344 |                   31 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  1.254 |      0.31  |                    7 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.596 |      0.021 |                    6 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35027985074626866
[2m[36m(func pid=106010)[0m top5: 0.8642723880597015
[2m[36m(func pid=106010)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=106010)[0m f1_macro: 0.34433736883595567
[2m[36m(func pid=106010)[0m f1_weighted: 0.37922602675041905
[2m[36m(func pid=106010)[0m f1_per_class: [0.282, 0.334, 0.686, 0.421, 0.037, 0.425, 0.37, 0.46, 0.165, 0.262]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6884 | Steps: 2 | Val loss: 2.2252 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.0768 | Steps: 2 | Val loss: 1.8554 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5908 | Steps: 2 | Val loss: 25.8831 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.1449 | Steps: 2 | Val loss: 1.8609 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=105422)[0m top1: 0.19169776119402984
[2m[36m(func pid=105422)[0m top5: 0.6674440298507462
[2m[36m(func pid=105422)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=105422)[0m f1_macro: 0.1570594532676864
[2m[36m(func pid=105422)[0m f1_weighted: 0.19886535330461105
[2m[36m(func pid=105422)[0m f1_per_class: [0.11, 0.272, 0.286, 0.187, 0.023, 0.334, 0.164, 0.13, 0.014, 0.05]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3069029850746269
[2m[36m(func pid=111263)[0m top5: 0.8577425373134329
[2m[36m(func pid=111263)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=111263)[0m f1_macro: 0.314009351956103
[2m[36m(func pid=111263)[0m f1_weighted: 0.2807434839370614
[2m[36m(func pid=111263)[0m f1_per_class: [0.529, 0.248, 0.75, 0.455, 0.097, 0.277, 0.13, 0.36, 0.025, 0.27]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.00792910447761194
[2m[36m(func pid=111842)[0m top5: 0.43050373134328357
[2m[36m(func pid=111842)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.003322974297053613
[2m[36m(func pid=111842)[0m f1_weighted: 0.003678863825007426
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.021, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:13 (running for 00:10:27.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.688 |      0.157 |                   32 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.145 |      0.343 |                   32 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  1.077 |      0.314 |                    8 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.591 |      0.003 |                    7 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3493470149253731
[2m[36m(func pid=106010)[0m top5: 0.8684701492537313
[2m[36m(func pid=106010)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=106010)[0m f1_macro: 0.3433139047667104
[2m[36m(func pid=106010)[0m f1_weighted: 0.3782938030890529
[2m[36m(func pid=106010)[0m f1_per_class: [0.256, 0.325, 0.686, 0.419, 0.045, 0.426, 0.374, 0.471, 0.163, 0.27]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6189 | Steps: 2 | Val loss: 2.2237 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7307 | Steps: 2 | Val loss: 1.6869 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3753 | Steps: 2 | Val loss: 48.7820 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=105422)[0m top1: 0.19402985074626866
[2m[36m(func pid=105422)[0m top5: 0.6702425373134329
[2m[36m(func pid=105422)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=105422)[0m f1_macro: 0.15992963533368104
[2m[36m(func pid=105422)[0m f1_weighted: 0.20142279726256743
[2m[36m(func pid=105422)[0m f1_per_class: [0.131, 0.273, 0.303, 0.177, 0.023, 0.333, 0.18, 0.14, 0.014, 0.026]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.1204 | Steps: 2 | Val loss: 1.8528 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=111263)[0m top1: 0.3829291044776119
[2m[36m(func pid=111263)[0m top5: 0.8894589552238806
[2m[36m(func pid=111263)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=111263)[0m f1_macro: 0.338280795665784
[2m[36m(func pid=111263)[0m f1_weighted: 0.3803728878227617
[2m[36m(func pid=111263)[0m f1_per_class: [0.384, 0.225, 0.649, 0.53, 0.133, 0.318, 0.382, 0.418, 0.125, 0.219]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.008395522388059701
[2m[36m(func pid=111842)[0m top5: 0.5419776119402985
[2m[36m(func pid=111842)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=111842)[0m f1_macro: 0.005372889015119351
[2m[36m(func pid=111842)[0m f1_weighted: 0.0014131741468922563
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.005, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:18 (running for 00:10:32.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.619 |      0.16  |                   33 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.12  |      0.347 |                   33 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.731 |      0.338 |                    9 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.375 |      0.005 |                    8 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35261194029850745
[2m[36m(func pid=106010)[0m top5: 0.8722014925373134
[2m[36m(func pid=106010)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=106010)[0m f1_macro: 0.34724168226959806
[2m[36m(func pid=106010)[0m f1_weighted: 0.3803467735449747
[2m[36m(func pid=106010)[0m f1_per_class: [0.305, 0.34, 0.686, 0.43, 0.049, 0.406, 0.368, 0.458, 0.17, 0.26]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6148 | Steps: 2 | Val loss: 2.2229 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5843 | Steps: 2 | Val loss: 1.6575 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9672 | Steps: 2 | Val loss: 22.8924 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.0421 | Steps: 2 | Val loss: 1.8543 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=105422)[0m top1: 0.1935634328358209
[2m[36m(func pid=105422)[0m top5: 0.6707089552238806
[2m[36m(func pid=105422)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=105422)[0m f1_macro: 0.16193558607730782
[2m[36m(func pid=105422)[0m f1_weighted: 0.20067782520404365
[2m[36m(func pid=105422)[0m f1_per_class: [0.125, 0.282, 0.303, 0.17, 0.023, 0.337, 0.176, 0.139, 0.014, 0.051]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.39552238805970147
[2m[36m(func pid=111263)[0m top5: 0.8973880597014925
[2m[36m(func pid=111263)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=111263)[0m f1_macro: 0.3476936579531296
[2m[36m(func pid=111263)[0m f1_weighted: 0.4117144975693236
[2m[36m(func pid=111263)[0m f1_per_class: [0.359, 0.235, 0.615, 0.515, 0.131, 0.315, 0.49, 0.45, 0.141, 0.224]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.0732276119402985
[2m[36m(func pid=111842)[0m top5: 0.5107276119402985
[2m[36m(func pid=111842)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=111842)[0m f1_macro: 0.033119737643785736
[2m[36m(func pid=111842)[0m f1_weighted: 0.04756970553991368
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.274, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:23 (running for 00:10:38.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.615 |      0.162 |                   34 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.042 |      0.349 |                   34 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.584 |      0.348 |                   10 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.967 |      0.033 |                    9 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3530783582089552
[2m[36m(func pid=106010)[0m top5: 0.8675373134328358
[2m[36m(func pid=106010)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=106010)[0m f1_macro: 0.3490302142356473
[2m[36m(func pid=106010)[0m f1_weighted: 0.3796408303270682
[2m[36m(func pid=106010)[0m f1_per_class: [0.329, 0.344, 0.686, 0.436, 0.056, 0.406, 0.357, 0.45, 0.172, 0.255]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6415 | Steps: 2 | Val loss: 2.2190 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4419 | Steps: 2 | Val loss: 1.7131 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1414 | Steps: 2 | Val loss: 65.6880 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0835 | Steps: 2 | Val loss: 1.8432 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=105422)[0m top1: 0.197294776119403
[2m[36m(func pid=105422)[0m top5: 0.6772388059701493
[2m[36m(func pid=105422)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.1662727338810312
[2m[36m(func pid=105422)[0m f1_weighted: 0.20662286061977794
[2m[36m(func pid=105422)[0m f1_per_class: [0.116, 0.281, 0.355, 0.188, 0.022, 0.347, 0.178, 0.136, 0.014, 0.026]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.36986940298507465
[2m[36m(func pid=111263)[0m top5: 0.9048507462686567
[2m[36m(func pid=111263)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=111263)[0m f1_macro: 0.35758920482917544
[2m[36m(func pid=111263)[0m f1_weighted: 0.3871461915544129
[2m[36m(func pid=111263)[0m f1_per_class: [0.515, 0.281, 0.686, 0.478, 0.126, 0.272, 0.428, 0.41, 0.144, 0.237]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.041044776119402986
[2m[36m(func pid=111842)[0m top5: 0.34281716417910446
[2m[36m(func pid=111842)[0m f1_micro: 0.041044776119402986
[2m[36m(func pid=111842)[0m f1_macro: 0.028627091458811003
[2m[36m(func pid=111842)[0m f1_weighted: 0.013948148590650303
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.233, 0.0, 0.024]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:28 (running for 00:10:43.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.641 |      0.166 |                   35 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.083 |      0.352 |                   35 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.442 |      0.358 |                   11 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.141 |      0.029 |                   10 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35494402985074625
[2m[36m(func pid=106010)[0m top5: 0.8736007462686567
[2m[36m(func pid=106010)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=106010)[0m f1_macro: 0.35218172681598997
[2m[36m(func pid=106010)[0m f1_weighted: 0.37953561252401985
[2m[36m(func pid=106010)[0m f1_per_class: [0.309, 0.354, 0.727, 0.437, 0.059, 0.414, 0.348, 0.452, 0.162, 0.26]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6024 | Steps: 2 | Val loss: 2.2195 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3353 | Steps: 2 | Val loss: 1.7942 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.3044 | Steps: 2 | Val loss: 67.6463 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0384 | Steps: 2 | Val loss: 1.8317 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=105422)[0m top1: 0.19916044776119404
[2m[36m(func pid=105422)[0m top5: 0.6767723880597015
[2m[36m(func pid=105422)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=105422)[0m f1_macro: 0.17240017752721065
[2m[36m(func pid=105422)[0m f1_weighted: 0.2094631447022178
[2m[36m(func pid=105422)[0m f1_per_class: [0.128, 0.284, 0.361, 0.182, 0.022, 0.352, 0.183, 0.146, 0.042, 0.026]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3460820895522388
[2m[36m(func pid=111263)[0m top5: 0.9071828358208955
[2m[36m(func pid=111263)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=111263)[0m f1_macro: 0.35004236077370077
[2m[36m(func pid=111263)[0m f1_weighted: 0.36606448719013335
[2m[36m(func pid=111263)[0m f1_per_class: [0.482, 0.289, 0.828, 0.434, 0.09, 0.208, 0.423, 0.39, 0.137, 0.219]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.020522388059701493
[2m[36m(func pid=111842)[0m top5: 0.24347014925373134
[2m[36m(func pid=111842)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=111842)[0m f1_macro: 0.015641512404077934
[2m[36m(func pid=111842)[0m f1_weighted: 0.005572718918453468
[2m[36m(func pid=111842)[0m f1_per_class: [0.04, 0.016, 0.05, 0.0, 0.0, 0.0, 0.0, 0.024, 0.0, 0.027]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:34 (running for 00:10:48.59)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.602 |      0.172 |                   36 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  1.038 |      0.349 |                   36 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.335 |      0.35  |                   12 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.304 |      0.016 |                   11 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35447761194029853
[2m[36m(func pid=106010)[0m top5: 0.875
[2m[36m(func pid=106010)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=106010)[0m f1_macro: 0.3494774584870493
[2m[36m(func pid=106010)[0m f1_weighted: 0.3776292131934147
[2m[36m(func pid=106010)[0m f1_per_class: [0.274, 0.351, 0.727, 0.437, 0.054, 0.413, 0.343, 0.47, 0.161, 0.266]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5898 | Steps: 2 | Val loss: 2.2189 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2193 | Steps: 2 | Val loss: 1.8316 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0689 | Steps: 2 | Val loss: 57.0852 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.9282 | Steps: 2 | Val loss: 1.8266 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=105422)[0m top1: 0.19962686567164178
[2m[36m(func pid=105422)[0m top5: 0.675839552238806
[2m[36m(func pid=105422)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=105422)[0m f1_macro: 0.1728525672564893
[2m[36m(func pid=105422)[0m f1_weighted: 0.21002146052994752
[2m[36m(func pid=105422)[0m f1_per_class: [0.128, 0.289, 0.361, 0.182, 0.021, 0.354, 0.181, 0.143, 0.043, 0.026]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.34841417910447764
[2m[36m(func pid=111263)[0m top5: 0.9020522388059702
[2m[36m(func pid=111263)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=111263)[0m f1_macro: 0.3485225292463897
[2m[36m(func pid=111263)[0m f1_weighted: 0.3703350130202927
[2m[36m(func pid=111263)[0m f1_per_class: [0.395, 0.287, 0.857, 0.405, 0.09, 0.195, 0.469, 0.413, 0.161, 0.211]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.15065298507462688
[2m[36m(func pid=111842)[0m top5: 0.4435634328358209
[2m[36m(func pid=111842)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=111842)[0m f1_macro: 0.039279935964030864
[2m[36m(func pid=111842)[0m f1_weighted: 0.06333573459744554
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:39 (running for 00:10:54.02)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.59  |      0.173 |                   37 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.928 |      0.357 |                   37 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.219 |      0.349 |                   13 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.069 |      0.039 |                   12 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.36007462686567165
[2m[36m(func pid=106010)[0m top5: 0.8801305970149254
[2m[36m(func pid=106010)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=106010)[0m f1_macro: 0.3574519808928511
[2m[36m(func pid=106010)[0m f1_weighted: 0.38310605768624195
[2m[36m(func pid=106010)[0m f1_per_class: [0.303, 0.351, 0.727, 0.448, 0.065, 0.409, 0.347, 0.472, 0.173, 0.28]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6179 | Steps: 2 | Val loss: 2.2187 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2751 | Steps: 2 | Val loss: 1.8477 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6158 | Steps: 2 | Val loss: 102.8629 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.9460 | Steps: 2 | Val loss: 1.8382 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=105422)[0m top1: 0.19309701492537312
[2m[36m(func pid=105422)[0m top5: 0.6791044776119403
[2m[36m(func pid=105422)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=105422)[0m f1_macro: 0.16982276774525512
[2m[36m(func pid=105422)[0m f1_weighted: 0.20308889077847384
[2m[36m(func pid=105422)[0m f1_per_class: [0.119, 0.285, 0.361, 0.179, 0.021, 0.338, 0.171, 0.128, 0.044, 0.052]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.35774253731343286
[2m[36m(func pid=111263)[0m top5: 0.8959888059701493
[2m[36m(func pid=111263)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=111263)[0m f1_macro: 0.34518757434145686
[2m[36m(func pid=111263)[0m f1_weighted: 0.3851250676688327
[2m[36m(func pid=111263)[0m f1_per_class: [0.28, 0.251, 0.828, 0.408, 0.105, 0.214, 0.53, 0.443, 0.179, 0.213]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.18610074626865672
[2m[36m(func pid=111842)[0m top5: 0.648320895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=111842)[0m f1_macro: 0.06814032928667081
[2m[36m(func pid=111842)[0m f1_weighted: 0.09393676558731749
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.336, 0.0, 0.0, 0.0, 0.312, 0.0, 0.0, 0.0, 0.033]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:45 (running for 00:10:59.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.618 |      0.17  |                   38 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.946 |      0.357 |                   38 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.275 |      0.345 |                   14 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.616 |      0.068 |                   13 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3498134328358209
[2m[36m(func pid=106010)[0m top5: 0.8805970149253731
[2m[36m(func pid=106010)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=106010)[0m f1_macro: 0.35677877173141426
[2m[36m(func pid=106010)[0m f1_weighted: 0.3771994093699238
[2m[36m(func pid=106010)[0m f1_per_class: [0.388, 0.334, 0.727, 0.412, 0.055, 0.398, 0.376, 0.449, 0.168, 0.261]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5699 | Steps: 2 | Val loss: 2.2168 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1689 | Steps: 2 | Val loss: 1.8708 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.5426 | Steps: 2 | Val loss: 90.2992 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.9065 | Steps: 2 | Val loss: 1.8400 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=111263)[0m top1: 0.3656716417910448
[2m[36m(func pid=111263)[0m top5: 0.8927238805970149
[2m[36m(func pid=111263)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=111263)[0m f1_macro: 0.3379766398206354
[2m[36m(func pid=111263)[0m f1_weighted: 0.3931475551507511
[2m[36m(func pid=111263)[0m f1_per_class: [0.236, 0.245, 0.75, 0.44, 0.105, 0.189, 0.54, 0.458, 0.197, 0.22]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m top1: 0.19263059701492538
[2m[36m(func pid=105422)[0m top5: 0.6814365671641791
[2m[36m(func pid=105422)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=105422)[0m f1_macro: 0.17612282178451072
[2m[36m(func pid=105422)[0m f1_weighted: 0.20298967912310437
[2m[36m(func pid=105422)[0m f1_per_class: [0.14, 0.284, 0.355, 0.174, 0.021, 0.329, 0.175, 0.132, 0.059, 0.093]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111842)[0m top1: 0.20755597014925373
[2m[36m(func pid=111842)[0m top5: 0.7033582089552238
[2m[36m(func pid=111842)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=111842)[0m f1_macro: 0.07064396561841675
[2m[36m(func pid=111842)[0m f1_weighted: 0.08490188823967991
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.365, 0.0, 0.0, 0.0, 0.0, 0.021, 0.259, 0.0, 0.061]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:20:50 (running for 00:11:04.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.57  |      0.176 |                   39 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.906 |      0.355 |                   39 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.169 |      0.338 |                   15 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  3.543 |      0.071 |                   14 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3493470149253731
[2m[36m(func pid=106010)[0m top5: 0.8801305970149254
[2m[36m(func pid=106010)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=106010)[0m f1_macro: 0.3547567311373158
[2m[36m(func pid=106010)[0m f1_weighted: 0.3764061081229337
[2m[36m(func pid=106010)[0m f1_per_class: [0.393, 0.346, 0.727, 0.414, 0.049, 0.381, 0.372, 0.445, 0.158, 0.261]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5373 | Steps: 2 | Val loss: 2.2158 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0923 | Steps: 2 | Val loss: 1.8888 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5093 | Steps: 2 | Val loss: 21.0165 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8588 | Steps: 2 | Val loss: 1.8238 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=111263)[0m top1: 0.3666044776119403
[2m[36m(func pid=111263)[0m top5: 0.9029850746268657
[2m[36m(func pid=111263)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=111263)[0m f1_macro: 0.33498733112865164
[2m[36m(func pid=111263)[0m f1_weighted: 0.39243742443126506
[2m[36m(func pid=111263)[0m f1_per_class: [0.231, 0.277, 0.727, 0.448, 0.104, 0.192, 0.516, 0.431, 0.192, 0.231]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.22994402985074627
[2m[36m(func pid=111842)[0m top5: 0.6529850746268657
[2m[36m(func pid=111842)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=111842)[0m f1_macro: 0.09469700392190197
[2m[36m(func pid=111842)[0m f1_weighted: 0.19376078422389642
[2m[36m(func pid=111842)[0m f1_per_class: [0.068, 0.139, 0.0, 0.0, 0.0, 0.0, 0.558, 0.0, 0.0, 0.182]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.1982276119402985
[2m[36m(func pid=105422)[0m top5: 0.6847014925373134
[2m[36m(func pid=105422)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=105422)[0m f1_macro: 0.18095064659331686
[2m[36m(func pid=105422)[0m f1_weighted: 0.2113328910976925
[2m[36m(func pid=105422)[0m f1_per_class: [0.149, 0.283, 0.361, 0.187, 0.02, 0.332, 0.188, 0.135, 0.061, 0.093]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:20:55 (running for 00:11:09.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.537 |      0.181 |                   40 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.859 |      0.355 |                   40 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.092 |      0.335 |                   16 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.509 |      0.095 |                   15 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3582089552238806
[2m[36m(func pid=106010)[0m top5: 0.8833955223880597
[2m[36m(func pid=106010)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=106010)[0m f1_macro: 0.35498199131329616
[2m[36m(func pid=106010)[0m f1_weighted: 0.38586187904036867
[2m[36m(func pid=106010)[0m f1_per_class: [0.367, 0.355, 0.706, 0.438, 0.044, 0.392, 0.375, 0.443, 0.157, 0.274]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9002 | Steps: 2 | Val loss: 12.7982 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5537 | Steps: 2 | Val loss: 2.2124 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1008 | Steps: 2 | Val loss: 1.8632 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7809 | Steps: 2 | Val loss: 1.8200 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m top1: 0.22294776119402984
[2m[36m(func pid=111842)[0m top5: 0.6469216417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=111842)[0m f1_macro: 0.07784593314910439
[2m[36m(func pid=111842)[0m f1_weighted: 0.18485450655302393
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.032, 0.023, 0.0, 0.0, 0.0, 0.596, 0.0, 0.0, 0.128]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.20102611940298507
[2m[36m(func pid=105422)[0m top5: 0.6902985074626866
[2m[36m(func pid=105422)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=105422)[0m f1_macro: 0.18563427653666364
[2m[36m(func pid=105422)[0m f1_weighted: 0.2166116894991218
[2m[36m(func pid=105422)[0m f1_per_class: [0.151, 0.281, 0.379, 0.191, 0.024, 0.331, 0.201, 0.147, 0.061, 0.09]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.39738805970149255
[2m[36m(func pid=111263)[0m top5: 0.9057835820895522
[2m[36m(func pid=111263)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=111263)[0m f1_macro: 0.3552911561512673
[2m[36m(func pid=111263)[0m f1_weighted: 0.42220862700722134
[2m[36m(func pid=111263)[0m f1_per_class: [0.259, 0.303, 0.727, 0.487, 0.113, 0.231, 0.544, 0.429, 0.218, 0.242]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:00 (running for 00:11:15.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.554 |      0.186 |                   41 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.781 |      0.354 |                   41 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.101 |      0.355 |                   17 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.9   |      0.078 |                   16 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35867537313432835
[2m[36m(func pid=106010)[0m top5: 0.8833955223880597
[2m[36m(func pid=106010)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=106010)[0m f1_macro: 0.35366552833556525
[2m[36m(func pid=106010)[0m f1_weighted: 0.38947056190349433
[2m[36m(func pid=106010)[0m f1_per_class: [0.332, 0.354, 0.706, 0.444, 0.048, 0.386, 0.386, 0.44, 0.164, 0.278]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9160 | Steps: 2 | Val loss: 9.8871 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5474 | Steps: 2 | Val loss: 2.2135 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0993 | Steps: 2 | Val loss: 1.8832 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8303 | Steps: 2 | Val loss: 1.8180 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=111842)[0m top1: 0.07136194029850747
[2m[36m(func pid=111842)[0m top5: 0.6119402985074627
[2m[36m(func pid=111842)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=111842)[0m f1_macro: 0.034786535863616116
[2m[36m(func pid=111842)[0m f1_weighted: 0.025667153381039493
[2m[36m(func pid=111842)[0m f1_per_class: [0.016, 0.089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.155, 0.0, 0.087]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.20009328358208955
[2m[36m(func pid=105422)[0m top5: 0.6888992537313433
[2m[36m(func pid=105422)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=105422)[0m f1_macro: 0.18523042666317696
[2m[36m(func pid=105422)[0m f1_weighted: 0.21496647535760963
[2m[36m(func pid=105422)[0m f1_per_class: [0.154, 0.283, 0.373, 0.178, 0.024, 0.324, 0.21, 0.141, 0.061, 0.104]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.4006529850746269
[2m[36m(func pid=111263)[0m top5: 0.9057835820895522
[2m[36m(func pid=111263)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=111263)[0m f1_macro: 0.36330612905912985
[2m[36m(func pid=111263)[0m f1_weighted: 0.4200274456071283
[2m[36m(func pid=111263)[0m f1_per_class: [0.321, 0.307, 0.8, 0.505, 0.097, 0.24, 0.52, 0.378, 0.209, 0.255]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:06 (running for 00:11:20.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.547 |      0.185 |                   42 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.83  |      0.352 |                   42 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.099 |      0.363 |                   18 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.916 |      0.035 |                   17 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35494402985074625
[2m[36m(func pid=106010)[0m top5: 0.8838619402985075
[2m[36m(func pid=106010)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=106010)[0m f1_macro: 0.3515215010679146
[2m[36m(func pid=106010)[0m f1_weighted: 0.3859079678241811
[2m[36m(func pid=106010)[0m f1_per_class: [0.309, 0.346, 0.706, 0.45, 0.047, 0.384, 0.369, 0.463, 0.19, 0.252]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7788 | Steps: 2 | Val loss: 15.0876 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5751 | Steps: 2 | Val loss: 2.2103 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0560 | Steps: 2 | Val loss: 1.9242 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7818 | Steps: 2 | Val loss: 1.8241 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=105422)[0m top1: 0.20335820895522388
[2m[36m(func pid=105422)[0m top5: 0.6935634328358209
[2m[36m(func pid=105422)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=105422)[0m f1_macro: 0.18300795203742043
[2m[36m(func pid=105422)[0m f1_weighted: 0.22114630865027107
[2m[36m(func pid=105422)[0m f1_per_class: [0.156, 0.28, 0.349, 0.195, 0.024, 0.321, 0.224, 0.103, 0.076, 0.101]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111842)[0m top1: 0.05083955223880597
[2m[36m(func pid=111842)[0m top5: 0.4939365671641791
[2m[36m(func pid=111842)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=111842)[0m f1_macro: 0.01812288516227525
[2m[36m(func pid=111842)[0m f1_weighted: 0.008134613526333453
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.051]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3987873134328358
[2m[36m(func pid=111263)[0m top5: 0.9020522388059702
[2m[36m(func pid=111263)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=111263)[0m f1_macro: 0.3591555751820624
[2m[36m(func pid=111263)[0m f1_weighted: 0.4139337271190219
[2m[36m(func pid=111263)[0m f1_per_class: [0.373, 0.307, 0.774, 0.5, 0.09, 0.246, 0.508, 0.318, 0.236, 0.239]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:11 (running for 00:11:26.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.575 |      0.183 |                   43 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.782 |      0.348 |                   43 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.056 |      0.359 |                   19 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.779 |      0.018 |                   18 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.34654850746268656
[2m[36m(func pid=106010)[0m top5: 0.8843283582089553
[2m[36m(func pid=106010)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=106010)[0m f1_macro: 0.34762650974084364
[2m[36m(func pid=106010)[0m f1_weighted: 0.3773064969525163
[2m[36m(func pid=106010)[0m f1_per_class: [0.282, 0.344, 0.706, 0.426, 0.061, 0.392, 0.363, 0.46, 0.184, 0.257]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1528 | Steps: 2 | Val loss: 20.3024 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5402 | Steps: 2 | Val loss: 2.2054 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0561 | Steps: 2 | Val loss: 1.9542 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7843 | Steps: 2 | Val loss: 1.8248 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=111842)[0m top1: 0.0228544776119403
[2m[36m(func pid=111842)[0m top5: 0.386660447761194
[2m[36m(func pid=111842)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=111842)[0m f1_macro: 0.012739896830825354
[2m[36m(func pid=111842)[0m f1_weighted: 0.011394837595963412
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044, 0.0, 0.034]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.21128731343283583
[2m[36m(func pid=105422)[0m top5: 0.7038246268656716
[2m[36m(func pid=105422)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=105422)[0m f1_macro: 0.18813668908669093
[2m[36m(func pid=105422)[0m f1_weighted: 0.23088080020565374
[2m[36m(func pid=105422)[0m f1_per_class: [0.15, 0.275, 0.349, 0.211, 0.029, 0.331, 0.238, 0.12, 0.074, 0.104]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.40671641791044777
[2m[36m(func pid=111263)[0m top5: 0.8997201492537313
[2m[36m(func pid=111263)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=111263)[0m f1_macro: 0.3740046775702469
[2m[36m(func pid=111263)[0m f1_weighted: 0.4238016855085487
[2m[36m(func pid=111263)[0m f1_per_class: [0.418, 0.317, 0.75, 0.505, 0.098, 0.253, 0.509, 0.387, 0.263, 0.241]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:16 (running for 00:11:31.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.54  |      0.188 |                   44 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.784 |      0.338 |                   44 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.056 |      0.374 |                   20 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.153 |      0.013 |                   19 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.333955223880597
[2m[36m(func pid=106010)[0m top5: 0.8847947761194029
[2m[36m(func pid=106010)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=106010)[0m f1_macro: 0.3380406654397545
[2m[36m(func pid=106010)[0m f1_weighted: 0.36574391071724355
[2m[36m(func pid=106010)[0m f1_per_class: [0.208, 0.338, 0.706, 0.402, 0.059, 0.397, 0.353, 0.465, 0.169, 0.283]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.3301 | Steps: 2 | Val loss: 8.1010 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5131 | Steps: 2 | Val loss: 2.2028 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0508 | Steps: 2 | Val loss: 2.0018 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7026 | Steps: 2 | Val loss: 1.8254 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=111842)[0m top1: 0.12639925373134328
[2m[36m(func pid=111842)[0m top5: 0.590018656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=111842)[0m f1_macro: 0.09562605611815414
[2m[36m(func pid=111842)[0m f1_weighted: 0.10567476682740241
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.21, 0.0, 0.007, 0.0, 0.363, 0.027, 0.305, 0.0, 0.045]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.21361940298507462
[2m[36m(func pid=105422)[0m top5: 0.7052238805970149
[2m[36m(func pid=105422)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=105422)[0m f1_macro: 0.19262888712496987
[2m[36m(func pid=105422)[0m f1_weighted: 0.23431912076444278
[2m[36m(func pid=105422)[0m f1_per_class: [0.17, 0.274, 0.333, 0.224, 0.029, 0.332, 0.235, 0.122, 0.073, 0.135]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.4001865671641791
[2m[36m(func pid=111263)[0m top5: 0.9001865671641791
[2m[36m(func pid=111263)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=111263)[0m f1_macro: 0.3789156765199313
[2m[36m(func pid=111263)[0m f1_weighted: 0.4246867148558264
[2m[36m(func pid=111263)[0m f1_per_class: [0.431, 0.315, 0.75, 0.497, 0.094, 0.283, 0.5, 0.438, 0.251, 0.229]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:22 (running for 00:11:36.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.513 |      0.193 |                   45 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.703 |      0.339 |                   45 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.051 |      0.379 |                   21 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.33  |      0.096 |                   20 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.33488805970149255
[2m[36m(func pid=106010)[0m top5: 0.8847947761194029
[2m[36m(func pid=106010)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=106010)[0m f1_macro: 0.3385244634615706
[2m[36m(func pid=106010)[0m f1_weighted: 0.36891906637685745
[2m[36m(func pid=106010)[0m f1_per_class: [0.193, 0.328, 0.706, 0.395, 0.064, 0.403, 0.375, 0.456, 0.185, 0.281]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9071 | Steps: 2 | Val loss: 8.4030 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4880 | Steps: 2 | Val loss: 2.2004 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0455 | Steps: 2 | Val loss: 2.0529 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6960 | Steps: 2 | Val loss: 1.8151 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.17164179104477612
[2m[36m(func pid=111842)[0m top5: 0.6511194029850746
[2m[36m(func pid=111842)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=111842)[0m f1_macro: 0.13885756201664606
[2m[36m(func pid=111842)[0m f1_weighted: 0.158049360759965
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.282, 0.097, 0.01, 0.062, 0.399, 0.131, 0.342, 0.019, 0.046]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.21361940298507462
[2m[36m(func pid=105422)[0m top5: 0.7066231343283582
[2m[36m(func pid=105422)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=105422)[0m f1_macro: 0.19200066939715615
[2m[36m(func pid=105422)[0m f1_weighted: 0.23543366826369488
[2m[36m(func pid=105422)[0m f1_per_class: [0.168, 0.262, 0.333, 0.231, 0.029, 0.341, 0.238, 0.114, 0.07, 0.133]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3843283582089552
[2m[36m(func pid=111263)[0m top5: 0.8987873134328358
[2m[36m(func pid=111263)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=111263)[0m f1_macro: 0.37168089525022746
[2m[36m(func pid=111263)[0m f1_weighted: 0.4133176540587326
[2m[36m(func pid=111263)[0m f1_per_class: [0.438, 0.301, 0.75, 0.48, 0.077, 0.284, 0.486, 0.441, 0.245, 0.213]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:27 (running for 00:11:41.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.488 |      0.192 |                   46 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.696 |      0.347 |                   46 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.045 |      0.372 |                   22 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.907 |      0.139 |                   21 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35774253731343286
[2m[36m(func pid=106010)[0m top5: 0.8810634328358209
[2m[36m(func pid=106010)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=106010)[0m f1_macro: 0.3472990842789823
[2m[36m(func pid=106010)[0m f1_weighted: 0.3942799011144756
[2m[36m(func pid=106010)[0m f1_per_class: [0.236, 0.347, 0.686, 0.451, 0.055, 0.363, 0.413, 0.426, 0.204, 0.292]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4669 | Steps: 2 | Val loss: 2.1973 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8532 | Steps: 2 | Val loss: 6.6241 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0459 | Steps: 2 | Val loss: 2.1368 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6914 | Steps: 2 | Val loss: 1.8219 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=111842)[0m top1: 0.1791044776119403
[2m[36m(func pid=111842)[0m top5: 0.6861007462686567
[2m[36m(func pid=111842)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=111842)[0m f1_macro: 0.1487781270331974
[2m[36m(func pid=111842)[0m f1_weighted: 0.15279270926020763
[2m[36m(func pid=111842)[0m f1_per_class: [0.087, 0.326, 0.109, 0.01, 0.048, 0.452, 0.069, 0.303, 0.014, 0.07]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.21688432835820895
[2m[36m(func pid=105422)[0m top5: 0.7089552238805971
[2m[36m(func pid=105422)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=105422)[0m f1_macro: 0.19304111777616884
[2m[36m(func pid=105422)[0m f1_weighted: 0.23938240751342127
[2m[36m(func pid=105422)[0m f1_per_class: [0.168, 0.261, 0.324, 0.241, 0.03, 0.344, 0.239, 0.121, 0.07, 0.132]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3582089552238806
[2m[36m(func pid=111263)[0m top5: 0.8950559701492538
[2m[36m(func pid=111263)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=111263)[0m f1_macro: 0.3600686268723518
[2m[36m(func pid=111263)[0m f1_weighted: 0.3846693190753934
[2m[36m(func pid=111263)[0m f1_per_class: [0.416, 0.29, 0.75, 0.435, 0.073, 0.271, 0.443, 0.447, 0.246, 0.23]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:32 (running for 00:11:46.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.467 |      0.193 |                   47 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.691 |      0.351 |                   47 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.046 |      0.36  |                   23 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.853 |      0.149 |                   22 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3619402985074627
[2m[36m(func pid=106010)[0m top5: 0.8782649253731343
[2m[36m(func pid=106010)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=106010)[0m f1_macro: 0.350744182190605
[2m[36m(func pid=106010)[0m f1_weighted: 0.3982167019835629
[2m[36m(func pid=106010)[0m f1_per_class: [0.317, 0.342, 0.686, 0.444, 0.041, 0.312, 0.45, 0.433, 0.191, 0.292]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4752 | Steps: 2 | Val loss: 2.1967 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7172 | Steps: 2 | Val loss: 4.1948 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0267 | Steps: 2 | Val loss: 2.2170 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7848 | Steps: 2 | Val loss: 1.8144 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=111842)[0m top1: 0.2635261194029851
[2m[36m(func pid=111842)[0m top5: 0.8129664179104478
[2m[36m(func pid=111842)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=111842)[0m f1_macro: 0.20385052679146698
[2m[36m(func pid=111842)[0m f1_weighted: 0.2627517104004201
[2m[36m(func pid=111842)[0m f1_per_class: [0.141, 0.347, 0.108, 0.113, 0.028, 0.372, 0.334, 0.375, 0.063, 0.157]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.21828358208955223
[2m[36m(func pid=105422)[0m top5: 0.7122201492537313
[2m[36m(func pid=105422)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=105422)[0m f1_macro: 0.2008852658352059
[2m[36m(func pid=105422)[0m f1_weighted: 0.2408473407122341
[2m[36m(func pid=105422)[0m f1_per_class: [0.216, 0.261, 0.333, 0.241, 0.025, 0.345, 0.238, 0.133, 0.07, 0.147]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3306902985074627
[2m[36m(func pid=111263)[0m top5: 0.8941231343283582
[2m[36m(func pid=111263)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=111263)[0m f1_macro: 0.34150014319676425
[2m[36m(func pid=111263)[0m f1_weighted: 0.35329616627577265
[2m[36m(func pid=111263)[0m f1_per_class: [0.371, 0.277, 0.774, 0.399, 0.084, 0.257, 0.395, 0.41, 0.246, 0.201]
[2m[36m(func pid=111263)[0m 
== Status ==
Current time: 2024-01-07 04:21:37 (running for 00:11:51.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.475 |      0.201 |                   48 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.785 |      0.349 |                   48 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.027 |      0.342 |                   24 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.717 |      0.204 |                   23 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.365205223880597
[2m[36m(func pid=106010)[0m top5: 0.8773320895522388
[2m[36m(func pid=106010)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=106010)[0m f1_macro: 0.3492665645604182
[2m[36m(func pid=106010)[0m f1_weighted: 0.4009945248179915
[2m[36m(func pid=106010)[0m f1_per_class: [0.297, 0.34, 0.632, 0.453, 0.049, 0.349, 0.437, 0.442, 0.204, 0.29]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7327 | Steps: 2 | Val loss: 3.7183 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4516 | Steps: 2 | Val loss: 2.1946 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0202 | Steps: 2 | Val loss: 2.2393 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5943 | Steps: 2 | Val loss: 1.7999 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=105422)[0m top1: 0.22154850746268656
[2m[36m(func pid=105422)[0m top5: 0.7117537313432836
[2m[36m(func pid=105422)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=105422)[0m f1_macro: 0.20234314212776477
[2m[36m(func pid=105422)[0m f1_weighted: 0.24613967472198137
[2m[36m(func pid=105422)[0m f1_per_class: [0.209, 0.261, 0.333, 0.244, 0.025, 0.361, 0.248, 0.135, 0.068, 0.139]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111842)[0m top1: 0.32136194029850745
[2m[36m(func pid=111842)[0m top5: 0.8638059701492538
[2m[36m(func pid=111842)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=111842)[0m f1_macro: 0.23868427298354034
[2m[36m(func pid=111842)[0m f1_weighted: 0.3374413569933395
[2m[36m(func pid=111842)[0m f1_per_class: [0.23, 0.256, 0.0, 0.208, 0.027, 0.304, 0.541, 0.471, 0.14, 0.21]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:21:42 (running for 00:11:56.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.452 |      0.202 |                   49 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.785 |      0.349 |                   48 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.02  |      0.337 |                   25 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.733 |      0.239 |                   24 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111263)[0m top1: 0.32136194029850745
[2m[36m(func pid=111263)[0m top5: 0.8852611940298507
[2m[36m(func pid=111263)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=111263)[0m f1_macro: 0.3365120499688863
[2m[36m(func pid=111263)[0m f1_weighted: 0.3421517292977111
[2m[36m(func pid=111263)[0m f1_per_class: [0.337, 0.273, 0.8, 0.375, 0.091, 0.256, 0.386, 0.404, 0.242, 0.2]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m top1: 0.36427238805970147
[2m[36m(func pid=106010)[0m top5: 0.8847947761194029
[2m[36m(func pid=106010)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=106010)[0m f1_macro: 0.35298382410824053
[2m[36m(func pid=106010)[0m f1_weighted: 0.3976360664568684
[2m[36m(func pid=106010)[0m f1_per_class: [0.25, 0.33, 0.686, 0.461, 0.054, 0.382, 0.41, 0.456, 0.209, 0.292]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6604 | Steps: 2 | Val loss: 4.3556 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4479 | Steps: 2 | Val loss: 2.1942 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6129 | Steps: 2 | Val loss: 1.7973 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0295 | Steps: 2 | Val loss: 2.2474 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=111842)[0m top1: 0.21222014925373134
[2m[36m(func pid=111842)[0m top5: 0.8171641791044776
[2m[36m(func pid=111842)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=111842)[0m f1_macro: 0.18539662453055966
[2m[36m(func pid=111842)[0m f1_weighted: 0.2298423275766845
[2m[36m(func pid=111842)[0m f1_per_class: [0.221, 0.249, 0.118, 0.315, 0.045, 0.233, 0.149, 0.311, 0.104, 0.11]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.22154850746268656
[2m[36m(func pid=105422)[0m top5: 0.7103544776119403
[2m[36m(func pid=105422)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=105422)[0m f1_macro: 0.20121496952125573
[2m[36m(func pid=105422)[0m f1_weighted: 0.2461182964636066
[2m[36m(func pid=105422)[0m f1_per_class: [0.202, 0.268, 0.338, 0.241, 0.025, 0.36, 0.246, 0.145, 0.055, 0.13]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:21:47 (running for 00:12:02.00)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.448 |      0.201 |                   50 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.613 |      0.353 |                   50 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.02  |      0.337 |                   25 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.66  |      0.185 |                   25 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.36473880597014924
[2m[36m(func pid=106010)[0m top5: 0.8847947761194029
[2m[36m(func pid=106010)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=106010)[0m f1_macro: 0.3534551335299949
[2m[36m(func pid=106010)[0m f1_weighted: 0.3956786085825668
[2m[36m(func pid=106010)[0m f1_per_class: [0.273, 0.328, 0.686, 0.463, 0.06, 0.382, 0.402, 0.456, 0.204, 0.282]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.32975746268656714
[2m[36m(func pid=111263)[0m top5: 0.8875932835820896
[2m[36m(func pid=111263)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=111263)[0m f1_macro: 0.34309441077736075
[2m[36m(func pid=111263)[0m f1_weighted: 0.3522769679887558
[2m[36m(func pid=111263)[0m f1_per_class: [0.332, 0.282, 0.828, 0.379, 0.095, 0.26, 0.408, 0.411, 0.246, 0.19]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7621 | Steps: 2 | Val loss: 4.1787 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4825 | Steps: 2 | Val loss: 2.1883 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5754 | Steps: 2 | Val loss: 1.7991 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0208 | Steps: 2 | Val loss: 2.2140 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=111842)[0m top1: 0.26725746268656714
[2m[36m(func pid=111842)[0m top5: 0.840018656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=111842)[0m f1_macro: 0.19897417266077083
[2m[36m(func pid=111842)[0m f1_weighted: 0.27795719952993947
[2m[36m(func pid=111842)[0m f1_per_class: [0.18, 0.298, 0.0, 0.297, 0.034, 0.24, 0.296, 0.328, 0.081, 0.235]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.22388059701492538
[2m[36m(func pid=105422)[0m top5: 0.7178171641791045
[2m[36m(func pid=105422)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=105422)[0m f1_macro: 0.20647181364640438
[2m[36m(func pid=105422)[0m f1_weighted: 0.24803447553838387
[2m[36m(func pid=105422)[0m f1_per_class: [0.217, 0.26, 0.349, 0.245, 0.025, 0.357, 0.248, 0.183, 0.042, 0.138]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:21:52 (running for 00:12:07.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.482 |      0.206 |                   51 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.575 |      0.359 |                   51 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.03  |      0.343 |                   26 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.762 |      0.199 |                   26 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3656716417910448
[2m[36m(func pid=106010)[0m top5: 0.882929104477612
[2m[36m(func pid=106010)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=106010)[0m f1_macro: 0.358802687642625
[2m[36m(func pid=106010)[0m f1_weighted: 0.3991020912803273
[2m[36m(func pid=106010)[0m f1_per_class: [0.333, 0.332, 0.706, 0.459, 0.064, 0.336, 0.43, 0.443, 0.207, 0.277]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3414179104477612
[2m[36m(func pid=111263)[0m top5: 0.8964552238805971
[2m[36m(func pid=111263)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=111263)[0m f1_macro: 0.3481543457399449
[2m[36m(func pid=111263)[0m f1_weighted: 0.3632732135619115
[2m[36m(func pid=111263)[0m f1_per_class: [0.352, 0.284, 0.8, 0.405, 0.087, 0.273, 0.411, 0.426, 0.24, 0.204]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8049 | Steps: 2 | Val loss: 5.4827 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4986 | Steps: 2 | Val loss: 2.1874 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6292 | Steps: 2 | Val loss: 1.8094 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0215 | Steps: 2 | Val loss: 2.1798 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=111842)[0m top1: 0.23694029850746268
[2m[36m(func pid=111842)[0m top5: 0.773320895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=111842)[0m f1_macro: 0.1890922495864736
[2m[36m(func pid=111842)[0m f1_weighted: 0.22716426618068486
[2m[36m(func pid=111842)[0m f1_per_class: [0.161, 0.294, 0.296, 0.075, 0.03, 0.127, 0.375, 0.337, 0.107, 0.089]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.21875
[2m[36m(func pid=105422)[0m top5: 0.7196828358208955
[2m[36m(func pid=105422)[0m f1_micro: 0.21875
[2m[36m(func pid=105422)[0m f1_macro: 0.20164071621750299
[2m[36m(func pid=105422)[0m f1_weighted: 0.24198787402754326
[2m[36m(func pid=105422)[0m f1_per_class: [0.208, 0.253, 0.319, 0.239, 0.03, 0.345, 0.244, 0.168, 0.055, 0.157]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:21:58 (running for 00:12:12.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.499 |      0.202 |                   52 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.629 |      0.351 |                   52 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.021 |      0.348 |                   27 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.805 |      0.189 |                   27 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35027985074626866
[2m[36m(func pid=106010)[0m top5: 0.8847947761194029
[2m[36m(func pid=106010)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=106010)[0m f1_macro: 0.351422525901617
[2m[36m(func pid=106010)[0m f1_weighted: 0.3846589128287741
[2m[36m(func pid=106010)[0m f1_per_class: [0.367, 0.319, 0.727, 0.436, 0.06, 0.305, 0.428, 0.422, 0.18, 0.271]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3572761194029851
[2m[36m(func pid=111263)[0m top5: 0.8992537313432836
[2m[36m(func pid=111263)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=111263)[0m f1_macro: 0.35728205662503504
[2m[36m(func pid=111263)[0m f1_weighted: 0.38026125952135076
[2m[36m(func pid=111263)[0m f1_per_class: [0.356, 0.288, 0.8, 0.435, 0.084, 0.271, 0.431, 0.455, 0.249, 0.203]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6218 | Steps: 2 | Val loss: 6.9807 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4166 | Steps: 2 | Val loss: 2.1877 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5067 | Steps: 2 | Val loss: 1.8044 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0187 | Steps: 2 | Val loss: 2.1255 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=111842)[0m top1: 0.20475746268656717
[2m[36m(func pid=111842)[0m top5: 0.7192164179104478
[2m[36m(func pid=111842)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=111842)[0m f1_macro: 0.1526661443086346
[2m[36m(func pid=111842)[0m f1_weighted: 0.19537536070630354
[2m[36m(func pid=111842)[0m f1_per_class: [0.136, 0.293, 0.143, 0.045, 0.02, 0.067, 0.329, 0.323, 0.103, 0.068]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.22108208955223882
[2m[36m(func pid=105422)[0m top5: 0.7201492537313433
[2m[36m(func pid=105422)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=105422)[0m f1_macro: 0.20527294409240918
[2m[36m(func pid=105422)[0m f1_weighted: 0.24537454847578138
[2m[36m(func pid=105422)[0m f1_per_class: [0.207, 0.255, 0.324, 0.242, 0.029, 0.351, 0.245, 0.18, 0.068, 0.153]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:03 (running for 00:12:17.61)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.417 |      0.205 |                   53 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.507 |      0.347 |                   53 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.022 |      0.357 |                   28 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.622 |      0.153 |                   28 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.35401119402985076
[2m[36m(func pid=106010)[0m top5: 0.886660447761194
[2m[36m(func pid=106010)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=106010)[0m f1_macro: 0.3472983954614172
[2m[36m(func pid=106010)[0m f1_weighted: 0.3891427559077888
[2m[36m(func pid=106010)[0m f1_per_class: [0.362, 0.322, 0.686, 0.451, 0.064, 0.301, 0.428, 0.432, 0.182, 0.245]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.37593283582089554
[2m[36m(func pid=111263)[0m top5: 0.9039179104477612
[2m[36m(func pid=111263)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=111263)[0m f1_macro: 0.3617810746002944
[2m[36m(func pid=111263)[0m f1_weighted: 0.4035534928103291
[2m[36m(func pid=111263)[0m f1_per_class: [0.379, 0.295, 0.75, 0.465, 0.073, 0.281, 0.474, 0.454, 0.253, 0.194]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6394 | Steps: 2 | Val loss: 8.5272 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4251 | Steps: 2 | Val loss: 2.1840 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4778 | Steps: 2 | Val loss: 1.7855 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0281 | Steps: 2 | Val loss: 2.1153 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=111842)[0m top1: 0.1767723880597015
[2m[36m(func pid=111842)[0m top5: 0.6833022388059702
[2m[36m(func pid=111842)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=111842)[0m f1_macro: 0.144675684006819
[2m[36m(func pid=111842)[0m f1_weighted: 0.1943124711375741
[2m[36m(func pid=111842)[0m f1_per_class: [0.077, 0.204, 0.171, 0.112, 0.0, 0.143, 0.294, 0.325, 0.063, 0.058]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.22388059701492538
[2m[36m(func pid=105422)[0m top5: 0.7262126865671642
[2m[36m(func pid=105422)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=105422)[0m f1_macro: 0.20789889150078406
[2m[36m(func pid=105422)[0m f1_weighted: 0.24855090093959295
[2m[36m(func pid=105422)[0m f1_per_class: [0.2, 0.254, 0.333, 0.256, 0.029, 0.358, 0.24, 0.189, 0.056, 0.164]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:08 (running for 00:12:22.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.425 |      0.208 |                   54 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.478 |      0.363 |                   54 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.019 |      0.362 |                   29 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.639 |      0.145 |                   29 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37173507462686567
[2m[36m(func pid=106010)[0m top5: 0.8903917910447762
[2m[36m(func pid=106010)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=106010)[0m f1_macro: 0.36255717572381296
[2m[36m(func pid=106010)[0m f1_weighted: 0.40497333533714713
[2m[36m(func pid=106010)[0m f1_per_class: [0.381, 0.333, 0.686, 0.465, 0.071, 0.35, 0.436, 0.447, 0.204, 0.253]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3843283582089552
[2m[36m(func pid=111263)[0m top5: 0.9039179104477612
[2m[36m(func pid=111263)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=111263)[0m f1_macro: 0.370029490702975
[2m[36m(func pid=111263)[0m f1_weighted: 0.4147029305878287
[2m[36m(func pid=111263)[0m f1_per_class: [0.378, 0.297, 0.75, 0.463, 0.079, 0.303, 0.497, 0.484, 0.25, 0.198]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.5559 | Steps: 2 | Val loss: 9.4734 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4436 | Steps: 2 | Val loss: 2.1837 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5996 | Steps: 2 | Val loss: 1.7691 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0153 | Steps: 2 | Val loss: 2.1116 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=111842)[0m top1: 0.17117537313432835
[2m[36m(func pid=111842)[0m top5: 0.6646455223880597
[2m[36m(func pid=111842)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=111842)[0m f1_macro: 0.12203317088298501
[2m[36m(func pid=111842)[0m f1_weighted: 0.16985679961046501
[2m[36m(func pid=111842)[0m f1_per_class: [0.099, 0.154, 0.0, 0.077, 0.0, 0.283, 0.232, 0.294, 0.018, 0.064]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.228544776119403
[2m[36m(func pid=105422)[0m top5: 0.7234141791044776
[2m[36m(func pid=105422)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.2098345780945336
[2m[36m(func pid=105422)[0m f1_weighted: 0.2531993909170887
[2m[36m(func pid=105422)[0m f1_per_class: [0.2, 0.267, 0.31, 0.247, 0.03, 0.376, 0.248, 0.192, 0.068, 0.161]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:13 (running for 00:12:28.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.444 |      0.21  |                   55 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.6   |      0.355 |                   55 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.028 |      0.37  |                   30 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.556 |      0.122 |                   30 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37593283582089554
[2m[36m(func pid=106010)[0m top5: 0.894589552238806
[2m[36m(func pid=106010)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=106010)[0m f1_macro: 0.35470637436017566
[2m[36m(func pid=106010)[0m f1_weighted: 0.40755607068333455
[2m[36m(func pid=106010)[0m f1_per_class: [0.3, 0.34, 0.667, 0.483, 0.061, 0.381, 0.419, 0.438, 0.207, 0.252]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.39225746268656714
[2m[36m(func pid=111263)[0m top5: 0.9020522388059702
[2m[36m(func pid=111263)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=111263)[0m f1_macro: 0.37292920500713583
[2m[36m(func pid=111263)[0m f1_weighted: 0.4246345261026832
[2m[36m(func pid=111263)[0m f1_per_class: [0.366, 0.303, 0.75, 0.465, 0.082, 0.323, 0.522, 0.467, 0.247, 0.203]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2579 | Steps: 2 | Val loss: 7.7070 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4396 | Steps: 2 | Val loss: 2.1822 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5304 | Steps: 2 | Val loss: 1.7715 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0154 | Steps: 2 | Val loss: 2.1386 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=111842)[0m top1: 0.20708955223880596
[2m[36m(func pid=111842)[0m top5: 0.7430037313432836
[2m[36m(func pid=111842)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=111842)[0m f1_macro: 0.12276330658204256
[2m[36m(func pid=111842)[0m f1_weighted: 0.21250658050122953
[2m[36m(func pid=111842)[0m f1_per_class: [0.081, 0.094, 0.0, 0.166, 0.0, 0.144, 0.386, 0.275, 0.0, 0.082]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.2234141791044776
[2m[36m(func pid=105422)[0m top5: 0.726679104477612
[2m[36m(func pid=105422)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=105422)[0m f1_macro: 0.20433422136137075
[2m[36m(func pid=105422)[0m f1_weighted: 0.24585299751317286
[2m[36m(func pid=105422)[0m f1_per_class: [0.201, 0.264, 0.306, 0.251, 0.034, 0.372, 0.227, 0.178, 0.055, 0.155]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:18 (running for 00:12:33.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.44  |      0.204 |                   56 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.53  |      0.35  |                   56 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.015 |      0.373 |                   31 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  2.258 |      0.123 |                   31 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3666044776119403
[2m[36m(func pid=106010)[0m top5: 0.8927238805970149
[2m[36m(func pid=106010)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=106010)[0m f1_macro: 0.3500123504247554
[2m[36m(func pid=106010)[0m f1_weighted: 0.3984823168143628
[2m[36m(func pid=106010)[0m f1_per_class: [0.235, 0.321, 0.686, 0.466, 0.063, 0.411, 0.406, 0.465, 0.182, 0.267]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.38992537313432835
[2m[36m(func pid=111263)[0m top5: 0.8936567164179104
[2m[36m(func pid=111263)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=111263)[0m f1_macro: 0.3726819592629385
[2m[36m(func pid=111263)[0m f1_weighted: 0.4237768196967974
[2m[36m(func pid=111263)[0m f1_per_class: [0.354, 0.304, 0.75, 0.465, 0.087, 0.327, 0.515, 0.481, 0.248, 0.196]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8554 | Steps: 2 | Val loss: 5.8540 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4025 | Steps: 2 | Val loss: 2.1820 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5019 | Steps: 2 | Val loss: 1.7689 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=111842)[0m top1: 0.30550373134328357
[2m[36m(func pid=111842)[0m top5: 0.7798507462686567
[2m[36m(func pid=111842)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=111842)[0m f1_macro: 0.19712940086749192
[2m[36m(func pid=111842)[0m f1_weighted: 0.3114612279824759
[2m[36m(func pid=111842)[0m f1_per_class: [0.176, 0.153, 0.0, 0.359, 0.159, 0.199, 0.453, 0.353, 0.032, 0.087]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0242 | Steps: 2 | Val loss: 2.1541 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=105422)[0m top1: 0.22667910447761194
[2m[36m(func pid=105422)[0m top5: 0.7271455223880597
[2m[36m(func pid=105422)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=105422)[0m f1_macro: 0.20725745299286175
[2m[36m(func pid=105422)[0m f1_weighted: 0.2501673102146837
[2m[36m(func pid=105422)[0m f1_per_class: [0.199, 0.272, 0.306, 0.257, 0.034, 0.372, 0.226, 0.212, 0.053, 0.142]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:24 (running for 00:12:38.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.402 |      0.207 |                   57 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.502 |      0.349 |                   57 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.015 |      0.373 |                   32 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.855 |      0.197 |                   32 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3605410447761194
[2m[36m(func pid=106010)[0m top5: 0.8903917910447762
[2m[36m(func pid=106010)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=106010)[0m f1_macro: 0.3489321755834514
[2m[36m(func pid=106010)[0m f1_weighted: 0.3922182611879267
[2m[36m(func pid=106010)[0m f1_per_class: [0.219, 0.298, 0.727, 0.456, 0.068, 0.407, 0.41, 0.456, 0.186, 0.261]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3880597014925373
[2m[36m(func pid=111263)[0m top5: 0.8917910447761194
[2m[36m(func pid=111263)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=111263)[0m f1_macro: 0.3690816324373939
[2m[36m(func pid=111263)[0m f1_weighted: 0.42245989121263267
[2m[36m(func pid=111263)[0m f1_per_class: [0.352, 0.296, 0.727, 0.481, 0.085, 0.333, 0.501, 0.468, 0.256, 0.191]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.3903 | Steps: 2 | Val loss: 5.1378 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3680 | Steps: 2 | Val loss: 2.1799 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5686 | Steps: 2 | Val loss: 1.7684 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=111842)[0m top1: 0.32649253731343286
[2m[36m(func pid=111842)[0m top5: 0.7728544776119403
[2m[36m(func pid=111842)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=111842)[0m f1_macro: 0.2008553131398539
[2m[36m(func pid=111842)[0m f1_weighted: 0.33842083558007724
[2m[36m(func pid=111842)[0m f1_per_class: [0.037, 0.157, 0.0, 0.379, 0.146, 0.263, 0.497, 0.394, 0.058, 0.077]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0145 | Steps: 2 | Val loss: 2.1641 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=105422)[0m top1: 0.23087686567164178
[2m[36m(func pid=105422)[0m top5: 0.7294776119402985
[2m[36m(func pid=105422)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=105422)[0m f1_macro: 0.20954896863221945
[2m[36m(func pid=105422)[0m f1_weighted: 0.253769208282466
[2m[36m(func pid=105422)[0m f1_per_class: [0.194, 0.277, 0.31, 0.256, 0.036, 0.38, 0.234, 0.214, 0.039, 0.156]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:29 (running for 00:12:43.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.368 |      0.21  |                   58 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.569 |      0.36  |                   58 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.024 |      0.369 |                   33 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.39  |      0.201 |                   33 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3829291044776119
[2m[36m(func pid=106010)[0m top5: 0.8950559701492538
[2m[36m(func pid=106010)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=106010)[0m f1_macro: 0.36002549674135553
[2m[36m(func pid=106010)[0m f1_weighted: 0.4128427427834678
[2m[36m(func pid=106010)[0m f1_per_class: [0.342, 0.344, 0.649, 0.483, 0.056, 0.365, 0.436, 0.452, 0.2, 0.274]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m top1: 0.3880597014925373
[2m[36m(func pid=111263)[0m top5: 0.8903917910447762
[2m[36m(func pid=111263)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=111263)[0m f1_macro: 0.37485749237875604
[2m[36m(func pid=111263)[0m f1_weighted: 0.42148648078776496
[2m[36m(func pid=111263)[0m f1_per_class: [0.357, 0.297, 0.75, 0.493, 0.095, 0.337, 0.48, 0.477, 0.268, 0.195]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8806 | Steps: 2 | Val loss: 5.2953 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3371 | Steps: 2 | Val loss: 2.1789 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4848 | Steps: 2 | Val loss: 1.7949 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=111842)[0m top1: 0.2462686567164179
[2m[36m(func pid=111842)[0m top5: 0.7355410447761194
[2m[36m(func pid=111842)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=111842)[0m f1_macro: 0.19955233984836399
[2m[36m(func pid=111842)[0m f1_weighted: 0.2704681574224599
[2m[36m(func pid=111842)[0m f1_per_class: [0.036, 0.206, 0.024, 0.153, 0.104, 0.307, 0.429, 0.383, 0.067, 0.286]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0143 | Steps: 2 | Val loss: 2.1633 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=105422)[0m top1: 0.228544776119403
[2m[36m(func pid=105422)[0m top5: 0.7252798507462687
[2m[36m(func pid=105422)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=105422)[0m f1_macro: 0.20856802455120818
[2m[36m(func pid=105422)[0m f1_weighted: 0.2515157707352991
[2m[36m(func pid=105422)[0m f1_per_class: [0.191, 0.271, 0.314, 0.257, 0.036, 0.379, 0.229, 0.206, 0.052, 0.149]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:34 (running for 00:12:49.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.337 |      0.209 |                   59 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.485 |      0.344 |                   59 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.015 |      0.375 |                   34 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.881 |      0.2   |                   34 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.36427238805970147
[2m[36m(func pid=106010)[0m top5: 0.8861940298507462
[2m[36m(func pid=106010)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=106010)[0m f1_macro: 0.3440717289712178
[2m[36m(func pid=106010)[0m f1_weighted: 0.39462725849649793
[2m[36m(func pid=106010)[0m f1_per_class: [0.345, 0.332, 0.649, 0.456, 0.06, 0.293, 0.442, 0.416, 0.199, 0.248]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0340 | Steps: 2 | Val loss: 5.5394 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=111263)[0m top1: 0.38619402985074625
[2m[36m(func pid=111263)[0m top5: 0.8931902985074627
[2m[36m(func pid=111263)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=111263)[0m f1_macro: 0.3750782286787583
[2m[36m(func pid=111263)[0m f1_weighted: 0.4185722542195165
[2m[36m(func pid=111263)[0m f1_per_class: [0.374, 0.293, 0.774, 0.508, 0.091, 0.336, 0.462, 0.464, 0.253, 0.197]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3724 | Steps: 2 | Val loss: 2.1782 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4833 | Steps: 2 | Val loss: 1.7995 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m top1: 0.24813432835820895
[2m[36m(func pid=111842)[0m top5: 0.7588619402985075
[2m[36m(func pid=111842)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=111842)[0m f1_macro: 0.2015079186410694
[2m[36m(func pid=111842)[0m f1_weighted: 0.26400823646895377
[2m[36m(func pid=111842)[0m f1_per_class: [0.113, 0.282, 0.035, 0.089, 0.065, 0.245, 0.436, 0.416, 0.074, 0.26]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0166 | Steps: 2 | Val loss: 2.1395 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=105422)[0m top1: 0.23041044776119404
[2m[36m(func pid=105422)[0m top5: 0.7290111940298507
[2m[36m(func pid=105422)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=105422)[0m f1_macro: 0.21077254864560535
[2m[36m(func pid=105422)[0m f1_weighted: 0.2530419114343438
[2m[36m(func pid=105422)[0m f1_per_class: [0.213, 0.273, 0.306, 0.258, 0.036, 0.38, 0.232, 0.194, 0.053, 0.163]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:39 (running for 00:12:54.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.372 |      0.211 |                   60 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.483 |      0.347 |                   60 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.014 |      0.375 |                   35 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.034 |      0.202 |                   35 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.36100746268656714
[2m[36m(func pid=106010)[0m top5: 0.886660447761194
[2m[36m(func pid=106010)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=106010)[0m f1_macro: 0.34672695564560124
[2m[36m(func pid=106010)[0m f1_weighted: 0.39192765026752047
[2m[36m(func pid=106010)[0m f1_per_class: [0.323, 0.335, 0.686, 0.449, 0.076, 0.278, 0.441, 0.424, 0.218, 0.238]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6477 | Steps: 2 | Val loss: 5.7176 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=111263)[0m top1: 0.38992537313432835
[2m[36m(func pid=111263)[0m top5: 0.8978544776119403
[2m[36m(func pid=111263)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=111263)[0m f1_macro: 0.3780772026784516
[2m[36m(func pid=111263)[0m f1_weighted: 0.42155535545593553
[2m[36m(func pid=111263)[0m f1_per_class: [0.384, 0.292, 0.774, 0.513, 0.09, 0.318, 0.469, 0.488, 0.252, 0.201]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3637 | Steps: 2 | Val loss: 2.1772 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4384 | Steps: 2 | Val loss: 1.7909 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=111842)[0m top1: 0.22667910447761194
[2m[36m(func pid=111842)[0m top5: 0.7513992537313433
[2m[36m(func pid=111842)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.17148584136481615
[2m[36m(func pid=111842)[0m f1_weighted: 0.2325866596677767
[2m[36m(func pid=111842)[0m f1_per_class: [0.098, 0.295, 0.028, 0.098, 0.051, 0.209, 0.336, 0.435, 0.036, 0.128]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0173 | Steps: 2 | Val loss: 2.1301 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=105422)[0m top1: 0.23227611940298507
[2m[36m(func pid=105422)[0m top5: 0.7313432835820896
[2m[36m(func pid=105422)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=105422)[0m f1_macro: 0.21280383955694654
[2m[36m(func pid=105422)[0m f1_weighted: 0.25614348734749043
[2m[36m(func pid=105422)[0m f1_per_class: [0.203, 0.276, 0.319, 0.256, 0.036, 0.381, 0.242, 0.199, 0.052, 0.164]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:44 (running for 00:12:59.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.364 |      0.213 |                   61 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.438 |      0.35  |                   61 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.017 |      0.378 |                   36 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.648 |      0.171 |                   36 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.365205223880597
[2m[36m(func pid=106010)[0m top5: 0.8889925373134329
[2m[36m(func pid=106010)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=106010)[0m f1_macro: 0.3500341653871134
[2m[36m(func pid=106010)[0m f1_weighted: 0.39830840136616413
[2m[36m(func pid=106010)[0m f1_per_class: [0.3, 0.331, 0.706, 0.459, 0.077, 0.316, 0.444, 0.424, 0.204, 0.241]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.5870 | Steps: 2 | Val loss: 5.4228 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=111263)[0m top1: 0.3927238805970149
[2m[36m(func pid=111263)[0m top5: 0.9011194029850746
[2m[36m(func pid=111263)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=111263)[0m f1_macro: 0.3762544969974838
[2m[36m(func pid=111263)[0m f1_weighted: 0.4210101365155353
[2m[36m(func pid=111263)[0m f1_per_class: [0.405, 0.285, 0.727, 0.53, 0.089, 0.311, 0.455, 0.502, 0.253, 0.207]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3444 | Steps: 2 | Val loss: 2.1733 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4558 | Steps: 2 | Val loss: 1.7809 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=111842)[0m top1: 0.20569029850746268
[2m[36m(func pid=111842)[0m top5: 0.8134328358208955
[2m[36m(func pid=111842)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=111842)[0m f1_macro: 0.1717074393458625
[2m[36m(func pid=111842)[0m f1_weighted: 0.19792326751475672
[2m[36m(func pid=111842)[0m f1_per_class: [0.183, 0.275, 0.158, 0.191, 0.05, 0.136, 0.175, 0.369, 0.051, 0.129]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0117 | Steps: 2 | Val loss: 2.1196 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=105422)[0m top1: 0.23740671641791045
[2m[36m(func pid=105422)[0m top5: 0.7350746268656716
[2m[36m(func pid=105422)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=105422)[0m f1_macro: 0.21649421155405707
[2m[36m(func pid=105422)[0m f1_weighted: 0.262151480483699
[2m[36m(func pid=105422)[0m f1_per_class: [0.204, 0.283, 0.328, 0.262, 0.036, 0.381, 0.253, 0.194, 0.053, 0.17]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:50 (running for 00:13:04.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.344 |      0.216 |                   62 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.456 |      0.35  |                   62 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.017 |      0.376 |                   37 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.587 |      0.172 |                   37 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37406716417910446
[2m[36m(func pid=106010)[0m top5: 0.8894589552238806
[2m[36m(func pid=106010)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=106010)[0m f1_macro: 0.3497412044807929
[2m[36m(func pid=106010)[0m f1_weighted: 0.40890512642616994
[2m[36m(func pid=106010)[0m f1_per_class: [0.272, 0.343, 0.649, 0.477, 0.068, 0.351, 0.438, 0.456, 0.207, 0.236]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7551 | Steps: 2 | Val loss: 5.6316 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=111263)[0m top1: 0.39505597014925375
[2m[36m(func pid=111263)[0m top5: 0.9001865671641791
[2m[36m(func pid=111263)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=111263)[0m f1_macro: 0.3776119557083843
[2m[36m(func pid=111263)[0m f1_weighted: 0.4208845022156673
[2m[36m(func pid=111263)[0m f1_per_class: [0.412, 0.299, 0.727, 0.536, 0.09, 0.298, 0.445, 0.494, 0.261, 0.213]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3410 | Steps: 2 | Val loss: 2.1739 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4059 | Steps: 2 | Val loss: 1.7907 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=111842)[0m top1: 0.23134328358208955
[2m[36m(func pid=111842)[0m top5: 0.8432835820895522
[2m[36m(func pid=111842)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=111842)[0m f1_macro: 0.19538448910778167
[2m[36m(func pid=111842)[0m f1_weighted: 0.2278895463300495
[2m[36m(func pid=111842)[0m f1_per_class: [0.283, 0.16, 0.113, 0.348, 0.083, 0.134, 0.182, 0.337, 0.153, 0.159]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0225 | Steps: 2 | Val loss: 2.1212 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=105422)[0m top1: 0.23694029850746268
[2m[36m(func pid=105422)[0m top5: 0.7378731343283582
[2m[36m(func pid=105422)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=105422)[0m f1_macro: 0.21921347634464045
[2m[36m(func pid=105422)[0m f1_weighted: 0.2613128999626195
[2m[36m(func pid=105422)[0m f1_per_class: [0.197, 0.287, 0.333, 0.258, 0.036, 0.388, 0.246, 0.196, 0.075, 0.175]
[2m[36m(func pid=105422)[0m 
== Status ==
Current time: 2024-01-07 04:22:55 (running for 00:13:09.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.341 |      0.219 |                   63 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.406 |      0.347 |                   63 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.012 |      0.378 |                   38 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.755 |      0.195 |                   38 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3656716417910448
[2m[36m(func pid=106010)[0m top5: 0.8927238805970149
[2m[36m(func pid=106010)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=106010)[0m f1_macro: 0.34716968314978
[2m[36m(func pid=106010)[0m f1_weighted: 0.40444569095224686
[2m[36m(func pid=106010)[0m f1_per_class: [0.216, 0.324, 0.686, 0.469, 0.056, 0.396, 0.428, 0.455, 0.212, 0.229]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4558 | Steps: 2 | Val loss: 5.8617 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=111263)[0m top1: 0.39552238805970147
[2m[36m(func pid=111263)[0m top5: 0.9034514925373134
[2m[36m(func pid=111263)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=111263)[0m f1_macro: 0.383026308271606
[2m[36m(func pid=111263)[0m f1_weighted: 0.4200332936538567
[2m[36m(func pid=111263)[0m f1_per_class: [0.418, 0.326, 0.8, 0.529, 0.084, 0.28, 0.443, 0.478, 0.25, 0.224]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3561 | Steps: 2 | Val loss: 2.1725 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4701 | Steps: 2 | Val loss: 1.8068 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=111842)[0m top1: 0.24766791044776118
[2m[36m(func pid=111842)[0m top5: 0.8148320895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=111842)[0m f1_macro: 0.19980211733013184
[2m[36m(func pid=111842)[0m f1_weighted: 0.2576395673555233
[2m[36m(func pid=111842)[0m f1_per_class: [0.146, 0.291, 0.157, 0.265, 0.067, 0.069, 0.301, 0.465, 0.086, 0.15]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.24207089552238806
[2m[36m(func pid=105422)[0m top5: 0.7322761194029851
[2m[36m(func pid=105422)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=105422)[0m f1_macro: 0.22412658363674734
[2m[36m(func pid=105422)[0m f1_weighted: 0.26753440751713203
[2m[36m(func pid=105422)[0m f1_per_class: [0.195, 0.289, 0.333, 0.256, 0.036, 0.39, 0.263, 0.215, 0.076, 0.188]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0235 | Steps: 2 | Val loss: 2.1195 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:23:00 (running for 00:13:15.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.356 |      0.224 |                   64 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.47  |      0.337 |                   64 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.022 |      0.383 |                   39 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.456 |      0.2   |                   39 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.34468283582089554
[2m[36m(func pid=106010)[0m top5: 0.8875932835820896
[2m[36m(func pid=106010)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=106010)[0m f1_macro: 0.33663415602285224
[2m[36m(func pid=106010)[0m f1_weighted: 0.38384392086293995
[2m[36m(func pid=106010)[0m f1_per_class: [0.186, 0.299, 0.706, 0.433, 0.051, 0.394, 0.412, 0.447, 0.199, 0.239]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9924 | Steps: 2 | Val loss: 5.7307 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=111263)[0m top1: 0.3969216417910448
[2m[36m(func pid=111263)[0m top5: 0.9029850746268657
[2m[36m(func pid=111263)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=111263)[0m f1_macro: 0.38617670955289285
[2m[36m(func pid=111263)[0m f1_weighted: 0.42217946712689614
[2m[36m(func pid=111263)[0m f1_per_class: [0.418, 0.325, 0.828, 0.529, 0.086, 0.279, 0.451, 0.472, 0.248, 0.226]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3726 | Steps: 2 | Val loss: 2.1733 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4152 | Steps: 2 | Val loss: 1.8134 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=111842)[0m top1: 0.29617537313432835
[2m[36m(func pid=111842)[0m top5: 0.8190298507462687
[2m[36m(func pid=111842)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=111842)[0m f1_macro: 0.23429000756292395
[2m[36m(func pid=111842)[0m f1_weighted: 0.30595148491689617
[2m[36m(func pid=111842)[0m f1_per_class: [0.12, 0.356, 0.228, 0.265, 0.101, 0.118, 0.402, 0.451, 0.152, 0.15]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.23787313432835822
[2m[36m(func pid=105422)[0m top5: 0.7341417910447762
[2m[36m(func pid=105422)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=105422)[0m f1_macro: 0.2173466792493449
[2m[36m(func pid=105422)[0m f1_weighted: 0.2642907850659871
[2m[36m(func pid=105422)[0m f1_per_class: [0.191, 0.289, 0.314, 0.263, 0.035, 0.367, 0.259, 0.192, 0.079, 0.183]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0227 | Steps: 2 | Val loss: 2.1067 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:23:05 (running for 00:13:20.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.373 |      0.217 |                   65 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.415 |      0.342 |                   65 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.023 |      0.386 |                   40 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.992 |      0.234 |                   40 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3596082089552239
[2m[36m(func pid=106010)[0m top5: 0.8861940298507462
[2m[36m(func pid=106010)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=106010)[0m f1_macro: 0.34192054202994915
[2m[36m(func pid=106010)[0m f1_weighted: 0.39695208099116236
[2m[36m(func pid=106010)[0m f1_per_class: [0.267, 0.339, 0.649, 0.441, 0.051, 0.327, 0.442, 0.464, 0.21, 0.228]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9001 | Steps: 2 | Val loss: 5.1881 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=111263)[0m top1: 0.40158582089552236
[2m[36m(func pid=111263)[0m top5: 0.9057835820895522
[2m[36m(func pid=111263)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=111263)[0m f1_macro: 0.39033188007858666
[2m[36m(func pid=111263)[0m f1_weighted: 0.4277746056075843
[2m[36m(func pid=111263)[0m f1_per_class: [0.443, 0.325, 0.828, 0.524, 0.087, 0.301, 0.469, 0.454, 0.238, 0.235]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.3695 | Steps: 2 | Val loss: 2.1724 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3706 | Steps: 2 | Val loss: 1.8690 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=111842)[0m top1: 0.3474813432835821
[2m[36m(func pid=111842)[0m top5: 0.8549440298507462
[2m[36m(func pid=111842)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=111842)[0m f1_macro: 0.30696020175676664
[2m[36m(func pid=111842)[0m f1_weighted: 0.34420168701257836
[2m[36m(func pid=111842)[0m f1_per_class: [0.26, 0.336, 0.353, 0.389, 0.104, 0.286, 0.34, 0.431, 0.186, 0.385]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.23694029850746268
[2m[36m(func pid=105422)[0m top5: 0.7327425373134329
[2m[36m(func pid=105422)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=105422)[0m f1_macro: 0.21314352614895077
[2m[36m(func pid=105422)[0m f1_weighted: 0.26323440846378793
[2m[36m(func pid=105422)[0m f1_per_class: [0.189, 0.293, 0.289, 0.264, 0.032, 0.376, 0.252, 0.191, 0.066, 0.18]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0274 | Steps: 2 | Val loss: 2.1151 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=106010)[0m top1: 0.34281716417910446
[2m[36m(func pid=106010)[0m top5: 0.8726679104477612
[2m[36m(func pid=106010)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=106010)[0m f1_macro: 0.3308377020750332
[2m[36m(func pid=106010)[0m f1_weighted: 0.377921547302228
[2m[36m(func pid=106010)[0m f1_per_class: [0.295, 0.317, 0.686, 0.396, 0.059, 0.206, 0.486, 0.425, 0.204, 0.236]
== Status ==
Current time: 2024-01-07 04:23:11 (running for 00:13:25.36)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.37  |      0.213 |                   66 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.371 |      0.331 |                   66 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.023 |      0.39  |                   41 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.9   |      0.307 |                   41 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7143 | Steps: 2 | Val loss: 5.0977 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=111263)[0m top1: 0.4039179104477612
[2m[36m(func pid=111263)[0m top5: 0.9029850746268657
[2m[36m(func pid=111263)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=111263)[0m f1_macro: 0.38254244648916924
[2m[36m(func pid=111263)[0m f1_weighted: 0.43030575628570916
[2m[36m(func pid=111263)[0m f1_per_class: [0.41, 0.319, 0.774, 0.524, 0.088, 0.299, 0.486, 0.445, 0.238, 0.242]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3582 | Steps: 2 | Val loss: 2.1713 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3607 | Steps: 2 | Val loss: 1.8771 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=111842)[0m top1: 0.3498134328358209
[2m[36m(func pid=111842)[0m top5: 0.8512126865671642
[2m[36m(func pid=111842)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.27622518094813636
[2m[36m(func pid=111842)[0m f1_weighted: 0.32879729720656264
[2m[36m(func pid=111842)[0m f1_per_class: [0.257, 0.235, 0.323, 0.511, 0.022, 0.288, 0.253, 0.378, 0.103, 0.392]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m top1: 0.23973880597014927
[2m[36m(func pid=105422)[0m top5: 0.7327425373134329
[2m[36m(func pid=105422)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=105422)[0m f1_macro: 0.21626993832942015
[2m[36m(func pid=105422)[0m f1_weighted: 0.26622914163731665
[2m[36m(func pid=105422)[0m f1_per_class: [0.197, 0.298, 0.289, 0.257, 0.032, 0.373, 0.264, 0.198, 0.076, 0.179]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0252 | Steps: 2 | Val loss: 2.1905 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 04:23:16 (running for 00:13:30.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.358 |      0.216 |                   67 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.361 |      0.324 |                   67 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.027 |      0.383 |                   42 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.714 |      0.276 |                   42 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3358208955223881
[2m[36m(func pid=106010)[0m top5: 0.8708022388059702
[2m[36m(func pid=106010)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=106010)[0m f1_macro: 0.3241288290446177
[2m[36m(func pid=106010)[0m f1_weighted: 0.37033340385744024
[2m[36m(func pid=106010)[0m f1_per_class: [0.287, 0.302, 0.706, 0.38, 0.062, 0.185, 0.499, 0.389, 0.198, 0.233]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7225 | Steps: 2 | Val loss: 5.0006 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=111263)[0m top1: 0.39598880597014924
[2m[36m(func pid=111263)[0m top5: 0.8973880597014925
[2m[36m(func pid=111263)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=111263)[0m f1_macro: 0.3688959862086269
[2m[36m(func pid=111263)[0m f1_weighted: 0.4187836843453389
[2m[36m(func pid=111263)[0m f1_per_class: [0.428, 0.327, 0.774, 0.52, 0.091, 0.306, 0.472, 0.293, 0.242, 0.236]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2947 | Steps: 2 | Val loss: 2.1680 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3036 | Steps: 2 | Val loss: 1.8581 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.35494402985074625
[2m[36m(func pid=111842)[0m top5: 0.8348880597014925
[2m[36m(func pid=111842)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=111842)[0m f1_macro: 0.2618735171630216
[2m[36m(func pid=111842)[0m f1_weighted: 0.30874080990275643
[2m[36m(func pid=111842)[0m f1_per_class: [0.242, 0.172, 0.351, 0.568, 0.055, 0.233, 0.195, 0.358, 0.133, 0.312]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0124 | Steps: 2 | Val loss: 2.2808 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:23:21 (running for 00:13:35.62)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.295 |      0.226 |                   68 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.361 |      0.324 |                   67 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.025 |      0.369 |                   43 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.722 |      0.262 |                   43 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m top1: 0.2462686567164179
[2m[36m(func pid=105422)[0m top5: 0.7378731343283582
[2m[36m(func pid=105422)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=105422)[0m f1_macro: 0.22570588853721868
[2m[36m(func pid=105422)[0m f1_weighted: 0.2686680075697199
[2m[36m(func pid=105422)[0m f1_per_class: [0.209, 0.312, 0.329, 0.257, 0.033, 0.386, 0.252, 0.222, 0.077, 0.179]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=106010)[0m top1: 0.34654850746268656
[2m[36m(func pid=106010)[0m top5: 0.8736007462686567
[2m[36m(func pid=106010)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=106010)[0m f1_macro: 0.3361936457390259
[2m[36m(func pid=106010)[0m f1_weighted: 0.3816868336532797
[2m[36m(func pid=106010)[0m f1_per_class: [0.299, 0.315, 0.706, 0.406, 0.061, 0.217, 0.482, 0.43, 0.218, 0.227]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5069 | Steps: 2 | Val loss: 4.5682 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=111263)[0m top1: 0.375
[2m[36m(func pid=111263)[0m top5: 0.8791977611940298
[2m[36m(func pid=111263)[0m f1_micro: 0.375
[2m[36m(func pid=111263)[0m f1_macro: 0.3394590761068098
[2m[36m(func pid=111263)[0m f1_weighted: 0.3937558878447757
[2m[36m(func pid=111263)[0m f1_per_class: [0.442, 0.318, 0.727, 0.512, 0.056, 0.279, 0.44, 0.144, 0.257, 0.22]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.3709 | Steps: 2 | Val loss: 2.1660 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3165 | Steps: 2 | Val loss: 1.8202 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=111842)[0m top1: 0.35261194029850745
[2m[36m(func pid=111842)[0m top5: 0.8157649253731343
[2m[36m(func pid=111842)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=111842)[0m f1_macro: 0.25582590046071624
[2m[36m(func pid=111842)[0m f1_weighted: 0.29371467981774046
[2m[36m(func pid=111842)[0m f1_per_class: [0.294, 0.056, 0.333, 0.585, 0.117, 0.144, 0.22, 0.371, 0.155, 0.283]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:23:26 (running for 00:13:40.95)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.371 |      0.224 |                   69 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.304 |      0.336 |                   68 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.012 |      0.339 |                   44 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.507 |      0.256 |                   44 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m top1: 0.24486940298507462
[2m[36m(func pid=105422)[0m top5: 0.7397388059701493
[2m[36m(func pid=105422)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=105422)[0m f1_macro: 0.22385137771547967
[2m[36m(func pid=105422)[0m f1_weighted: 0.2681502286593905
[2m[36m(func pid=105422)[0m f1_per_class: [0.206, 0.309, 0.312, 0.255, 0.033, 0.373, 0.257, 0.23, 0.075, 0.188]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0291 | Steps: 2 | Val loss: 2.3355 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=106010)[0m top1: 0.35447761194029853
[2m[36m(func pid=106010)[0m top5: 0.8833955223880597
[2m[36m(func pid=106010)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=106010)[0m f1_macro: 0.3439631722498783
[2m[36m(func pid=106010)[0m f1_weighted: 0.3884041471749079
[2m[36m(func pid=106010)[0m f1_per_class: [0.324, 0.328, 0.706, 0.437, 0.061, 0.264, 0.447, 0.443, 0.218, 0.211]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8817 | Steps: 2 | Val loss: 4.7361 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=111263)[0m top1: 0.3619402985074627
[2m[36m(func pid=111263)[0m top5: 0.8740671641791045
[2m[36m(func pid=111263)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=111263)[0m f1_macro: 0.3219140826137915
[2m[36m(func pid=111263)[0m f1_weighted: 0.3787655541582904
[2m[36m(func pid=111263)[0m f1_per_class: [0.42, 0.322, 0.706, 0.497, 0.057, 0.266, 0.425, 0.061, 0.257, 0.209]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.20615671641791045
[2m[36m(func pid=111842)[0m top5: 0.7430037313432836
[2m[36m(func pid=111842)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=111842)[0m f1_macro: 0.17838862603288294
[2m[36m(func pid=111842)[0m f1_weighted: 0.21838664577438646
[2m[36m(func pid=111842)[0m f1_per_class: [0.155, 0.074, 0.286, 0.422, 0.098, 0.055, 0.176, 0.349, 0.074, 0.095]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2477 | Steps: 2 | Val loss: 2.1631 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3680 | Steps: 2 | Val loss: 1.7787 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:23:31 (running for 00:13:46.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.371 |      0.224 |                   69 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.368 |      0.353 |                   70 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.029 |      0.322 |                   45 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.882 |      0.178 |                   45 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3736007462686567
[2m[36m(func pid=106010)[0m top5: 0.8880597014925373
[2m[36m(func pid=106010)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=106010)[0m f1_macro: 0.35265225032764097
[2m[36m(func pid=106010)[0m f1_weighted: 0.4048282118477029
[2m[36m(func pid=106010)[0m f1_per_class: [0.322, 0.337, 0.686, 0.488, 0.061, 0.334, 0.422, 0.449, 0.211, 0.216]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.24766791044776118
[2m[36m(func pid=105422)[0m top5: 0.7364738805970149
[2m[36m(func pid=105422)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=105422)[0m f1_macro: 0.23030386512065154
[2m[36m(func pid=105422)[0m f1_weighted: 0.27008948585338743
[2m[36m(func pid=105422)[0m f1_per_class: [0.208, 0.307, 0.324, 0.264, 0.037, 0.38, 0.251, 0.244, 0.078, 0.211]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0631 | Steps: 2 | Val loss: 2.4314 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6917 | Steps: 2 | Val loss: 5.0096 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=111263)[0m top1: 0.3269589552238806
[2m[36m(func pid=111263)[0m top5: 0.8880597014925373
[2m[36m(func pid=111263)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=111263)[0m f1_macro: 0.31932135600703954
[2m[36m(func pid=111263)[0m f1_weighted: 0.35421094300470635
[2m[36m(func pid=111263)[0m f1_per_class: [0.314, 0.28, 0.649, 0.435, 0.061, 0.169, 0.397, 0.447, 0.239, 0.202]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.13759328358208955
[2m[36m(func pid=111842)[0m top5: 0.6935634328358209
[2m[36m(func pid=111842)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=111842)[0m f1_macro: 0.14764985163223407
[2m[36m(func pid=111842)[0m f1_weighted: 0.1536773712451667
[2m[36m(func pid=111842)[0m f1_per_class: [0.097, 0.071, 0.244, 0.173, 0.106, 0.107, 0.187, 0.293, 0.087, 0.112]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2991 | Steps: 2 | Val loss: 1.7812 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3230 | Steps: 2 | Val loss: 2.1582 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 04:23:37 (running for 00:13:51.49)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.248 |      0.23  |                   70 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.299 |      0.35  |                   71 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.063 |      0.319 |                   46 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.692 |      0.148 |                   46 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3689365671641791
[2m[36m(func pid=106010)[0m top5: 0.8922574626865671
[2m[36m(func pid=106010)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=106010)[0m f1_macro: 0.3504776931959971
[2m[36m(func pid=106010)[0m f1_weighted: 0.3999978925292337
[2m[36m(func pid=106010)[0m f1_per_class: [0.26, 0.324, 0.727, 0.49, 0.052, 0.379, 0.4, 0.44, 0.207, 0.226]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.25046641791044777
[2m[36m(func pid=105422)[0m top5: 0.7434701492537313
[2m[36m(func pid=105422)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=105422)[0m f1_macro: 0.23129513991975484
[2m[36m(func pid=105422)[0m f1_weighted: 0.272559777030505
[2m[36m(func pid=105422)[0m f1_per_class: [0.216, 0.312, 0.324, 0.27, 0.033, 0.392, 0.247, 0.241, 0.067, 0.211]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8232 | Steps: 2 | Val loss: 4.2231 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0124 | Steps: 2 | Val loss: 3.3708 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=111263)[0m top1: 0.21222014925373134
[2m[36m(func pid=111263)[0m top5: 0.8013059701492538
[2m[36m(func pid=111263)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=111263)[0m f1_macro: 0.2048683433129994
[2m[36m(func pid=111263)[0m f1_weighted: 0.21121990120722087
[2m[36m(func pid=111263)[0m f1_per_class: [0.222, 0.203, 0.4, 0.262, 0.059, 0.016, 0.226, 0.341, 0.155, 0.165]
[2m[36m(func pid=111842)[0m top1: 0.2042910447761194
[2m[36m(func pid=111842)[0m top5: 0.7611940298507462
[2m[36m(func pid=111842)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=111842)[0m f1_macro: 0.21985051896321375
[2m[36m(func pid=111842)[0m f1_weighted: 0.18901886383926828
[2m[36m(func pid=111842)[0m f1_per_class: [0.109, 0.178, 0.247, 0.095, 0.107, 0.413, 0.166, 0.376, 0.142, 0.366]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3146 | Steps: 2 | Val loss: 1.8068 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2877 | Steps: 2 | Val loss: 2.1561 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 04:23:42 (running for 00:13:56.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.323 |      0.231 |                   71 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.315 |      0.344 |                   72 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.012 |      0.205 |                   47 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.823 |      0.22  |                   47 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.34841417910447764
[2m[36m(func pid=106010)[0m top5: 0.8885261194029851
[2m[36m(func pid=106010)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=106010)[0m f1_macro: 0.34401150026490734
[2m[36m(func pid=106010)[0m f1_weighted: 0.3816996816259955
[2m[36m(func pid=106010)[0m f1_per_class: [0.207, 0.299, 0.774, 0.449, 0.053, 0.389, 0.389, 0.452, 0.189, 0.238]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.24953358208955223
[2m[36m(func pid=105422)[0m top5: 0.7458022388059702
[2m[36m(func pid=105422)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=105422)[0m f1_macro: 0.23674370380078305
[2m[36m(func pid=105422)[0m f1_weighted: 0.27133050082379484
[2m[36m(func pid=105422)[0m f1_per_class: [0.217, 0.31, 0.358, 0.267, 0.032, 0.389, 0.243, 0.249, 0.081, 0.22]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0855 | Steps: 2 | Val loss: 3.0238 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5112 | Steps: 2 | Val loss: 3.8896 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=111263)[0m top1: 0.21921641791044777
[2m[36m(func pid=111263)[0m top5: 0.8152985074626866
[2m[36m(func pid=111263)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=111263)[0m f1_macro: 0.19966012459246807
[2m[36m(func pid=111263)[0m f1_weighted: 0.22763052788194227
[2m[36m(func pid=111263)[0m f1_per_class: [0.191, 0.211, 0.255, 0.249, 0.059, 0.023, 0.28, 0.361, 0.218, 0.15]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m top1: 0.22108208955223882
[2m[36m(func pid=111842)[0m top5: 0.8022388059701493
[2m[36m(func pid=111842)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=111842)[0m f1_macro: 0.2286819330304199
[2m[36m(func pid=111842)[0m f1_weighted: 0.21035450542777875
[2m[36m(func pid=111842)[0m f1_per_class: [0.12, 0.278, 0.256, 0.129, 0.135, 0.435, 0.149, 0.347, 0.099, 0.338]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3245 | Steps: 2 | Val loss: 1.8166 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2647 | Steps: 2 | Val loss: 2.1570 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:23:47 (running for 00:14:02.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.288 |      0.237 |                   72 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.324 |      0.341 |                   73 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.085 |      0.2   |                   48 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.511 |      0.229 |                   48 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.34048507462686567
[2m[36m(func pid=106010)[0m top5: 0.8833955223880597
[2m[36m(func pid=106010)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=106010)[0m f1_macro: 0.3408583348178006
[2m[36m(func pid=106010)[0m f1_weighted: 0.37414733802542355
[2m[36m(func pid=106010)[0m f1_per_class: [0.191, 0.269, 0.774, 0.438, 0.066, 0.392, 0.391, 0.454, 0.192, 0.242]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=105422)[0m top1: 0.2462686567164179
[2m[36m(func pid=105422)[0m top5: 0.7439365671641791
[2m[36m(func pid=105422)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=105422)[0m f1_macro: 0.2348044570314372
[2m[36m(func pid=105422)[0m f1_weighted: 0.2665936821580548
[2m[36m(func pid=105422)[0m f1_per_class: [0.216, 0.31, 0.353, 0.259, 0.036, 0.389, 0.234, 0.256, 0.081, 0.214]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0594 | Steps: 2 | Val loss: 2.3567 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.8005 | Steps: 2 | Val loss: 3.5157 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=111842)[0m top1: 0.27238805970149255
[2m[36m(func pid=111842)[0m top5: 0.8558768656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=111842)[0m f1_macro: 0.2742233099134527
[2m[36m(func pid=111842)[0m f1_weighted: 0.27597038878558855
[2m[36m(func pid=111842)[0m f1_per_class: [0.142, 0.367, 0.519, 0.201, 0.141, 0.338, 0.279, 0.376, 0.081, 0.299]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.310634328358209
[2m[36m(func pid=111263)[0m top5: 0.8638059701492538
[2m[36m(func pid=111263)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=111263)[0m f1_macro: 0.2733263819302183
[2m[36m(func pid=111263)[0m f1_weighted: 0.3415873583993864
[2m[36m(func pid=111263)[0m f1_per_class: [0.224, 0.232, 0.407, 0.369, 0.077, 0.126, 0.479, 0.425, 0.224, 0.171]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2637 | Steps: 2 | Val loss: 1.7857 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3304 | Steps: 2 | Val loss: 2.1572 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:23:53 (running for 00:14:07.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00004 | RUNNING    | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.265 |      0.235 |                   73 |
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.264 |      0.347 |                   74 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.059 |      0.273 |                   49 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.8   |      0.274 |                   49 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.36007462686567165
[2m[36m(func pid=106010)[0m top5: 0.8917910447761194
[2m[36m(func pid=106010)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=106010)[0m f1_macro: 0.3474961859503081
[2m[36m(func pid=106010)[0m f1_weighted: 0.3897894930643383
[2m[36m(func pid=106010)[0m f1_per_class: [0.254, 0.302, 0.727, 0.474, 0.065, 0.391, 0.388, 0.456, 0.187, 0.231]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2708 | Steps: 2 | Val loss: 3.6577 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=105422)[0m top1: 0.24766791044776118
[2m[36m(func pid=105422)[0m top5: 0.7434701492537313
[2m[36m(func pid=105422)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=105422)[0m f1_macro: 0.23392376364783457
[2m[36m(func pid=105422)[0m f1_weighted: 0.26913494071856725
[2m[36m(func pid=105422)[0m f1_per_class: [0.211, 0.316, 0.348, 0.262, 0.036, 0.393, 0.236, 0.253, 0.077, 0.206]
[2m[36m(func pid=105422)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0630 | Steps: 2 | Val loss: 2.2445 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=111842)[0m top1: 0.34375
[2m[36m(func pid=111842)[0m top5: 0.8703358208955224
[2m[36m(func pid=111842)[0m f1_micro: 0.34375
[2m[36m(func pid=111842)[0m f1_macro: 0.29269882202267583
[2m[36m(func pid=111842)[0m f1_weighted: 0.36243799391664183
[2m[36m(func pid=111842)[0m f1_per_class: [0.166, 0.396, 0.375, 0.285, 0.116, 0.298, 0.478, 0.437, 0.105, 0.273]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.34375
[2m[36m(func pid=111263)[0m top5: 0.8805970149253731
[2m[36m(func pid=111263)[0m f1_micro: 0.34375
[2m[36m(func pid=111263)[0m f1_macro: 0.3346783673126796
[2m[36m(func pid=111263)[0m f1_weighted: 0.38101071499328437
[2m[36m(func pid=111263)[0m f1_per_class: [0.265, 0.235, 0.774, 0.408, 0.075, 0.222, 0.53, 0.396, 0.216, 0.225]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3576 | Steps: 2 | Val loss: 1.7798 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=105422)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.3300 | Steps: 2 | Val loss: 2.1551 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:23:58 (running for 00:14:13.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.264 |      0.347 |                   74 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.063 |      0.335 |                   50 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.271 |      0.293 |                   50 |
| train_35a0b_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=105422)[0m top1: 0.25093283582089554
[2m[36m(func pid=105422)[0m top5: 0.7448694029850746
[2m[36m(func pid=105422)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=105422)[0m f1_macro: 0.23290991893357935
[2m[36m(func pid=105422)[0m f1_weighted: 0.27202071984377446
[2m[36m(func pid=105422)[0m f1_per_class: [0.206, 0.321, 0.333, 0.266, 0.033, 0.393, 0.24, 0.261, 0.066, 0.21]
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6198 | Steps: 2 | Val loss: 3.5754 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=106010)[0m top1: 0.3843283582089552
[2m[36m(func pid=106010)[0m top5: 0.8941231343283582
[2m[36m(func pid=106010)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=106010)[0m f1_macro: 0.3652925521805898
[2m[36m(func pid=106010)[0m f1_weighted: 0.4146053778862079
[2m[36m(func pid=106010)[0m f1_per_class: [0.337, 0.341, 0.667, 0.507, 0.065, 0.371, 0.41, 0.482, 0.242, 0.233]
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0534 | Steps: 2 | Val loss: 2.3833 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.34421641791044777
[2m[36m(func pid=111842)[0m top5: 0.8652052238805971
[2m[36m(func pid=111842)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=111842)[0m f1_macro: 0.30859354489805835
[2m[36m(func pid=111842)[0m f1_weighted: 0.35895570749517486
[2m[36m(func pid=111842)[0m f1_per_class: [0.216, 0.381, 0.571, 0.383, 0.067, 0.263, 0.384, 0.49, 0.057, 0.273]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.32509328358208955
[2m[36m(func pid=111263)[0m top5: 0.871268656716418
[2m[36m(func pid=111263)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=111263)[0m f1_macro: 0.34112569782734814
[2m[36m(func pid=111263)[0m f1_weighted: 0.3627195071044946
[2m[36m(func pid=111263)[0m f1_per_class: [0.355, 0.251, 0.889, 0.4, 0.085, 0.251, 0.462, 0.326, 0.234, 0.159]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2750 | Steps: 2 | Val loss: 1.7851 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5819 | Steps: 2 | Val loss: 3.6208 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0310 | Steps: 2 | Val loss: 2.7162 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=106010)[0m top1: 0.3694029850746269
[2m[36m(func pid=106010)[0m top5: 0.8917910447761194
[2m[36m(func pid=106010)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=106010)[0m f1_macro: 0.3546029935770529
[2m[36m(func pid=106010)[0m f1_weighted: 0.40180639606283164
[2m[36m(func pid=106010)[0m f1_per_class: [0.356, 0.332, 0.706, 0.477, 0.058, 0.288, 0.44, 0.442, 0.234, 0.214]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3148320895522388
[2m[36m(func pid=111842)[0m top5: 0.8330223880597015
[2m[36m(func pid=111842)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=111842)[0m f1_macro: 0.2718991329839498
[2m[36m(func pid=111842)[0m f1_weighted: 0.30958806214712475
[2m[36m(func pid=111842)[0m f1_per_class: [0.283, 0.374, 0.348, 0.418, 0.065, 0.213, 0.213, 0.483, 0.038, 0.286]
== Status ==
Current time: 2024-01-07 04:24:05 (running for 00:14:19.37)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.275 |      0.355 |                   76 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.053 |      0.341 |                   51 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.62  |      0.309 |                   51 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=122997)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=122997)[0m Configuration completed!
[2m[36m(func pid=122997)[0m New optimizer parameters:
[2m[36m(func pid=122997)[0m SGD (
[2m[36m(func pid=122997)[0m Parameter Group 0
[2m[36m(func pid=122997)[0m     dampening: 0
[2m[36m(func pid=122997)[0m     differentiable: False
[2m[36m(func pid=122997)[0m     foreach: None
[2m[36m(func pid=122997)[0m     lr: 0.0001
[2m[36m(func pid=122997)[0m     maximize: False
[2m[36m(func pid=122997)[0m     momentum: 0.99
[2m[36m(func pid=122997)[0m     nesterov: False
[2m[36m(func pid=122997)[0m     weight_decay: 0.0001
[2m[36m(func pid=122997)[0m )
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111263)[0m top1: 0.28218283582089554
[2m[36m(func pid=111263)[0m top5: 0.8316231343283582
[2m[36m(func pid=111263)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=111263)[0m f1_macro: 0.29988509485059545
[2m[36m(func pid=111263)[0m f1_weighted: 0.3140490569821816
[2m[36m(func pid=111263)[0m f1_per_class: [0.393, 0.278, 0.75, 0.345, 0.077, 0.206, 0.37, 0.256, 0.212, 0.112]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2533 | Steps: 2 | Val loss: 1.7869 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4419 | Steps: 2 | Val loss: 4.0206 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0397 | Steps: 2 | Val loss: 2.8265 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:24:10 (running for 00:14:24.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.253 |      0.354 |                   77 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.031 |      0.3   |                   52 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.582 |      0.272 |                   52 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37453358208955223
[2m[36m(func pid=106010)[0m top5: 0.8894589552238806
[2m[36m(func pid=106010)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=106010)[0m f1_macro: 0.35445108198235514
[2m[36m(func pid=106010)[0m f1_weighted: 0.4061326999913669
[2m[36m(func pid=106010)[0m f1_per_class: [0.358, 0.341, 0.706, 0.477, 0.061, 0.274, 0.456, 0.436, 0.224, 0.212]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9470 | Steps: 2 | Val loss: 2.3189 | Batch size: 32 | lr: 0.0001 | Duration: 4.52s
[2m[36m(func pid=111842)[0m top1: 0.2933768656716418
[2m[36m(func pid=111842)[0m top5: 0.7826492537313433
[2m[36m(func pid=111842)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=111842)[0m f1_macro: 0.24153372333407447
[2m[36m(func pid=111842)[0m f1_weighted: 0.29088562858932754
[2m[36m(func pid=111842)[0m f1_per_class: [0.26, 0.359, 0.259, 0.394, 0.115, 0.224, 0.209, 0.341, 0.035, 0.219]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.28451492537313433
[2m[36m(func pid=111263)[0m top5: 0.8106343283582089
[2m[36m(func pid=111263)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=111263)[0m f1_macro: 0.29994375087308295
[2m[36m(func pid=111263)[0m f1_weighted: 0.31492890613430796
[2m[36m(func pid=111263)[0m f1_per_class: [0.417, 0.305, 0.75, 0.336, 0.084, 0.196, 0.372, 0.234, 0.205, 0.099]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2848 | Steps: 2 | Val loss: 1.7773 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=122997)[0m top1: 0.17350746268656717
[2m[36m(func pid=122997)[0m top5: 0.5340485074626866
[2m[36m(func pid=122997)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=122997)[0m f1_macro: 0.11867199521287645
[2m[36m(func pid=122997)[0m f1_weighted: 0.12254784891034151
[2m[36m(func pid=122997)[0m f1_per_class: [0.312, 0.343, 0.0, 0.09, 0.0, 0.213, 0.015, 0.013, 0.0, 0.2]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2980 | Steps: 2 | Val loss: 3.3931 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 04:24:15 (running for 00:14:29.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.285 |      0.362 |                   78 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.04  |      0.3   |                   53 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.442 |      0.242 |                   53 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.947 |      0.119 |                    1 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.38152985074626866
[2m[36m(func pid=106010)[0m top5: 0.8899253731343284
[2m[36m(func pid=106010)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=106010)[0m f1_macro: 0.36168376345765907
[2m[36m(func pid=106010)[0m f1_weighted: 0.41487771319632843
[2m[36m(func pid=106010)[0m f1_per_class: [0.344, 0.336, 0.706, 0.484, 0.062, 0.301, 0.466, 0.465, 0.225, 0.228]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0193 | Steps: 2 | Val loss: 2.7328 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9671 | Steps: 2 | Val loss: 2.3297 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=111842)[0m top1: 0.34328358208955223
[2m[36m(func pid=111842)[0m top5: 0.8526119402985075
[2m[36m(func pid=111842)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=111842)[0m f1_macro: 0.27709020883467106
[2m[36m(func pid=111842)[0m f1_weighted: 0.35150016803230677
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.372, 0.232, 0.375, 0.132, 0.293, 0.374, 0.466, 0.105, 0.421]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.2933768656716418
[2m[36m(func pid=111263)[0m top5: 0.8292910447761194
[2m[36m(func pid=111263)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=111263)[0m f1_macro: 0.301642492663762
[2m[36m(func pid=111263)[0m f1_weighted: 0.32682405681658805
[2m[36m(func pid=111263)[0m f1_per_class: [0.384, 0.303, 0.75, 0.341, 0.076, 0.211, 0.407, 0.237, 0.19, 0.118]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2725 | Steps: 2 | Val loss: 1.7820 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=122997)[0m top1: 0.17817164179104478
[2m[36m(func pid=122997)[0m top5: 0.5237873134328358
[2m[36m(func pid=122997)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=122997)[0m f1_macro: 0.10427222712240762
[2m[36m(func pid=122997)[0m f1_weighted: 0.12627004650064025
[2m[36m(func pid=122997)[0m f1_per_class: [0.187, 0.32, 0.0, 0.106, 0.011, 0.274, 0.012, 0.023, 0.0, 0.109]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3716 | Steps: 2 | Val loss: 3.3867 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:24:20 (running for 00:14:35.20)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.273 |      0.357 |                   79 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.019 |      0.302 |                   54 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.298 |      0.277 |                   54 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.967 |      0.104 |                    2 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.38526119402985076
[2m[36m(func pid=106010)[0m top5: 0.8885261194029851
[2m[36m(func pid=106010)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=106010)[0m f1_macro: 0.35653944273134114
[2m[36m(func pid=106010)[0m f1_weighted: 0.4199106863925933
[2m[36m(func pid=106010)[0m f1_per_class: [0.308, 0.34, 0.649, 0.496, 0.068, 0.327, 0.463, 0.469, 0.212, 0.233]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0237 | Steps: 2 | Val loss: 2.4892 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9800 | Steps: 2 | Val loss: 2.3407 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=111842)[0m top1: 0.34281716417910446
[2m[36m(func pid=111842)[0m top5: 0.8759328358208955
[2m[36m(func pid=111842)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=111842)[0m f1_macro: 0.27679423372446454
[2m[36m(func pid=111842)[0m f1_weighted: 0.3464766848298565
[2m[36m(func pid=111842)[0m f1_per_class: [0.0, 0.357, 0.25, 0.338, 0.139, 0.344, 0.384, 0.475, 0.072, 0.41]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.32649253731343286
[2m[36m(func pid=111263)[0m top5: 0.8661380597014925
[2m[36m(func pid=111263)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=111263)[0m f1_macro: 0.3211597807044763
[2m[36m(func pid=111263)[0m f1_weighted: 0.36294838022363024
[2m[36m(func pid=111263)[0m f1_per_class: [0.319, 0.303, 0.75, 0.352, 0.078, 0.241, 0.494, 0.298, 0.211, 0.166]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2555 | Steps: 2 | Val loss: 1.7948 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=122997)[0m top1: 0.17257462686567165
[2m[36m(func pid=122997)[0m top5: 0.5083955223880597
[2m[36m(func pid=122997)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=122997)[0m f1_macro: 0.09923328866405864
[2m[36m(func pid=122997)[0m f1_weighted: 0.12105031184627381
[2m[36m(func pid=122997)[0m f1_per_class: [0.184, 0.294, 0.0, 0.101, 0.011, 0.303, 0.006, 0.02, 0.0, 0.074]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3954 | Steps: 2 | Val loss: 3.4408 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 04:24:26 (running for 00:14:40.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.256 |      0.355 |                   80 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.024 |      0.321 |                   55 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.372 |      0.277 |                   55 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.98  |      0.099 |                    3 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3787313432835821
[2m[36m(func pid=106010)[0m top5: 0.8908582089552238
[2m[36m(func pid=106010)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=106010)[0m f1_macro: 0.35520243645615107
[2m[36m(func pid=106010)[0m f1_weighted: 0.41420691221691064
[2m[36m(func pid=106010)[0m f1_per_class: [0.28, 0.347, 0.667, 0.491, 0.068, 0.357, 0.436, 0.459, 0.222, 0.225]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0468 | Steps: 2 | Val loss: 2.2452 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9524 | Steps: 2 | Val loss: 2.3473 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=111842)[0m top1: 0.35074626865671643
[2m[36m(func pid=111842)[0m top5: 0.863339552238806
[2m[36m(func pid=111842)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=111842)[0m f1_macro: 0.28886706091907854
[2m[36m(func pid=111842)[0m f1_weighted: 0.3590671980576453
[2m[36m(func pid=111842)[0m f1_per_class: [0.022, 0.322, 0.338, 0.351, 0.209, 0.38, 0.417, 0.492, 0.072, 0.286]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m top1: 0.37220149253731344
[2m[36m(func pid=111263)[0m top5: 0.9015858208955224
[2m[36m(func pid=111263)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=111263)[0m f1_macro: 0.3623358028203091
[2m[36m(func pid=111263)[0m f1_weighted: 0.40577621144043824
[2m[36m(func pid=111263)[0m f1_per_class: [0.311, 0.329, 0.889, 0.421, 0.084, 0.23, 0.539, 0.388, 0.221, 0.212]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2247 | Steps: 2 | Val loss: 1.8044 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=122997)[0m top1: 0.1623134328358209
[2m[36m(func pid=122997)[0m top5: 0.5046641791044776
[2m[36m(func pid=122997)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=122997)[0m f1_macro: 0.09290028263490649
[2m[36m(func pid=122997)[0m f1_weighted: 0.1168912170427706
[2m[36m(func pid=122997)[0m f1_per_class: [0.148, 0.272, 0.0, 0.098, 0.01, 0.292, 0.015, 0.018, 0.0, 0.077]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2443 | Steps: 2 | Val loss: 3.4386 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 04:24:31 (running for 00:14:45.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.225 |      0.351 |                   81 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.047 |      0.362 |                   56 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.395 |      0.289 |                   56 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.952 |      0.093 |                    4 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.373134328358209
[2m[36m(func pid=106010)[0m top5: 0.8913246268656716
[2m[36m(func pid=106010)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=106010)[0m f1_macro: 0.3509400897530567
[2m[36m(func pid=106010)[0m f1_weighted: 0.4103016008814636
[2m[36m(func pid=106010)[0m f1_per_class: [0.248, 0.334, 0.667, 0.483, 0.068, 0.369, 0.435, 0.453, 0.232, 0.22]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0104 | Steps: 2 | Val loss: 2.1384 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.3498134328358209
[2m[36m(func pid=111842)[0m top5: 0.8535447761194029
[2m[36m(func pid=111842)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.2865477128879073
[2m[36m(func pid=111842)[0m f1_weighted: 0.36086420088062254
[2m[36m(func pid=111842)[0m f1_per_class: [0.099, 0.258, 0.293, 0.438, 0.126, 0.396, 0.368, 0.494, 0.081, 0.315]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9492 | Steps: 2 | Val loss: 2.3618 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=111263)[0m top1: 0.394589552238806
[2m[36m(func pid=111263)[0m top5: 0.9029850746268657
[2m[36m(func pid=111263)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=111263)[0m f1_macro: 0.35288847766191866
[2m[36m(func pid=111263)[0m f1_weighted: 0.41425733394783715
[2m[36m(func pid=111263)[0m f1_per_class: [0.347, 0.376, 0.889, 0.505, 0.065, 0.142, 0.527, 0.214, 0.199, 0.264]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2273 | Steps: 2 | Val loss: 1.8071 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=122997)[0m top1: 0.15205223880597016
[2m[36m(func pid=122997)[0m top5: 0.49580223880597013
[2m[36m(func pid=122997)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=122997)[0m f1_macro: 0.08321042222684417
[2m[36m(func pid=122997)[0m f1_weighted: 0.11229330420768081
[2m[36m(func pid=122997)[0m f1_per_class: [0.108, 0.253, 0.0, 0.093, 0.01, 0.292, 0.018, 0.023, 0.0, 0.036]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2058 | Steps: 2 | Val loss: 3.5818 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 04:24:36 (running for 00:14:51.02)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.227 |      0.351 |                   82 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.01  |      0.353 |                   57 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.244 |      0.287 |                   57 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.949 |      0.083 |                    5 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3656716417910448
[2m[36m(func pid=106010)[0m top5: 0.8941231343283582
[2m[36m(func pid=106010)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=106010)[0m f1_macro: 0.35145052892881734
[2m[36m(func pid=106010)[0m f1_weighted: 0.4008943531117137
[2m[36m(func pid=106010)[0m f1_per_class: [0.238, 0.335, 0.706, 0.474, 0.068, 0.368, 0.409, 0.481, 0.212, 0.222]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0573 | Steps: 2 | Val loss: 2.3318 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=111842)[0m top1: 0.3423507462686567
[2m[36m(func pid=111842)[0m top5: 0.8348880597014925
[2m[36m(func pid=111842)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=111842)[0m f1_macro: 0.2880902387725308
[2m[36m(func pid=111842)[0m f1_weighted: 0.35032392646484617
[2m[36m(func pid=111842)[0m f1_per_class: [0.179, 0.18, 0.333, 0.489, 0.092, 0.385, 0.325, 0.505, 0.087, 0.305]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9101 | Steps: 2 | Val loss: 2.3676 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=111263)[0m top1: 0.3558768656716418
[2m[36m(func pid=111263)[0m top5: 0.8847947761194029
[2m[36m(func pid=111263)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=111263)[0m f1_macro: 0.3322835440858062
[2m[36m(func pid=111263)[0m f1_weighted: 0.3698485050798227
[2m[36m(func pid=111263)[0m f1_per_class: [0.4, 0.385, 0.857, 0.393, 0.049, 0.113, 0.497, 0.156, 0.223, 0.25]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2242 | Steps: 2 | Val loss: 1.8061 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=122997)[0m top1: 0.14505597014925373
[2m[36m(func pid=122997)[0m top5: 0.49720149253731344
[2m[36m(func pid=122997)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=122997)[0m f1_macro: 0.0848595682116953
[2m[36m(func pid=122997)[0m f1_weighted: 0.11398241472452843
[2m[36m(func pid=122997)[0m f1_per_class: [0.092, 0.222, 0.059, 0.098, 0.0, 0.29, 0.035, 0.041, 0.013, 0.0]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1490 | Steps: 2 | Val loss: 3.7959 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:24:42 (running for 00:14:56.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.224 |      0.353 |                   83 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.057 |      0.332 |                   58 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.206 |      0.288 |                   58 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.91  |      0.085 |                    6 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3656716417910448
[2m[36m(func pid=106010)[0m top5: 0.8969216417910447
[2m[36m(func pid=106010)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=106010)[0m f1_macro: 0.35317507414978677
[2m[36m(func pid=106010)[0m f1_weighted: 0.3998194291545949
[2m[36m(func pid=106010)[0m f1_per_class: [0.246, 0.337, 0.706, 0.465, 0.075, 0.374, 0.409, 0.492, 0.199, 0.23]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2153 | Steps: 2 | Val loss: 2.4970 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=111842)[0m top1: 0.33348880597014924
[2m[36m(func pid=111842)[0m top5: 0.8208955223880597
[2m[36m(func pid=111842)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=111842)[0m f1_macro: 0.277735927320363
[2m[36m(func pid=111842)[0m f1_weighted: 0.344320286984992
[2m[36m(func pid=111842)[0m f1_per_class: [0.175, 0.163, 0.25, 0.487, 0.079, 0.406, 0.314, 0.486, 0.08, 0.337]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9000 | Steps: 2 | Val loss: 2.3638 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=111263)[0m top1: 0.3162313432835821
[2m[36m(func pid=111263)[0m top5: 0.8815298507462687
[2m[36m(func pid=111263)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=111263)[0m f1_macro: 0.33657463124921205
[2m[36m(func pid=111263)[0m f1_weighted: 0.33834485764501776
[2m[36m(func pid=111263)[0m f1_per_class: [0.312, 0.381, 0.889, 0.323, 0.051, 0.281, 0.373, 0.304, 0.207, 0.245]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2574 | Steps: 2 | Val loss: 1.8113 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=122997)[0m top1: 0.13992537313432835
[2m[36m(func pid=122997)[0m top5: 0.5060634328358209
[2m[36m(func pid=122997)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=122997)[0m f1_macro: 0.08203717459598868
[2m[36m(func pid=122997)[0m f1_weighted: 0.11637065408480904
[2m[36m(func pid=122997)[0m f1_per_class: [0.062, 0.206, 0.054, 0.099, 0.009, 0.277, 0.057, 0.046, 0.011, 0.0]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5075 | Steps: 2 | Val loss: 3.7964 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2255 | Steps: 2 | Val loss: 2.7247 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:24:47 (running for 00:15:01.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.257 |      0.351 |                   84 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.215 |      0.337 |                   59 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.149 |      0.278 |                   59 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.9   |      0.082 |                    7 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3670708955223881
[2m[36m(func pid=106010)[0m top5: 0.8941231343283582
[2m[36m(func pid=106010)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=106010)[0m f1_macro: 0.3513795815664881
[2m[36m(func pid=106010)[0m f1_weighted: 0.4016989449972418
[2m[36m(func pid=106010)[0m f1_per_class: [0.248, 0.352, 0.686, 0.451, 0.064, 0.38, 0.42, 0.483, 0.195, 0.235]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3591417910447761
[2m[36m(func pid=111842)[0m top5: 0.8339552238805971
[2m[36m(func pid=111842)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=111842)[0m f1_macro: 0.27994308009021135
[2m[36m(func pid=111842)[0m f1_weighted: 0.38181721499352445
[2m[36m(func pid=111842)[0m f1_per_class: [0.177, 0.157, 0.19, 0.485, 0.047, 0.421, 0.449, 0.437, 0.09, 0.346]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8934 | Steps: 2 | Val loss: 2.3538 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=111263)[0m top1: 0.25466417910447764
[2m[36m(func pid=111263)[0m top5: 0.7952425373134329
[2m[36m(func pid=111263)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=111263)[0m f1_macro: 0.27394757584937846
[2m[36m(func pid=111263)[0m f1_weighted: 0.2665280156337909
[2m[36m(func pid=111263)[0m f1_per_class: [0.253, 0.328, 0.828, 0.364, 0.029, 0.253, 0.175, 0.169, 0.153, 0.187]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2125 | Steps: 2 | Val loss: 1.8122 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0643 | Steps: 2 | Val loss: 4.2444 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=122997)[0m top1: 0.1417910447761194
[2m[36m(func pid=122997)[0m top5: 0.5186567164179104
[2m[36m(func pid=122997)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=122997)[0m f1_macro: 0.08406508778145337
[2m[36m(func pid=122997)[0m f1_weighted: 0.12119857440163075
[2m[36m(func pid=122997)[0m f1_per_class: [0.053, 0.182, 0.051, 0.109, 0.009, 0.288, 0.07, 0.059, 0.019, 0.0]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0876 | Steps: 2 | Val loss: 2.5448 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:24:52 (running for 00:15:07.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.212 |      0.352 |                   85 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.226 |      0.274 |                   60 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.508 |      0.28  |                   60 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.893 |      0.084 |                    8 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3689365671641791
[2m[36m(func pid=106010)[0m top5: 0.8899253731343284
[2m[36m(func pid=106010)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=106010)[0m f1_macro: 0.3519000010894687
[2m[36m(func pid=106010)[0m f1_weighted: 0.40287220836477217
[2m[36m(func pid=106010)[0m f1_per_class: [0.255, 0.355, 0.686, 0.453, 0.072, 0.371, 0.424, 0.471, 0.204, 0.229]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.33908582089552236
[2m[36m(func pid=111842)[0m top5: 0.8306902985074627
[2m[36m(func pid=111842)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=111842)[0m f1_macro: 0.25706558223226317
[2m[36m(func pid=111842)[0m f1_weighted: 0.3731007120913828
[2m[36m(func pid=111842)[0m f1_per_class: [0.169, 0.165, 0.13, 0.428, 0.048, 0.368, 0.505, 0.398, 0.046, 0.313]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8936 | Steps: 2 | Val loss: 2.3466 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=111263)[0m top1: 0.28218283582089554
[2m[36m(func pid=111263)[0m top5: 0.8190298507462687
[2m[36m(func pid=111263)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=111263)[0m f1_macro: 0.2695314182104155
[2m[36m(func pid=111263)[0m f1_weighted: 0.29160971304503763
[2m[36m(func pid=111263)[0m f1_per_class: [0.308, 0.305, 0.774, 0.44, 0.033, 0.115, 0.252, 0.159, 0.184, 0.124]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2282 | Steps: 2 | Val loss: 1.8133 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.4004 | Steps: 2 | Val loss: 4.2065 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=122997)[0m top1: 0.14458955223880596
[2m[36m(func pid=122997)[0m top5: 0.5335820895522388
[2m[36m(func pid=122997)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=122997)[0m f1_macro: 0.09051664862576767
[2m[36m(func pid=122997)[0m f1_weighted: 0.12935057553516965
[2m[36m(func pid=122997)[0m f1_per_class: [0.058, 0.175, 0.095, 0.124, 0.008, 0.287, 0.088, 0.061, 0.009, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:24:57 (running for 00:15:12.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.228 |      0.356 |                   86 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.088 |      0.27  |                   61 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.064 |      0.257 |                   61 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.894 |      0.091 |                    9 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3736007462686567
[2m[36m(func pid=106010)[0m top5: 0.8843283582089553
[2m[36m(func pid=106010)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=106010)[0m f1_macro: 0.356059146580114
[2m[36m(func pid=106010)[0m f1_weighted: 0.40629136811747735
[2m[36m(func pid=106010)[0m f1_per_class: [0.27, 0.355, 0.686, 0.46, 0.074, 0.355, 0.432, 0.473, 0.225, 0.231]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0359 | Steps: 2 | Val loss: 2.2954 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=111842)[0m top1: 0.30783582089552236
[2m[36m(func pid=111842)[0m top5: 0.8274253731343284
[2m[36m(func pid=111842)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=111842)[0m f1_macro: 0.25658543589307625
[2m[36m(func pid=111842)[0m f1_weighted: 0.3278434128638074
[2m[36m(func pid=111842)[0m f1_per_class: [0.172, 0.204, 0.151, 0.417, 0.094, 0.324, 0.347, 0.405, 0.097, 0.355]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8386 | Steps: 2 | Val loss: 2.3356 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=111263)[0m top1: 0.33861940298507465
[2m[36m(func pid=111263)[0m top5: 0.8479477611940298
[2m[36m(func pid=111263)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=111263)[0m f1_macro: 0.3001880227398168
[2m[36m(func pid=111263)[0m f1_weighted: 0.3560879223074177
[2m[36m(func pid=111263)[0m f1_per_class: [0.38, 0.306, 0.706, 0.491, 0.032, 0.045, 0.419, 0.268, 0.207, 0.147]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2407 | Steps: 2 | Val loss: 1.8260 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1739 | Steps: 2 | Val loss: 4.1126 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=122997)[0m top1: 0.1515858208955224
[2m[36m(func pid=122997)[0m top5: 0.5396455223880597
[2m[36m(func pid=122997)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=122997)[0m f1_macro: 0.10257221321167176
[2m[36m(func pid=122997)[0m f1_weighted: 0.14061828875505614
[2m[36m(func pid=122997)[0m f1_per_class: [0.065, 0.172, 0.163, 0.137, 0.008, 0.294, 0.11, 0.058, 0.019, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:03 (running for 00:15:17.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.241 |      0.355 |                   87 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.036 |      0.3   |                   62 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  1.4   |      0.257 |                   62 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.839 |      0.103 |                   10 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.36986940298507465
[2m[36m(func pid=106010)[0m top5: 0.8847947761194029
[2m[36m(func pid=106010)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=106010)[0m f1_macro: 0.3548530455311074
[2m[36m(func pid=106010)[0m f1_weighted: 0.4036824102136592
[2m[36m(func pid=106010)[0m f1_per_class: [0.251, 0.355, 0.686, 0.452, 0.071, 0.369, 0.427, 0.462, 0.229, 0.247]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1764 | Steps: 2 | Val loss: 2.2008 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=111842)[0m top1: 0.28125
[2m[36m(func pid=111842)[0m top5: 0.8456156716417911
[2m[36m(func pid=111842)[0m f1_micro: 0.28125
[2m[36m(func pid=111842)[0m f1_macro: 0.2568585945458955
[2m[36m(func pid=111842)[0m f1_weighted: 0.2715298311800709
[2m[36m(func pid=111842)[0m f1_per_class: [0.207, 0.253, 0.296, 0.433, 0.089, 0.227, 0.148, 0.374, 0.128, 0.414]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8162 | Steps: 2 | Val loss: 2.3196 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=111263)[0m top1: 0.3787313432835821
[2m[36m(func pid=111263)[0m top5: 0.8292910447761194
[2m[36m(func pid=111263)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=111263)[0m f1_macro: 0.31748082297714797
[2m[36m(func pid=111263)[0m f1_weighted: 0.4016979527124783
[2m[36m(func pid=111263)[0m f1_per_class: [0.491, 0.283, 0.533, 0.549, 0.034, 0.045, 0.509, 0.356, 0.211, 0.165]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.1912 | Steps: 2 | Val loss: 1.8161 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4055 | Steps: 2 | Val loss: 4.4223 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=122997)[0m top1: 0.1599813432835821
[2m[36m(func pid=122997)[0m top5: 0.5578358208955224
[2m[36m(func pid=122997)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=122997)[0m f1_macro: 0.11289021500020902
[2m[36m(func pid=122997)[0m f1_weighted: 0.14971643337889726
[2m[36m(func pid=122997)[0m f1_per_class: [0.071, 0.181, 0.179, 0.142, 0.022, 0.306, 0.118, 0.091, 0.019, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:08 (running for 00:15:22.94)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.191 |      0.357 |                   88 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.176 |      0.317 |                   63 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.174 |      0.257 |                   63 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.816 |      0.113 |                   11 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3736007462686567
[2m[36m(func pid=106010)[0m top5: 0.8875932835820896
[2m[36m(func pid=106010)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=106010)[0m f1_macro: 0.35712529122075926
[2m[36m(func pid=106010)[0m f1_weighted: 0.406334162091021
[2m[36m(func pid=106010)[0m f1_per_class: [0.249, 0.35, 0.706, 0.463, 0.077, 0.367, 0.428, 0.471, 0.217, 0.242]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0396 | Steps: 2 | Val loss: 2.2672 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.24580223880597016
[2m[36m(func pid=111842)[0m top5: 0.8498134328358209
[2m[36m(func pid=111842)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=111842)[0m f1_macro: 0.2729226385367418
[2m[36m(func pid=111842)[0m f1_weighted: 0.25036423400915614
[2m[36m(func pid=111842)[0m f1_per_class: [0.278, 0.197, 0.593, 0.388, 0.059, 0.158, 0.173, 0.349, 0.112, 0.423]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8265 | Steps: 2 | Val loss: 2.3008 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=111263)[0m top1: 0.37033582089552236
[2m[36m(func pid=111263)[0m top5: 0.8274253731343284
[2m[36m(func pid=111263)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=111263)[0m f1_macro: 0.2946674683852001
[2m[36m(func pid=111263)[0m f1_weighted: 0.39512595855034377
[2m[36m(func pid=111263)[0m f1_per_class: [0.233, 0.276, 0.444, 0.519, 0.056, 0.06, 0.521, 0.4, 0.211, 0.227]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.1945 | Steps: 2 | Val loss: 1.8134 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7916 | Steps: 2 | Val loss: 4.1439 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=122997)[0m top1: 0.1646455223880597
[2m[36m(func pid=122997)[0m top5: 0.5867537313432836
[2m[36m(func pid=122997)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=122997)[0m f1_macro: 0.1204436957629337
[2m[36m(func pid=122997)[0m f1_weighted: 0.16052916649242271
[2m[36m(func pid=122997)[0m f1_per_class: [0.077, 0.178, 0.222, 0.156, 0.021, 0.303, 0.146, 0.072, 0.029, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:13 (running for 00:15:28.20)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.194 |      0.356 |                   89 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.04  |      0.295 |                   64 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.405 |      0.273 |                   64 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.826 |      0.12  |                   12 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3712686567164179
[2m[36m(func pid=106010)[0m top5: 0.8852611940298507
[2m[36m(func pid=106010)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=106010)[0m f1_macro: 0.35583602828559785
[2m[36m(func pid=106010)[0m f1_weighted: 0.4049368780224294
[2m[36m(func pid=106010)[0m f1_per_class: [0.241, 0.344, 0.706, 0.459, 0.077, 0.368, 0.43, 0.478, 0.215, 0.241]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0368 | Steps: 2 | Val loss: 2.3733 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=111842)[0m top1: 0.2896455223880597
[2m[36m(func pid=111842)[0m top5: 0.8260261194029851
[2m[36m(func pid=111842)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=111842)[0m f1_macro: 0.284200769027158
[2m[36m(func pid=111842)[0m f1_weighted: 0.32571272237274335
[2m[36m(func pid=111842)[0m f1_per_class: [0.229, 0.162, 0.6, 0.336, 0.054, 0.228, 0.47, 0.391, 0.099, 0.273]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8072 | Steps: 2 | Val loss: 2.2885 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=111263)[0m top1: 0.3498134328358209
[2m[36m(func pid=111263)[0m top5: 0.8260261194029851
[2m[36m(func pid=111263)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=111263)[0m f1_macro: 0.2771807707767525
[2m[36m(func pid=111263)[0m f1_weighted: 0.3762542574594117
[2m[36m(func pid=111263)[0m f1_per_class: [0.145, 0.285, 0.393, 0.424, 0.093, 0.051, 0.559, 0.355, 0.213, 0.254]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2341 | Steps: 2 | Val loss: 1.8113 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3523 | Steps: 2 | Val loss: 4.2858 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=122997)[0m top1: 0.16977611940298507
[2m[36m(func pid=122997)[0m top5: 0.6035447761194029
[2m[36m(func pid=122997)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=122997)[0m f1_macro: 0.1254581504906429
[2m[36m(func pid=122997)[0m f1_weighted: 0.16772412622397806
[2m[36m(func pid=122997)[0m f1_per_class: [0.096, 0.179, 0.222, 0.172, 0.021, 0.312, 0.15, 0.074, 0.03, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:19 (running for 00:15:33.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.234 |      0.361 |                   90 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.037 |      0.277 |                   65 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.792 |      0.284 |                   65 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.807 |      0.125 |                   13 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37406716417910446
[2m[36m(func pid=106010)[0m top5: 0.8861940298507462
[2m[36m(func pid=106010)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=106010)[0m f1_macro: 0.3609500085051985
[2m[36m(func pid=106010)[0m f1_weighted: 0.406057762109687
[2m[36m(func pid=106010)[0m f1_per_class: [0.288, 0.353, 0.727, 0.463, 0.074, 0.365, 0.425, 0.471, 0.211, 0.233]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3045708955223881
[2m[36m(func pid=111842)[0m top5: 0.8083022388059702
[2m[36m(func pid=111842)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=111842)[0m f1_macro: 0.24813533879558514
[2m[36m(func pid=111842)[0m f1_weighted: 0.32891112439311326
[2m[36m(func pid=111842)[0m f1_per_class: [0.095, 0.193, 0.444, 0.244, 0.056, 0.303, 0.564, 0.236, 0.108, 0.238]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0786 | Steps: 2 | Val loss: 2.2973 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7462 | Steps: 2 | Val loss: 2.2735 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=111263)[0m top1: 0.3987873134328358
[2m[36m(func pid=111263)[0m top5: 0.8316231343283582
[2m[36m(func pid=111263)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=111263)[0m f1_macro: 0.30056332216333015
[2m[36m(func pid=111263)[0m f1_weighted: 0.40224993298414086
[2m[36m(func pid=111263)[0m f1_per_class: [0.217, 0.285, 0.511, 0.522, 0.126, 0.023, 0.577, 0.244, 0.209, 0.291]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.1998 | Steps: 2 | Val loss: 1.8193 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4553 | Steps: 2 | Val loss: 3.9222 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=122997)[0m top1: 0.1767723880597015
[2m[36m(func pid=122997)[0m top5: 0.6222014925373134
[2m[36m(func pid=122997)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=122997)[0m f1_macro: 0.13126547052689488
[2m[36m(func pid=122997)[0m f1_weighted: 0.1795479312222745
[2m[36m(func pid=122997)[0m f1_per_class: [0.107, 0.185, 0.231, 0.189, 0.02, 0.316, 0.168, 0.078, 0.02, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:24 (running for 00:15:39.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.2   |      0.36  |                   91 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.079 |      0.301 |                   66 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.352 |      0.248 |                   66 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.746 |      0.131 |                   14 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37220149253731344
[2m[36m(func pid=106010)[0m top5: 0.8885261194029851
[2m[36m(func pid=106010)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=106010)[0m f1_macro: 0.3598120057234909
[2m[36m(func pid=106010)[0m f1_weighted: 0.4032517412728283
[2m[36m(func pid=106010)[0m f1_per_class: [0.346, 0.353, 0.727, 0.463, 0.064, 0.33, 0.431, 0.442, 0.218, 0.225]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.27238805970149255
[2m[36m(func pid=111842)[0m top5: 0.8138992537313433
[2m[36m(func pid=111842)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=111842)[0m f1_macro: 0.255304031993976
[2m[36m(func pid=111842)[0m f1_weighted: 0.2936910419311111
[2m[36m(func pid=111842)[0m f1_per_class: [0.289, 0.286, 0.462, 0.223, 0.07, 0.264, 0.4, 0.339, 0.062, 0.158]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0225 | Steps: 2 | Val loss: 2.2684 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7246 | Steps: 2 | Val loss: 2.2618 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=111263)[0m top1: 0.43796641791044777
[2m[36m(func pid=111263)[0m top5: 0.8269589552238806
[2m[36m(func pid=111263)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=111263)[0m f1_macro: 0.33512638936730094
[2m[36m(func pid=111263)[0m f1_weighted: 0.4130499873630468
[2m[36m(func pid=111263)[0m f1_per_class: [0.338, 0.228, 0.706, 0.586, 0.135, 0.016, 0.579, 0.203, 0.255, 0.305]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.1833 | Steps: 2 | Val loss: 1.8296 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2233 | Steps: 2 | Val loss: 4.5109 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=122997)[0m top1: 0.1837686567164179
[2m[36m(func pid=122997)[0m top5: 0.6347947761194029
[2m[36m(func pid=122997)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=122997)[0m f1_macro: 0.14045592569156576
[2m[36m(func pid=122997)[0m f1_weighted: 0.19031430577161343
[2m[36m(func pid=122997)[0m f1_per_class: [0.105, 0.185, 0.282, 0.201, 0.019, 0.328, 0.187, 0.078, 0.02, 0.0]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m top1: 0.1875
[2m[36m(func pid=111842)[0m top5: 0.7971082089552238
[2m[36m(func pid=111842)[0m f1_micro: 0.1875
[2m[36m(func pid=111842)[0m f1_macro: 0.18736001576726025
[2m[36m(func pid=111842)[0m f1_weighted: 0.17922787506469873
[2m[36m(func pid=111842)[0m f1_per_class: [0.247, 0.347, 0.306, 0.178, 0.111, 0.166, 0.076, 0.316, 0.028, 0.098]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:25:29 (running for 00:15:44.33)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.2   |      0.36  |                   91 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.022 |      0.335 |                   67 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.223 |      0.187 |                   68 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.725 |      0.14  |                   15 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37173507462686567
[2m[36m(func pid=106010)[0m top5: 0.8857276119402985
[2m[36m(func pid=106010)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=106010)[0m f1_macro: 0.35998432224755617
[2m[36m(func pid=106010)[0m f1_weighted: 0.4051805328879105
[2m[36m(func pid=106010)[0m f1_per_class: [0.342, 0.353, 0.727, 0.458, 0.059, 0.301, 0.45, 0.454, 0.226, 0.23]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0386 | Steps: 2 | Val loss: 2.2631 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7235 | Steps: 2 | Val loss: 2.2475 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.1811 | Steps: 2 | Val loss: 1.8054 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=111263)[0m top1: 0.44076492537313433
[2m[36m(func pid=111263)[0m top5: 0.8316231343283582
[2m[36m(func pid=111263)[0m f1_micro: 0.44076492537313433
[2m[36m(func pid=111263)[0m f1_macro: 0.3577712752288469
[2m[36m(func pid=111263)[0m f1_weighted: 0.4115626346761681
[2m[36m(func pid=111263)[0m f1_per_class: [0.395, 0.236, 0.857, 0.581, 0.124, 0.008, 0.568, 0.202, 0.266, 0.34]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3645 | Steps: 2 | Val loss: 4.5763 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=122997)[0m top1: 0.1921641791044776
[2m[36m(func pid=122997)[0m top5: 0.6534514925373134
[2m[36m(func pid=122997)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=122997)[0m f1_macro: 0.145835967930075
[2m[36m(func pid=122997)[0m f1_weighted: 0.20285319760806578
[2m[36m(func pid=122997)[0m f1_per_class: [0.102, 0.19, 0.268, 0.211, 0.024, 0.344, 0.208, 0.09, 0.019, 0.0]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:35 (running for 00:15:49.63)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.181 |      0.359 |                   93 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.039 |      0.358 |                   68 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.223 |      0.187 |                   68 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.723 |      0.146 |                   16 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.37919776119402987
[2m[36m(func pid=106010)[0m top5: 0.8875932835820896
[2m[36m(func pid=106010)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=106010)[0m f1_macro: 0.3586029319546407
[2m[36m(func pid=106010)[0m f1_weighted: 0.41280126305313514
[2m[36m(func pid=106010)[0m f1_per_class: [0.327, 0.354, 0.686, 0.475, 0.062, 0.312, 0.455, 0.458, 0.236, 0.222]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.18563432835820895
[2m[36m(func pid=111842)[0m top5: 0.7966417910447762
[2m[36m(func pid=111842)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=111842)[0m f1_macro: 0.17298595119921076
[2m[36m(func pid=111842)[0m f1_weighted: 0.17611516574699124
[2m[36m(func pid=111842)[0m f1_per_class: [0.225, 0.334, 0.18, 0.213, 0.135, 0.152, 0.056, 0.276, 0.038, 0.122]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0381 | Steps: 2 | Val loss: 2.2294 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6921 | Steps: 2 | Val loss: 2.2341 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.1754 | Steps: 2 | Val loss: 1.7957 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4163 | Steps: 2 | Val loss: 4.3303 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=111263)[0m top1: 0.4337686567164179
[2m[36m(func pid=111263)[0m top5: 0.855410447761194
[2m[36m(func pid=111263)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=111263)[0m f1_macro: 0.3635634403960124
[2m[36m(func pid=111263)[0m f1_weighted: 0.4139824416476435
[2m[36m(func pid=111263)[0m f1_per_class: [0.391, 0.275, 0.857, 0.57, 0.083, 0.016, 0.544, 0.292, 0.26, 0.346]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=122997)[0m top1: 0.19869402985074627
[2m[36m(func pid=122997)[0m top5: 0.6669776119402985
[2m[36m(func pid=122997)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=122997)[0m f1_macro: 0.15230794150632065
[2m[36m(func pid=122997)[0m f1_weighted: 0.21418434318654275
[2m[36m(func pid=122997)[0m f1_per_class: [0.118, 0.207, 0.256, 0.225, 0.018, 0.342, 0.224, 0.081, 0.019, 0.032]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:40 (running for 00:15:54.93)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.181 |      0.359 |                   93 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.038 |      0.364 |                   69 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.416 |      0.198 |                   70 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.692 |      0.152 |                   17 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.386660447761194
[2m[36m(func pid=106010)[0m top5: 0.8964552238805971
[2m[36m(func pid=106010)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=106010)[0m f1_macro: 0.35796424990023934
[2m[36m(func pid=106010)[0m f1_weighted: 0.42032408019487155
[2m[36m(func pid=106010)[0m f1_per_class: [0.296, 0.359, 0.667, 0.496, 0.066, 0.341, 0.45, 0.456, 0.224, 0.225]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111842)[0m top1: 0.23087686567164178
[2m[36m(func pid=111842)[0m top5: 0.7966417910447762
[2m[36m(func pid=111842)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=111842)[0m f1_macro: 0.19817122725906453
[2m[36m(func pid=111842)[0m f1_weighted: 0.2295309885378234
[2m[36m(func pid=111842)[0m f1_per_class: [0.2, 0.241, 0.152, 0.262, 0.088, 0.274, 0.191, 0.275, 0.088, 0.211]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0268 | Steps: 2 | Val loss: 2.2039 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6974 | Steps: 2 | Val loss: 2.2209 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2664 | Steps: 2 | Val loss: 4.4380 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.1937 | Steps: 2 | Val loss: 1.7800 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=111263)[0m top1: 0.4244402985074627
[2m[36m(func pid=111263)[0m top5: 0.8582089552238806
[2m[36m(func pid=111263)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=111263)[0m f1_macro: 0.36643277094242366
[2m[36m(func pid=111263)[0m f1_weighted: 0.4169521274956705
[2m[36m(func pid=111263)[0m f1_per_class: [0.348, 0.307, 0.857, 0.564, 0.082, 0.046, 0.525, 0.341, 0.255, 0.34]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=122997)[0m top1: 0.20942164179104478
[2m[36m(func pid=122997)[0m top5: 0.6786380597014925
[2m[36m(func pid=122997)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=122997)[0m f1_macro: 0.16162508095315323
[2m[36m(func pid=122997)[0m f1_weighted: 0.22833556793046217
[2m[36m(func pid=122997)[0m f1_per_class: [0.126, 0.237, 0.259, 0.228, 0.017, 0.353, 0.245, 0.08, 0.038, 0.033]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:45 (running for 00:16:00.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.175 |      0.358 |                   94 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.027 |      0.366 |                   70 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.266 |      0.224 |                   71 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.697 |      0.162 |                   18 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.2756529850746269
[2m[36m(func pid=111842)[0m top5: 0.7817164179104478
[2m[36m(func pid=111842)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=111842)[0m f1_macro: 0.2243455293709721
[2m[36m(func pid=111842)[0m f1_weighted: 0.29608856070844847
[2m[36m(func pid=111842)[0m f1_per_class: [0.277, 0.161, 0.141, 0.305, 0.088, 0.361, 0.387, 0.25, 0.094, 0.18]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=106010)[0m top1: 0.3885261194029851
[2m[36m(func pid=106010)[0m top5: 0.9029850746268657
[2m[36m(func pid=106010)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=106010)[0m f1_macro: 0.3585975195034109
[2m[36m(func pid=106010)[0m f1_weighted: 0.4216097732433
[2m[36m(func pid=106010)[0m f1_per_class: [0.277, 0.358, 0.667, 0.5, 0.071, 0.355, 0.445, 0.472, 0.224, 0.219]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0172 | Steps: 2 | Val loss: 2.2328 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6368 | Steps: 2 | Val loss: 2.2086 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2020 | Steps: 2 | Val loss: 1.7929 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1363 | Steps: 2 | Val loss: 4.7297 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=111263)[0m top1: 0.4221082089552239
[2m[36m(func pid=111263)[0m top5: 0.8586753731343284
[2m[36m(func pid=111263)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=111263)[0m f1_macro: 0.374845144331315
[2m[36m(func pid=111263)[0m f1_weighted: 0.42568035652527725
[2m[36m(func pid=111263)[0m f1_per_class: [0.312, 0.33, 0.857, 0.555, 0.089, 0.111, 0.512, 0.433, 0.243, 0.306]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=122997)[0m top1: 0.21735074626865672
[2m[36m(func pid=122997)[0m top5: 0.6930970149253731
[2m[36m(func pid=122997)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=122997)[0m f1_macro: 0.16675795237889401
[2m[36m(func pid=122997)[0m f1_weighted: 0.23979446375983543
[2m[36m(func pid=122997)[0m f1_per_class: [0.129, 0.23, 0.256, 0.246, 0.017, 0.358, 0.264, 0.104, 0.031, 0.031]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=111842)[0m top1: 0.27658582089552236
[2m[36m(func pid=111842)[0m top5: 0.7677238805970149
[2m[36m(func pid=111842)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=111842)[0m f1_macro: 0.2198049805309759
[2m[36m(func pid=111842)[0m f1_weighted: 0.30308352337490935
[2m[36m(func pid=111842)[0m f1_per_class: [0.281, 0.163, 0.135, 0.308, 0.068, 0.435, 0.395, 0.173, 0.082, 0.158]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 04:25:51 (running for 00:16:05.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.194 |      0.359 |                   95 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.017 |      0.375 |                   71 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.136 |      0.22  |                   72 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.637 |      0.167 |                   19 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=106010)[0m top1: 0.3824626865671642
[2m[36m(func pid=106010)[0m top5: 0.9006529850746269
[2m[36m(func pid=106010)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=106010)[0m f1_macro: 0.3515364576385279
[2m[36m(func pid=106010)[0m f1_weighted: 0.41706898681401483
[2m[36m(func pid=106010)[0m f1_per_class: [0.265, 0.352, 0.632, 0.495, 0.068, 0.354, 0.441, 0.455, 0.233, 0.22]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0131 | Steps: 2 | Val loss: 2.2849 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.5881 | Steps: 2 | Val loss: 2.1961 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5910 | Steps: 2 | Val loss: 4.8411 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.1762 | Steps: 2 | Val loss: 1.7890 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=111263)[0m top1: 0.4141791044776119
[2m[36m(func pid=111263)[0m top5: 0.863339552238806
[2m[36m(func pid=111263)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=111263)[0m f1_macro: 0.37115153206118795
[2m[36m(func pid=111263)[0m f1_weighted: 0.42422630603699557
[2m[36m(func pid=111263)[0m f1_per_class: [0.305, 0.348, 0.786, 0.534, 0.11, 0.145, 0.505, 0.432, 0.238, 0.309]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=122997)[0m top1: 0.22621268656716417
[2m[36m(func pid=122997)[0m top5: 0.7136194029850746
[2m[36m(func pid=122997)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=122997)[0m f1_macro: 0.17365866015139886
[2m[36m(func pid=122997)[0m f1_weighted: 0.25191068806021727
[2m[36m(func pid=122997)[0m f1_per_class: [0.137, 0.247, 0.262, 0.253, 0.017, 0.36, 0.285, 0.122, 0.02, 0.033]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:25:56 (running for 00:16:10.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.202 |      0.352 |                   96 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.013 |      0.371 |                   72 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.591 |      0.194 |                   73 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.588 |      0.174 |                   20 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.2140858208955224
[2m[36m(func pid=111842)[0m top5: 0.7280783582089553
[2m[36m(func pid=111842)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=111842)[0m f1_macro: 0.19358488928074588
[2m[36m(func pid=111842)[0m f1_weighted: 0.2396892289923861
[2m[36m(func pid=111842)[0m f1_per_class: [0.333, 0.18, 0.12, 0.292, 0.042, 0.331, 0.231, 0.113, 0.118, 0.175]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=106010)[0m top1: 0.38619402985074625
[2m[36m(func pid=106010)[0m top5: 0.9015858208955224
[2m[36m(func pid=106010)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=106010)[0m f1_macro: 0.35857210995387667
[2m[36m(func pid=106010)[0m f1_weighted: 0.41886372183470544
[2m[36m(func pid=106010)[0m f1_per_class: [0.286, 0.363, 0.667, 0.49, 0.067, 0.346, 0.445, 0.464, 0.228, 0.23]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0172 | Steps: 2 | Val loss: 2.2972 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5677 | Steps: 2 | Val loss: 2.1872 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1827 | Steps: 2 | Val loss: 5.6692 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.1863 | Steps: 2 | Val loss: 1.7914 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=111263)[0m top1: 0.41277985074626866
[2m[36m(func pid=111263)[0m top5: 0.8698694029850746
[2m[36m(func pid=111263)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=111263)[0m f1_macro: 0.37566186314122074
[2m[36m(func pid=111263)[0m f1_weighted: 0.43027556772248005
[2m[36m(func pid=111263)[0m f1_per_class: [0.289, 0.372, 0.8, 0.508, 0.095, 0.185, 0.52, 0.442, 0.234, 0.311]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=122997)[0m top1: 0.2294776119402985
[2m[36m(func pid=122997)[0m top5: 0.7248134328358209
[2m[36m(func pid=122997)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=122997)[0m f1_macro: 0.17659062136211695
[2m[36m(func pid=122997)[0m f1_weighted: 0.2568180336088179
[2m[36m(func pid=122997)[0m f1_per_class: [0.137, 0.255, 0.262, 0.255, 0.017, 0.362, 0.294, 0.123, 0.03, 0.032]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:26:01 (running for 00:16:16.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.338
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.176 |      0.359 |                   97 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.017 |      0.376 |                   73 |
| train_35a0b_00007 | RUNNING    | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.183 |      0.144 |                   74 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.568 |      0.177 |                   21 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.14972014925373134
[2m[36m(func pid=111842)[0m top5: 0.6576492537313433
[2m[36m(func pid=111842)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=111842)[0m f1_macro: 0.14423137377849632
[2m[36m(func pid=111842)[0m f1_weighted: 0.1617230624764138
[2m[36m(func pid=111842)[0m f1_per_class: [0.303, 0.151, 0.129, 0.267, 0.051, 0.134, 0.093, 0.116, 0.077, 0.12]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=106010)[0m top1: 0.386660447761194
[2m[36m(func pid=106010)[0m top5: 0.8992537313432836
[2m[36m(func pid=106010)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=106010)[0m f1_macro: 0.3605115575510555
[2m[36m(func pid=106010)[0m f1_weighted: 0.41876248741780164
[2m[36m(func pid=106010)[0m f1_per_class: [0.278, 0.365, 0.667, 0.485, 0.069, 0.356, 0.443, 0.462, 0.239, 0.24]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0137 | Steps: 2 | Val loss: 2.2570 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5687 | Steps: 2 | Val loss: 2.1769 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0711 | Steps: 2 | Val loss: 5.8936 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.1437 | Steps: 2 | Val loss: 1.7972 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=111263)[0m top1: 0.41324626865671643
[2m[36m(func pid=111263)[0m top5: 0.8759328358208955
[2m[36m(func pid=111263)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=111263)[0m f1_macro: 0.3711133559733367
[2m[36m(func pid=111263)[0m f1_weighted: 0.4359911599012687
[2m[36m(func pid=111263)[0m f1_per_class: [0.277, 0.389, 0.75, 0.481, 0.072, 0.158, 0.56, 0.482, 0.238, 0.304]
[2m[36m(func pid=111263)[0m 
[2m[36m(func pid=122997)[0m top1: 0.23740671641791045
[2m[36m(func pid=122997)[0m top5: 0.738339552238806
[2m[36m(func pid=122997)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=122997)[0m f1_macro: 0.18794188934349232
[2m[36m(func pid=122997)[0m f1_weighted: 0.26725364624337505
[2m[36m(func pid=122997)[0m f1_per_class: [0.144, 0.261, 0.282, 0.267, 0.017, 0.357, 0.307, 0.154, 0.031, 0.06]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:26:06 (running for 00:16:21.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.337
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00005 | RUNNING    | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.186 |      0.361 |                   98 |
| train_35a0b_00006 | RUNNING    | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.014 |      0.371 |                   74 |
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.569 |      0.188 |                   22 |
| train_35a0b_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.1417910447761194
[2m[36m(func pid=111842)[0m top5: 0.6343283582089553
[2m[36m(func pid=111842)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.13742343333364776
[2m[36m(func pid=111842)[0m f1_weighted: 0.1440225785411997
[2m[36m(func pid=111842)[0m f1_per_class: [0.249, 0.159, 0.118, 0.237, 0.055, 0.106, 0.062, 0.169, 0.07, 0.15]
[2m[36m(func pid=106010)[0m top1: 0.38619402985074625
[2m[36m(func pid=106010)[0m top5: 0.9011194029850746
[2m[36m(func pid=106010)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=106010)[0m f1_macro: 0.3621121249095539
[2m[36m(func pid=106010)[0m f1_weighted: 0.4184238590706513
[2m[36m(func pid=106010)[0m f1_per_class: [0.263, 0.364, 0.686, 0.484, 0.071, 0.356, 0.445, 0.459, 0.244, 0.25]
[2m[36m(func pid=106010)[0m 
[2m[36m(func pid=111263)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0938 | Steps: 2 | Val loss: 2.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5112 | Steps: 2 | Val loss: 2.1673 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=106010)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.1594 | Steps: 2 | Val loss: 1.8091 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=111263)[0m top1: 0.3894589552238806
[2m[36m(func pid=111263)[0m top5: 0.8675373134328358
[2m[36m(func pid=111263)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=111263)[0m f1_macro: 0.3306187369622183
[2m[36m(func pid=111263)[0m f1_weighted: 0.41122137852829693
[2m[36m(func pid=111263)[0m f1_per_class: [0.265, 0.407, 0.706, 0.441, 0.051, 0.083, 0.578, 0.273, 0.216, 0.286]
[2m[36m(func pid=122997)[0m top1: 0.24766791044776118
[2m[36m(func pid=122997)[0m top5: 0.7486007462686567
[2m[36m(func pid=122997)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=122997)[0m f1_macro: 0.19552430437858587
[2m[36m(func pid=122997)[0m f1_weighted: 0.2763173279624285
[2m[36m(func pid=122997)[0m f1_per_class: [0.153, 0.27, 0.289, 0.268, 0.022, 0.376, 0.322, 0.153, 0.043, 0.058]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=106010)[0m top1: 0.3833955223880597
[2m[36m(func pid=106010)[0m top5: 0.8978544776119403
[2m[36m(func pid=106010)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=106010)[0m f1_macro: 0.3662579620467314
[2m[36m(func pid=106010)[0m f1_weighted: 0.4175590918412632
[2m[36m(func pid=106010)[0m f1_per_class: [0.259, 0.36, 0.727, 0.478, 0.068, 0.365, 0.444, 0.467, 0.242, 0.251]
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4596 | Steps: 2 | Val loss: 2.1599 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=128361)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128361)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=128361)[0m Configuration completed!
[2m[36m(func pid=128361)[0m New optimizer parameters:
[2m[36m(func pid=128361)[0m SGD (
[2m[36m(func pid=128361)[0m Parameter Group 0
[2m[36m(func pid=128361)[0m     dampening: 0
[2m[36m(func pid=128361)[0m     differentiable: False
[2m[36m(func pid=128361)[0m     foreach: None
[2m[36m(func pid=128361)[0m     lr: 0.001
[2m[36m(func pid=128361)[0m     maximize: False
[2m[36m(func pid=128361)[0m     momentum: 0.99
[2m[36m(func pid=128361)[0m     nesterov: False
[2m[36m(func pid=128361)[0m     weight_decay: 0.0001
[2m[36m(func pid=128361)[0m )
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m top1: 0.25279850746268656
[2m[36m(func pid=122997)[0m top5: 0.7583955223880597
[2m[36m(func pid=122997)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=122997)[0m f1_macro: 0.20101482319590688
[2m[36m(func pid=122997)[0m f1_weighted: 0.28207913941304064
[2m[36m(func pid=122997)[0m f1_per_class: [0.155, 0.266, 0.296, 0.275, 0.028, 0.378, 0.336, 0.155, 0.043, 0.079]
== Status ==
Current time: 2024-01-07 04:26:12 (running for 00:16:27.25)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 3 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.511 |      0.196 |                   23 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 04:26:19 (running for 00:16:33.49)
Memory usage on this node: 20.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 3 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.46  |      0.201 |                   24 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128502)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=128502)[0m Configuration completed!
[2m[36m(func pid=128502)[0m New optimizer parameters:
[2m[36m(func pid=128502)[0m SGD (
[2m[36m(func pid=128502)[0m Parameter Group 0
[2m[36m(func pid=128502)[0m     dampening: 0
[2m[36m(func pid=128502)[0m     differentiable: False
[2m[36m(func pid=128502)[0m     foreach: None
[2m[36m(func pid=128502)[0m     lr: 0.01
[2m[36m(func pid=128502)[0m     maximize: False
[2m[36m(func pid=128502)[0m     momentum: 0.99
[2m[36m(func pid=128502)[0m     nesterov: False
[2m[36m(func pid=128502)[0m     weight_decay: 0.0001
[2m[36m(func pid=128502)[0m )
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9996 | Steps: 2 | Val loss: 2.3217 | Batch size: 32 | lr: 0.001 | Duration: 4.88s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4593 | Steps: 2 | Val loss: 2.1503 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=128361)[0m top1: 0.1791044776119403
[2m[36m(func pid=128361)[0m top5: 0.53125
[2m[36m(func pid=128361)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=128361)[0m f1_macro: 0.1202500653222521
[2m[36m(func pid=128361)[0m f1_weighted: 0.12639716314606758
[2m[36m(func pid=128361)[0m f1_per_class: [0.326, 0.348, 0.0, 0.088, 0.0, 0.232, 0.018, 0.027, 0.0, 0.164]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9613 | Steps: 2 | Val loss: 2.2891 | Batch size: 32 | lr: 0.01 | Duration: 4.71s
[2m[36m(func pid=122997)[0m top1: 0.26026119402985076
[2m[36m(func pid=122997)[0m top5: 0.7663246268656716
[2m[36m(func pid=122997)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=122997)[0m f1_macro: 0.20769152704508156
[2m[36m(func pid=122997)[0m f1_weighted: 0.2901010712647271
[2m[36m(func pid=122997)[0m f1_per_class: [0.162, 0.27, 0.3, 0.28, 0.029, 0.385, 0.347, 0.174, 0.054, 0.075]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9375 | Steps: 2 | Val loss: 2.3312 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=128502)[0m top1: 0.14878731343283583
[2m[36m(func pid=128502)[0m top5: 0.6096082089552238
[2m[36m(func pid=128502)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=128502)[0m f1_macro: 0.091865647296186
[2m[36m(func pid=128502)[0m f1_weighted: 0.1381923139327787
[2m[36m(func pid=128502)[0m f1_per_class: [0.089, 0.303, 0.0, 0.053, 0.0, 0.119, 0.173, 0.046, 0.0, 0.135]
== Status ==
Current time: 2024-01-07 04:26:24 (running for 00:16:38.88)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.459 |      0.208 |                   25 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  3     |      0.12  |                    1 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=129297)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=129297)[0m Configuration completed!
[2m[36m(func pid=129297)[0m New optimizer parameters:
[2m[36m(func pid=129297)[0m SGD (
[2m[36m(func pid=129297)[0m Parameter Group 0
[2m[36m(func pid=129297)[0m     dampening: 0
[2m[36m(func pid=129297)[0m     differentiable: False
[2m[36m(func pid=129297)[0m     foreach: None
[2m[36m(func pid=129297)[0m     lr: 0.1
[2m[36m(func pid=129297)[0m     maximize: False
[2m[36m(func pid=129297)[0m     momentum: 0.99
[2m[36m(func pid=129297)[0m     nesterov: False
[2m[36m(func pid=129297)[0m     weight_decay: 0.0001
[2m[36m(func pid=129297)[0m )
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4120 | Steps: 2 | Val loss: 2.1430 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=128361)[0m top1: 0.1828358208955224
[2m[36m(func pid=128361)[0m top5: 0.5200559701492538
[2m[36m(func pid=128361)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=128361)[0m f1_macro: 0.10722723774019975
[2m[36m(func pid=128361)[0m f1_weighted: 0.1219399690472642
[2m[36m(func pid=128361)[0m f1_per_class: [0.248, 0.33, 0.0, 0.086, 0.012, 0.284, 0.003, 0.025, 0.0, 0.083]
[2m[36m(func pid=128361)[0m 
== Status ==
Current time: 2024-01-07 04:26:29 (running for 00:16:44.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.412 |      0.22  |                   26 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.937 |      0.107 |                    2 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  2.961 |      0.092 |                    1 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.26865671641791045
[2m[36m(func pid=122997)[0m top5: 0.7737873134328358
[2m[36m(func pid=122997)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=122997)[0m f1_macro: 0.22023394886408507
[2m[36m(func pid=122997)[0m f1_weighted: 0.29918874389282224
[2m[36m(func pid=122997)[0m f1_per_class: [0.182, 0.266, 0.293, 0.308, 0.03, 0.395, 0.348, 0.159, 0.066, 0.157]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7380 | Steps: 2 | Val loss: 2.2635 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9362 | Steps: 2 | Val loss: 2.3356 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9212 | Steps: 2 | Val loss: 2.3514 | Batch size: 32 | lr: 0.1 | Duration: 4.56s
[2m[36m(func pid=128502)[0m top1: 0.13805970149253732
[2m[36m(func pid=128502)[0m top5: 0.636660447761194
[2m[36m(func pid=128502)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=128502)[0m f1_macro: 0.09548573932730477
[2m[36m(func pid=128502)[0m f1_weighted: 0.16513146311045482
[2m[36m(func pid=128502)[0m f1_per_class: [0.078, 0.141, 0.083, 0.171, 0.009, 0.063, 0.273, 0.014, 0.0, 0.122]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4403 | Steps: 2 | Val loss: 2.1366 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=128361)[0m top1: 0.1935634328358209
[2m[36m(func pid=128361)[0m top5: 0.5289179104477612
[2m[36m(func pid=128361)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=128361)[0m f1_macro: 0.11253786200322125
[2m[36m(func pid=128361)[0m f1_weighted: 0.13095665616766755
[2m[36m(func pid=128361)[0m f1_per_class: [0.22, 0.326, 0.043, 0.089, 0.012, 0.326, 0.015, 0.05, 0.0, 0.044]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.04291044776119403
[2m[36m(func pid=129297)[0m top5: 0.6688432835820896
[2m[36m(func pid=129297)[0m f1_micro: 0.04291044776119403
[2m[36m(func pid=129297)[0m f1_macro: 0.052791469797635396
[2m[36m(func pid=129297)[0m f1_weighted: 0.03501976013781161
[2m[36m(func pid=129297)[0m f1_per_class: [0.026, 0.03, 0.02, 0.003, 0.0, 0.0, 0.018, 0.385, 0.0, 0.045]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:26:35 (running for 00:16:49.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.44  |      0.221 |                   27 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.936 |      0.113 |                    3 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  2.738 |      0.095 |                    2 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.921 |      0.053 |                    1 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.27052238805970147
[2m[36m(func pid=122997)[0m top5: 0.7807835820895522
[2m[36m(func pid=122997)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=122997)[0m f1_macro: 0.2211665807640853
[2m[36m(func pid=122997)[0m f1_weighted: 0.300390988042494
[2m[36m(func pid=122997)[0m f1_per_class: [0.208, 0.265, 0.267, 0.317, 0.029, 0.397, 0.343, 0.151, 0.06, 0.176]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4282 | Steps: 2 | Val loss: 2.1968 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8445 | Steps: 2 | Val loss: 2.3191 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9082 | Steps: 2 | Val loss: 2.8213 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=128502)[0m top1: 0.22014925373134328
[2m[36m(func pid=128502)[0m top5: 0.6730410447761194
[2m[36m(func pid=128502)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=128502)[0m f1_macro: 0.13465450980601373
[2m[36m(func pid=128502)[0m f1_weighted: 0.23861718486050593
[2m[36m(func pid=128502)[0m f1_per_class: [0.098, 0.068, 0.118, 0.369, 0.012, 0.008, 0.382, 0.046, 0.046, 0.2]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3204 | Steps: 2 | Val loss: 2.1289 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=128361)[0m top1: 0.21082089552238806
[2m[36m(func pid=128361)[0m top5: 0.5485074626865671
[2m[36m(func pid=128361)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=128361)[0m f1_macro: 0.15782327038517452
[2m[36m(func pid=128361)[0m f1_weighted: 0.1625089986246339
[2m[36m(func pid=128361)[0m f1_per_class: [0.327, 0.311, 0.25, 0.103, 0.023, 0.4, 0.079, 0.038, 0.0, 0.049]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.05083955223880597
[2m[36m(func pid=129297)[0m top5: 0.5755597014925373
[2m[36m(func pid=129297)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=129297)[0m f1_macro: 0.039502843829895155
[2m[36m(func pid=129297)[0m f1_weighted: 0.033522149717294544
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.016, 0.021, 0.003, 0.0, 0.0, 0.064, 0.158, 0.0, 0.133]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:26:40 (running for 00:16:54.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.32  |      0.226 |                   28 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.844 |      0.158 |                    4 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  2.428 |      0.135 |                    3 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.908 |      0.04  |                    2 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2719216417910448
[2m[36m(func pid=122997)[0m top5: 0.7863805970149254
[2m[36m(func pid=122997)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=122997)[0m f1_macro: 0.22558557924123163
[2m[36m(func pid=122997)[0m f1_weighted: 0.3022068510619708
[2m[36m(func pid=122997)[0m f1_per_class: [0.209, 0.258, 0.267, 0.319, 0.028, 0.391, 0.351, 0.152, 0.061, 0.22]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.1633 | Steps: 2 | Val loss: 2.1184 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7314 | Steps: 2 | Val loss: 2.2922 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.3970 | Steps: 2 | Val loss: 4.2288 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=128502)[0m top1: 0.3208955223880597
[2m[36m(func pid=128502)[0m top5: 0.6856343283582089
[2m[36m(func pid=128502)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=128502)[0m f1_macro: 0.20861742376091316
[2m[36m(func pid=128502)[0m f1_weighted: 0.3069850028616715
[2m[36m(func pid=128502)[0m f1_per_class: [0.197, 0.042, 0.086, 0.516, 0.141, 0.016, 0.408, 0.372, 0.075, 0.232]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4126 | Steps: 2 | Val loss: 2.1248 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=128361)[0m top1: 0.17257462686567165
[2m[36m(func pid=128361)[0m top5: 0.5760261194029851
[2m[36m(func pid=128361)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=128361)[0m f1_macro: 0.13060062530023425
[2m[36m(func pid=128361)[0m f1_weighted: 0.15779118165146064
[2m[36m(func pid=128361)[0m f1_per_class: [0.145, 0.284, 0.292, 0.091, 0.022, 0.216, 0.17, 0.045, 0.0, 0.04]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.0625
[2m[36m(func pid=129297)[0m top5: 0.5615671641791045
[2m[36m(func pid=129297)[0m f1_micro: 0.0625
[2m[36m(func pid=129297)[0m f1_macro: 0.03663665072578203
[2m[36m(func pid=129297)[0m f1_weighted: 0.07244419992771421
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.0, 0.026, 0.084, 0.0, 0.0, 0.149, 0.07, 0.0, 0.037]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:26:45 (running for 00:16:59.96)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.413 |      0.224 |                   29 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.731 |      0.131 |                    5 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  2.163 |      0.209 |                    4 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.397 |      0.037 |                    3 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.26865671641791045
[2m[36m(func pid=122997)[0m top5: 0.7835820895522388
[2m[36m(func pid=122997)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=122997)[0m f1_macro: 0.22388046097043407
[2m[36m(func pid=122997)[0m f1_weighted: 0.29879825323086046
[2m[36m(func pid=122997)[0m f1_per_class: [0.211, 0.255, 0.245, 0.31, 0.032, 0.388, 0.349, 0.165, 0.052, 0.231]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.7261 | Steps: 2 | Val loss: 2.0216 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6435 | Steps: 2 | Val loss: 2.2595 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.2836 | Steps: 2 | Val loss: 2.6689 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=128502)[0m top1: 0.34095149253731344
[2m[36m(func pid=128502)[0m top5: 0.722481343283582
[2m[36m(func pid=128502)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=128502)[0m f1_macro: 0.22295213076552073
[2m[36m(func pid=128502)[0m f1_weighted: 0.2933324374127136
[2m[36m(func pid=128502)[0m f1_per_class: [0.384, 0.047, 0.153, 0.533, 0.092, 0.076, 0.308, 0.383, 0.063, 0.19]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m top1: 0.18610074626865672
[2m[36m(func pid=128361)[0m top5: 0.625
[2m[36m(func pid=128361)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=128361)[0m f1_macro: 0.1424871970866679
[2m[36m(func pid=128361)[0m f1_weighted: 0.182255196818087
[2m[36m(func pid=128361)[0m f1_per_class: [0.137, 0.302, 0.34, 0.091, 0.015, 0.113, 0.275, 0.073, 0.0, 0.08]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3613 | Steps: 2 | Val loss: 2.1217 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=129297)[0m top1: 0.34375
[2m[36m(func pid=129297)[0m top5: 0.7481343283582089
[2m[36m(func pid=129297)[0m f1_micro: 0.34375
[2m[36m(func pid=129297)[0m f1_macro: 0.12643842363031782
[2m[36m(func pid=129297)[0m f1_weighted: 0.26010357706474857
[2m[36m(func pid=129297)[0m f1_per_class: [0.1, 0.0, 0.0, 0.526, 0.0, 0.0, 0.337, 0.125, 0.076, 0.101]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:26:51 (running for 00:17:05.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.361 |      0.224 |                   30 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.643 |      0.142 |                    6 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.726 |      0.223 |                    5 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.284 |      0.126 |                    4 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2681902985074627
[2m[36m(func pid=122997)[0m top5: 0.78125
[2m[36m(func pid=122997)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=122997)[0m f1_macro: 0.2242271423536577
[2m[36m(func pid=122997)[0m f1_weighted: 0.296224182946518
[2m[36m(func pid=122997)[0m f1_per_class: [0.217, 0.258, 0.211, 0.291, 0.034, 0.383, 0.355, 0.175, 0.054, 0.264]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3342 | Steps: 2 | Val loss: 1.8602 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5395 | Steps: 2 | Val loss: 2.2324 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.0659 | Steps: 2 | Val loss: 37.7030 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=128361)[0m top1: 0.19402985074626866
[2m[36m(func pid=128361)[0m top5: 0.6529850746268657
[2m[36m(func pid=128361)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=128361)[0m f1_macro: 0.1575177576928346
[2m[36m(func pid=128361)[0m f1_weighted: 0.1939600356812604
[2m[36m(func pid=128361)[0m f1_per_class: [0.148, 0.291, 0.369, 0.096, 0.02, 0.072, 0.317, 0.13, 0.0, 0.133]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.3670708955223881
[2m[36m(func pid=128502)[0m top5: 0.7989738805970149
[2m[36m(func pid=128502)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=128502)[0m f1_macro: 0.2671467896957159
[2m[36m(func pid=128502)[0m f1_weighted: 0.31896614861004646
[2m[36m(func pid=128502)[0m f1_per_class: [0.355, 0.047, 0.444, 0.541, 0.152, 0.157, 0.354, 0.381, 0.037, 0.203]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2917 | Steps: 2 | Val loss: 2.1165 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=129297)[0m top1: 0.027985074626865673
[2m[36m(func pid=129297)[0m top5: 0.47574626865671643
[2m[36m(func pid=129297)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=129297)[0m f1_macro: 0.015567965480560195
[2m[36m(func pid=129297)[0m f1_weighted: 0.02166422292392001
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.124, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:26:56 (running for 00:17:10.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.292 |      0.23  |                   31 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.539 |      0.158 |                    7 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.334 |      0.267 |                    6 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.066 |      0.016 |                    5 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2751865671641791
[2m[36m(func pid=122997)[0m top5: 0.7821828358208955
[2m[36m(func pid=122997)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=122997)[0m f1_macro: 0.2303287033126494
[2m[36m(func pid=122997)[0m f1_weighted: 0.30513989949323894
[2m[36m(func pid=122997)[0m f1_per_class: [0.221, 0.268, 0.216, 0.308, 0.034, 0.389, 0.355, 0.211, 0.056, 0.246]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4537 | Steps: 2 | Val loss: 2.2066 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.0120 | Steps: 2 | Val loss: 1.7414 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.1370 | Steps: 2 | Val loss: 2680.4924 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=128361)[0m top1: 0.2080223880597015
[2m[36m(func pid=128361)[0m top5: 0.6777052238805971
[2m[36m(func pid=128361)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=128361)[0m f1_macro: 0.17658083061105118
[2m[36m(func pid=128361)[0m f1_weighted: 0.21403898623286205
[2m[36m(func pid=128361)[0m f1_per_class: [0.181, 0.271, 0.32, 0.121, 0.035, 0.114, 0.336, 0.217, 0.0, 0.17]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.3880597014925373
[2m[36m(func pid=128502)[0m top5: 0.8526119402985075
[2m[36m(func pid=128502)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=128502)[0m f1_macro: 0.3070191023431354
[2m[36m(func pid=128502)[0m f1_weighted: 0.3363786655293328
[2m[36m(func pid=128502)[0m f1_per_class: [0.481, 0.028, 0.649, 0.561, 0.157, 0.251, 0.358, 0.387, 0.0, 0.198]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2300 | Steps: 2 | Val loss: 2.1114 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=129297)[0m top1: 0.006063432835820896
[2m[36m(func pid=129297)[0m top5: 0.5093283582089553
[2m[36m(func pid=129297)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=129297)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=129297)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:01 (running for 00:17:16.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.23  |      0.233 |                   32 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.454 |      0.177 |                    8 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.012 |      0.307 |                    7 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  3.137 |      0.001 |                    6 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2733208955223881
[2m[36m(func pid=122997)[0m top5: 0.7859141791044776
[2m[36m(func pid=122997)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=122997)[0m f1_macro: 0.23287798267793688
[2m[36m(func pid=122997)[0m f1_weighted: 0.304063021016442
[2m[36m(func pid=122997)[0m f1_per_class: [0.228, 0.257, 0.229, 0.299, 0.032, 0.402, 0.359, 0.215, 0.068, 0.241]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7425 | Steps: 2 | Val loss: 1.7015 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3183 | Steps: 2 | Val loss: 2.1657 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.2045 | Steps: 2 | Val loss: 11863.5039 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=128502)[0m top1: 0.41091417910447764
[2m[36m(func pid=128502)[0m top5: 0.8680037313432836
[2m[36m(func pid=128502)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=128502)[0m f1_macro: 0.3423315789841631
[2m[36m(func pid=128502)[0m f1_weighted: 0.37684974741615485
[2m[36m(func pid=128502)[0m f1_per_class: [0.433, 0.044, 0.828, 0.566, 0.15, 0.332, 0.442, 0.409, 0.021, 0.198]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2059 | Steps: 2 | Val loss: 2.1065 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=128361)[0m top1: 0.22061567164179105
[2m[36m(func pid=128361)[0m top5: 0.7313432835820896
[2m[36m(func pid=128361)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=128361)[0m f1_macro: 0.18853502026906355
[2m[36m(func pid=128361)[0m f1_weighted: 0.2293344141925284
[2m[36m(func pid=128361)[0m f1_per_class: [0.212, 0.25, 0.308, 0.131, 0.035, 0.152, 0.364, 0.27, 0.0, 0.163]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.17210820895522388
[2m[36m(func pid=129297)[0m top5: 0.5727611940298507
[2m[36m(func pid=129297)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=129297)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=129297)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:07 (running for 00:17:21.42)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.206 |      0.233 |                   33 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.318 |      0.189 |                    9 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.742 |      0.342 |                    8 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  4.204 |      0.029 |                    7 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2719216417910448
[2m[36m(func pid=122997)[0m top5: 0.7877798507462687
[2m[36m(func pid=122997)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=122997)[0m f1_macro: 0.23300943948886715
[2m[36m(func pid=122997)[0m f1_weighted: 0.30316451118045207
[2m[36m(func pid=122997)[0m f1_per_class: [0.253, 0.247, 0.224, 0.304, 0.032, 0.389, 0.357, 0.223, 0.07, 0.23]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4969 | Steps: 2 | Val loss: 1.7758 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2052 | Steps: 2 | Val loss: 2.1271 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8497 | Steps: 2 | Val loss: 13292.8242 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=128502)[0m top1: 0.40111940298507465
[2m[36m(func pid=128502)[0m top5: 0.8768656716417911
[2m[36m(func pid=128502)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=128502)[0m f1_macro: 0.3455459674855771
[2m[36m(func pid=128502)[0m f1_weighted: 0.39589271473700455
[2m[36m(func pid=128502)[0m f1_per_class: [0.356, 0.162, 0.828, 0.542, 0.136, 0.38, 0.461, 0.299, 0.088, 0.204]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m top1: 0.2439365671641791
[2m[36m(func pid=128361)[0m top5: 0.769589552238806
[2m[36m(func pid=128361)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=128361)[0m f1_macro: 0.22375872974167627
[2m[36m(func pid=128361)[0m f1_weighted: 0.25861252259982387
[2m[36m(func pid=128361)[0m f1_per_class: [0.266, 0.23, 0.329, 0.208, 0.039, 0.172, 0.376, 0.31, 0.0, 0.308]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1630 | Steps: 2 | Val loss: 2.1067 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=129297)[0m top1: 0.006063432835820896
[2m[36m(func pid=129297)[0m top5: 0.5093283582089553
[2m[36m(func pid=129297)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=129297)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=129297)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:12 (running for 00:17:26.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.163 |      0.231 |                   34 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.205 |      0.224 |                   10 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.497 |      0.346 |                    9 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.85  |      0.001 |                    8 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.26492537313432835
[2m[36m(func pid=122997)[0m top5: 0.7882462686567164
[2m[36m(func pid=122997)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=122997)[0m f1_macro: 0.23122025556129477
[2m[36m(func pid=122997)[0m f1_weighted: 0.29665205408643386
[2m[36m(func pid=122997)[0m f1_per_class: [0.261, 0.244, 0.231, 0.298, 0.03, 0.38, 0.346, 0.227, 0.067, 0.228]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1296 | Steps: 2 | Val loss: 2.0868 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3901 | Steps: 2 | Val loss: 1.9754 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4278 | Steps: 2 | Val loss: 26676.2617 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=128361)[0m top1: 0.26026119402985076
[2m[36m(func pid=128361)[0m top5: 0.7952425373134329
[2m[36m(func pid=128361)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=128361)[0m f1_macro: 0.23760300202686077
[2m[36m(func pid=128361)[0m f1_weighted: 0.2793519197224502
[2m[36m(func pid=128361)[0m f1_per_class: [0.315, 0.216, 0.282, 0.266, 0.043, 0.187, 0.382, 0.337, 0.021, 0.324]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.37406716417910446
[2m[36m(func pid=128502)[0m top5: 0.8843283582089553
[2m[36m(func pid=128502)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=128502)[0m f1_macro: 0.34325059986051176
[2m[36m(func pid=128502)[0m f1_weighted: 0.3727933646127386
[2m[36m(func pid=128502)[0m f1_per_class: [0.488, 0.21, 0.75, 0.508, 0.147, 0.415, 0.384, 0.158, 0.192, 0.182]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2009 | Steps: 2 | Val loss: 2.1090 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=129297)[0m top1: 0.006063432835820896
[2m[36m(func pid=129297)[0m top5: 0.5093283582089553
[2m[36m(func pid=129297)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=129297)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=129297)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:17 (running for 00:17:32.13)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.201 |      0.232 |                   35 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.13  |      0.238 |                   11 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.39  |      0.343 |                   10 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.428 |      0.001 |                    9 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.26259328358208955
[2m[36m(func pid=122997)[0m top5: 0.7798507462686567
[2m[36m(func pid=122997)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=122997)[0m f1_macro: 0.23230622240704993
[2m[36m(func pid=122997)[0m f1_weighted: 0.29447869533919696
[2m[36m(func pid=122997)[0m f1_per_class: [0.274, 0.241, 0.212, 0.279, 0.029, 0.376, 0.355, 0.251, 0.058, 0.248]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2994 | Steps: 2 | Val loss: 2.1812 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0106 | Steps: 2 | Val loss: 2.0672 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0923 | Steps: 2 | Val loss: 21011.1113 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=128502)[0m top1: 0.3596082089552239
[2m[36m(func pid=128502)[0m top5: 0.8997201492537313
[2m[36m(func pid=128502)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=128502)[0m f1_macro: 0.3185324598876728
[2m[36m(func pid=128502)[0m f1_weighted: 0.35389138310028806
[2m[36m(func pid=128502)[0m f1_per_class: [0.333, 0.247, 0.706, 0.486, 0.175, 0.404, 0.339, 0.141, 0.183, 0.171]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.0979 | Steps: 2 | Val loss: 2.1116 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=128361)[0m top1: 0.2560634328358209
[2m[36m(func pid=128361)[0m top5: 0.800839552238806
[2m[36m(func pid=128361)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=128361)[0m f1_macro: 0.239604656527433
[2m[36m(func pid=128361)[0m f1_weighted: 0.2786140830405493
[2m[36m(func pid=128361)[0m f1_per_class: [0.292, 0.212, 0.279, 0.278, 0.043, 0.194, 0.368, 0.336, 0.022, 0.37]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.006063432835820896
[2m[36m(func pid=129297)[0m top5: 0.5093283582089553
[2m[36m(func pid=129297)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=129297)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=129297)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:23 (running for 00:17:37.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.098 |      0.232 |                   36 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.011 |      0.24  |                   12 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.299 |      0.319 |                   11 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.092 |      0.001 |                   10 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2574626865671642
[2m[36m(func pid=122997)[0m top5: 0.7761194029850746
[2m[36m(func pid=122997)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=122997)[0m f1_macro: 0.2322453570325605
[2m[36m(func pid=122997)[0m f1_weighted: 0.2898098566308081
[2m[36m(func pid=122997)[0m f1_per_class: [0.286, 0.234, 0.203, 0.263, 0.031, 0.378, 0.354, 0.27, 0.046, 0.258]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1353 | Steps: 2 | Val loss: 2.3215 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0172 | Steps: 2 | Val loss: 2.0549 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3509 | Steps: 2 | Val loss: 4227.2930 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=128502)[0m top1: 0.3736007462686567
[2m[36m(func pid=128502)[0m top5: 0.9319029850746269
[2m[36m(func pid=128502)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=128502)[0m f1_macro: 0.3161015273687468
[2m[36m(func pid=128502)[0m f1_weighted: 0.3691262910778603
[2m[36m(func pid=128502)[0m f1_per_class: [0.082, 0.251, 0.774, 0.493, 0.182, 0.378, 0.382, 0.276, 0.169, 0.175]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.0825 | Steps: 2 | Val loss: 2.1124 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=128361)[0m top1: 0.25652985074626866
[2m[36m(func pid=128361)[0m top5: 0.8017723880597015
[2m[36m(func pid=128361)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=128361)[0m f1_macro: 0.23841676800775913
[2m[36m(func pid=128361)[0m f1_weighted: 0.28220349684622625
[2m[36m(func pid=128361)[0m f1_per_class: [0.289, 0.205, 0.242, 0.292, 0.046, 0.224, 0.356, 0.355, 0.044, 0.33]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.08628731343283583
[2m[36m(func pid=129297)[0m top5: 0.5881529850746269
[2m[36m(func pid=129297)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=129297)[0m f1_macro: 0.038563134978229316
[2m[36m(func pid=129297)[0m f1_weighted: 0.06407284603831352
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.372, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:28 (running for 00:17:42.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  2.082 |      0.237 |                   37 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  2.017 |      0.238 |                   13 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.135 |      0.316 |                   12 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.351 |      0.039 |                   11 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.2574626865671642
[2m[36m(func pid=122997)[0m top5: 0.7686567164179104
[2m[36m(func pid=122997)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=122997)[0m f1_macro: 0.23724489622737654
[2m[36m(func pid=122997)[0m f1_weighted: 0.2869429861349578
[2m[36m(func pid=122997)[0m f1_per_class: [0.295, 0.254, 0.222, 0.237, 0.032, 0.363, 0.354, 0.292, 0.075, 0.248]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.8640 | Steps: 2 | Val loss: 2.0479 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0868 | Steps: 2 | Val loss: 2.3250 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1327 | Steps: 2 | Val loss: 11579.8506 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=128361)[0m top1: 0.25419776119402987
[2m[36m(func pid=128361)[0m top5: 0.8050373134328358
[2m[36m(func pid=128361)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=128361)[0m f1_macro: 0.23410927806812515
[2m[36m(func pid=128361)[0m f1_weighted: 0.28343653975506744
[2m[36m(func pid=128361)[0m f1_per_class: [0.269, 0.192, 0.226, 0.296, 0.042, 0.253, 0.354, 0.36, 0.063, 0.286]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.3903917910447761
[2m[36m(func pid=128502)[0m top5: 0.9384328358208955
[2m[36m(func pid=128502)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=128502)[0m f1_macro: 0.32437193477547077
[2m[36m(func pid=128502)[0m f1_weighted: 0.39462964701202946
[2m[36m(func pid=128502)[0m f1_per_class: [0.085, 0.227, 0.783, 0.508, 0.168, 0.301, 0.477, 0.421, 0.095, 0.179]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.9893 | Steps: 2 | Val loss: 2.1052 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=129297)[0m top1: 0.17210820895522388
[2m[36m(func pid=129297)[0m top5: 0.5727611940298507
[2m[36m(func pid=129297)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=129297)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=129297)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:33 (running for 00:17:48.16)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.989 |      0.239 |                   38 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.864 |      0.234 |                   14 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.087 |      0.324 |                   13 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.133 |      0.029 |                   12 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.26026119402985076
[2m[36m(func pid=122997)[0m top5: 0.7742537313432836
[2m[36m(func pid=122997)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=122997)[0m f1_macro: 0.23941772069028971
[2m[36m(func pid=122997)[0m f1_weighted: 0.29094405568481135
[2m[36m(func pid=122997)[0m f1_per_class: [0.275, 0.249, 0.233, 0.235, 0.031, 0.373, 0.367, 0.305, 0.076, 0.248]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7076 | Steps: 2 | Val loss: 2.0392 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2390 | Steps: 2 | Val loss: 2.4319 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.6834 | Steps: 2 | Val loss: 10239.3311 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=128361)[0m top1: 0.2537313432835821
[2m[36m(func pid=128361)[0m top5: 0.8027052238805971
[2m[36m(func pid=128361)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=128361)[0m f1_macro: 0.23124344542681152
[2m[36m(func pid=128361)[0m f1_weighted: 0.28326459626567013
[2m[36m(func pid=128361)[0m f1_per_class: [0.277, 0.187, 0.212, 0.32, 0.043, 0.227, 0.345, 0.342, 0.076, 0.281]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.9847 | Steps: 2 | Val loss: 2.0953 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=128502)[0m top1: 0.41511194029850745
[2m[36m(func pid=128502)[0m top5: 0.9277052238805971
[2m[36m(func pid=128502)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=128502)[0m f1_macro: 0.31453963646593486
[2m[36m(func pid=128502)[0m f1_weighted: 0.4057876211833283
[2m[36m(func pid=128502)[0m f1_per_class: [0.125, 0.159, 0.762, 0.573, 0.153, 0.278, 0.512, 0.399, 0.04, 0.146]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.17210820895522388
[2m[36m(func pid=129297)[0m top5: 0.5736940298507462
[2m[36m(func pid=129297)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=129297)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=129297)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:39 (running for 00:17:53.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.985 |      0.246 |                   39 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.708 |      0.231 |                   15 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.239 |      0.315 |                   14 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.683 |      0.029 |                   13 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.26725746268656714
[2m[36m(func pid=122997)[0m top5: 0.7765858208955224
[2m[36m(func pid=122997)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=122997)[0m f1_macro: 0.24610352188590778
[2m[36m(func pid=122997)[0m f1_weighted: 0.2994165247702693
[2m[36m(func pid=122997)[0m f1_per_class: [0.297, 0.249, 0.238, 0.244, 0.032, 0.375, 0.382, 0.314, 0.093, 0.238]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5625 | Steps: 2 | Val loss: 2.0303 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0514 | Steps: 2 | Val loss: 3.0391 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.6946 | Steps: 2 | Val loss: 7352.9624 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=128361)[0m top1: 0.25326492537313433
[2m[36m(func pid=128361)[0m top5: 0.800839552238806
[2m[36m(func pid=128361)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=128361)[0m f1_macro: 0.23319539209725537
[2m[36m(func pid=128361)[0m f1_weighted: 0.28251911124382634
[2m[36m(func pid=128361)[0m f1_per_class: [0.249, 0.181, 0.24, 0.341, 0.046, 0.234, 0.32, 0.366, 0.09, 0.265]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.36986940298507465
[2m[36m(func pid=128502)[0m top5: 0.8875932835820896
[2m[36m(func pid=128502)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=128502)[0m f1_macro: 0.2107193629705609
[2m[36m(func pid=128502)[0m f1_weighted: 0.34880962140834776
[2m[36m(func pid=128502)[0m f1_per_class: [0.125, 0.042, 0.143, 0.593, 0.16, 0.166, 0.444, 0.305, 0.04, 0.088]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.9168 | Steps: 2 | Val loss: 2.0870 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=129297)[0m top1: 0.17164179104477612
[2m[36m(func pid=129297)[0m top5: 0.5755597014925373
[2m[36m(func pid=129297)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=129297)[0m f1_macro: 0.02929936305732484
[2m[36m(func pid=129297)[0m f1_weighted: 0.050426608993250306
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:44 (running for 00:17:59.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.917 |      0.243 |                   40 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.562 |      0.233 |                   16 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.051 |      0.211 |                   15 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.695 |      0.029 |                   14 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.27005597014925375
[2m[36m(func pid=122997)[0m top5: 0.7779850746268657
[2m[36m(func pid=122997)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=122997)[0m f1_macro: 0.24339023971003484
[2m[36m(func pid=122997)[0m f1_weighted: 0.3027550356892895
[2m[36m(func pid=122997)[0m f1_per_class: [0.289, 0.24, 0.218, 0.254, 0.034, 0.372, 0.393, 0.306, 0.092, 0.236]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.4391 | Steps: 2 | Val loss: 1.9978 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3606 | Steps: 2 | Val loss: 2.8461 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.6470 | Steps: 2 | Val loss: 5972.7461 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=128361)[0m top1: 0.2751865671641791
[2m[36m(func pid=128361)[0m top5: 0.7994402985074627
[2m[36m(func pid=128361)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=128361)[0m f1_macro: 0.24525834056238655
[2m[36m(func pid=128361)[0m f1_weighted: 0.3044262539675528
[2m[36m(func pid=128361)[0m f1_per_class: [0.249, 0.194, 0.235, 0.389, 0.057, 0.266, 0.323, 0.38, 0.111, 0.247]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.3316231343283582
[2m[36m(func pid=128502)[0m top5: 0.8680037313432836
[2m[36m(func pid=128502)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=128502)[0m f1_macro: 0.2791221957093034
[2m[36m(func pid=128502)[0m f1_weighted: 0.3360987528597576
[2m[36m(func pid=128502)[0m f1_per_class: [0.235, 0.024, 0.706, 0.547, 0.147, 0.103, 0.438, 0.383, 0.103, 0.105]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9293 | Steps: 2 | Val loss: 2.0745 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=129297)[0m top1: 0.17117537313432835
[2m[36m(func pid=129297)[0m top5: 0.5760261194029851
[2m[36m(func pid=129297)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=129297)[0m f1_macro: 0.02923138191955396
[2m[36m(func pid=129297)[0m f1_weighted: 0.05030960787460547
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.2268 | Steps: 2 | Val loss: 1.9358 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:27:50 (running for 00:18:04.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.929 |      0.249 |                   41 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.439 |      0.245 |                   17 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.361 |      0.279 |                   16 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  4.647 |      0.029 |                   15 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.28078358208955223
[2m[36m(func pid=122997)[0m top5: 0.7803171641791045
[2m[36m(func pid=122997)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=122997)[0m f1_macro: 0.24895641809823282
[2m[36m(func pid=122997)[0m f1_weighted: 0.314879757136472
[2m[36m(func pid=122997)[0m f1_per_class: [0.269, 0.24, 0.233, 0.285, 0.035, 0.357, 0.407, 0.319, 0.102, 0.241]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2027 | Steps: 2 | Val loss: 4.3621 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5567 | Steps: 2 | Val loss: 3262.6194 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=128361)[0m top1: 0.314365671641791
[2m[36m(func pid=128361)[0m top5: 0.8120335820895522
[2m[36m(func pid=128361)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=128361)[0m f1_macro: 0.2702116603479397
[2m[36m(func pid=128361)[0m f1_weighted: 0.3442849125661269
[2m[36m(func pid=128361)[0m f1_per_class: [0.273, 0.197, 0.242, 0.447, 0.068, 0.31, 0.373, 0.421, 0.137, 0.233]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.18983208955223882
[2m[36m(func pid=128502)[0m top5: 0.7840485074626866
[2m[36m(func pid=128502)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=128502)[0m f1_macro: 0.22525741420035245
[2m[36m(func pid=128502)[0m f1_weighted: 0.1970385201656454
[2m[36m(func pid=128502)[0m f1_per_class: [0.116, 0.126, 0.706, 0.142, 0.073, 0.079, 0.304, 0.365, 0.178, 0.163]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8979 | Steps: 2 | Val loss: 2.0612 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=129297)[0m top1: 0.16884328358208955
[2m[36m(func pid=129297)[0m top5: 0.5825559701492538
[2m[36m(func pid=129297)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=129297)[0m f1_macro: 0.028936850519584334
[2m[36m(func pid=129297)[0m f1_weighted: 0.049802695157306995
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:27:55 (running for 00:18:09.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.898 |      0.253 |                   42 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.227 |      0.27  |                   18 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.203 |      0.225 |                   17 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.557 |      0.029 |                   16 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.29197761194029853
[2m[36m(func pid=122997)[0m top5: 0.7887126865671642
[2m[36m(func pid=122997)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=122997)[0m f1_macro: 0.25340492435368367
[2m[36m(func pid=122997)[0m f1_weighted: 0.3264744143099903
[2m[36m(func pid=122997)[0m f1_per_class: [0.265, 0.242, 0.238, 0.305, 0.038, 0.361, 0.424, 0.33, 0.095, 0.236]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1350 | Steps: 2 | Val loss: 1.8674 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1827 | Steps: 2 | Val loss: 5.9793 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9149 | Steps: 2 | Val loss: 2106.0286 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=128361)[0m top1: 0.35867537313432835
[2m[36m(func pid=128361)[0m top5: 0.8283582089552238
[2m[36m(func pid=128361)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=128361)[0m f1_macro: 0.30278781426412715
[2m[36m(func pid=128361)[0m f1_weighted: 0.38265730775074774
[2m[36m(func pid=128361)[0m f1_per_class: [0.344, 0.213, 0.296, 0.497, 0.082, 0.357, 0.413, 0.454, 0.152, 0.22]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9649 | Steps: 2 | Val loss: 2.0465 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=128502)[0m top1: 0.10914179104477612
[2m[36m(func pid=128502)[0m top5: 0.6152052238805971
[2m[36m(func pid=128502)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=128502)[0m f1_macro: 0.1689416715770164
[2m[36m(func pid=128502)[0m f1_weighted: 0.08846912164970364
[2m[36m(func pid=128502)[0m f1_per_class: [0.1, 0.206, 0.727, 0.013, 0.057, 0.033, 0.068, 0.241, 0.072, 0.172]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.16744402985074627
[2m[36m(func pid=129297)[0m top5: 0.5918843283582089
[2m[36m(func pid=129297)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=129297)[0m f1_macro: 0.02884692647649658
[2m[36m(func pid=129297)[0m f1_weighted: 0.04964792849732853
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.0750 | Steps: 2 | Val loss: 1.8136 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 04:28:00 (running for 00:18:15.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.965 |      0.258 |                   43 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.135 |      0.303 |                   19 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.183 |      0.169 |                   18 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.915 |      0.029 |                   17 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.300839552238806
[2m[36m(func pid=122997)[0m top5: 0.7924440298507462
[2m[36m(func pid=122997)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=122997)[0m f1_macro: 0.25843951373559454
[2m[36m(func pid=122997)[0m f1_weighted: 0.3359517830854511
[2m[36m(func pid=122997)[0m f1_per_class: [0.26, 0.235, 0.25, 0.333, 0.04, 0.344, 0.436, 0.345, 0.107, 0.234]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6062 | Steps: 2 | Val loss: 6.6742 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.6838 | Steps: 2 | Val loss: 1457.1155 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=128361)[0m top1: 0.3736007462686567
[2m[36m(func pid=128361)[0m top5: 0.8344216417910447
[2m[36m(func pid=128361)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=128361)[0m f1_macro: 0.31615669163012583
[2m[36m(func pid=128361)[0m f1_weighted: 0.39259457226710237
[2m[36m(func pid=128361)[0m f1_per_class: [0.383, 0.218, 0.348, 0.51, 0.091, 0.354, 0.43, 0.44, 0.171, 0.217]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.8174 | Steps: 2 | Val loss: 2.0302 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=128502)[0m top1: 0.09794776119402986
[2m[36m(func pid=128502)[0m top5: 0.45149253731343286
[2m[36m(func pid=128502)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=128502)[0m f1_macro: 0.0628198561646981
[2m[36m(func pid=128502)[0m f1_weighted: 0.06792650922405107
[2m[36m(func pid=128502)[0m f1_per_class: [0.128, 0.336, 0.0, 0.0, 0.053, 0.039, 0.0, 0.038, 0.0, 0.033]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.16744402985074627
[2m[36m(func pid=129297)[0m top5: 0.5984141791044776
[2m[36m(func pid=129297)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=129297)[0m f1_macro: 0.04158112731856654
[2m[36m(func pid=129297)[0m f1_weighted: 0.05534792273102342
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.285, 0.0, 0.019, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9884 | Steps: 2 | Val loss: 1.7729 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:28:06 (running for 00:18:20.75)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.817 |      0.268 |                   44 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  1.075 |      0.316 |                   20 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.606 |      0.063 |                   19 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.684 |      0.042 |                   18 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.3138992537313433
[2m[36m(func pid=122997)[0m top5: 0.7957089552238806
[2m[36m(func pid=122997)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=122997)[0m f1_macro: 0.26770972476396876
[2m[36m(func pid=122997)[0m f1_weighted: 0.3495235715771683
[2m[36m(func pid=122997)[0m f1_per_class: [0.255, 0.249, 0.289, 0.358, 0.043, 0.354, 0.446, 0.337, 0.119, 0.227]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4568 | Steps: 2 | Val loss: 6.3929 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7075 | Steps: 2 | Val loss: 1061.0559 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=128361)[0m top1: 0.384794776119403
[2m[36m(func pid=128361)[0m top5: 0.8376865671641791
[2m[36m(func pid=128361)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=128361)[0m f1_macro: 0.32284817100865215
[2m[36m(func pid=128361)[0m f1_weighted: 0.39924482542509443
[2m[36m(func pid=128361)[0m f1_per_class: [0.419, 0.212, 0.387, 0.526, 0.098, 0.375, 0.433, 0.446, 0.13, 0.203]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7863 | Steps: 2 | Val loss: 2.0236 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=128502)[0m top1: 0.11194029850746269
[2m[36m(func pid=128502)[0m top5: 0.5256529850746269
[2m[36m(func pid=128502)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=128502)[0m f1_macro: 0.08029453989952454
[2m[36m(func pid=128502)[0m f1_weighted: 0.07844361974887631
[2m[36m(func pid=128502)[0m f1_per_class: [0.125, 0.341, 0.019, 0.016, 0.084, 0.092, 0.0, 0.0, 0.0, 0.126]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.166044776119403
[2m[36m(func pid=129297)[0m top5: 0.5904850746268657
[2m[36m(func pid=129297)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=129297)[0m f1_macro: 0.04724281971427159
[2m[36m(func pid=129297)[0m f1_weighted: 0.056295343687907784
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.283, 0.0, 0.023, 0.167, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.9645 | Steps: 2 | Val loss: 1.7360 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:28:11 (running for 00:18:26.01)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.786 |      0.267 |                   45 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.988 |      0.323 |                   21 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.457 |      0.08  |                   20 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.707 |      0.047 |                   19 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.31576492537313433
[2m[36m(func pid=122997)[0m top5: 0.7957089552238806
[2m[36m(func pid=122997)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=122997)[0m f1_macro: 0.26728162081504314
[2m[36m(func pid=122997)[0m f1_weighted: 0.35183446472015195
[2m[36m(func pid=122997)[0m f1_per_class: [0.239, 0.251, 0.296, 0.362, 0.045, 0.356, 0.449, 0.344, 0.116, 0.215]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3813 | Steps: 2 | Val loss: 4.6086 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.6195 | Steps: 2 | Val loss: 798.0751 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=128361)[0m top1: 0.38572761194029853
[2m[36m(func pid=128361)[0m top5: 0.8502798507462687
[2m[36m(func pid=128361)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=128361)[0m f1_macro: 0.3349695617601606
[2m[36m(func pid=128361)[0m f1_weighted: 0.39429954078097446
[2m[36m(func pid=128361)[0m f1_per_class: [0.473, 0.208, 0.444, 0.53, 0.119, 0.376, 0.411, 0.422, 0.145, 0.221]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7538 | Steps: 2 | Val loss: 2.0106 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=128502)[0m top1: 0.1865671641791045
[2m[36m(func pid=128502)[0m top5: 0.7611940298507462
[2m[36m(func pid=128502)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=128502)[0m f1_macro: 0.14211842153012264
[2m[36m(func pid=128502)[0m f1_weighted: 0.1522026532482128
[2m[36m(func pid=128502)[0m f1_per_class: [0.112, 0.299, 0.098, 0.128, 0.131, 0.2, 0.066, 0.308, 0.0, 0.079]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.1623134328358209
[2m[36m(func pid=129297)[0m top5: 0.5732276119402985
[2m[36m(func pid=129297)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=129297)[0m f1_macro: 0.04368644681471691
[2m[36m(func pid=129297)[0m f1_weighted: 0.051504553992595695
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.282, 0.0, 0.007, 0.148, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:28:17 (running for 00:18:31.37)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.754 |      0.271 |                   46 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.965 |      0.335 |                   22 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.381 |      0.142 |                   21 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.62  |      0.044 |                   20 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.32369402985074625
[2m[36m(func pid=122997)[0m top5: 0.7957089552238806
[2m[36m(func pid=122997)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=122997)[0m f1_macro: 0.2711182457119823
[2m[36m(func pid=122997)[0m f1_weighted: 0.359522538681665
[2m[36m(func pid=122997)[0m f1_per_class: [0.247, 0.253, 0.282, 0.378, 0.047, 0.347, 0.458, 0.364, 0.11, 0.225]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.9017 | Steps: 2 | Val loss: 1.7151 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3911 | Steps: 2 | Val loss: 3.9908 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6338 | Steps: 2 | Val loss: 622.8954 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=128361)[0m top1: 0.3931902985074627
[2m[36m(func pid=128361)[0m top5: 0.8563432835820896
[2m[36m(func pid=128361)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=128361)[0m f1_macro: 0.36591674702970967
[2m[36m(func pid=128361)[0m f1_weighted: 0.40231113712967076
[2m[36m(func pid=128361)[0m f1_per_class: [0.508, 0.256, 0.6, 0.52, 0.117, 0.401, 0.395, 0.439, 0.194, 0.228]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7779 | Steps: 2 | Val loss: 1.9973 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=128502)[0m top1: 0.30597014925373134
[2m[36m(func pid=128502)[0m top5: 0.8246268656716418
[2m[36m(func pid=128502)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=128502)[0m f1_macro: 0.20222572760004542
[2m[36m(func pid=128502)[0m f1_weighted: 0.28858257828363487
[2m[36m(func pid=128502)[0m f1_per_class: [0.128, 0.269, 0.142, 0.18, 0.265, 0.015, 0.537, 0.402, 0.031, 0.054]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.15485074626865672
[2m[36m(func pid=129297)[0m top5: 0.5541044776119403
[2m[36m(func pid=129297)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=129297)[0m f1_macro: 0.035114719498205896
[2m[36m(func pid=129297)[0m f1_weighted: 0.048594790125086294
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.274, 0.0, 0.003, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:28:22 (running for 00:18:36.87)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.778 |      0.27  |                   47 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.902 |      0.366 |                   23 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.391 |      0.202 |                   22 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.634 |      0.035 |                   21 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.3260261194029851
[2m[36m(func pid=122997)[0m top5: 0.8041044776119403
[2m[36m(func pid=122997)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=122997)[0m f1_macro: 0.27003388463299727
[2m[36m(func pid=122997)[0m f1_weighted: 0.36177147585595115
[2m[36m(func pid=122997)[0m f1_per_class: [0.233, 0.249, 0.293, 0.392, 0.05, 0.332, 0.462, 0.358, 0.112, 0.22]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7520 | Steps: 2 | Val loss: 1.7185 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1801 | Steps: 2 | Val loss: 3.9117 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5212 | Steps: 2 | Val loss: 490.7132 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=128361)[0m top1: 0.3880597014925373
[2m[36m(func pid=128361)[0m top5: 0.8656716417910447
[2m[36m(func pid=128361)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=128361)[0m f1_macro: 0.37220636820061553
[2m[36m(func pid=128361)[0m f1_weighted: 0.39648388755943437
[2m[36m(func pid=128361)[0m f1_per_class: [0.517, 0.288, 0.667, 0.507, 0.116, 0.395, 0.371, 0.433, 0.191, 0.237]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7518 | Steps: 2 | Val loss: 1.9848 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=128502)[0m top1: 0.3316231343283582
[2m[36m(func pid=128502)[0m top5: 0.8736007462686567
[2m[36m(func pid=128502)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=128502)[0m f1_macro: 0.18306231455717722
[2m[36m(func pid=128502)[0m f1_weighted: 0.3207965067382542
[2m[36m(func pid=128502)[0m f1_per_class: [0.094, 0.156, 0.142, 0.548, 0.091, 0.043, 0.383, 0.286, 0.038, 0.049]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.14972014925373134
[2m[36m(func pid=129297)[0m top5: 0.5387126865671642
[2m[36m(func pid=129297)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=129297)[0m f1_macro: 0.03454307746490409
[2m[36m(func pid=129297)[0m f1_weighted: 0.04760685502380232
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.268, 0.0, 0.003, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5899 | Steps: 2 | Val loss: 1.7290 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:28:27 (running for 00:18:42.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.752 |      0.274 |                   48 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.752 |      0.372 |                   24 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.18  |      0.183 |                   23 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.521 |      0.035 |                   22 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.333955223880597
[2m[36m(func pid=122997)[0m top5: 0.8097014925373134
[2m[36m(func pid=122997)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=122997)[0m f1_macro: 0.27439395550845014
[2m[36m(func pid=122997)[0m f1_weighted: 0.36981707576357853
[2m[36m(func pid=122997)[0m f1_per_class: [0.234, 0.239, 0.286, 0.414, 0.057, 0.342, 0.468, 0.367, 0.113, 0.223]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2624 | Steps: 2 | Val loss: 4.3310 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.3335 | Steps: 2 | Val loss: 410.9998 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=128361)[0m top1: 0.38199626865671643
[2m[36m(func pid=128361)[0m top5: 0.8689365671641791
[2m[36m(func pid=128361)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=128361)[0m f1_macro: 0.3722039783826994
[2m[36m(func pid=128361)[0m f1_weighted: 0.3918759951607283
[2m[36m(func pid=128361)[0m f1_per_class: [0.5, 0.295, 0.706, 0.496, 0.113, 0.398, 0.362, 0.447, 0.162, 0.243]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6364 | Steps: 2 | Val loss: 1.9696 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=128502)[0m top1: 0.3069029850746269
[2m[36m(func pid=128502)[0m top5: 0.9015858208955224
[2m[36m(func pid=128502)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=128502)[0m f1_macro: 0.1546112964517556
[2m[36m(func pid=128502)[0m f1_weighted: 0.2615103120661774
[2m[36m(func pid=128502)[0m f1_per_class: [0.08, 0.051, 0.211, 0.593, 0.0, 0.054, 0.21, 0.244, 0.022, 0.082]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.146455223880597
[2m[36m(func pid=129297)[0m top5: 0.5415111940298507
[2m[36m(func pid=129297)[0m f1_micro: 0.146455223880597
[2m[36m(func pid=129297)[0m f1_macro: 0.04101079442286189
[2m[36m(func pid=129297)[0m f1_weighted: 0.04967907521351929
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.267, 0.0, 0.01, 0.133, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:28:33 (running for 00:18:47.59)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.636 |      0.277 |                   49 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.59  |      0.372 |                   25 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.262 |      0.155 |                   24 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.333 |      0.041 |                   23 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.33908582089552236
[2m[36m(func pid=122997)[0m top5: 0.8185634328358209
[2m[36m(func pid=122997)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=122997)[0m f1_macro: 0.27670782130166616
[2m[36m(func pid=122997)[0m f1_weighted: 0.3729573488544926
[2m[36m(func pid=122997)[0m f1_per_class: [0.231, 0.24, 0.316, 0.421, 0.062, 0.328, 0.478, 0.368, 0.105, 0.219]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5826 | Steps: 2 | Val loss: 1.7123 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2980 | Steps: 2 | Val loss: 5.2171 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4932 | Steps: 2 | Val loss: 325.0121 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=128361)[0m top1: 0.3894589552238806
[2m[36m(func pid=128361)[0m top5: 0.8768656716417911
[2m[36m(func pid=128361)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=128361)[0m f1_macro: 0.38221067673449766
[2m[36m(func pid=128361)[0m f1_weighted: 0.4035471900298422
[2m[36m(func pid=128361)[0m f1_per_class: [0.517, 0.306, 0.727, 0.489, 0.115, 0.406, 0.395, 0.457, 0.16, 0.25]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6025 | Steps: 2 | Val loss: 1.9583 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=128502)[0m top1: 0.2574626865671642
[2m[36m(func pid=128502)[0m top5: 0.878731343283582
[2m[36m(func pid=128502)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=128502)[0m f1_macro: 0.11492604813641076
[2m[36m(func pid=128502)[0m f1_weighted: 0.2352694095585077
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.047, 0.0, 0.57, 0.0, 0.036, 0.161, 0.254, 0.025, 0.057]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.14412313432835822
[2m[36m(func pid=129297)[0m top5: 0.550839552238806
[2m[36m(func pid=129297)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=129297)[0m f1_macro: 0.0481899796964272
[2m[36m(func pid=129297)[0m f1_weighted: 0.054198657349808325
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.27, 0.0, 0.007, 0.138, 0.0, 0.012, 0.0, 0.026, 0.029]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5729 | Steps: 2 | Val loss: 1.6965 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:28:38 (running for 00:18:52.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.602 |      0.286 |                   50 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.583 |      0.382 |                   26 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.298 |      0.115 |                   25 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.493 |      0.048 |                   24 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.34654850746268656
[2m[36m(func pid=122997)[0m top5: 0.8250932835820896
[2m[36m(func pid=122997)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=122997)[0m f1_macro: 0.2863114463180953
[2m[36m(func pid=122997)[0m f1_weighted: 0.3801221088589764
[2m[36m(func pid=122997)[0m f1_per_class: [0.257, 0.241, 0.364, 0.438, 0.061, 0.32, 0.482, 0.381, 0.11, 0.208]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1849 | Steps: 2 | Val loss: 4.4504 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.3294 | Steps: 2 | Val loss: 263.0233 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=128361)[0m top1: 0.40205223880597013
[2m[36m(func pid=128361)[0m top5: 0.8815298507462687
[2m[36m(func pid=128361)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=128361)[0m f1_macro: 0.38374326295986455
[2m[36m(func pid=128361)[0m f1_weighted: 0.41790831925880806
[2m[36m(func pid=128361)[0m f1_per_class: [0.5, 0.316, 0.706, 0.513, 0.116, 0.376, 0.427, 0.442, 0.195, 0.247]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6306 | Steps: 2 | Val loss: 1.9496 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=128502)[0m top1: 0.21688432835820895
[2m[36m(func pid=128502)[0m top5: 0.8838619402985075
[2m[36m(func pid=128502)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=128502)[0m f1_macro: 0.11987525052548371
[2m[36m(func pid=128502)[0m f1_weighted: 0.23683357953750997
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.109, 0.0, 0.254, 0.0, 0.111, 0.396, 0.278, 0.0, 0.051]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.13666044776119404
[2m[36m(func pid=129297)[0m top5: 0.5643656716417911
[2m[36m(func pid=129297)[0m f1_micro: 0.13666044776119404
[2m[36m(func pid=129297)[0m f1_macro: 0.05145554343747623
[2m[36m(func pid=129297)[0m f1_weighted: 0.05551875249758951
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.273, 0.0, 0.0, 0.176, 0.0, 0.021, 0.011, 0.0, 0.033]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6083 | Steps: 2 | Val loss: 1.6726 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:28:44 (running for 00:18:58.52)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.631 |      0.288 |                   51 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.573 |      0.384 |                   27 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.185 |      0.12  |                   26 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.329 |      0.051 |                   25 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.34421641791044777
[2m[36m(func pid=122997)[0m top5: 0.8278917910447762
[2m[36m(func pid=122997)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=122997)[0m f1_macro: 0.2878673192413266
[2m[36m(func pid=122997)[0m f1_weighted: 0.3771742398198013
[2m[36m(func pid=122997)[0m f1_per_class: [0.26, 0.234, 0.369, 0.436, 0.075, 0.332, 0.475, 0.375, 0.11, 0.213]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4776 | Steps: 2 | Val loss: 4.6359 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.4511 | Steps: 2 | Val loss: 213.8459 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=128361)[0m top1: 0.41651119402985076
[2m[36m(func pid=128361)[0m top5: 0.8885261194029851
[2m[36m(func pid=128361)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=128361)[0m f1_macro: 0.3876430183405238
[2m[36m(func pid=128361)[0m f1_weighted: 0.4335526279861061
[2m[36m(func pid=128361)[0m f1_per_class: [0.522, 0.329, 0.667, 0.525, 0.119, 0.38, 0.459, 0.426, 0.214, 0.235]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5017 | Steps: 2 | Val loss: 1.9370 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=128502)[0m top1: 0.15438432835820895
[2m[36m(func pid=128502)[0m top5: 0.84375
[2m[36m(func pid=128502)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=128502)[0m f1_macro: 0.08629074340168424
[2m[36m(func pid=128502)[0m f1_weighted: 0.14788356957041457
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.193, 0.0, 0.069, 0.0, 0.055, 0.256, 0.203, 0.0, 0.086]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.13199626865671643
[2m[36m(func pid=129297)[0m top5: 0.5886194029850746
[2m[36m(func pid=129297)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=129297)[0m f1_macro: 0.06832150150382094
[2m[36m(func pid=129297)[0m f1_weighted: 0.05680221070901924
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.282, 0.0, 0.0, 0.312, 0.0, 0.012, 0.032, 0.0, 0.045]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4083 | Steps: 2 | Val loss: 1.6652 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 04:28:49 (running for 00:19:03.67)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.502 |      0.292 |                   52 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.608 |      0.388 |                   28 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.478 |      0.086 |                   27 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.451 |      0.068 |                   26 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.34888059701492535
[2m[36m(func pid=122997)[0m top5: 0.8362873134328358
[2m[36m(func pid=122997)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=122997)[0m f1_macro: 0.29173807040701716
[2m[36m(func pid=122997)[0m f1_weighted: 0.38066768093140285
[2m[36m(func pid=122997)[0m f1_per_class: [0.281, 0.234, 0.393, 0.448, 0.076, 0.32, 0.479, 0.368, 0.112, 0.206]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6746 | Steps: 2 | Val loss: 8.8937 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.4567 | Steps: 2 | Val loss: 179.6675 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=128361)[0m top1: 0.425839552238806
[2m[36m(func pid=128361)[0m top5: 0.8899253731343284
[2m[36m(func pid=128361)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=128361)[0m f1_macro: 0.38683057873427984
[2m[36m(func pid=128361)[0m f1_weighted: 0.444057308389714
[2m[36m(func pid=128361)[0m f1_per_class: [0.541, 0.325, 0.615, 0.537, 0.112, 0.367, 0.49, 0.423, 0.226, 0.232]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5126 | Steps: 2 | Val loss: 1.9253 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=128502)[0m top1: 0.11986940298507463
[2m[36m(func pid=128502)[0m top5: 0.789179104477612
[2m[36m(func pid=128502)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=128502)[0m f1_macro: 0.06263580084577172
[2m[36m(func pid=128502)[0m f1_weighted: 0.14466487384603666
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.196, 0.0, 0.023, 0.0, 0.015, 0.34, 0.02, 0.0, 0.032]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.13712686567164178
[2m[36m(func pid=129297)[0m top5: 0.6077425373134329
[2m[36m(func pid=129297)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=129297)[0m f1_macro: 0.0690275385631383
[2m[36m(func pid=129297)[0m f1_weighted: 0.06376723533177839
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.302, 0.0, 0.0, 0.2, 0.0, 0.012, 0.081, 0.04, 0.055]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:28:54 (running for 00:19:08.89)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.513 |      0.294 |                   53 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.408 |      0.387 |                   29 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.675 |      0.063 |                   28 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.457 |      0.069 |                   27 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.3521455223880597
[2m[36m(func pid=122997)[0m top5: 0.8372201492537313
[2m[36m(func pid=122997)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=122997)[0m f1_macro: 0.29379613569490143
[2m[36m(func pid=122997)[0m f1_weighted: 0.3831328111868506
[2m[36m(func pid=122997)[0m f1_per_class: [0.285, 0.239, 0.387, 0.456, 0.083, 0.321, 0.476, 0.367, 0.118, 0.207]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3489 | Steps: 2 | Val loss: 1.6786 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4352 | Steps: 2 | Val loss: 17.0936 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.8139 | Steps: 2 | Val loss: 150.9001 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=128361)[0m top1: 0.4230410447761194
[2m[36m(func pid=128361)[0m top5: 0.8941231343283582
[2m[36m(func pid=128361)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=128361)[0m f1_macro: 0.3802493780552331
[2m[36m(func pid=128361)[0m f1_weighted: 0.44508996026852005
[2m[36m(func pid=128361)[0m f1_per_class: [0.531, 0.324, 0.615, 0.542, 0.093, 0.338, 0.505, 0.41, 0.221, 0.222]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5637 | Steps: 2 | Val loss: 1.9195 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=128502)[0m top1: 0.032182835820895525
[2m[36m(func pid=128502)[0m top5: 0.6655783582089553
[2m[36m(func pid=128502)[0m f1_micro: 0.032182835820895525
[2m[36m(func pid=128502)[0m f1_macro: 0.020766491678345995
[2m[36m(func pid=128502)[0m f1_weighted: 0.038713517133680035
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.091, 0.0, 0.013, 0.0, 0.024, 0.055, 0.0, 0.0, 0.025]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.1478544776119403
[2m[36m(func pid=129297)[0m top5: 0.6389925373134329
[2m[36m(func pid=129297)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=129297)[0m f1_macro: 0.07937905211571737
[2m[36m(func pid=129297)[0m f1_weighted: 0.07522194896118013
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.322, 0.0, 0.0, 0.174, 0.0, 0.021, 0.163, 0.066, 0.047]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:28:59 (running for 00:19:14.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.564 |      0.3   |                   54 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.349 |      0.38  |                   30 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.435 |      0.021 |                   29 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.814 |      0.079 |                   28 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.35634328358208955
[2m[36m(func pid=122997)[0m top5: 0.8404850746268657
[2m[36m(func pid=122997)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=122997)[0m f1_macro: 0.29960098253808537
[2m[36m(func pid=122997)[0m f1_weighted: 0.38845657764492525
[2m[36m(func pid=122997)[0m f1_per_class: [0.296, 0.246, 0.393, 0.463, 0.084, 0.329, 0.475, 0.378, 0.141, 0.191]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3616 | Steps: 2 | Val loss: 1.6998 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8213 | Steps: 2 | Val loss: 9.4513 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5307 | Steps: 2 | Val loss: 119.0913 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=128361)[0m top1: 0.4244402985074627
[2m[36m(func pid=128361)[0m top5: 0.898320895522388
[2m[36m(func pid=128361)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=128361)[0m f1_macro: 0.3848688759772886
[2m[36m(func pid=128361)[0m f1_weighted: 0.44879493261757525
[2m[36m(func pid=128361)[0m f1_per_class: [0.556, 0.323, 0.632, 0.54, 0.074, 0.332, 0.517, 0.417, 0.237, 0.221]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4895 | Steps: 2 | Val loss: 1.9104 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=128502)[0m top1: 0.09701492537313433
[2m[36m(func pid=128502)[0m top5: 0.6464552238805971
[2m[36m(func pid=128502)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=128502)[0m f1_macro: 0.06694381390177478
[2m[36m(func pid=128502)[0m f1_weighted: 0.12206054045350484
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.047, 0.037, 0.044, 0.057, 0.19, 0.264, 0.0, 0.0, 0.029]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.15298507462686567
[2m[36m(func pid=129297)[0m top5: 0.6665111940298507
[2m[36m(func pid=129297)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=129297)[0m f1_macro: 0.07885737198568885
[2m[36m(func pid=129297)[0m f1_weighted: 0.08141907830891613
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.333, 0.0, 0.01, 0.1, 0.016, 0.012, 0.223, 0.054, 0.04]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2707 | Steps: 2 | Val loss: 1.7191 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 04:29:05 (running for 00:19:19.67)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.489 |      0.306 |                   55 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.362 |      0.385 |                   31 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.821 |      0.067 |                   30 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.531 |      0.079 |                   29 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.36380597014925375
[2m[36m(func pid=122997)[0m top5: 0.8456156716417911
[2m[36m(func pid=122997)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=122997)[0m f1_macro: 0.3060296834001527
[2m[36m(func pid=122997)[0m f1_weighted: 0.39584500817257123
[2m[36m(func pid=122997)[0m f1_per_class: [0.315, 0.245, 0.407, 0.476, 0.083, 0.333, 0.481, 0.4, 0.135, 0.184]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4549 | Steps: 2 | Val loss: 8.2907 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4251 | Steps: 2 | Val loss: 97.1193 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=128361)[0m top1: 0.41651119402985076
[2m[36m(func pid=128361)[0m top5: 0.902518656716418
[2m[36m(func pid=128361)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=128361)[0m f1_macro: 0.37169217652831044
[2m[36m(func pid=128361)[0m f1_weighted: 0.44292109038688054
[2m[36m(func pid=128361)[0m f1_per_class: [0.495, 0.312, 0.615, 0.538, 0.07, 0.314, 0.523, 0.403, 0.213, 0.233]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4497 | Steps: 2 | Val loss: 1.9018 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=128502)[0m top1: 0.13199626865671643
[2m[36m(func pid=128502)[0m top5: 0.558768656716418
[2m[36m(func pid=128502)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=128502)[0m f1_macro: 0.09201537801147845
[2m[36m(func pid=128502)[0m f1_weighted: 0.13853743622604456
[2m[36m(func pid=128502)[0m f1_per_class: [0.029, 0.078, 0.056, 0.158, 0.205, 0.008, 0.241, 0.093, 0.0, 0.052]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.15811567164179105
[2m[36m(func pid=129297)[0m top5: 0.6819029850746269
[2m[36m(func pid=129297)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=129297)[0m f1_macro: 0.0951870349120142
[2m[36m(func pid=129297)[0m f1_weighted: 0.08764805224854035
[2m[36m(func pid=129297)[0m f1_per_class: [0.0, 0.341, 0.0, 0.013, 0.19, 0.008, 0.009, 0.332, 0.017, 0.041]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2501 | Steps: 2 | Val loss: 1.7412 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=122997)[0m top1: 0.365205223880597
[2m[36m(func pid=122997)[0m top5: 0.8488805970149254
[2m[36m(func pid=122997)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=122997)[0m f1_macro: 0.3113833437609044
[2m[36m(func pid=122997)[0m f1_weighted: 0.39792149774055596
[2m[36m(func pid=122997)[0m f1_per_class: [0.326, 0.234, 0.429, 0.477, 0.084, 0.332, 0.49, 0.409, 0.15, 0.183]
== Status ==
Current time: 2024-01-07 04:29:10 (running for 00:19:25.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.45  |      0.311 |                   56 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.271 |      0.372 |                   32 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.455 |      0.092 |                   31 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.425 |      0.095 |                   30 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7736 | Steps: 2 | Val loss: 9.5549 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.1982 | Steps: 2 | Val loss: 83.2320 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=128361)[0m top1: 0.41651119402985076
[2m[36m(func pid=128361)[0m top5: 0.90625
[2m[36m(func pid=128361)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=128361)[0m f1_macro: 0.37460303521781946
[2m[36m(func pid=128361)[0m f1_weighted: 0.4447754900540788
[2m[36m(func pid=128361)[0m f1_per_class: [0.495, 0.308, 0.585, 0.532, 0.066, 0.333, 0.524, 0.415, 0.243, 0.244]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.4399 | Steps: 2 | Val loss: 1.8944 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=128502)[0m top1: 0.14458955223880596
[2m[36m(func pid=128502)[0m top5: 0.6114738805970149
[2m[36m(func pid=128502)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=128502)[0m f1_macro: 0.12100113912790711
[2m[36m(func pid=128502)[0m f1_weighted: 0.15681491088426697
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.114, 0.035, 0.172, 0.171, 0.0, 0.21, 0.401, 0.027, 0.08]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.16651119402985073
[2m[36m(func pid=129297)[0m top5: 0.6879664179104478
[2m[36m(func pid=129297)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=129297)[0m f1_macro: 0.1064116482036789
[2m[36m(func pid=129297)[0m f1_weighted: 0.09692619109233018
[2m[36m(func pid=129297)[0m f1_per_class: [0.022, 0.347, 0.0, 0.01, 0.231, 0.016, 0.033, 0.336, 0.016, 0.053]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2001 | Steps: 2 | Val loss: 1.7857 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:29:15 (running for 00:19:30.33)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.44  |      0.318 |                   57 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.25  |      0.375 |                   33 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.774 |      0.121 |                   32 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.198 |      0.106 |                   31 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.3670708955223881
[2m[36m(func pid=122997)[0m top5: 0.8526119402985075
[2m[36m(func pid=122997)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=122997)[0m f1_macro: 0.3176580369081856
[2m[36m(func pid=122997)[0m f1_weighted: 0.398472262862637
[2m[36m(func pid=122997)[0m f1_per_class: [0.368, 0.241, 0.444, 0.477, 0.088, 0.32, 0.488, 0.412, 0.157, 0.18]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3236 | Steps: 2 | Val loss: 9.7375 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2688 | Steps: 2 | Val loss: 73.3097 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=128361)[0m top1: 0.4025186567164179
[2m[36m(func pid=128361)[0m top5: 0.90625
[2m[36m(func pid=128361)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=128361)[0m f1_macro: 0.36133763589550627
[2m[36m(func pid=128361)[0m f1_weighted: 0.4336107943254323
[2m[36m(func pid=128361)[0m f1_per_class: [0.438, 0.301, 0.545, 0.51, 0.068, 0.354, 0.509, 0.421, 0.225, 0.242]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3371 | Steps: 2 | Val loss: 1.8930 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=128502)[0m top1: 0.13013059701492538
[2m[36m(func pid=128502)[0m top5: 0.7047574626865671
[2m[36m(func pid=128502)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=128502)[0m f1_macro: 0.1055490623364828
[2m[36m(func pid=128502)[0m f1_weighted: 0.15924928388346302
[2m[36m(func pid=128502)[0m f1_per_class: [0.013, 0.131, 0.037, 0.175, 0.133, 0.016, 0.235, 0.221, 0.027, 0.066]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.1791044776119403
[2m[36m(func pid=129297)[0m top5: 0.6879664179104478
[2m[36m(func pid=129297)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=129297)[0m f1_macro: 0.09986740839387712
[2m[36m(func pid=129297)[0m f1_weighted: 0.10938844663814237
[2m[36m(func pid=129297)[0m f1_per_class: [0.018, 0.357, 0.0, 0.0, 0.125, 0.016, 0.087, 0.301, 0.019, 0.075]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:29:21 (running for 00:19:35.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.337 |      0.314 |                   58 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.2   |      0.361 |                   34 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.324 |      0.106 |                   33 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.269 |      0.1   |                   32 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.363339552238806
[2m[36m(func pid=122997)[0m top5: 0.8535447761194029
[2m[36m(func pid=122997)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=122997)[0m f1_macro: 0.31443845769998763
[2m[36m(func pid=122997)[0m f1_weighted: 0.3963338486941851
[2m[36m(func pid=122997)[0m f1_per_class: [0.367, 0.239, 0.436, 0.474, 0.085, 0.319, 0.487, 0.403, 0.156, 0.177]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2385 | Steps: 2 | Val loss: 1.8320 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1042 | Steps: 2 | Val loss: 7.9220 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.3577 | Steps: 2 | Val loss: 65.9435 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=128361)[0m top1: 0.3941231343283582
[2m[36m(func pid=128361)[0m top5: 0.9001865671641791
[2m[36m(func pid=128361)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=128361)[0m f1_macro: 0.3527010274387523
[2m[36m(func pid=128361)[0m f1_weighted: 0.42855828036309784
[2m[36m(func pid=128361)[0m f1_per_class: [0.37, 0.308, 0.5, 0.502, 0.068, 0.379, 0.488, 0.439, 0.211, 0.261]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.3804 | Steps: 2 | Val loss: 1.8889 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=128502)[0m top1: 0.20848880597014927
[2m[36m(func pid=128502)[0m top5: 0.7518656716417911
[2m[36m(func pid=128502)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=128502)[0m f1_macro: 0.14391073596225035
[2m[36m(func pid=128502)[0m f1_weighted: 0.24303644552283934
[2m[36m(func pid=128502)[0m f1_per_class: [0.033, 0.171, 0.052, 0.174, 0.112, 0.06, 0.471, 0.228, 0.064, 0.075]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.1958955223880597
[2m[36m(func pid=129297)[0m top5: 0.7005597014925373
[2m[36m(func pid=129297)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=129297)[0m f1_macro: 0.10963987105077652
[2m[36m(func pid=129297)[0m f1_weighted: 0.1285370136971021
[2m[36m(func pid=129297)[0m f1_per_class: [0.026, 0.367, 0.0, 0.0, 0.1, 0.008, 0.146, 0.295, 0.034, 0.12]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m top1: 0.36240671641791045
[2m[36m(func pid=122997)[0m top5: 0.8544776119402985
[2m[36m(func pid=122997)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=122997)[0m f1_macro: 0.31836187060201887
[2m[36m(func pid=122997)[0m f1_weighted: 0.3960446573300581
[2m[36m(func pid=122997)[0m f1_per_class: [0.381, 0.243, 0.444, 0.471, 0.084, 0.324, 0.482, 0.405, 0.177, 0.172]
== Status ==
Current time: 2024-01-07 04:29:26 (running for 00:19:40.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.38  |      0.318 |                   59 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.239 |      0.353 |                   35 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.104 |      0.144 |                   34 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.358 |      0.11  |                   33 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1608 | Steps: 2 | Val loss: 1.8834 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3517 | Steps: 2 | Val loss: 7.7803 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.4211 | Steps: 2 | Val loss: 55.3101 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=128361)[0m top1: 0.3885261194029851
[2m[36m(func pid=128361)[0m top5: 0.9006529850746269
[2m[36m(func pid=128361)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=128361)[0m f1_macro: 0.35354468854284016
[2m[36m(func pid=128361)[0m f1_weighted: 0.4266205767961346
[2m[36m(func pid=128361)[0m f1_per_class: [0.331, 0.31, 0.511, 0.477, 0.067, 0.382, 0.497, 0.481, 0.212, 0.269]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3212 | Steps: 2 | Val loss: 1.8835 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=128502)[0m top1: 0.2868470149253731
[2m[36m(func pid=128502)[0m top5: 0.8194962686567164
[2m[36m(func pid=128502)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=128502)[0m f1_macro: 0.17655657629531823
[2m[36m(func pid=128502)[0m f1_weighted: 0.29338870533276556
[2m[36m(func pid=128502)[0m f1_per_class: [0.122, 0.091, 0.096, 0.189, 0.092, 0.132, 0.621, 0.325, 0.045, 0.052]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.20149253731343283
[2m[36m(func pid=129297)[0m top5: 0.7178171641791045
[2m[36m(func pid=129297)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=129297)[0m f1_macro: 0.12270455618235057
[2m[36m(func pid=129297)[0m f1_weighted: 0.14041238799165975
[2m[36m(func pid=129297)[0m f1_per_class: [0.024, 0.361, 0.0, 0.0, 0.105, 0.047, 0.167, 0.309, 0.069, 0.144]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:29:31 (running for 00:19:46.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.321 |      0.317 |                   60 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.161 |      0.354 |                   36 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.352 |      0.177 |                   35 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.421 |      0.123 |                   34 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.36007462686567165
[2m[36m(func pid=122997)[0m top5: 0.8563432835820896
[2m[36m(func pid=122997)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=122997)[0m f1_macro: 0.3173107090637758
[2m[36m(func pid=122997)[0m f1_weighted: 0.39429798681465006
[2m[36m(func pid=122997)[0m f1_per_class: [0.375, 0.243, 0.444, 0.467, 0.081, 0.313, 0.483, 0.406, 0.18, 0.18]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2253 | Steps: 2 | Val loss: 1.9219 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4985 | Steps: 2 | Val loss: 9.5343 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2992 | Steps: 2 | Val loss: 44.7556 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=128361)[0m top1: 0.3829291044776119
[2m[36m(func pid=128361)[0m top5: 0.9001865671641791
[2m[36m(func pid=128361)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=128361)[0m f1_macro: 0.34766840652915976
[2m[36m(func pid=128361)[0m f1_weighted: 0.42365241311978213
[2m[36m(func pid=128361)[0m f1_per_class: [0.276, 0.306, 0.49, 0.455, 0.069, 0.404, 0.505, 0.493, 0.206, 0.275]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2994 | Steps: 2 | Val loss: 1.8747 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=128502)[0m top1: 0.3208955223880597
[2m[36m(func pid=128502)[0m top5: 0.8348880597014925
[2m[36m(func pid=128502)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=128502)[0m f1_macro: 0.2065563048944084
[2m[36m(func pid=128502)[0m f1_weighted: 0.3248131319291739
[2m[36m(func pid=128502)[0m f1_per_class: [0.13, 0.05, 0.109, 0.331, 0.114, 0.094, 0.592, 0.496, 0.103, 0.047]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.18983208955223882
[2m[36m(func pid=129297)[0m top5: 0.7285447761194029
[2m[36m(func pid=129297)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=129297)[0m f1_macro: 0.12528031068107323
[2m[36m(func pid=129297)[0m f1_weighted: 0.13538657964977885
[2m[36m(func pid=129297)[0m f1_per_class: [0.017, 0.34, 0.0, 0.02, 0.133, 0.1, 0.121, 0.324, 0.072, 0.126]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:29:37 (running for 00:19:51.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.299 |      0.32  |                   61 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.225 |      0.348 |                   37 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.499 |      0.207 |                   36 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.299 |      0.125 |                   35 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.3591417910447761
[2m[36m(func pid=122997)[0m top5: 0.8605410447761194
[2m[36m(func pid=122997)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=122997)[0m f1_macro: 0.32016272144319713
[2m[36m(func pid=122997)[0m f1_weighted: 0.3939558204192777
[2m[36m(func pid=122997)[0m f1_per_class: [0.398, 0.255, 0.462, 0.454, 0.082, 0.314, 0.486, 0.4, 0.183, 0.169]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.1075 | Steps: 2 | Val loss: 1.9365 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5933 | Steps: 2 | Val loss: 12.8076 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.2116 | Steps: 2 | Val loss: 39.9039 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=128361)[0m top1: 0.38152985074626866
[2m[36m(func pid=128361)[0m top5: 0.8973880597014925
[2m[36m(func pid=128361)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=128361)[0m f1_macro: 0.34562075932555236
[2m[36m(func pid=128361)[0m f1_weighted: 0.4235626817356557
[2m[36m(func pid=128361)[0m f1_per_class: [0.232, 0.284, 0.511, 0.459, 0.071, 0.386, 0.52, 0.5, 0.224, 0.271]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2733 | Steps: 2 | Val loss: 1.8704 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=128502)[0m top1: 0.29617537313432835
[2m[36m(func pid=128502)[0m top5: 0.832089552238806
[2m[36m(func pid=128502)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=128502)[0m f1_macro: 0.16152754441740974
[2m[36m(func pid=128502)[0m f1_weighted: 0.2921449465730665
[2m[36m(func pid=128502)[0m f1_per_class: [0.038, 0.074, 0.187, 0.445, 0.0, 0.008, 0.436, 0.38, 0.0, 0.047]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.17117537313432835
[2m[36m(func pid=129297)[0m top5: 0.722481343283582
[2m[36m(func pid=129297)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=129297)[0m f1_macro: 0.15803514330117335
[2m[36m(func pid=129297)[0m f1_weighted: 0.11237705189761538
[2m[36m(func pid=129297)[0m f1_per_class: [0.02, 0.329, 0.364, 0.041, 0.229, 0.124, 0.015, 0.322, 0.041, 0.095]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:29:42 (running for 00:19:56.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.273 |      0.321 |                   62 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.108 |      0.346 |                   38 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.593 |      0.162 |                   37 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.212 |      0.158 |                   36 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.355410447761194
[2m[36m(func pid=122997)[0m top5: 0.8600746268656716
[2m[36m(func pid=122997)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=122997)[0m f1_macro: 0.3211026058517546
[2m[36m(func pid=122997)[0m f1_weighted: 0.3895204219569992
[2m[36m(func pid=122997)[0m f1_per_class: [0.385, 0.259, 0.471, 0.448, 0.08, 0.332, 0.467, 0.397, 0.197, 0.177]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1488 | Steps: 2 | Val loss: 1.9413 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4411 | Steps: 2 | Val loss: 10.4794 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.4458 | Steps: 2 | Val loss: 36.6441 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=128361)[0m top1: 0.38386194029850745
[2m[36m(func pid=128361)[0m top5: 0.9039179104477612
[2m[36m(func pid=128361)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=128361)[0m f1_macro: 0.3431000906436534
[2m[36m(func pid=128361)[0m f1_weighted: 0.42553199071605613
[2m[36m(func pid=128361)[0m f1_per_class: [0.228, 0.281, 0.533, 0.463, 0.061, 0.357, 0.54, 0.481, 0.216, 0.271]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2275 | Steps: 2 | Val loss: 1.8673 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=129297)[0m top1: 0.16557835820895522
[2m[36m(func pid=129297)[0m top5: 0.7024253731343284
[2m[36m(func pid=129297)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=129297)[0m f1_macro: 0.18346551631420266
[2m[36m(func pid=129297)[0m f1_weighted: 0.11183747226210544
[2m[36m(func pid=129297)[0m f1_per_class: [0.02, 0.319, 0.667, 0.056, 0.227, 0.106, 0.006, 0.339, 0.021, 0.074]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.28171641791044777
[2m[36m(func pid=128502)[0m top5: 0.7817164179104478
[2m[36m(func pid=128502)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=128502)[0m f1_macro: 0.1583774434876386
[2m[36m(func pid=128502)[0m f1_weighted: 0.29555878706135386
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.173, 0.079, 0.377, 0.0, 0.008, 0.445, 0.452, 0.0, 0.05]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m top1: 0.3493470149253731
[2m[36m(func pid=122997)[0m top5: 0.8596082089552238
[2m[36m(func pid=122997)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=122997)[0m f1_macro: 0.3195635945067908
[2m[36m(func pid=122997)[0m f1_weighted: 0.38393274129629795
[2m[36m(func pid=122997)[0m f1_per_class: [0.378, 0.258, 0.49, 0.439, 0.077, 0.34, 0.458, 0.377, 0.198, 0.181]
[2m[36m(func pid=122997)[0m 
== Status ==
Current time: 2024-01-07 04:29:47 (running for 00:20:02.02)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.228 |      0.32  |                   63 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.149 |      0.343 |                   39 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.441 |      0.158 |                   38 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.446 |      0.183 |                   37 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2044 | Steps: 2 | Val loss: 1.9575 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1546 | Steps: 2 | Val loss: 32.7784 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.1067 | Steps: 2 | Val loss: 14.4624 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.1804 | Steps: 2 | Val loss: 1.8706 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=128361)[0m top1: 0.38899253731343286
[2m[36m(func pid=128361)[0m top5: 0.9043843283582089
[2m[36m(func pid=128361)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=128361)[0m f1_macro: 0.34982504153626
[2m[36m(func pid=128361)[0m f1_weighted: 0.42962214302828994
[2m[36m(func pid=128361)[0m f1_per_class: [0.222, 0.291, 0.615, 0.463, 0.065, 0.35, 0.55, 0.485, 0.196, 0.259]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.17350746268656717
[2m[36m(func pid=129297)[0m top5: 0.7285447761194029
[2m[36m(func pid=129297)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=129297)[0m f1_macro: 0.17658484185006662
[2m[36m(func pid=129297)[0m f1_weighted: 0.11341738323309378
[2m[36m(func pid=129297)[0m f1_per_class: [0.025, 0.325, 0.571, 0.035, 0.228, 0.114, 0.033, 0.281, 0.051, 0.103]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.25
[2m[36m(func pid=128502)[0m top5: 0.835820895522388
[2m[36m(func pid=128502)[0m f1_micro: 0.25
[2m[36m(func pid=128502)[0m f1_macro: 0.12452176505950456
[2m[36m(func pid=128502)[0m f1_weighted: 0.2341054535298634
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.319, 0.0, 0.428, 0.04, 0.008, 0.146, 0.247, 0.0, 0.057]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:29:53 (running for 00:20:07.36)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.18  |      0.319 |                   64 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.204 |      0.35  |                   40 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  3.107 |      0.125 |                   39 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.155 |      0.177 |                   38 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.3451492537313433
[2m[36m(func pid=122997)[0m top5: 0.8596082089552238
[2m[36m(func pid=122997)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=122997)[0m f1_macro: 0.3187889375442105
[2m[36m(func pid=122997)[0m f1_weighted: 0.3797143691905867
[2m[36m(func pid=122997)[0m f1_per_class: [0.374, 0.26, 0.49, 0.429, 0.074, 0.348, 0.449, 0.378, 0.2, 0.186]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0960 | Steps: 2 | Val loss: 1.9753 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2868 | Steps: 2 | Val loss: 30.1852 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5509 | Steps: 2 | Val loss: 14.3170 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2280 | Steps: 2 | Val loss: 1.8662 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=128361)[0m top1: 0.38619402985074625
[2m[36m(func pid=128361)[0m top5: 0.9090485074626866
[2m[36m(func pid=128361)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=128361)[0m f1_macro: 0.34660896931231633
[2m[36m(func pid=128361)[0m f1_weighted: 0.4255546633474338
[2m[36m(func pid=128361)[0m f1_per_class: [0.228, 0.276, 0.632, 0.469, 0.065, 0.34, 0.549, 0.457, 0.196, 0.255]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m top1: 0.19636194029850745
[2m[36m(func pid=129297)[0m top5: 0.7248134328358209
[2m[36m(func pid=129297)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=129297)[0m f1_macro: 0.17370878469448806
[2m[36m(func pid=129297)[0m f1_weighted: 0.1472608104130074
[2m[36m(func pid=129297)[0m f1_per_class: [0.028, 0.321, 0.5, 0.026, 0.176, 0.069, 0.177, 0.297, 0.023, 0.121]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.2332089552238806
[2m[36m(func pid=128502)[0m top5: 0.867070895522388
[2m[36m(func pid=128502)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=128502)[0m f1_macro: 0.15711276046012843
[2m[36m(func pid=128502)[0m f1_weighted: 0.21592110450390528
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.369, 0.0, 0.354, 0.167, 0.024, 0.115, 0.2, 0.0, 0.343]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:29:58 (running for 00:20:12.65)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.228 |      0.32  |                   65 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.096 |      0.347 |                   41 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.551 |      0.157 |                   40 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.287 |      0.174 |                   39 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.34701492537313433
[2m[36m(func pid=122997)[0m top5: 0.8605410447761194
[2m[36m(func pid=122997)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=122997)[0m f1_macro: 0.31982396985000766
[2m[36m(func pid=122997)[0m f1_weighted: 0.3817028005716078
[2m[36m(func pid=122997)[0m f1_per_class: [0.387, 0.266, 0.471, 0.43, 0.074, 0.34, 0.45, 0.392, 0.203, 0.185]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0944 | Steps: 2 | Val loss: 1.9923 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.0869 | Steps: 2 | Val loss: 29.1961 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7492 | Steps: 2 | Val loss: 10.0541 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=128361)[0m top1: 0.3871268656716418
[2m[36m(func pid=128361)[0m top5: 0.9113805970149254
[2m[36m(func pid=128361)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=128361)[0m f1_macro: 0.34348251425614584
[2m[36m(func pid=128361)[0m f1_weighted: 0.4239230779139354
[2m[36m(func pid=128361)[0m f1_per_class: [0.23, 0.287, 0.632, 0.469, 0.069, 0.313, 0.552, 0.433, 0.19, 0.259]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1693 | Steps: 2 | Val loss: 1.8583 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=129297)[0m top1: 0.2719216417910448
[2m[36m(func pid=129297)[0m top5: 0.7220149253731343
[2m[36m(func pid=129297)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=129297)[0m f1_macro: 0.2180438633293936
[2m[36m(func pid=129297)[0m f1_weighted: 0.23595115261927138
[2m[36m(func pid=129297)[0m f1_per_class: [0.033, 0.343, 0.545, 0.02, 0.193, 0.062, 0.454, 0.348, 0.059, 0.123]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.3824626865671642
[2m[36m(func pid=128502)[0m top5: 0.8964552238805971
[2m[36m(func pid=128502)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=128502)[0m f1_macro: 0.22726851375827167
[2m[36m(func pid=128502)[0m f1_weighted: 0.35062597829946557
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.421, 0.143, 0.249, 0.0, 0.16, 0.548, 0.376, 0.0, 0.375]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:30:03 (running for 00:20:18.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.169 |      0.322 |                   66 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.094 |      0.343 |                   42 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.749 |      0.227 |                   41 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.087 |      0.218 |                   40 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.345615671641791
[2m[36m(func pid=122997)[0m top5: 0.8628731343283582
[2m[36m(func pid=122997)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=122997)[0m f1_macro: 0.32166164912230455
[2m[36m(func pid=122997)[0m f1_weighted: 0.37985360891450454
[2m[36m(func pid=122997)[0m f1_per_class: [0.385, 0.267, 0.49, 0.43, 0.075, 0.344, 0.442, 0.388, 0.217, 0.18]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1484 | Steps: 2 | Val loss: 2.0308 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.2963 | Steps: 2 | Val loss: 29.4608 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4539 | Steps: 2 | Val loss: 13.6429 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=128361)[0m top1: 0.384794776119403
[2m[36m(func pid=128361)[0m top5: 0.9081156716417911
[2m[36m(func pid=128361)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=128361)[0m f1_macro: 0.3315779380657443
[2m[36m(func pid=128361)[0m f1_weighted: 0.4199228189221657
[2m[36m(func pid=128361)[0m f1_per_class: [0.235, 0.295, 0.585, 0.482, 0.072, 0.299, 0.539, 0.382, 0.173, 0.254]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.0785 | Steps: 2 | Val loss: 1.8476 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=129297)[0m top1: 0.23833955223880596
[2m[36m(func pid=129297)[0m top5: 0.6730410447761194
[2m[36m(func pid=129297)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=129297)[0m f1_macro: 0.2159168797113839
[2m[36m(func pid=129297)[0m f1_weighted: 0.20605175684809685
[2m[36m(func pid=129297)[0m f1_per_class: [0.014, 0.351, 0.667, 0.023, 0.156, 0.121, 0.322, 0.385, 0.026, 0.095]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:30:08 (running for 00:20:23.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.169 |      0.322 |                   66 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.148 |      0.332 |                   43 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  2.454 |      0.264 |                   42 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.296 |      0.216 |                   41 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128502)[0m top1: 0.3568097014925373
[2m[36m(func pid=128502)[0m top5: 0.8927238805970149
[2m[36m(func pid=128502)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=128502)[0m f1_macro: 0.26438928759161695
[2m[36m(func pid=128502)[0m f1_weighted: 0.3136132892298222
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.432, 0.786, 0.191, 0.0, 0.082, 0.493, 0.375, 0.0, 0.286]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m top1: 0.34794776119402987
[2m[36m(func pid=122997)[0m top5: 0.8661380597014925
[2m[36m(func pid=122997)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=122997)[0m f1_macro: 0.3268511362046031
[2m[36m(func pid=122997)[0m f1_weighted: 0.3822404262195759
[2m[36m(func pid=122997)[0m f1_per_class: [0.384, 0.277, 0.522, 0.432, 0.074, 0.344, 0.439, 0.394, 0.217, 0.185]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0693 | Steps: 2 | Val loss: 2.0555 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3032 | Steps: 2 | Val loss: 30.3070 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=128361)[0m top1: 0.39552238805970147
[2m[36m(func pid=128361)[0m top5: 0.9118470149253731
[2m[36m(func pid=128361)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=128361)[0m f1_macro: 0.33646623139332804
[2m[36m(func pid=128361)[0m f1_weighted: 0.42636738358732634
[2m[36m(func pid=128361)[0m f1_per_class: [0.288, 0.302, 0.558, 0.499, 0.076, 0.298, 0.538, 0.385, 0.175, 0.244]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6975 | Steps: 2 | Val loss: 19.3656 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.1208 | Steps: 2 | Val loss: 1.8459 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 04:30:14 (running for 00:20:28.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.078 |      0.327 |                   67 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.069 |      0.336 |                   44 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  2.454 |      0.264 |                   42 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.303 |      0.182 |                   42 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.17630597014925373
[2m[36m(func pid=129297)[0m top5: 0.6166044776119403
[2m[36m(func pid=129297)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=129297)[0m f1_macro: 0.18194857957942626
[2m[36m(func pid=129297)[0m f1_weighted: 0.11622176256083877
[2m[36m(func pid=129297)[0m f1_per_class: [0.025, 0.338, 0.629, 0.044, 0.208, 0.11, 0.018, 0.353, 0.021, 0.073]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.2332089552238806
[2m[36m(func pid=128502)[0m top5: 0.8745335820895522
[2m[36m(func pid=128502)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=128502)[0m f1_macro: 0.15971313055461242
[2m[36m(func pid=128502)[0m f1_weighted: 0.2034615879815214
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.432, 0.324, 0.27, 0.0, 0.007, 0.122, 0.207, 0.0, 0.235]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=122997)[0m top1: 0.35074626865671643
[2m[36m(func pid=122997)[0m top5: 0.8684701492537313
[2m[36m(func pid=122997)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=122997)[0m f1_macro: 0.32849701481547405
[2m[36m(func pid=122997)[0m f1_weighted: 0.3846155205455507
[2m[36m(func pid=122997)[0m f1_per_class: [0.378, 0.283, 0.522, 0.428, 0.078, 0.348, 0.445, 0.403, 0.218, 0.182]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0670 | Steps: 2 | Val loss: 2.1004 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.3882 | Steps: 2 | Val loss: 29.6293 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=128361)[0m top1: 0.3978544776119403
[2m[36m(func pid=128361)[0m top5: 0.9090485074626866
[2m[36m(func pid=128361)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=128361)[0m f1_macro: 0.33627633130367285
[2m[36m(func pid=128361)[0m f1_weighted: 0.4252613068379103
[2m[36m(func pid=128361)[0m f1_per_class: [0.316, 0.312, 0.533, 0.506, 0.084, 0.287, 0.527, 0.369, 0.187, 0.242]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0223 | Steps: 2 | Val loss: 1.8404 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7003 | Steps: 2 | Val loss: 23.1684 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=129297)[0m top1: 0.16044776119402984
[2m[36m(func pid=129297)[0m top5: 0.5993470149253731
[2m[36m(func pid=129297)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=129297)[0m f1_macro: 0.17520598258428682
[2m[36m(func pid=129297)[0m f1_weighted: 0.11586777440165136
[2m[36m(func pid=129297)[0m f1_per_class: [0.032, 0.307, 0.629, 0.082, 0.156, 0.116, 0.0, 0.336, 0.029, 0.064]
[2m[36m(func pid=129297)[0m 
== Status ==
Current time: 2024-01-07 04:30:19 (running for 00:20:33.93)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.121 |      0.328 |                   68 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.067 |      0.336 |                   45 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.698 |      0.16  |                   43 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.388 |      0.175 |                   43 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=122997)[0m top1: 0.35027985074626866
[2m[36m(func pid=122997)[0m top5: 0.8652052238805971
[2m[36m(func pid=122997)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=122997)[0m f1_macro: 0.32685127848609996
[2m[36m(func pid=122997)[0m f1_weighted: 0.38433194780037555
[2m[36m(func pid=122997)[0m f1_per_class: [0.352, 0.283, 0.533, 0.431, 0.079, 0.345, 0.444, 0.406, 0.213, 0.183]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m top1: 0.2019589552238806
[2m[36m(func pid=128502)[0m top5: 0.8456156716417911
[2m[36m(func pid=128502)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=128502)[0m f1_macro: 0.14338902329811523
[2m[36m(func pid=128502)[0m f1_weighted: 0.16665295131553612
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.411, 0.324, 0.243, 0.0, 0.0, 0.044, 0.176, 0.0, 0.235]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0514 | Steps: 2 | Val loss: 2.1536 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 8.2979 | Steps: 2 | Val loss: 32.0730 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.9910 | Steps: 2 | Val loss: 1.8357 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=128361)[0m top1: 0.39598880597014924
[2m[36m(func pid=128361)[0m top5: 0.9085820895522388
[2m[36m(func pid=128361)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=128361)[0m f1_macro: 0.3343629092820236
[2m[36m(func pid=128361)[0m f1_weighted: 0.420349327673237
[2m[36m(func pid=128361)[0m f1_per_class: [0.321, 0.324, 0.533, 0.498, 0.088, 0.246, 0.524, 0.368, 0.205, 0.235]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0529 | Steps: 2 | Val loss: 18.9287 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:30:24 (running for 00:20:39.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.022 |      0.327 |                   69 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.051 |      0.334 |                   46 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.7   |      0.143 |                   44 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  8.298 |      0.108 |                   44 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.15438432835820895
[2m[36m(func pid=129297)[0m top5: 0.6194029850746269
[2m[36m(func pid=129297)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=129297)[0m f1_macro: 0.10820586701057995
[2m[36m(func pid=129297)[0m f1_weighted: 0.10414137052201743
[2m[36m(func pid=129297)[0m f1_per_class: [0.034, 0.302, 0.0, 0.069, 0.167, 0.104, 0.0, 0.296, 0.038, 0.072]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m top1: 0.35261194029850745
[2m[36m(func pid=122997)[0m top5: 0.867070895522388
[2m[36m(func pid=122997)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=122997)[0m f1_macro: 0.32770176754905966
[2m[36m(func pid=122997)[0m f1_weighted: 0.38710415856416425
[2m[36m(func pid=122997)[0m f1_per_class: [0.342, 0.282, 0.545, 0.443, 0.079, 0.349, 0.442, 0.401, 0.21, 0.183]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m top1: 0.18796641791044777
[2m[36m(func pid=128502)[0m top5: 0.8157649253731343
[2m[36m(func pid=128502)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=128502)[0m f1_macro: 0.12175564527457525
[2m[36m(func pid=128502)[0m f1_weighted: 0.14442942889655683
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.365, 0.279, 0.23, 0.0, 0.0, 0.012, 0.181, 0.0, 0.151]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0495 | Steps: 2 | Val loss: 2.1816 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.3991 | Steps: 2 | Val loss: 34.2362 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=128361)[0m top1: 0.3978544776119403
[2m[36m(func pid=128361)[0m top5: 0.9076492537313433
[2m[36m(func pid=128361)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=128361)[0m f1_macro: 0.3344736135558777
[2m[36m(func pid=128361)[0m f1_weighted: 0.4198397205689539
[2m[36m(func pid=128361)[0m f1_per_class: [0.323, 0.328, 0.558, 0.509, 0.096, 0.219, 0.519, 0.378, 0.199, 0.216]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.2082 | Steps: 2 | Val loss: 1.8249 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.2704 | Steps: 2 | Val loss: 15.5006 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:30:30 (running for 00:20:44.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  0.991 |      0.328 |                   70 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.049 |      0.334 |                   47 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.053 |      0.122 |                   45 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.399 |      0.119 |                   45 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.16511194029850745
[2m[36m(func pid=129297)[0m top5: 0.6347947761194029
[2m[36m(func pid=129297)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=129297)[0m f1_macro: 0.1190115212436054
[2m[36m(func pid=129297)[0m f1_weighted: 0.11077058487047345
[2m[36m(func pid=129297)[0m f1_per_class: [0.045, 0.312, 0.0, 0.054, 0.216, 0.141, 0.015, 0.293, 0.028, 0.087]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m top1: 0.35401119402985076
[2m[36m(func pid=122997)[0m top5: 0.8698694029850746
[2m[36m(func pid=122997)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=122997)[0m f1_macro: 0.32814975646987465
[2m[36m(func pid=122997)[0m f1_weighted: 0.3875153936094278
[2m[36m(func pid=122997)[0m f1_per_class: [0.315, 0.28, 0.558, 0.438, 0.09, 0.351, 0.449, 0.409, 0.206, 0.185]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128502)[0m top1: 0.17117537313432835
[2m[36m(func pid=128502)[0m top5: 0.7845149253731343
[2m[36m(func pid=128502)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=128502)[0m f1_macro: 0.10866714306302831
[2m[36m(func pid=128502)[0m f1_weighted: 0.11377233781881929
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.317, 0.338, 0.092, 0.0, 0.0, 0.063, 0.211, 0.0, 0.067]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0391 | Steps: 2 | Val loss: 2.2291 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.6140 | Steps: 2 | Val loss: 30.8765 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=128361)[0m top1: 0.394589552238806
[2m[36m(func pid=128361)[0m top5: 0.9071828358208955
[2m[36m(func pid=128361)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=128361)[0m f1_macro: 0.32901039535676296
[2m[36m(func pid=128361)[0m f1_weighted: 0.4160256552299994
[2m[36m(func pid=128361)[0m f1_per_class: [0.317, 0.333, 0.558, 0.505, 0.098, 0.188, 0.523, 0.361, 0.207, 0.201]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9873 | Steps: 2 | Val loss: 1.8230 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.4005 | Steps: 2 | Val loss: 22.2484 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:30:35 (running for 00:20:50.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.208 |      0.328 |                   71 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.039 |      0.329 |                   48 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.27  |      0.109 |                   46 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  3.614 |      0.151 |                   46 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.20569029850746268
[2m[36m(func pid=129297)[0m top5: 0.6637126865671642
[2m[36m(func pid=129297)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=129297)[0m f1_macro: 0.150680634971957
[2m[36m(func pid=129297)[0m f1_weighted: 0.16342071519607443
[2m[36m(func pid=129297)[0m f1_per_class: [0.057, 0.349, 0.0, 0.069, 0.232, 0.154, 0.137, 0.328, 0.073, 0.107]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m top1: 0.35494402985074625
[2m[36m(func pid=122997)[0m top5: 0.8708022388059702
[2m[36m(func pid=122997)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=122997)[0m f1_macro: 0.3300629652727515
[2m[36m(func pid=122997)[0m f1_weighted: 0.38815463292513214
[2m[36m(func pid=122997)[0m f1_per_class: [0.308, 0.283, 0.571, 0.443, 0.091, 0.355, 0.443, 0.412, 0.203, 0.191]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0675 | Steps: 2 | Val loss: 2.2547 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=128502)[0m top1: 0.26026119402985076
[2m[36m(func pid=128502)[0m top5: 0.7439365671641791
[2m[36m(func pid=128502)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=128502)[0m f1_macro: 0.11210031383789346
[2m[36m(func pid=128502)[0m f1_weighted: 0.22313058503546573
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.289, 0.0, 0.021, 0.0, 0.0, 0.514, 0.238, 0.0, 0.059]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0985 | Steps: 2 | Val loss: 21.6239 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=128361)[0m top1: 0.3983208955223881
[2m[36m(func pid=128361)[0m top5: 0.9076492537313433
[2m[36m(func pid=128361)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=128361)[0m f1_macro: 0.33852291364132625
[2m[36m(func pid=128361)[0m f1_weighted: 0.42037621488076204
[2m[36m(func pid=128361)[0m f1_per_class: [0.356, 0.33, 0.615, 0.52, 0.108, 0.191, 0.524, 0.336, 0.205, 0.198]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.0041 | Steps: 2 | Val loss: 1.8115 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2839 | Steps: 2 | Val loss: 35.6271 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 04:30:41 (running for 00:20:55.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  0.987 |      0.33  |                   72 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.068 |      0.339 |                   49 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  3.401 |      0.112 |                   47 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.098 |      0.21  |                   47 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.23647388059701493
[2m[36m(func pid=129297)[0m top5: 0.6996268656716418
[2m[36m(func pid=129297)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=129297)[0m f1_macro: 0.21022890361560925
[2m[36m(func pid=129297)[0m f1_weighted: 0.20521936043897251
[2m[36m(func pid=129297)[0m f1_per_class: [0.078, 0.392, 0.421, 0.124, 0.225, 0.12, 0.204, 0.328, 0.075, 0.136]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m top1: 0.3582089552238806
[2m[36m(func pid=122997)[0m top5: 0.8722014925373134
[2m[36m(func pid=122997)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=122997)[0m f1_macro: 0.3308409226836236
[2m[36m(func pid=122997)[0m f1_weighted: 0.391090675086178
[2m[36m(func pid=122997)[0m f1_per_class: [0.286, 0.288, 0.571, 0.444, 0.096, 0.353, 0.448, 0.424, 0.214, 0.184]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0524 | Steps: 2 | Val loss: 2.2837 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=128502)[0m top1: 0.1958955223880597
[2m[36m(func pid=128502)[0m top5: 0.7005597014925373
[2m[36m(func pid=128502)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=128502)[0m f1_macro: 0.07378190247029423
[2m[36m(func pid=128502)[0m f1_weighted: 0.1715009136872064
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.242, 0.0, 0.023, 0.0, 0.0, 0.41, 0.012, 0.0, 0.051]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 7.4313 | Steps: 2 | Val loss: 18.9434 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=128361)[0m top1: 0.3983208955223881
[2m[36m(func pid=128361)[0m top5: 0.9048507462686567
[2m[36m(func pid=128361)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=128361)[0m f1_macro: 0.34344981633863975
[2m[36m(func pid=128361)[0m f1_weighted: 0.42049424921017714
[2m[36m(func pid=128361)[0m f1_per_class: [0.407, 0.328, 0.6, 0.521, 0.113, 0.201, 0.516, 0.338, 0.221, 0.189]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9962 | Steps: 2 | Val loss: 1.8075 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9809 | Steps: 2 | Val loss: 40.3318 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:30:46 (running for 00:21:01.00)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.004 |      0.331 |                   73 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.052 |      0.343 |                   50 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.284 |      0.074 |                   48 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  7.431 |      0.169 |                   48 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.20055970149253732
[2m[36m(func pid=129297)[0m top5: 0.7014925373134329
[2m[36m(func pid=129297)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=129297)[0m f1_macro: 0.16917542477430253
[2m[36m(func pid=129297)[0m f1_weighted: 0.18491060095583875
[2m[36m(func pid=129297)[0m f1_per_class: [0.083, 0.451, 0.069, 0.124, 0.196, 0.161, 0.096, 0.325, 0.061, 0.127]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=122997)[0m top1: 0.3605410447761194
[2m[36m(func pid=122997)[0m top5: 0.8726679104477612
[2m[36m(func pid=122997)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=122997)[0m f1_macro: 0.33090491838183833
[2m[36m(func pid=122997)[0m f1_weighted: 0.3926701122340766
[2m[36m(func pid=122997)[0m f1_per_class: [0.283, 0.295, 0.545, 0.441, 0.092, 0.366, 0.446, 0.433, 0.217, 0.19]
[2m[36m(func pid=122997)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0449 | Steps: 2 | Val loss: 2.3278 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=128502)[0m top1: 0.12080223880597014
[2m[36m(func pid=128502)[0m top5: 0.6800373134328358
[2m[36m(func pid=128502)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=128502)[0m f1_macro: 0.05527650159859472
[2m[36m(func pid=128502)[0m f1_weighted: 0.10651522422236073
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.179, 0.0, 0.039, 0.0, 0.121, 0.17, 0.0, 0.0, 0.045]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m top1: 0.3969216417910448
[2m[36m(func pid=128361)[0m top5: 0.9034514925373134
[2m[36m(func pid=128361)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=128361)[0m f1_macro: 0.3488863672626144
[2m[36m(func pid=128361)[0m f1_weighted: 0.4195253205933833
[2m[36m(func pid=128361)[0m f1_per_class: [0.417, 0.33, 0.615, 0.518, 0.113, 0.222, 0.502, 0.347, 0.234, 0.189]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1724 | Steps: 2 | Val loss: 18.2930 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=122997)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0768 | Steps: 2 | Val loss: 1.8045 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6993 | Steps: 2 | Val loss: 38.7479 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:30:52 (running for 00:21:06.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00008 | RUNNING    | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  0.996 |      0.331 |                   74 |
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.045 |      0.349 |                   51 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.981 |      0.055 |                   49 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.172 |      0.141 |                   49 |
| train_35a0b_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129297)[0m top1: 0.15065298507462688
[2m[36m(func pid=129297)[0m top5: 0.6651119402985075
[2m[36m(func pid=129297)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=129297)[0m f1_macro: 0.1409072744781309
[2m[36m(func pid=129297)[0m f1_weighted: 0.131764687897838
[2m[36m(func pid=129297)[0m f1_per_class: [0.082, 0.402, 0.067, 0.068, 0.205, 0.148, 0.006, 0.346, 0.012, 0.073]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0223 | Steps: 2 | Val loss: 2.3434 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=122997)[0m top1: 0.3572761194029851
[2m[36m(func pid=122997)[0m top5: 0.8736007462686567
[2m[36m(func pid=122997)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=122997)[0m f1_macro: 0.3304180205662072
[2m[36m(func pid=122997)[0m f1_weighted: 0.39102230472206984
[2m[36m(func pid=122997)[0m f1_per_class: [0.283, 0.294, 0.571, 0.442, 0.089, 0.357, 0.446, 0.423, 0.212, 0.187]
[2m[36m(func pid=128502)[0m top1: 0.10121268656716417
[2m[36m(func pid=128502)[0m top5: 0.59375
[2m[36m(func pid=128502)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=128502)[0m f1_macro: 0.05806939690161701
[2m[36m(func pid=128502)[0m f1_weighted: 0.07480039315740662
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.136, 0.0, 0.003, 0.0, 0.39, 0.019, 0.0, 0.0, 0.033]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m top1: 0.39598880597014924
[2m[36m(func pid=128361)[0m top5: 0.9020522388059702
[2m[36m(func pid=128361)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=128361)[0m f1_macro: 0.3473607234722808
[2m[36m(func pid=128361)[0m f1_weighted: 0.4204363417249559
[2m[36m(func pid=128361)[0m f1_per_class: [0.4, 0.316, 0.615, 0.516, 0.111, 0.237, 0.511, 0.35, 0.236, 0.183]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.3273 | Steps: 2 | Val loss: 17.0882 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.9719 | Steps: 2 | Val loss: 39.2107 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0225 | Steps: 2 | Val loss: 2.3842 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=129297)[0m top1: 0.12313432835820895
[2m[36m(func pid=129297)[0m top5: 0.648320895522388
[2m[36m(func pid=129297)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=129297)[0m f1_macro: 0.13119225852932553
[2m[36m(func pid=129297)[0m f1_weighted: 0.1032616242081404
[2m[36m(func pid=129297)[0m f1_per_class: [0.094, 0.339, 0.087, 0.02, 0.219, 0.114, 0.0, 0.36, 0.025, 0.053]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.10261194029850747
[2m[36m(func pid=128502)[0m top5: 0.4552238805970149
[2m[36m(func pid=128502)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=128502)[0m f1_macro: 0.052257083711034304
[2m[36m(func pid=128502)[0m f1_weighted: 0.06349440526481323
[2m[36m(func pid=128502)[0m f1_per_class: [0.0, 0.12, 0.0, 0.0, 0.0, 0.372, 0.0, 0.0, 0.0, 0.031]
[2m[36m(func pid=128361)[0m top1: 0.39505597014925375
[2m[36m(func pid=128361)[0m top5: 0.9001865671641791
[2m[36m(func pid=128361)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=128361)[0m f1_macro: 0.3475035630518969
[2m[36m(func pid=128361)[0m f1_weighted: 0.42124141258480985
[2m[36m(func pid=128361)[0m f1_per_class: [0.382, 0.317, 0.615, 0.509, 0.106, 0.253, 0.512, 0.365, 0.233, 0.184]
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3158 | Steps: 2 | Val loss: 22.3954 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:30:57 (running for 00:21:11.93)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.022 |      0.347 |                   52 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.699 |      0.058 |                   50 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.327 |      0.131 |                   50 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=140594)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=140594)[0m Configuration completed!
[2m[36m(func pid=140594)[0m New optimizer parameters:
[2m[36m(func pid=140594)[0m SGD (
[2m[36m(func pid=140594)[0m Parameter Group 0
[2m[36m(func pid=140594)[0m     dampening: 0
[2m[36m(func pid=140594)[0m     differentiable: False
[2m[36m(func pid=140594)[0m     foreach: None
[2m[36m(func pid=140594)[0m     lr: 0.0001
[2m[36m(func pid=140594)[0m     maximize: False
[2m[36m(func pid=140594)[0m     momentum: 0.9
[2m[36m(func pid=140594)[0m     nesterov: False
[2m[36m(func pid=140594)[0m     weight_decay: 0.0001
[2m[36m(func pid=140594)[0m )
[2m[36m(func pid=140594)[0m 
== Status ==
Current time: 2024-01-07 04:31:03 (running for 00:21:17.43)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.022 |      0.348 |                   53 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.972 |      0.052 |                   51 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.316 |      0.142 |                   51 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.1455223880597015
[2m[36m(func pid=129297)[0m top5: 0.7686567164179104
[2m[36m(func pid=129297)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=129297)[0m f1_macro: 0.14215491506914996
[2m[36m(func pid=129297)[0m f1_weighted: 0.11105555408629963
[2m[36m(func pid=129297)[0m f1_per_class: [0.076, 0.273, 0.115, 0.059, 0.124, 0.191, 0.0, 0.273, 0.141, 0.169]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0341 | Steps: 2 | Val loss: 2.4050 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5034 | Steps: 2 | Val loss: 38.6771 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0060 | Steps: 2 | Val loss: 2.3197 | Batch size: 32 | lr: 0.0001 | Duration: 4.86s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4788 | Steps: 2 | Val loss: 33.7998 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=128361)[0m top1: 0.3969216417910448
[2m[36m(func pid=128361)[0m top5: 0.9001865671641791
[2m[36m(func pid=128361)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=128361)[0m f1_macro: 0.34829086448312596
[2m[36m(func pid=128361)[0m f1_weighted: 0.42396994653148945
[2m[36m(func pid=128361)[0m f1_per_class: [0.355, 0.312, 0.6, 0.509, 0.106, 0.278, 0.51, 0.395, 0.233, 0.185]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.1021455223880597
[2m[36m(func pid=128502)[0m top5: 0.3516791044776119
[2m[36m(func pid=128502)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=128502)[0m f1_macro: 0.07506717407239247
[2m[36m(func pid=128502)[0m f1_weighted: 0.05913360200297087
[2m[36m(func pid=128502)[0m f1_per_class: [0.036, 0.1, 0.235, 0.0, 0.0, 0.339, 0.0, 0.0, 0.021, 0.019]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:31:08 (running for 00:21:22.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.034 |      0.348 |                   54 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.503 |      0.075 |                   52 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.316 |      0.142 |                   51 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  3.006 |      0.12  |                    1 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.17397388059701493
[2m[36m(func pid=140594)[0m top5: 0.5326492537313433
[2m[36m(func pid=140594)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=140594)[0m f1_macro: 0.12002552973736866
[2m[36m(func pid=140594)[0m f1_weighted: 0.12239574767857399
[2m[36m(func pid=140594)[0m f1_per_class: [0.329, 0.346, 0.0, 0.088, 0.0, 0.208, 0.015, 0.014, 0.0, 0.2]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=129297)[0m top1: 0.1599813432835821
[2m[36m(func pid=129297)[0m top5: 0.7639925373134329
[2m[36m(func pid=129297)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=129297)[0m f1_macro: 0.15173256437242325
[2m[36m(func pid=129297)[0m f1_weighted: 0.14090177994618594
[2m[36m(func pid=129297)[0m f1_per_class: [0.086, 0.252, 0.092, 0.14, 0.104, 0.148, 0.053, 0.273, 0.122, 0.247]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0126 | Steps: 2 | Val loss: 2.4331 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4995 | Steps: 2 | Val loss: 36.8222 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9751 | Steps: 2 | Val loss: 2.3254 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.2974 | Steps: 2 | Val loss: 32.7150 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=128361)[0m top1: 0.39132462686567165
[2m[36m(func pid=128361)[0m top5: 0.8987873134328358
[2m[36m(func pid=128361)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=128361)[0m f1_macro: 0.3396366480693159
[2m[36m(func pid=128361)[0m f1_weighted: 0.41892093419470516
[2m[36m(func pid=128361)[0m f1_per_class: [0.329, 0.299, 0.571, 0.507, 0.103, 0.262, 0.508, 0.42, 0.218, 0.179]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m top1: 0.11054104477611941
[2m[36m(func pid=128502)[0m top5: 0.28638059701492535
[2m[36m(func pid=128502)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=128502)[0m f1_macro: 0.08426354706142855
[2m[36m(func pid=128502)[0m f1_weighted: 0.055259123186916906
[2m[36m(func pid=128502)[0m f1_per_class: [0.17, 0.064, 0.188, 0.0, 0.0, 0.322, 0.0, 0.0, 0.079, 0.019]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.18330223880597016
[2m[36m(func pid=140594)[0m top5: 0.5261194029850746
[2m[36m(func pid=140594)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=140594)[0m f1_macro: 0.10905229338020554
[2m[36m(func pid=140594)[0m f1_weighted: 0.12744336800787492
[2m[36m(func pid=140594)[0m f1_per_class: [0.216, 0.329, 0.0, 0.097, 0.011, 0.288, 0.012, 0.024, 0.0, 0.113]
[2m[36m(func pid=140594)[0m 
== Status ==
Current time: 2024-01-07 04:31:13 (running for 00:21:27.79)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.013 |      0.34  |                   55 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.5   |      0.084 |                   53 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.479 |      0.152 |                   52 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.975 |      0.109 |                    2 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.20988805970149255
[2m[36m(func pid=129297)[0m top5: 0.7728544776119403
[2m[36m(func pid=129297)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=129297)[0m f1_macro: 0.1822803091067116
[2m[36m(func pid=129297)[0m f1_weighted: 0.19884121930796692
[2m[36m(func pid=129297)[0m f1_per_class: [0.116, 0.367, 0.098, 0.176, 0.134, 0.086, 0.162, 0.296, 0.135, 0.253]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0165 | Steps: 2 | Val loss: 2.4656 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7675 | Steps: 2 | Val loss: 32.7719 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9657 | Steps: 2 | Val loss: 2.3373 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=128361)[0m top1: 0.3908582089552239
[2m[36m(func pid=128361)[0m top5: 0.8964552238805971
[2m[36m(func pid=128361)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=128361)[0m f1_macro: 0.33780184293685894
[2m[36m(func pid=128361)[0m f1_weighted: 0.4195238580454174
[2m[36m(func pid=128361)[0m f1_per_class: [0.309, 0.299, 0.558, 0.512, 0.11, 0.264, 0.505, 0.432, 0.215, 0.176]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4145 | Steps: 2 | Val loss: 21.9581 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=128502)[0m top1: 0.09841417910447761
[2m[36m(func pid=128502)[0m top5: 0.34281716417910446
[2m[36m(func pid=128502)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=128502)[0m f1_macro: 0.05880751590912987
[2m[36m(func pid=128502)[0m f1_weighted: 0.04499242518787994
[2m[36m(func pid=128502)[0m f1_per_class: [0.055, 0.032, 0.074, 0.0, 0.0, 0.307, 0.0, 0.0, 0.066, 0.054]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:31:18 (running for 00:21:33.00)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.017 |      0.338 |                   56 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.768 |      0.059 |                   54 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.297 |      0.182 |                   53 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.966 |      0.095 |                    3 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.16791044776119404
[2m[36m(func pid=140594)[0m top5: 0.5097947761194029
[2m[36m(func pid=140594)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=140594)[0m f1_macro: 0.09512075403124721
[2m[36m(func pid=140594)[0m f1_weighted: 0.11786867998266001
[2m[36m(func pid=140594)[0m f1_per_class: [0.16, 0.283, 0.0, 0.097, 0.011, 0.297, 0.009, 0.021, 0.0, 0.074]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=129297)[0m top1: 0.25046641791044777
[2m[36m(func pid=129297)[0m top5: 0.804570895522388
[2m[36m(func pid=129297)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=129297)[0m f1_macro: 0.2064633538544045
[2m[36m(func pid=129297)[0m f1_weighted: 0.2170076167696713
[2m[36m(func pid=129297)[0m f1_per_class: [0.119, 0.418, 0.18, 0.116, 0.148, 0.153, 0.219, 0.32, 0.098, 0.293]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0261 | Steps: 2 | Val loss: 2.4907 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9553 | Steps: 2 | Val loss: 30.6502 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9569 | Steps: 2 | Val loss: 2.3539 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=128361)[0m top1: 0.3917910447761194
[2m[36m(func pid=128361)[0m top5: 0.8922574626865671
[2m[36m(func pid=128361)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=128361)[0m f1_macro: 0.33914825223579675
[2m[36m(func pid=128361)[0m f1_weighted: 0.4219231010241239
[2m[36m(func pid=128361)[0m f1_per_class: [0.299, 0.295, 0.558, 0.51, 0.115, 0.277, 0.51, 0.443, 0.213, 0.17]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4935 | Steps: 2 | Val loss: 21.9377 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=128502)[0m top1: 0.08488805970149253
[2m[36m(func pid=128502)[0m top5: 0.45988805970149255
[2m[36m(func pid=128502)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=128502)[0m f1_macro: 0.06136608712435796
[2m[36m(func pid=128502)[0m f1_weighted: 0.05252309483409112
[2m[36m(func pid=128502)[0m f1_per_class: [0.032, 0.07, 0.057, 0.0, 0.0, 0.275, 0.021, 0.0, 0.0, 0.159]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.15438432835820895
[2m[36m(func pid=140594)[0m top5: 0.5
[2m[36m(func pid=140594)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=140594)[0m f1_macro: 0.08923123184804363
[2m[36m(func pid=140594)[0m f1_weighted: 0.11141038595412597
[2m[36m(func pid=140594)[0m f1_per_class: [0.15, 0.256, 0.0, 0.095, 0.01, 0.284, 0.012, 0.017, 0.0, 0.069]
[2m[36m(func pid=140594)[0m 
== Status ==
Current time: 2024-01-07 04:31:24 (running for 00:21:38.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.026 |      0.339 |                   57 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.955 |      0.061 |                   55 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.414 |      0.206 |                   54 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.957 |      0.089 |                    4 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.23414179104477612
[2m[36m(func pid=129297)[0m top5: 0.8050373134328358
[2m[36m(func pid=129297)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=129297)[0m f1_macro: 0.22798371483103547
[2m[36m(func pid=129297)[0m f1_weighted: 0.19514423262241049
[2m[36m(func pid=129297)[0m f1_per_class: [0.106, 0.358, 0.48, 0.093, 0.083, 0.176, 0.187, 0.308, 0.125, 0.364]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0164 | Steps: 2 | Val loss: 2.5357 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8148 | Steps: 2 | Val loss: 27.6529 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9439 | Steps: 2 | Val loss: 2.3801 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=128361)[0m top1: 0.386660447761194
[2m[36m(func pid=128361)[0m top5: 0.886660447761194
[2m[36m(func pid=128361)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=128361)[0m f1_macro: 0.33377348584221866
[2m[36m(func pid=128361)[0m f1_weighted: 0.41859332995445664
[2m[36m(func pid=128361)[0m f1_per_class: [0.289, 0.285, 0.545, 0.506, 0.109, 0.263, 0.515, 0.448, 0.212, 0.166]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.5948 | Steps: 2 | Val loss: 28.9695 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=128502)[0m top1: 0.08582089552238806
[2m[36m(func pid=128502)[0m top5: 0.5638992537313433
[2m[36m(func pid=128502)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=128502)[0m f1_macro: 0.07805829863385763
[2m[36m(func pid=128502)[0m f1_weighted: 0.07786863634085446
[2m[36m(func pid=128502)[0m f1_per_class: [0.027, 0.143, 0.06, 0.0, 0.0, 0.236, 0.076, 0.0, 0.0, 0.239]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.14598880597014927
[2m[36m(func pid=140594)[0m top5: 0.48367537313432835
[2m[36m(func pid=140594)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=140594)[0m f1_macro: 0.0761719032729316
[2m[36m(func pid=140594)[0m f1_weighted: 0.10827134145177741
[2m[36m(func pid=140594)[0m f1_per_class: [0.101, 0.226, 0.0, 0.097, 0.0, 0.291, 0.018, 0.029, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 04:31:29 (running for 00:21:43.54)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.016 |      0.334 |                   58 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.815 |      0.078 |                   56 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.494 |      0.228 |                   55 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.944 |      0.076 |                    5 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=129297)[0m top1: 0.2355410447761194
[2m[36m(func pid=129297)[0m top5: 0.7859141791044776
[2m[36m(func pid=129297)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=129297)[0m f1_macro: 0.2581262500423534
[2m[36m(func pid=129297)[0m f1_weighted: 0.18055921196906397
[2m[36m(func pid=129297)[0m f1_per_class: [0.083, 0.348, 0.696, 0.038, 0.235, 0.285, 0.147, 0.342, 0.074, 0.333]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0284 | Steps: 2 | Val loss: 2.5865 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.8561 | Steps: 2 | Val loss: 26.4465 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9357 | Steps: 2 | Val loss: 2.3841 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=128361)[0m top1: 0.3824626865671642
[2m[36m(func pid=128361)[0m top5: 0.8843283582089553
[2m[36m(func pid=128361)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=128361)[0m f1_macro: 0.3315230409223892
[2m[36m(func pid=128361)[0m f1_weighted: 0.4160659471603122
[2m[36m(func pid=128361)[0m f1_per_class: [0.278, 0.28, 0.545, 0.505, 0.109, 0.269, 0.509, 0.446, 0.212, 0.162]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 7.7217 | Steps: 2 | Val loss: 43.4779 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=128502)[0m top1: 0.10307835820895522
[2m[36m(func pid=128502)[0m top5: 0.6147388059701493
[2m[36m(func pid=128502)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=128502)[0m f1_macro: 0.08585668876588312
[2m[36m(func pid=128502)[0m f1_weighted: 0.11644000040274904
[2m[36m(func pid=128502)[0m f1_per_class: [0.027, 0.23, 0.024, 0.0, 0.0, 0.119, 0.194, 0.036, 0.0, 0.23]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.13805970149253732
[2m[36m(func pid=140594)[0m top5: 0.48134328358208955
[2m[36m(func pid=140594)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=140594)[0m f1_macro: 0.07953490215498653
[2m[36m(func pid=140594)[0m f1_weighted: 0.1094546367750489
[2m[36m(func pid=140594)[0m f1_per_class: [0.074, 0.195, 0.059, 0.105, 0.0, 0.276, 0.035, 0.04, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
== Status ==
Current time: 2024-01-07 04:31:34 (running for 00:21:49.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.028 |      0.332 |                   59 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.856 |      0.086 |                   57 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.595 |      0.258 |                   56 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.936 |      0.08  |                    6 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.2178171641791045
[2m[36m(func pid=129297)[0m top5: 0.7453358208955224
[2m[36m(func pid=129297)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=129297)[0m f1_macro: 0.1560477532649777
[2m[36m(func pid=129297)[0m f1_weighted: 0.13630890504474638
[2m[36m(func pid=129297)[0m f1_per_class: [0.075, 0.323, 0.0, 0.023, 0.164, 0.371, 0.015, 0.386, 0.0, 0.203]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0701 | Steps: 2 | Val loss: 2.5920 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3070 | Steps: 2 | Val loss: 25.7075 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9337 | Steps: 2 | Val loss: 2.3734 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=128361)[0m top1: 0.3824626865671642
[2m[36m(func pid=128361)[0m top5: 0.8843283582089553
[2m[36m(func pid=128361)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=128361)[0m f1_macro: 0.3343648607170014
[2m[36m(func pid=128361)[0m f1_weighted: 0.4177400666381343
[2m[36m(func pid=128361)[0m f1_per_class: [0.27, 0.268, 0.545, 0.501, 0.093, 0.283, 0.514, 0.466, 0.24, 0.164]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.7839 | Steps: 2 | Val loss: 44.2224 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=128502)[0m top1: 0.14505597014925373
[2m[36m(func pid=128502)[0m top5: 0.6385261194029851
[2m[36m(func pid=128502)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=128502)[0m f1_macro: 0.12148181132711433
[2m[36m(func pid=128502)[0m f1_weighted: 0.15620742785113453
[2m[36m(func pid=128502)[0m f1_per_class: [0.029, 0.287, 0.022, 0.0, 0.0, 0.045, 0.274, 0.278, 0.0, 0.281]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:31:40 (running for 00:21:54.45)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.07  |      0.334 |                   60 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.307 |      0.121 |                   58 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  7.722 |      0.156 |                   57 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.934 |      0.079 |                    7 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.13759328358208955
[2m[36m(func pid=140594)[0m top5: 0.48927238805970147
[2m[36m(func pid=140594)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=140594)[0m f1_macro: 0.07859405019980613
[2m[36m(func pid=140594)[0m f1_weighted: 0.11224482501843495
[2m[36m(func pid=140594)[0m f1_per_class: [0.054, 0.188, 0.053, 0.097, 0.009, 0.277, 0.057, 0.041, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0080 | Steps: 2 | Val loss: 2.6854 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=129297)[0m top1: 0.1791044776119403
[2m[36m(func pid=129297)[0m top5: 0.7486007462686567
[2m[36m(func pid=129297)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=129297)[0m f1_macro: 0.11795674344596194
[2m[36m(func pid=129297)[0m f1_weighted: 0.1146853586667103
[2m[36m(func pid=129297)[0m f1_per_class: [0.065, 0.301, 0.0, 0.044, 0.115, 0.164, 0.028, 0.347, 0.0, 0.117]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9450 | Steps: 2 | Val loss: 24.9674 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9201 | Steps: 2 | Val loss: 2.3600 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=128361)[0m top1: 0.3675373134328358
[2m[36m(func pid=128361)[0m top5: 0.8819962686567164
[2m[36m(func pid=128361)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=128361)[0m f1_macro: 0.3328922176711879
[2m[36m(func pid=128361)[0m f1_weighted: 0.40867927964296885
[2m[36m(func pid=128361)[0m f1_per_class: [0.213, 0.26, 0.585, 0.457, 0.093, 0.298, 0.522, 0.479, 0.264, 0.158]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6756 | Steps: 2 | Val loss: 34.4874 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=128502)[0m top1: 0.14365671641791045
[2m[36m(func pid=128502)[0m top5: 0.6511194029850746
[2m[36m(func pid=128502)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=128502)[0m f1_macro: 0.11719771060472581
[2m[36m(func pid=128502)[0m f1_weighted: 0.1405383455531496
[2m[36m(func pid=128502)[0m f1_per_class: [0.035, 0.303, 0.015, 0.0, 0.0, 0.031, 0.211, 0.315, 0.0, 0.262]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:31:45 (running for 00:21:59.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.008 |      0.333 |                   61 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.945 |      0.117 |                   59 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.784 |      0.118 |                   58 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.92  |      0.081 |                    8 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.13805970149253732
[2m[36m(func pid=140594)[0m top5: 0.5065298507462687
[2m[36m(func pid=140594)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=140594)[0m f1_macro: 0.08050918811172181
[2m[36m(func pid=140594)[0m f1_weighted: 0.11631283785599122
[2m[36m(func pid=140594)[0m f1_per_class: [0.05, 0.184, 0.059, 0.101, 0.009, 0.276, 0.068, 0.048, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0155 | Steps: 2 | Val loss: 2.7674 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=129297)[0m top1: 0.1791044776119403
[2m[36m(func pid=129297)[0m top5: 0.7765858208955224
[2m[36m(func pid=129297)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=129297)[0m f1_macro: 0.11755154706847741
[2m[36m(func pid=129297)[0m f1_weighted: 0.13219865835805192
[2m[36m(func pid=129297)[0m f1_per_class: [0.083, 0.28, 0.0, 0.116, 0.114, 0.054, 0.078, 0.315, 0.0, 0.135]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8086 | Steps: 2 | Val loss: 26.4117 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=128361)[0m top1: 0.353544776119403
[2m[36m(func pid=128361)[0m top5: 0.8777985074626866
[2m[36m(func pid=128361)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=128361)[0m f1_macro: 0.326000426332445
[2m[36m(func pid=128361)[0m f1_weighted: 0.39544709481233803
[2m[36m(func pid=128361)[0m f1_per_class: [0.197, 0.246, 0.6, 0.425, 0.089, 0.314, 0.512, 0.485, 0.232, 0.159]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9328 | Steps: 2 | Val loss: 2.3512 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 9.2865 | Steps: 2 | Val loss: 23.7053 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=128502)[0m top1: 0.1455223880597015
[2m[36m(func pid=128502)[0m top5: 0.6240671641791045
[2m[36m(func pid=128502)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=128502)[0m f1_macro: 0.1303788282724853
[2m[36m(func pid=128502)[0m f1_weighted: 0.15108220554821516
[2m[36m(func pid=128502)[0m f1_per_class: [0.053, 0.296, 0.2, 0.0, 0.0, 0.023, 0.256, 0.288, 0.0, 0.188]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:31:50 (running for 00:22:05.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.015 |      0.326 |                   62 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.809 |      0.13  |                   60 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.676 |      0.118 |                   59 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.933 |      0.087 |                    9 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.1455223880597015
[2m[36m(func pid=140594)[0m top5: 0.5158582089552238
[2m[36m(func pid=140594)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=140594)[0m f1_macro: 0.08722392088614747
[2m[36m(func pid=140594)[0m f1_weighted: 0.12816821793948954
[2m[36m(func pid=140594)[0m f1_per_class: [0.047, 0.179, 0.062, 0.117, 0.016, 0.287, 0.088, 0.065, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0125 | Steps: 2 | Val loss: 2.8365 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=129297)[0m top1: 0.19076492537313433
[2m[36m(func pid=129297)[0m top5: 0.7989738805970149
[2m[36m(func pid=129297)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=129297)[0m f1_macro: 0.13374665581832962
[2m[36m(func pid=129297)[0m f1_weighted: 0.16843195325972882
[2m[36m(func pid=129297)[0m f1_per_class: [0.092, 0.242, 0.0, 0.178, 0.119, 0.031, 0.168, 0.32, 0.0, 0.187]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.1654 | Steps: 2 | Val loss: 37.4759 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=128361)[0m top1: 0.345615671641791
[2m[36m(func pid=128361)[0m top5: 0.8773320895522388
[2m[36m(func pid=128361)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=128361)[0m f1_macro: 0.32753683329667305
[2m[36m(func pid=128361)[0m f1_weighted: 0.38866495848071614
[2m[36m(func pid=128361)[0m f1_per_class: [0.186, 0.252, 0.649, 0.393, 0.092, 0.328, 0.511, 0.493, 0.212, 0.159]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8954 | Steps: 2 | Val loss: 2.3372 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.1342 | Steps: 2 | Val loss: 15.1748 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=128502)[0m top1: 0.13759328358208955
[2m[36m(func pid=128502)[0m top5: 0.5970149253731343
[2m[36m(func pid=128502)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=128502)[0m f1_macro: 0.10294681671610606
[2m[36m(func pid=128502)[0m f1_weighted: 0.15466162017996712
[2m[36m(func pid=128502)[0m f1_per_class: [0.057, 0.177, 0.0, 0.003, 0.0, 0.038, 0.338, 0.247, 0.025, 0.143]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:31:56 (running for 00:22:10.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.013 |      0.328 |                   63 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.165 |      0.103 |                   61 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  9.287 |      0.134 |                   60 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.895 |      0.089 |                   10 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.1478544776119403
[2m[36m(func pid=140594)[0m top5: 0.5303171641791045
[2m[36m(func pid=140594)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=140594)[0m f1_macro: 0.08853756577286073
[2m[36m(func pid=140594)[0m f1_weighted: 0.1310071301278441
[2m[36m(func pid=140594)[0m f1_per_class: [0.047, 0.184, 0.057, 0.128, 0.014, 0.292, 0.082, 0.07, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0060 | Steps: 2 | Val loss: 2.9070 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=129297)[0m top1: 0.25652985074626866
[2m[36m(func pid=129297)[0m top5: 0.8059701492537313
[2m[36m(func pid=129297)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=129297)[0m f1_macro: 0.18509058398716133
[2m[36m(func pid=129297)[0m f1_weighted: 0.2685845694257914
[2m[36m(func pid=129297)[0m f1_per_class: [0.105, 0.232, 0.0, 0.291, 0.143, 0.127, 0.349, 0.401, 0.016, 0.188]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1913 | Steps: 2 | Val loss: 42.9376 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=128361)[0m top1: 0.33955223880597013
[2m[36m(func pid=128361)[0m top5: 0.8736007462686567
[2m[36m(func pid=128361)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=128361)[0m f1_macro: 0.32476731691695476
[2m[36m(func pid=128361)[0m f1_weighted: 0.3825526893379294
[2m[36m(func pid=128361)[0m f1_per_class: [0.174, 0.261, 0.649, 0.376, 0.095, 0.33, 0.501, 0.498, 0.211, 0.153]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9218 | Steps: 2 | Val loss: 2.3279 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3882 | Steps: 2 | Val loss: 9.1535 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=128502)[0m top1: 0.1455223880597015
[2m[36m(func pid=128502)[0m top5: 0.613339552238806
[2m[36m(func pid=128502)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=128502)[0m f1_macro: 0.08986654937781546
[2m[36m(func pid=128502)[0m f1_weighted: 0.1649060827642814
[2m[36m(func pid=128502)[0m f1_per_class: [0.06, 0.123, 0.0, 0.032, 0.0, 0.059, 0.398, 0.115, 0.022, 0.091]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:32:01 (running for 00:22:15.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.006 |      0.325 |                   64 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  1.191 |      0.09  |                   62 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  3.134 |      0.185 |                   61 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.922 |      0.093 |                   11 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.14458955223880596
[2m[36m(func pid=140594)[0m top5: 0.5401119402985075
[2m[36m(func pid=140594)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=140594)[0m f1_macro: 0.09280897086111539
[2m[36m(func pid=140594)[0m f1_weighted: 0.12715959846847352
[2m[36m(func pid=140594)[0m f1_per_class: [0.045, 0.178, 0.108, 0.119, 0.02, 0.295, 0.078, 0.073, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0245 | Steps: 2 | Val loss: 2.9645 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=129297)[0m top1: 0.2933768656716418
[2m[36m(func pid=129297)[0m top5: 0.7985074626865671
[2m[36m(func pid=129297)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=129297)[0m f1_macro: 0.21931255381427248
[2m[36m(func pid=129297)[0m f1_weighted: 0.32716278612968286
[2m[36m(func pid=129297)[0m f1_per_class: [0.115, 0.276, 0.0, 0.301, 0.083, 0.234, 0.451, 0.456, 0.089, 0.187]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.9352 | Steps: 2 | Val loss: 26.2597 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=128361)[0m top1: 0.3414179104477612
[2m[36m(func pid=128361)[0m top5: 0.8717350746268657
[2m[36m(func pid=128361)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=128361)[0m f1_macro: 0.3299550477204606
[2m[36m(func pid=128361)[0m f1_weighted: 0.3846545381007929
[2m[36m(func pid=128361)[0m f1_per_class: [0.178, 0.271, 0.686, 0.366, 0.094, 0.337, 0.507, 0.506, 0.215, 0.14]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8692 | Steps: 2 | Val loss: 2.3220 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.8545 | Steps: 2 | Val loss: 6.5486 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=128502)[0m top1: 0.1767723880597015
[2m[36m(func pid=128502)[0m top5: 0.6907649253731343
[2m[36m(func pid=128502)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=128502)[0m f1_macro: 0.11488933541732944
[2m[36m(func pid=128502)[0m f1_weighted: 0.2077612724621021
[2m[36m(func pid=128502)[0m f1_per_class: [0.07, 0.198, 0.0, 0.097, 0.0, 0.115, 0.409, 0.124, 0.059, 0.076]
[2m[36m(func pid=128502)[0m 
== Status ==
Current time: 2024-01-07 04:32:06 (running for 00:22:20.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.024 |      0.33  |                   65 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.935 |      0.115 |                   63 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.388 |      0.219 |                   62 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.922 |      0.093 |                   11 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.14738805970149255
[2m[36m(func pid=140594)[0m top5: 0.5466417910447762
[2m[36m(func pid=140594)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=140594)[0m f1_macro: 0.09971822796403079
[2m[36m(func pid=140594)[0m f1_weighted: 0.1301601731782081
[2m[36m(func pid=140594)[0m f1_per_class: [0.044, 0.181, 0.162, 0.123, 0.019, 0.306, 0.078, 0.073, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0115 | Steps: 2 | Val loss: 3.0124 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=129297)[0m top1: 0.22574626865671643
[2m[36m(func pid=129297)[0m top5: 0.7831156716417911
[2m[36m(func pid=129297)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=129297)[0m f1_macro: 0.1801298849129259
[2m[36m(func pid=129297)[0m f1_weighted: 0.26899924477431914
[2m[36m(func pid=129297)[0m f1_per_class: [0.105, 0.25, 0.0, 0.254, 0.0, 0.157, 0.346, 0.495, 0.059, 0.136]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7103 | Steps: 2 | Val loss: 13.6079 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=128361)[0m top1: 0.33955223880597013
[2m[36m(func pid=128361)[0m top5: 0.8745335820895522
[2m[36m(func pid=128361)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=128361)[0m f1_macro: 0.3335627344796951
[2m[36m(func pid=128361)[0m f1_weighted: 0.3810124033342234
[2m[36m(func pid=128361)[0m f1_per_class: [0.181, 0.271, 0.727, 0.349, 0.096, 0.354, 0.504, 0.494, 0.214, 0.144]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8698 | Steps: 2 | Val loss: 2.3195 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1964 | Steps: 2 | Val loss: 4.9524 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 04:32:11 (running for 00:22:26.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.011 |      0.334 |                   66 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.71  |      0.133 |                   64 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.854 |      0.18  |                   63 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.869 |      0.1   |                   12 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=128502)[0m top1: 0.23507462686567165
[2m[36m(func pid=128502)[0m top5: 0.8078358208955224
[2m[36m(func pid=128502)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=128502)[0m f1_macro: 0.13318689830996966
[2m[36m(func pid=128502)[0m f1_weighted: 0.24273900569365783
[2m[36m(func pid=128502)[0m f1_per_class: [0.115, 0.321, 0.0, 0.141, 0.0, 0.127, 0.416, 0.069, 0.084, 0.059]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.1478544776119403
[2m[36m(func pid=140594)[0m top5: 0.550839552238806
[2m[36m(func pid=140594)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=140594)[0m f1_macro: 0.10923511360737337
[2m[36m(func pid=140594)[0m f1_weighted: 0.13330064014386303
[2m[36m(func pid=140594)[0m f1_per_class: [0.067, 0.176, 0.227, 0.128, 0.018, 0.308, 0.082, 0.075, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0147 | Steps: 2 | Val loss: 3.0591 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=129297)[0m top1: 0.19263059701492538
[2m[36m(func pid=129297)[0m top5: 0.7793843283582089
[2m[36m(func pid=129297)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=129297)[0m f1_macro: 0.16548323658505096
[2m[36m(func pid=129297)[0m f1_weighted: 0.2130313610630053
[2m[36m(func pid=129297)[0m f1_per_class: [0.112, 0.254, 0.103, 0.294, 0.0, 0.098, 0.141, 0.48, 0.06, 0.114]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5126 | Steps: 2 | Val loss: 10.8187 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=128361)[0m top1: 0.3344216417910448
[2m[36m(func pid=128361)[0m top5: 0.8782649253731343
[2m[36m(func pid=128361)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=128361)[0m f1_macro: 0.330060898918939
[2m[36m(func pid=128361)[0m f1_weighted: 0.3741124266419503
[2m[36m(func pid=128361)[0m f1_per_class: [0.179, 0.283, 0.727, 0.33, 0.098, 0.341, 0.498, 0.489, 0.221, 0.135]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8732 | Steps: 2 | Val loss: 2.3143 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.3554 | Steps: 2 | Val loss: 4.6385 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 04:32:17 (running for 00:22:31.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.015 |      0.33  |                   67 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.513 |      0.132 |                   65 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.196 |      0.165 |                   64 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.87  |      0.109 |                   13 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=128502)[0m top1: 0.24486940298507462
[2m[36m(func pid=128502)[0m top5: 0.8395522388059702
[2m[36m(func pid=128502)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=128502)[0m f1_macro: 0.13232514822037042
[2m[36m(func pid=128502)[0m f1_weighted: 0.23595610439621803
[2m[36m(func pid=128502)[0m f1_per_class: [0.151, 0.324, 0.0, 0.153, 0.0, 0.129, 0.382, 0.051, 0.07, 0.064]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.19402985074626866
[2m[36m(func pid=129297)[0m top5: 0.7621268656716418
[2m[36m(func pid=129297)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=129297)[0m f1_macro: 0.17022823587069266
[2m[36m(func pid=129297)[0m f1_weighted: 0.1993298479356549
[2m[36m(func pid=129297)[0m f1_per_class: [0.111, 0.28, 0.238, 0.345, 0.0, 0.101, 0.037, 0.443, 0.059, 0.088]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=140594)[0m top1: 0.15065298507462688
[2m[36m(func pid=140594)[0m top5: 0.5541044776119403
[2m[36m(func pid=140594)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=140594)[0m f1_macro: 0.11354833564418639
[2m[36m(func pid=140594)[0m f1_weighted: 0.13788643044596713
[2m[36m(func pid=140594)[0m f1_per_class: [0.054, 0.182, 0.267, 0.144, 0.018, 0.306, 0.08, 0.074, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0043 | Steps: 2 | Val loss: 3.1297 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=128361)[0m top1: 0.3306902985074627
[2m[36m(func pid=128361)[0m top5: 0.8777985074626866
[2m[36m(func pid=128361)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=128361)[0m f1_macro: 0.3263270348511192
[2m[36m(func pid=128361)[0m f1_weighted: 0.3711674542523557
[2m[36m(func pid=128361)[0m f1_per_class: [0.188, 0.282, 0.706, 0.33, 0.094, 0.346, 0.489, 0.476, 0.225, 0.128]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8168 | Steps: 2 | Val loss: 9.9087 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6610 | Steps: 2 | Val loss: 5.0753 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8283 | Steps: 2 | Val loss: 2.3063 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 04:32:23 (running for 00:22:37.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.004 |      0.326 |                   68 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.513 |      0.132 |                   65 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.661 |      0.168 |                   66 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.873 |      0.114 |                   14 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.16744402985074627
[2m[36m(func pid=129297)[0m top5: 0.7430037313432836
[2m[36m(func pid=129297)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=129297)[0m f1_macro: 0.16768455469904328
[2m[36m(func pid=129297)[0m f1_weighted: 0.17256059173459126
[2m[36m(func pid=129297)[0m f1_per_class: [0.09, 0.257, 0.182, 0.299, 0.195, 0.056, 0.019, 0.442, 0.072, 0.066]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128502)[0m top1: 0.2392723880597015
[2m[36m(func pid=128502)[0m top5: 0.8423507462686567
[2m[36m(func pid=128502)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=128502)[0m f1_macro: 0.1565330021181804
[2m[36m(func pid=128502)[0m f1_weighted: 0.22304509592734645
[2m[36m(func pid=128502)[0m f1_per_class: [0.098, 0.344, 0.267, 0.157, 0.0, 0.127, 0.318, 0.039, 0.1, 0.114]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.1501865671641791
[2m[36m(func pid=140594)[0m top5: 0.5583022388059702
[2m[36m(func pid=140594)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=140594)[0m f1_macro: 0.11419035689312369
[2m[36m(func pid=140594)[0m f1_weighted: 0.13893438550527656
[2m[36m(func pid=140594)[0m f1_per_class: [0.038, 0.183, 0.28, 0.142, 0.017, 0.309, 0.084, 0.079, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0040 | Steps: 2 | Val loss: 3.1894 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=128361)[0m top1: 0.3292910447761194
[2m[36m(func pid=128361)[0m top5: 0.8768656716417911
[2m[36m(func pid=128361)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=128361)[0m f1_macro: 0.3234699674723903
[2m[36m(func pid=128361)[0m f1_weighted: 0.3701322999478903
[2m[36m(func pid=128361)[0m f1_per_class: [0.182, 0.287, 0.706, 0.329, 0.093, 0.346, 0.488, 0.466, 0.215, 0.124]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7297 | Steps: 2 | Val loss: 9.3199 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.7776 | Steps: 2 | Val loss: 5.9076 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8379 | Steps: 2 | Val loss: 2.2984 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:32:28 (running for 00:22:43.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.004 |      0.323 |                   69 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.817 |      0.157 |                   66 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.661 |      0.168 |                   66 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.838 |      0.112 |                   16 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.15205223880597016
[2m[36m(func pid=140594)[0m top5: 0.5727611940298507
[2m[36m(func pid=140594)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=140594)[0m f1_macro: 0.11222053767055837
[2m[36m(func pid=140594)[0m f1_weighted: 0.144065718932502
[2m[36m(func pid=140594)[0m f1_per_class: [0.026, 0.178, 0.264, 0.144, 0.011, 0.312, 0.103, 0.072, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=129297)[0m top1: 0.12033582089552239
[2m[36m(func pid=129297)[0m top5: 0.7318097014925373
[2m[36m(func pid=129297)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=129297)[0m f1_macro: 0.13255921301331305
[2m[36m(func pid=129297)[0m f1_weighted: 0.12197015547151574
[2m[36m(func pid=129297)[0m f1_per_class: [0.08, 0.095, 0.084, 0.234, 0.202, 0.035, 0.009, 0.463, 0.071, 0.052]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0116 | Steps: 2 | Val loss: 3.2609 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=128502)[0m top1: 0.23880597014925373
[2m[36m(func pid=128502)[0m top5: 0.8330223880597015
[2m[36m(func pid=128502)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=128502)[0m f1_macro: 0.18741914588904737
[2m[36m(func pid=128502)[0m f1_weighted: 0.21475433984063558
[2m[36m(func pid=128502)[0m f1_per_class: [0.095, 0.362, 0.562, 0.178, 0.0, 0.115, 0.254, 0.049, 0.125, 0.133]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m top1: 0.3255597014925373
[2m[36m(func pid=128361)[0m top5: 0.875
[2m[36m(func pid=128361)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=128361)[0m f1_macro: 0.32060412393674376
[2m[36m(func pid=128361)[0m f1_weighted: 0.3655338352774463
[2m[36m(func pid=128361)[0m f1_per_class: [0.185, 0.29, 0.706, 0.32, 0.095, 0.336, 0.487, 0.438, 0.23, 0.121]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8162 | Steps: 2 | Val loss: 2.2979 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.6658 | Steps: 2 | Val loss: 8.0907 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4397 | Steps: 2 | Val loss: 8.8607 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:32:34 (running for 00:22:48.61)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.012 |      0.321 |                   70 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.73  |      0.187 |                   67 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  2.778 |      0.133 |                   67 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.816 |      0.114 |                   17 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.15298507462686567
[2m[36m(func pid=140594)[0m top5: 0.5708955223880597
[2m[36m(func pid=140594)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=140594)[0m f1_macro: 0.11385650738758282
[2m[36m(func pid=140594)[0m f1_weighted: 0.14662186091535995
[2m[36m(func pid=140594)[0m f1_per_class: [0.039, 0.178, 0.246, 0.161, 0.015, 0.312, 0.094, 0.083, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128502)[0m top1: 0.2332089552238806
[2m[36m(func pid=128502)[0m top5: 0.8306902985074627
[2m[36m(func pid=128502)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=128502)[0m f1_macro: 0.18939043609390416
[2m[36m(func pid=128502)[0m f1_weighted: 0.20352820567062024
[2m[36m(func pid=128502)[0m f1_per_class: [0.11, 0.378, 0.529, 0.18, 0.0, 0.102, 0.195, 0.125, 0.12, 0.154]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.08022388059701492
[2m[36m(func pid=129297)[0m top5: 0.707089552238806
[2m[36m(func pid=129297)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=129297)[0m f1_macro: 0.10499578841034199
[2m[36m(func pid=129297)[0m f1_weighted: 0.07941071692448383
[2m[36m(func pid=129297)[0m f1_per_class: [0.072, 0.045, 0.092, 0.141, 0.19, 0.029, 0.003, 0.391, 0.042, 0.044]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0029 | Steps: 2 | Val loss: 3.3364 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=128361)[0m top1: 0.31949626865671643
[2m[36m(func pid=128361)[0m top5: 0.8745335820895522
[2m[36m(func pid=128361)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=128361)[0m f1_macro: 0.3171898284344128
[2m[36m(func pid=128361)[0m f1_weighted: 0.3583570895760074
[2m[36m(func pid=128361)[0m f1_per_class: [0.18, 0.289, 0.706, 0.309, 0.096, 0.326, 0.477, 0.437, 0.23, 0.122]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8008 | Steps: 2 | Val loss: 2.2940 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4182 | Steps: 2 | Val loss: 8.3156 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.8340 | Steps: 2 | Val loss: 10.6341 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=140594)[0m top1: 0.15764925373134328
[2m[36m(func pid=140594)[0m top5: 0.5788246268656716
[2m[36m(func pid=140594)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=140594)[0m f1_macro: 0.11624640740223677
[2m[36m(func pid=140594)[0m f1_weighted: 0.15524283616562448
[2m[36m(func pid=140594)[0m f1_per_class: [0.04, 0.182, 0.233, 0.175, 0.01, 0.318, 0.104, 0.09, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
== Status ==
Current time: 2024-01-07 04:32:39 (running for 00:22:54.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.003 |      0.317 |                   71 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.44  |      0.189 |                   68 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.666 |      0.105 |                   68 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.801 |      0.116 |                   18 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=128502)[0m top1: 0.22294776119402984
[2m[36m(func pid=128502)[0m top5: 0.8302238805970149
[2m[36m(func pid=128502)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=128502)[0m f1_macro: 0.1926296003987303
[2m[36m(func pid=128502)[0m f1_weighted: 0.1857080657661186
[2m[36m(func pid=128502)[0m f1_per_class: [0.12, 0.369, 0.6, 0.189, 0.0, 0.094, 0.117, 0.221, 0.116, 0.1]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.07742537313432836
[2m[36m(func pid=129297)[0m top5: 0.7098880597014925
[2m[36m(func pid=129297)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=129297)[0m f1_macro: 0.09501651861160251
[2m[36m(func pid=129297)[0m f1_weighted: 0.08532486816743048
[2m[36m(func pid=129297)[0m f1_per_class: [0.073, 0.058, 0.084, 0.177, 0.17, 0.021, 0.003, 0.324, 0.0, 0.04]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0340 | Steps: 2 | Val loss: 3.3838 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=128361)[0m top1: 0.31949626865671643
[2m[36m(func pid=128361)[0m top5: 0.8736007462686567
[2m[36m(func pid=128361)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=128361)[0m f1_macro: 0.3177545034775732
[2m[36m(func pid=128361)[0m f1_weighted: 0.3558544030635495
[2m[36m(func pid=128361)[0m f1_per_class: [0.182, 0.299, 0.706, 0.293, 0.09, 0.338, 0.473, 0.433, 0.235, 0.129]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4716 | Steps: 2 | Val loss: 8.0237 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.0588 | Steps: 2 | Val loss: 13.1324 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7949 | Steps: 2 | Val loss: 2.2888 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 04:32:45 (running for 00:22:59.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.034 |      0.318 |                   72 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.418 |      0.193 |                   69 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  3.059 |      0.07  |                   70 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.801 |      0.116 |                   18 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.07835820895522388
[2m[36m(func pid=129297)[0m top5: 0.7276119402985075
[2m[36m(func pid=129297)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=129297)[0m f1_macro: 0.07042029384463375
[2m[36m(func pid=129297)[0m f1_weighted: 0.0894889225276794
[2m[36m(func pid=129297)[0m f1_per_class: [0.072, 0.087, 0.079, 0.241, 0.171, 0.007, 0.009, 0.0, 0.0, 0.038]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0030 | Steps: 2 | Val loss: 3.4108 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=128502)[0m top1: 0.21595149253731344
[2m[36m(func pid=128502)[0m top5: 0.8250932835820896
[2m[36m(func pid=128502)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=128502)[0m f1_macro: 0.14106730872774048
[2m[36m(func pid=128502)[0m f1_weighted: 0.17381285156692325
[2m[36m(func pid=128502)[0m f1_per_class: [0.095, 0.354, 0.0, 0.171, 0.0, 0.1, 0.096, 0.293, 0.127, 0.174]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.15904850746268656
[2m[36m(func pid=140594)[0m top5: 0.5862873134328358
[2m[36m(func pid=140594)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=140594)[0m f1_macro: 0.11789123150196582
[2m[36m(func pid=140594)[0m f1_weighted: 0.16016246174838789
[2m[36m(func pid=140594)[0m f1_per_class: [0.036, 0.17, 0.241, 0.192, 0.01, 0.314, 0.113, 0.093, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m top1: 0.310634328358209
[2m[36m(func pid=128361)[0m top5: 0.8745335820895522
[2m[36m(func pid=128361)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=128361)[0m f1_macro: 0.31314332430182457
[2m[36m(func pid=128361)[0m f1_weighted: 0.343672190816075
[2m[36m(func pid=128361)[0m f1_per_class: [0.177, 0.301, 0.75, 0.264, 0.082, 0.316, 0.471, 0.405, 0.229, 0.135]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7309 | Steps: 2 | Val loss: 12.0852 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4797 | Steps: 2 | Val loss: 8.2300 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7614 | Steps: 2 | Val loss: 2.2833 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:32:50 (running for 00:23:04.83)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.003 |      0.313 |                   73 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.472 |      0.141 |                   70 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.731 |      0.083 |                   71 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.795 |      0.118 |                   19 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.09235074626865672
[2m[36m(func pid=129297)[0m top5: 0.7402052238805971
[2m[36m(func pid=129297)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=129297)[0m f1_macro: 0.08261873548430046
[2m[36m(func pid=129297)[0m f1_weighted: 0.11154083811932186
[2m[36m(func pid=129297)[0m f1_per_class: [0.074, 0.111, 0.085, 0.29, 0.169, 0.012, 0.015, 0.03, 0.0, 0.038]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0030 | Steps: 2 | Val loss: 3.4786 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=128502)[0m top1: 0.20335820895522388
[2m[36m(func pid=128502)[0m top5: 0.816231343283582
[2m[36m(func pid=128502)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=128502)[0m f1_macro: 0.13232305333411015
[2m[36m(func pid=128502)[0m f1_weighted: 0.1707538761913523
[2m[36m(func pid=128502)[0m f1_per_class: [0.097, 0.325, 0.0, 0.169, 0.0, 0.072, 0.121, 0.275, 0.105, 0.159]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=140594)[0m top1: 0.15951492537313433
[2m[36m(func pid=140594)[0m top5: 0.5960820895522388
[2m[36m(func pid=140594)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=140594)[0m f1_macro: 0.11900286278162175
[2m[36m(func pid=140594)[0m f1_weighted: 0.16020542724626477
[2m[36m(func pid=140594)[0m f1_per_class: [0.036, 0.163, 0.276, 0.201, 0.01, 0.319, 0.11, 0.075, 0.0, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128361)[0m top1: 0.30830223880597013
[2m[36m(func pid=128361)[0m top5: 0.8726679104477612
[2m[36m(func pid=128361)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=128361)[0m f1_macro: 0.3186470963612353
[2m[36m(func pid=128361)[0m f1_weighted: 0.339095479852049
[2m[36m(func pid=128361)[0m f1_per_class: [0.173, 0.301, 0.8, 0.24, 0.101, 0.318, 0.479, 0.387, 0.25, 0.138]
[2m[36m(func pid=128361)[0m 
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3433 | Steps: 2 | Val loss: 7.8992 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7588 | Steps: 2 | Val loss: 2.2789 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1925 | Steps: 2 | Val loss: 8.5253 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 04:32:55 (running for 00:23:10.24)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00009 | RUNNING    | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.003 |      0.319 |                   74 |
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.48  |      0.132 |                   71 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.343 |      0.142 |                   72 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.761 |      0.119 |                   20 |
| train_35a0b_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=129297)[0m top1: 0.14832089552238806
[2m[36m(func pid=129297)[0m top5: 0.7672574626865671
[2m[36m(func pid=129297)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=129297)[0m f1_macro: 0.14183776335022008
[2m[36m(func pid=129297)[0m f1_weighted: 0.17722618423803524
[2m[36m(func pid=129297)[0m f1_per_class: [0.087, 0.217, 0.083, 0.359, 0.14, 0.041, 0.027, 0.366, 0.049, 0.048]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=128361)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0024 | Steps: 2 | Val loss: 3.5298 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=140594)[0m top1: 0.16277985074626866
[2m[36m(func pid=140594)[0m top5: 0.597481343283582
[2m[36m(func pid=140594)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=140594)[0m f1_macro: 0.12171862259969957
[2m[36m(func pid=140594)[0m f1_weighted: 0.16490757556238567
[2m[36m(func pid=140594)[0m f1_per_class: [0.036, 0.151, 0.267, 0.212, 0.01, 0.322, 0.117, 0.092, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128502)[0m top1: 0.1884328358208955
[2m[36m(func pid=128502)[0m top5: 0.7952425373134329
[2m[36m(func pid=128502)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=128502)[0m f1_macro: 0.11889123382480282
[2m[36m(func pid=128502)[0m f1_weighted: 0.16599920812576505
[2m[36m(func pid=128502)[0m f1_per_class: [0.084, 0.294, 0.0, 0.17, 0.0, 0.044, 0.146, 0.23, 0.083, 0.138]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=128361)[0m top1: 0.3050373134328358
[2m[36m(func pid=128361)[0m top5: 0.871268656716418
[2m[36m(func pid=128361)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=128361)[0m f1_macro: 0.3156153143577146
[2m[36m(func pid=128361)[0m f1_weighted: 0.3358753964017457
[2m[36m(func pid=128361)[0m f1_per_class: [0.17, 0.302, 0.8, 0.229, 0.101, 0.315, 0.481, 0.377, 0.246, 0.135]
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.3791 | Steps: 2 | Val loss: 5.4243 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7484 | Steps: 2 | Val loss: 2.2764 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6424 | Steps: 2 | Val loss: 8.8686 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=129297)[0m top1: 0.18889925373134328
[2m[36m(func pid=129297)[0m top5: 0.7756529850746269
[2m[36m(func pid=129297)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=129297)[0m f1_macro: 0.1707161805619686
[2m[36m(func pid=129297)[0m f1_weighted: 0.19994500252468317
[2m[36m(func pid=129297)[0m f1_per_class: [0.091, 0.371, 0.164, 0.341, 0.121, 0.062, 0.009, 0.429, 0.053, 0.066]
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=140594)[0m top1: 0.16184701492537312
[2m[36m(func pid=140594)[0m top5: 0.6021455223880597
[2m[36m(func pid=140594)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=140594)[0m f1_macro: 0.11929292091319739
[2m[36m(func pid=140594)[0m f1_weighted: 0.16737956944906013
[2m[36m(func pid=140594)[0m f1_per_class: [0.033, 0.147, 0.254, 0.214, 0.01, 0.316, 0.13, 0.088, 0.0, 0.0]
[2m[36m(func pid=128502)[0m top1: 0.1814365671641791
[2m[36m(func pid=128502)[0m top5: 0.75
[2m[36m(func pid=128502)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=128502)[0m f1_macro: 0.11850896414892664
[2m[36m(func pid=128502)[0m f1_weighted: 0.1675982654080181
[2m[36m(func pid=128502)[0m f1_per_class: [0.073, 0.273, 0.0, 0.149, 0.0, 0.067, 0.171, 0.261, 0.079, 0.113]
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5880 | Steps: 2 | Val loss: 4.3334 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=129297)[0m top1: 0.16837686567164178
[2m[36m(func pid=129297)[0m top5: 0.7980410447761194
[2m[36m(func pid=129297)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=129297)[0m f1_macro: 0.16024250368776527
[2m[36m(func pid=129297)[0m f1_weighted: 0.15814517959255095
[2m[36m(func pid=129297)[0m f1_per_class: [0.083, 0.355, 0.194, 0.185, 0.145, 0.103, 0.025, 0.33, 0.058, 0.125]
== Status ==
Current time: 2024-01-07 04:33:01 (running for 00:23:15.79)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.334
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.192 |      0.119 |                   72 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.379 |      0.171 |                   73 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.759 |      0.122 |                   21 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


== Status ==
Current time: 2024-01-07 04:33:07 (running for 00:23:21.46)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.334
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.192 |      0.119 |                   72 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.588 |      0.16  |                   74 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.759 |      0.122 |                   21 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m 
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=145749)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=145749)[0m Configuration completed!
[2m[36m(func pid=145749)[0m New optimizer parameters:
[2m[36m(func pid=145749)[0m SGD (
[2m[36m(func pid=145749)[0m Parameter Group 0
[2m[36m(func pid=145749)[0m     dampening: 0
[2m[36m(func pid=145749)[0m     differentiable: False
[2m[36m(func pid=145749)[0m     foreach: None
[2m[36m(func pid=145749)[0m     lr: 0.001
[2m[36m(func pid=145749)[0m     maximize: False
[2m[36m(func pid=145749)[0m     momentum: 0.9
[2m[36m(func pid=145749)[0m     nesterov: False
[2m[36m(func pid=145749)[0m     weight_decay: 0.0001
[2m[36m(func pid=145749)[0m )
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6973 | Steps: 2 | Val loss: 9.0548 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=129297)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5484 | Steps: 2 | Val loss: 4.4867 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7328 | Steps: 2 | Val loss: 2.2710 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9726 | Steps: 2 | Val loss: 2.3077 | Batch size: 32 | lr: 0.001 | Duration: 4.70s
== Status ==
Current time: 2024-01-07 04:33:12 (running for 00:23:26.49)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.334
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.642 |      0.119 |                   73 |
| train_35a0b_00011 | RUNNING    | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.588 |      0.16  |                   74 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.748 |      0.119 |                   22 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=128502)[0m top1: 0.17397388059701493
[2m[36m(func pid=128502)[0m top5: 0.7122201492537313
[2m[36m(func pid=128502)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=128502)[0m f1_macro: 0.11757436831622758
[2m[36m(func pid=128502)[0m f1_weighted: 0.17264614875793352
[2m[36m(func pid=128502)[0m f1_per_class: [0.076, 0.259, 0.0, 0.147, 0.0, 0.075, 0.2, 0.233, 0.075, 0.11]
[2m[36m(func pid=128502)[0m 
[2m[36m(func pid=129297)[0m top1: 0.14132462686567165
[2m[36m(func pid=129297)[0m top5: 0.8083022388059702
[2m[36m(func pid=129297)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=129297)[0m f1_macro: 0.15048945532030228
[2m[36m(func pid=129297)[0m f1_weighted: 0.11325797266060218
[2m[36m(func pid=129297)[0m f1_per_class: [0.077, 0.28, 0.194, 0.052, 0.154, 0.148, 0.022, 0.328, 0.077, 0.176]
[2m[36m(func pid=140594)[0m top1: 0.16884328358208955
[2m[36m(func pid=140594)[0m top5: 0.613339552238806
[2m[36m(func pid=140594)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=140594)[0m f1_macro: 0.12585537924725002
[2m[36m(func pid=140594)[0m f1_weighted: 0.17489635682633983
[2m[36m(func pid=140594)[0m f1_per_class: [0.056, 0.17, 0.25, 0.217, 0.01, 0.322, 0.135, 0.088, 0.011, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m top1: 0.17723880597014927
[2m[36m(func pid=145749)[0m top5: 0.5438432835820896
[2m[36m(func pid=145749)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=145749)[0m f1_macro: 0.11810221056660164
[2m[36m(func pid=145749)[0m f1_weighted: 0.1275979900074536
[2m[36m(func pid=145749)[0m f1_per_class: [0.322, 0.348, 0.0, 0.099, 0.0, 0.205, 0.024, 0.013, 0.0, 0.169]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=128502)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4810 | Steps: 2 | Val loss: 9.1004 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7261 | Steps: 2 | Val loss: 2.2661 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9696 | Steps: 2 | Val loss: 2.3026 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=140594)[0m top1: 0.16744402985074627
[2m[36m(func pid=140594)[0m top5: 0.6222014925373134
[2m[36m(func pid=140594)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=140594)[0m f1_macro: 0.1264583408011528
[2m[36m(func pid=140594)[0m f1_weighted: 0.17484401964475152
[2m[36m(func pid=140594)[0m f1_per_class: [0.066, 0.167, 0.246, 0.219, 0.01, 0.318, 0.135, 0.094, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=128502)[0m top1: 0.15671641791044777
[2m[36m(func pid=128502)[0m top5: 0.6949626865671642
[2m[36m(func pid=128502)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=128502)[0m f1_macro: 0.11006935093065388
[2m[36m(func pid=128502)[0m f1_weighted: 0.16053891544546803
[2m[36m(func pid=128502)[0m f1_per_class: [0.084, 0.238, 0.0, 0.161, 0.0, 0.084, 0.166, 0.174, 0.076, 0.119]
[2m[36m(func pid=145749)[0m top1: 0.18796641791044777
[2m[36m(func pid=145749)[0m top5: 0.5513059701492538
[2m[36m(func pid=145749)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=145749)[0m f1_macro: 0.1256772684200655
[2m[36m(func pid=145749)[0m f1_weighted: 0.1364452248915086
[2m[36m(func pid=145749)[0m f1_per_class: [0.255, 0.33, 0.0, 0.117, 0.012, 0.284, 0.009, 0.079, 0.0, 0.17]
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7285 | Steps: 2 | Val loss: 2.2641 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 04:33:17 (running for 00:23:32.24)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.333
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00010 | RUNNING    | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.697 |      0.118 |                   74 |
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.726 |      0.126 |                   24 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.973 |      0.118 |                    1 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=146482)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=146482)[0m Configuration completed!
[2m[36m(func pid=146482)[0m New optimizer parameters:
[2m[36m(func pid=146482)[0m SGD (
[2m[36m(func pid=146482)[0m Parameter Group 0
[2m[36m(func pid=146482)[0m     dampening: 0
[2m[36m(func pid=146482)[0m     differentiable: False
[2m[36m(func pid=146482)[0m     foreach: None
[2m[36m(func pid=146482)[0m     lr: 0.01
[2m[36m(func pid=146482)[0m     maximize: False
[2m[36m(func pid=146482)[0m     momentum: 0.9
[2m[36m(func pid=146482)[0m     nesterov: False
[2m[36m(func pid=146482)[0m     weight_decay: 0.0001
[2m[36m(func pid=146482)[0m )
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.16837686567164178
[2m[36m(func pid=140594)[0m top5: 0.6217350746268657
[2m[36m(func pid=140594)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=140594)[0m f1_macro: 0.12822114076205832
[2m[36m(func pid=140594)[0m f1_weighted: 0.17613854733431358
[2m[36m(func pid=140594)[0m f1_per_class: [0.065, 0.168, 0.25, 0.216, 0.011, 0.318, 0.139, 0.106, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9082 | Steps: 2 | Val loss: 2.3092 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9937 | Steps: 2 | Val loss: 2.3811 | Batch size: 32 | lr: 0.01 | Duration: 4.76s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7046 | Steps: 2 | Val loss: 2.2562 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=145749)[0m top1: 0.18236940298507462
[2m[36m(func pid=145749)[0m top5: 0.5592350746268657
[2m[36m(func pid=145749)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=145749)[0m f1_macro: 0.12508268337936432
[2m[36m(func pid=145749)[0m f1_weighted: 0.14172357857794535
[2m[36m(func pid=145749)[0m f1_per_class: [0.187, 0.293, 0.059, 0.117, 0.0, 0.319, 0.033, 0.122, 0.0, 0.122]
[2m[36m(func pid=146482)[0m top1: 0.09748134328358209
[2m[36m(func pid=146482)[0m top5: 0.4510261194029851
[2m[36m(func pid=146482)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=146482)[0m f1_macro: 0.060487903160454715
[2m[36m(func pid=146482)[0m f1_weighted: 0.07017720904309553
[2m[36m(func pid=146482)[0m f1_per_class: [0.123, 0.183, 0.0, 0.048, 0.0, 0.176, 0.0, 0.037, 0.0, 0.038]
[2m[36m(func pid=140594)[0m top1: 0.17117537313432835
[2m[36m(func pid=140594)[0m top5: 0.6357276119402985
[2m[36m(func pid=140594)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=140594)[0m f1_macro: 0.13051473128662688
[2m[36m(func pid=140594)[0m f1_weighted: 0.17885359766348946
[2m[36m(func pid=140594)[0m f1_per_class: [0.066, 0.169, 0.265, 0.222, 0.011, 0.324, 0.14, 0.099, 0.011, 0.0]
== Status ==
Current time: 2024-01-07 04:33:23 (running for 00:23:37.98)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.729 |      0.128 |                   25 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.97  |      0.126 |                    2 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 04:33:30 (running for 00:23:44.75)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.729 |      0.128 |                   25 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.908 |      0.125 |                    3 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=147051)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=147051)[0m Configuration completed!
[2m[36m(func pid=147051)[0m New optimizer parameters:
[2m[36m(func pid=147051)[0m SGD (
[2m[36m(func pid=147051)[0m Parameter Group 0
[2m[36m(func pid=147051)[0m     dampening: 0
[2m[36m(func pid=147051)[0m     differentiable: False
[2m[36m(func pid=147051)[0m     foreach: None
[2m[36m(func pid=147051)[0m     lr: 0.1
[2m[36m(func pid=147051)[0m     maximize: False
[2m[36m(func pid=147051)[0m     momentum: 0.9
[2m[36m(func pid=147051)[0m     nesterov: False
[2m[36m(func pid=147051)[0m     weight_decay: 0.0001
[2m[36m(func pid=147051)[0m )
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7022 | Steps: 2 | Val loss: 2.2540 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8491 | Steps: 2 | Val loss: 2.3125 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7493 | Steps: 2 | Val loss: 2.3121 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9664 | Steps: 2 | Val loss: 2.4842 | Batch size: 32 | lr: 0.1 | Duration: 4.79s
== Status ==
Current time: 2024-01-07 04:33:35 (running for 00:23:49.80)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.705 |      0.131 |                   26 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.908 |      0.125 |                    3 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  2.994 |      0.06  |                    1 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.17490671641791045
[2m[36m(func pid=140594)[0m top5: 0.6431902985074627
[2m[36m(func pid=140594)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=140594)[0m f1_macro: 0.13815603934225612
[2m[36m(func pid=140594)[0m f1_weighted: 0.18219042514845812
[2m[36m(func pid=140594)[0m f1_per_class: [0.088, 0.182, 0.297, 0.217, 0.01, 0.326, 0.145, 0.106, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m top1: 0.16184701492537312
[2m[36m(func pid=145749)[0m top5: 0.5573694029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=145749)[0m f1_macro: 0.10766676226162561
[2m[36m(func pid=145749)[0m f1_weighted: 0.1393308073422269
[2m[36m(func pid=145749)[0m f1_per_class: [0.129, 0.279, 0.044, 0.096, 0.007, 0.259, 0.084, 0.106, 0.0, 0.073]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.11986940298507463
[2m[36m(func pid=146482)[0m top5: 0.5405783582089553
[2m[36m(func pid=146482)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=146482)[0m f1_macro: 0.08722963348556838
[2m[36m(func pid=146482)[0m f1_weighted: 0.09033598055142815
[2m[36m(func pid=146482)[0m f1_per_class: [0.128, 0.221, 0.066, 0.049, 0.0, 0.205, 0.009, 0.157, 0.0, 0.037]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.01632462686567164
[2m[36m(func pid=147051)[0m top5: 0.4239738805970149
[2m[36m(func pid=147051)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=147051)[0m f1_macro: 0.020348331539587556
[2m[36m(func pid=147051)[0m f1_weighted: 0.013193645039297276
[2m[36m(func pid=147051)[0m f1_per_class: [0.015, 0.01, 0.017, 0.007, 0.0, 0.0, 0.015, 0.065, 0.0, 0.075]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7482 | Steps: 2 | Val loss: 2.2997 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7329 | Steps: 2 | Val loss: 2.2494 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5052 | Steps: 2 | Val loss: 2.2263 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.0689 | Steps: 2 | Val loss: 2.1617 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:33:41 (running for 00:23:55.38)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.702 |      0.138 |                   27 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.748 |      0.123 |                    5 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  2.749 |      0.087 |                    2 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.966 |      0.02  |                    1 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.16417910447761194
[2m[36m(func pid=145749)[0m top5: 0.5755597014925373
[2m[36m(func pid=145749)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=145749)[0m f1_macro: 0.1225933638824896
[2m[36m(func pid=145749)[0m f1_weighted: 0.1532437376716372
[2m[36m(func pid=145749)[0m f1_per_class: [0.114, 0.28, 0.109, 0.097, 0.018, 0.242, 0.131, 0.115, 0.0, 0.119]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.17490671641791045
[2m[36m(func pid=140594)[0m top5: 0.6478544776119403
[2m[36m(func pid=140594)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=140594)[0m f1_macro: 0.13853838883601705
[2m[36m(func pid=140594)[0m f1_weighted: 0.18192944614789608
[2m[36m(func pid=140594)[0m f1_per_class: [0.088, 0.188, 0.297, 0.207, 0.016, 0.323, 0.15, 0.106, 0.01, 0.0]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.13712686567164178
[2m[36m(func pid=146482)[0m top5: 0.683768656716418
[2m[36m(func pid=146482)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=146482)[0m f1_macro: 0.11175969305077467
[2m[36m(func pid=146482)[0m f1_weighted: 0.14736585202670388
[2m[36m(func pid=146482)[0m f1_per_class: [0.133, 0.239, 0.05, 0.159, 0.048, 0.166, 0.094, 0.179, 0.033, 0.018]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.15811567164179105
[2m[36m(func pid=147051)[0m top5: 0.7835820895522388
[2m[36m(func pid=147051)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=147051)[0m f1_macro: 0.1109660803755043
[2m[36m(func pid=147051)[0m f1_weighted: 0.13940248546291095
[2m[36m(func pid=147051)[0m f1_per_class: [0.071, 0.19, 0.042, 0.26, 0.071, 0.015, 0.052, 0.218, 0.0, 0.19]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6886 | Steps: 2 | Val loss: 2.2787 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6751 | Steps: 2 | Val loss: 2.2464 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.1487 | Steps: 2 | Val loss: 2.1109 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.2941 | Steps: 2 | Val loss: 1.9894 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:33:46 (running for 00:24:00.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.733 |      0.139 |                   28 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.689 |      0.149 |                    6 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  2.505 |      0.112 |                    3 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.069 |      0.111 |                    2 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.18330223880597016
[2m[36m(func pid=145749)[0m top5: 0.5960820895522388
[2m[36m(func pid=145749)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=145749)[0m f1_macro: 0.14880693377229948
[2m[36m(func pid=145749)[0m f1_weighted: 0.1762266938652588
[2m[36m(func pid=145749)[0m f1_per_class: [0.138, 0.291, 0.241, 0.103, 0.023, 0.292, 0.172, 0.126, 0.0, 0.103]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.1814365671641791
[2m[36m(func pid=140594)[0m top5: 0.644589552238806
[2m[36m(func pid=140594)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=140594)[0m f1_macro: 0.14637169686105173
[2m[36m(func pid=140594)[0m f1_weighted: 0.18984802844129625
[2m[36m(func pid=140594)[0m f1_per_class: [0.088, 0.212, 0.306, 0.21, 0.016, 0.323, 0.158, 0.108, 0.021, 0.023]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.24813432835820895
[2m[36m(func pid=146482)[0m top5: 0.7709888059701493
[2m[36m(func pid=146482)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=146482)[0m f1_macro: 0.19796651795377662
[2m[36m(func pid=146482)[0m f1_weighted: 0.279744909628733
[2m[36m(func pid=146482)[0m f1_per_class: [0.185, 0.207, 0.083, 0.319, 0.062, 0.387, 0.282, 0.344, 0.042, 0.069]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.2653917910447761
[2m[36m(func pid=147051)[0m top5: 0.8069029850746269
[2m[36m(func pid=147051)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=147051)[0m f1_macro: 0.12124193293335968
[2m[36m(func pid=147051)[0m f1_weighted: 0.23252171478185046
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.226, 0.044, 0.362, 0.0, 0.085, 0.264, 0.011, 0.034, 0.185]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6406 | Steps: 2 | Val loss: 2.2555 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6586 | Steps: 2 | Val loss: 2.2455 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8519 | Steps: 2 | Val loss: 1.9732 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6924 | Steps: 2 | Val loss: 2.9250 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:33:51 (running for 00:24:06.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.675 |      0.146 |                   29 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.641 |      0.178 |                    7 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  2.149 |      0.198 |                    4 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.294 |      0.121 |                    3 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.19776119402985073
[2m[36m(func pid=145749)[0m top5: 0.6245335820895522
[2m[36m(func pid=145749)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=145749)[0m f1_macro: 0.17788838050958897
[2m[36m(func pid=145749)[0m f1_weighted: 0.1935870756217263
[2m[36m(func pid=145749)[0m f1_per_class: [0.188, 0.292, 0.344, 0.111, 0.016, 0.326, 0.192, 0.18, 0.0, 0.13]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.18610074626865672
[2m[36m(func pid=140594)[0m top5: 0.6427238805970149
[2m[36m(func pid=140594)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=140594)[0m f1_macro: 0.1529792806067757
[2m[36m(func pid=140594)[0m f1_weighted: 0.19185039450209757
[2m[36m(func pid=140594)[0m f1_per_class: [0.094, 0.227, 0.301, 0.198, 0.02, 0.338, 0.156, 0.128, 0.023, 0.045]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.35074626865671643
[2m[36m(func pid=146482)[0m top5: 0.8292910447761194
[2m[36m(func pid=146482)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=146482)[0m f1_macro: 0.28526037916815383
[2m[36m(func pid=146482)[0m f1_weighted: 0.3733086458166841
[2m[36m(func pid=146482)[0m f1_per_class: [0.429, 0.238, 0.308, 0.435, 0.058, 0.425, 0.442, 0.276, 0.032, 0.21]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.12080223880597014
[2m[36m(func pid=147051)[0m top5: 0.6408582089552238
[2m[36m(func pid=147051)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=147051)[0m f1_macro: 0.0449697756973795
[2m[36m(func pid=147051)[0m f1_weighted: 0.09657175278338404
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.0, 0.0, 0.333, 0.0, 0.008, 0.0, 0.0, 0.072, 0.037]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5785 | Steps: 2 | Val loss: 2.2304 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6503 | Steps: 2 | Val loss: 2.2437 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4922 | Steps: 2 | Val loss: 1.8312 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0950 | Steps: 2 | Val loss: 8.5687 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:33:57 (running for 00:24:11.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.659 |      0.153 |                   30 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.578 |      0.183 |                    8 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  1.852 |      0.285 |                    5 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.692 |      0.045 |                    4 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.20569029850746268
[2m[36m(func pid=145749)[0m top5: 0.6511194029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=145749)[0m f1_macro: 0.182882562398585
[2m[36m(func pid=145749)[0m f1_weighted: 0.20778294011438594
[2m[36m(func pid=145749)[0m f1_per_class: [0.209, 0.286, 0.314, 0.136, 0.026, 0.335, 0.211, 0.205, 0.0, 0.107]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.1921641791044776
[2m[36m(func pid=140594)[0m top5: 0.648320895522388
[2m[36m(func pid=140594)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=140594)[0m f1_macro: 0.15723574172064
[2m[36m(func pid=140594)[0m f1_weighted: 0.1970490179478125
[2m[36m(func pid=140594)[0m f1_per_class: [0.1, 0.245, 0.314, 0.198, 0.02, 0.349, 0.16, 0.112, 0.024, 0.049]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.40904850746268656
[2m[36m(func pid=146482)[0m top5: 0.8600746268656716
[2m[36m(func pid=146482)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=146482)[0m f1_macro: 0.31434308255315346
[2m[36m(func pid=146482)[0m f1_weighted: 0.41857715334486467
[2m[36m(func pid=146482)[0m f1_per_class: [0.549, 0.231, 0.324, 0.492, 0.077, 0.444, 0.548, 0.175, 0.0, 0.302]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.07649253731343283
[2m[36m(func pid=147051)[0m top5: 0.5601679104477612
[2m[36m(func pid=147051)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=147051)[0m f1_macro: 0.04844292154647626
[2m[36m(func pid=147051)[0m f1_weighted: 0.08737596520132812
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.0, 0.023, 0.116, 0.0, 0.159, 0.12, 0.0, 0.0, 0.066]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4999 | Steps: 2 | Val loss: 2.2045 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6236 | Steps: 2 | Val loss: 2.2397 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.2431 | Steps: 2 | Val loss: 1.7698 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.2027 | Steps: 2 | Val loss: 13.5144 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:34:02 (running for 00:24:16.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.65  |      0.157 |                   31 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.5   |      0.199 |                    9 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  1.492 |      0.314 |                    6 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.095 |      0.048 |                    5 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.21222014925373134
[2m[36m(func pid=145749)[0m top5: 0.6865671641791045
[2m[36m(func pid=145749)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=145749)[0m f1_macro: 0.19873643787613626
[2m[36m(func pid=145749)[0m f1_weighted: 0.22083783328930948
[2m[36m(func pid=145749)[0m f1_per_class: [0.227, 0.265, 0.358, 0.161, 0.024, 0.365, 0.225, 0.226, 0.0, 0.136]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.19169776119402984
[2m[36m(func pid=140594)[0m top5: 0.652518656716418
[2m[36m(func pid=140594)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=140594)[0m f1_macro: 0.15684502615477083
[2m[36m(func pid=140594)[0m f1_weighted: 0.19807846439961724
[2m[36m(func pid=140594)[0m f1_per_class: [0.102, 0.245, 0.301, 0.201, 0.019, 0.349, 0.159, 0.121, 0.026, 0.045]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3983208955223881
[2m[36m(func pid=146482)[0m top5: 0.8582089552238806
[2m[36m(func pid=146482)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=146482)[0m f1_macro: 0.3196633629532166
[2m[36m(func pid=146482)[0m f1_weighted: 0.41217690830665277
[2m[36m(func pid=146482)[0m f1_per_class: [0.435, 0.228, 0.353, 0.513, 0.089, 0.33, 0.511, 0.414, 0.019, 0.304]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.1259328358208955
[2m[36m(func pid=147051)[0m top5: 0.40158582089552236
[2m[36m(func pid=147051)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=147051)[0m f1_macro: 0.039444850255661065
[2m[36m(func pid=147051)[0m f1_weighted: 0.06788782530008831
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4207 | Steps: 2 | Val loss: 2.1707 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6566 | Steps: 2 | Val loss: 2.2379 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8845 | Steps: 2 | Val loss: 1.7993 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6029 | Steps: 2 | Val loss: 22.5103 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:34:07 (running for 00:24:22.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.624 |      0.157 |                   32 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.421 |      0.209 |                   10 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  1.243 |      0.32  |                    7 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.203 |      0.039 |                    6 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.22527985074626866
[2m[36m(func pid=145749)[0m top5: 0.7276119402985075
[2m[36m(func pid=145749)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=145749)[0m f1_macro: 0.2093705920185827
[2m[36m(func pid=145749)[0m f1_weighted: 0.244456850309144
[2m[36m(func pid=145749)[0m f1_per_class: [0.223, 0.251, 0.4, 0.211, 0.02, 0.35, 0.271, 0.217, 0.0, 0.15]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.18936567164179105
[2m[36m(func pid=140594)[0m top5: 0.6553171641791045
[2m[36m(func pid=140594)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=140594)[0m f1_macro: 0.15580318151475253
[2m[36m(func pid=140594)[0m f1_weighted: 0.19782017643845892
[2m[36m(func pid=140594)[0m f1_per_class: [0.099, 0.236, 0.297, 0.202, 0.023, 0.347, 0.165, 0.118, 0.024, 0.048]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3474813432835821
[2m[36m(func pid=146482)[0m top5: 0.8451492537313433
[2m[36m(func pid=146482)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=146482)[0m f1_macro: 0.31866945677697145
[2m[36m(func pid=146482)[0m f1_weighted: 0.36041373059829623
[2m[36m(func pid=146482)[0m f1_per_class: [0.413, 0.234, 0.49, 0.496, 0.081, 0.347, 0.339, 0.383, 0.099, 0.304]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.05830223880597015
[2m[36m(func pid=147051)[0m top5: 0.5657649253731343
[2m[36m(func pid=147051)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=147051)[0m f1_macro: 0.0244012531225757
[2m[36m(func pid=147051)[0m f1_weighted: 0.011601214031085988
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.0, 0.024, 0.0, 0.0, 0.0, 0.0, 0.193, 0.0, 0.027]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3103 | Steps: 2 | Val loss: 2.1456 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6191 | Steps: 2 | Val loss: 2.2326 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7383 | Steps: 2 | Val loss: 1.7892 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.5301 | Steps: 2 | Val loss: 19.7015 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:34:12 (running for 00:24:27.34)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.657 |      0.156 |                   33 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.31  |      0.223 |                   11 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.885 |      0.319 |                    8 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.603 |      0.024 |                    7 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.23600746268656717
[2m[36m(func pid=145749)[0m top5: 0.7486007462686567
[2m[36m(func pid=145749)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=145749)[0m f1_macro: 0.22259712676502036
[2m[36m(func pid=145749)[0m f1_weighted: 0.25917136518758005
[2m[36m(func pid=145749)[0m f1_per_class: [0.241, 0.252, 0.393, 0.239, 0.023, 0.304, 0.309, 0.203, 0.0, 0.261]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.18563432835820895
[2m[36m(func pid=140594)[0m top5: 0.6693097014925373
[2m[36m(func pid=140594)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=140594)[0m f1_macro: 0.15638136392732035
[2m[36m(func pid=140594)[0m f1_weighted: 0.19829022495804882
[2m[36m(func pid=140594)[0m f1_per_class: [0.094, 0.201, 0.31, 0.22, 0.024, 0.328, 0.171, 0.145, 0.022, 0.049]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.34701492537313433
[2m[36m(func pid=146482)[0m top5: 0.855410447761194
[2m[36m(func pid=146482)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=146482)[0m f1_macro: 0.3283899762050609
[2m[36m(func pid=146482)[0m f1_weighted: 0.34929155989921773
[2m[36m(func pid=146482)[0m f1_per_class: [0.458, 0.242, 0.558, 0.504, 0.098, 0.322, 0.291, 0.386, 0.123, 0.301]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.14598880597014927
[2m[36m(func pid=147051)[0m top5: 0.6916977611940298
[2m[36m(func pid=147051)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=147051)[0m f1_macro: 0.039462423765492825
[2m[36m(func pid=147051)[0m f1_weighted: 0.06398289712293344
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2559 | Steps: 2 | Val loss: 2.1266 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6020 | Steps: 2 | Val loss: 2.2293 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5471 | Steps: 2 | Val loss: 1.7281 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4970 | Steps: 2 | Val loss: 61.5179 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:34:18 (running for 00:24:32.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.619 |      0.156 |                   34 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.256 |      0.221 |                   12 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.738 |      0.328 |                    9 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  3.53  |      0.039 |                    8 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.2439365671641791
[2m[36m(func pid=145749)[0m top5: 0.7635261194029851
[2m[36m(func pid=145749)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=145749)[0m f1_macro: 0.22092701158519312
[2m[36m(func pid=145749)[0m f1_weighted: 0.2699123171215004
[2m[36m(func pid=145749)[0m f1_per_class: [0.224, 0.248, 0.348, 0.264, 0.027, 0.287, 0.329, 0.223, 0.0, 0.259]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.18983208955223882
[2m[36m(func pid=140594)[0m top5: 0.6753731343283582
[2m[36m(func pid=140594)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=140594)[0m f1_macro: 0.15678276573197408
[2m[36m(func pid=140594)[0m f1_weighted: 0.2045969189213482
[2m[36m(func pid=140594)[0m f1_per_class: [0.1, 0.186, 0.31, 0.246, 0.02, 0.33, 0.179, 0.136, 0.01, 0.051]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3694029850746269
[2m[36m(func pid=146482)[0m top5: 0.8717350746268657
[2m[36m(func pid=146482)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=146482)[0m f1_macro: 0.35209854074972513
[2m[36m(func pid=146482)[0m f1_weighted: 0.37692387616427653
[2m[36m(func pid=146482)[0m f1_per_class: [0.508, 0.258, 0.615, 0.531, 0.085, 0.29, 0.353, 0.383, 0.161, 0.336]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.16930970149253732
[2m[36m(func pid=147051)[0m top5: 0.4846082089552239
[2m[36m(func pid=147051)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=147051)[0m f1_macro: 0.02908653846153846
[2m[36m(func pid=147051)[0m f1_weighted: 0.050060320393226165
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.1909 | Steps: 2 | Val loss: 2.1122 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6106 | Steps: 2 | Val loss: 2.2261 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4576 | Steps: 2 | Val loss: 1.7421 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.8907 | Steps: 2 | Val loss: 74.3818 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:34:23 (running for 00:24:37.87)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.602 |      0.157 |                   35 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.191 |      0.219 |                   13 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.547 |      0.352 |                   10 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.497 |      0.029 |                    9 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.24813432835820895
[2m[36m(func pid=145749)[0m top5: 0.7784514925373134
[2m[36m(func pid=145749)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=145749)[0m f1_macro: 0.21892254165108255
[2m[36m(func pid=145749)[0m f1_weighted: 0.27715560890674995
[2m[36m(func pid=145749)[0m f1_per_class: [0.22, 0.227, 0.308, 0.301, 0.027, 0.277, 0.334, 0.222, 0.02, 0.254]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.1921641791044776
[2m[36m(func pid=140594)[0m top5: 0.6814365671641791
[2m[36m(func pid=140594)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=140594)[0m f1_macro: 0.15872094519994356
[2m[36m(func pid=140594)[0m f1_weighted: 0.20821432137214188
[2m[36m(func pid=140594)[0m f1_per_class: [0.116, 0.173, 0.31, 0.255, 0.021, 0.328, 0.189, 0.145, 0.0, 0.051]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3712686567164179
[2m[36m(func pid=146482)[0m top5: 0.8791977611940298
[2m[36m(func pid=146482)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=146482)[0m f1_macro: 0.33657735426467555
[2m[36m(func pid=146482)[0m f1_weighted: 0.38372356117637985
[2m[36m(func pid=146482)[0m f1_per_class: [0.486, 0.245, 0.585, 0.544, 0.096, 0.22, 0.409, 0.359, 0.137, 0.286]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.16371268656716417
[2m[36m(func pid=147051)[0m top5: 0.6273320895522388
[2m[36m(func pid=147051)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=147051)[0m f1_macro: 0.03630659799163146
[2m[36m(func pid=147051)[0m f1_weighted: 0.053239704722412094
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.285, 0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.067]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.1275 | Steps: 2 | Val loss: 2.1038 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5826 | Steps: 2 | Val loss: 2.2250 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3447 | Steps: 2 | Val loss: 1.7832 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.6937 | Steps: 2 | Val loss: 72.9476 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:34:28 (running for 00:24:43.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.611 |      0.159 |                   36 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.127 |      0.223 |                   14 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.458 |      0.337 |                   11 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.891 |      0.036 |                   10 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.251865671641791
[2m[36m(func pid=145749)[0m top5: 0.7845149253731343
[2m[36m(func pid=145749)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=145749)[0m f1_macro: 0.22279378545779466
[2m[36m(func pid=145749)[0m f1_weighted: 0.28252130827521615
[2m[36m(func pid=145749)[0m f1_per_class: [0.211, 0.229, 0.308, 0.313, 0.034, 0.297, 0.329, 0.236, 0.02, 0.25]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.19916044776119404
[2m[36m(func pid=140594)[0m top5: 0.6823694029850746
[2m[36m(func pid=140594)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=140594)[0m f1_macro: 0.16503923935159298
[2m[36m(func pid=140594)[0m f1_weighted: 0.2158975171272273
[2m[36m(func pid=140594)[0m f1_per_class: [0.119, 0.174, 0.333, 0.277, 0.026, 0.341, 0.19, 0.138, 0.0, 0.053]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.375
[2m[36m(func pid=146482)[0m top5: 0.8847947761194029
[2m[36m(func pid=146482)[0m f1_micro: 0.375
[2m[36m(func pid=146482)[0m f1_macro: 0.34330430139992246
[2m[36m(func pid=146482)[0m f1_weighted: 0.3943131497468272
[2m[36m(func pid=146482)[0m f1_per_class: [0.49, 0.272, 0.585, 0.502, 0.097, 0.218, 0.467, 0.363, 0.135, 0.303]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.15671641791044777
[2m[36m(func pid=147051)[0m top5: 0.6222014925373134
[2m[36m(func pid=147051)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=147051)[0m f1_macro: 0.040161360705092
[2m[36m(func pid=147051)[0m f1_weighted: 0.06098268670484339
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.045, 0.0, 0.0, 0.086]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0340 | Steps: 2 | Val loss: 2.0939 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5631 | Steps: 2 | Val loss: 2.2212 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2773 | Steps: 2 | Val loss: 1.7500 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0823 | Steps: 2 | Val loss: 33.9678 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:34:34 (running for 00:24:48.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.583 |      0.165 |                   37 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.034 |      0.228 |                   15 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.345 |      0.343 |                   12 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.694 |      0.04  |                   11 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.26026119402985076
[2m[36m(func pid=145749)[0m top5: 0.7859141791044776
[2m[36m(func pid=145749)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=145749)[0m f1_macro: 0.22754462474777984
[2m[36m(func pid=145749)[0m f1_weighted: 0.2910981784994416
[2m[36m(func pid=145749)[0m f1_per_class: [0.221, 0.233, 0.289, 0.329, 0.043, 0.294, 0.338, 0.251, 0.021, 0.255]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.20522388059701493
[2m[36m(func pid=140594)[0m top5: 0.6902985074626866
[2m[36m(func pid=140594)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=140594)[0m f1_macro: 0.171193577088765
[2m[36m(func pid=140594)[0m f1_weighted: 0.22368869704146263
[2m[36m(func pid=140594)[0m f1_per_class: [0.127, 0.182, 0.324, 0.286, 0.026, 0.34, 0.199, 0.151, 0.0, 0.077]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.4085820895522388
[2m[36m(func pid=146482)[0m top5: 0.9043843283582089
[2m[36m(func pid=146482)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=146482)[0m f1_macro: 0.3699676963285234
[2m[36m(func pid=146482)[0m f1_weighted: 0.42182970659768415
[2m[36m(func pid=146482)[0m f1_per_class: [0.5, 0.31, 0.615, 0.528, 0.086, 0.324, 0.457, 0.414, 0.183, 0.281]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.28638059701492535
[2m[36m(func pid=147051)[0m top5: 0.7588619402985075
[2m[36m(func pid=147051)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=147051)[0m f1_macro: 0.06803988623171182
[2m[36m(func pid=147051)[0m f1_weighted: 0.16190795854227796
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.102, 0.0, 0.458, 0.0, 0.0, 0.053, 0.0, 0.0, 0.067]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.0035 | Steps: 2 | Val loss: 2.0767 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5738 | Steps: 2 | Val loss: 2.2153 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1910 | Steps: 2 | Val loss: 1.7899 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8813 | Steps: 2 | Val loss: 24.8386 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:34:39 (running for 00:24:53.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.563 |      0.171 |                   38 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  2.003 |      0.235 |                   16 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.277 |      0.37  |                   13 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.082 |      0.068 |                   12 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.27098880597014924
[2m[36m(func pid=145749)[0m top5: 0.7943097014925373
[2m[36m(func pid=145749)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=145749)[0m f1_macro: 0.2354863735175068
[2m[36m(func pid=145749)[0m f1_weighted: 0.3021073696074034
[2m[36m(func pid=145749)[0m f1_per_class: [0.222, 0.245, 0.293, 0.341, 0.042, 0.291, 0.351, 0.274, 0.043, 0.253]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2080223880597015
[2m[36m(func pid=140594)[0m top5: 0.6949626865671642
[2m[36m(func pid=140594)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=140594)[0m f1_macro: 0.17476143877400646
[2m[36m(func pid=140594)[0m f1_weighted: 0.22642359899748302
[2m[36m(func pid=140594)[0m f1_per_class: [0.135, 0.212, 0.328, 0.28, 0.026, 0.34, 0.196, 0.15, 0.0, 0.08]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.41091417910447764
[2m[36m(func pid=146482)[0m top5: 0.8992537313432836
[2m[36m(func pid=146482)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=146482)[0m f1_macro: 0.3548342385498083
[2m[36m(func pid=146482)[0m f1_weighted: 0.42913065933098354
[2m[36m(func pid=146482)[0m f1_per_class: [0.403, 0.302, 0.615, 0.527, 0.076, 0.377, 0.488, 0.326, 0.237, 0.197]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.302705223880597
[2m[36m(func pid=147051)[0m top5: 0.7294776119402985
[2m[36m(func pid=147051)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=147051)[0m f1_macro: 0.08966460955174096
[2m[36m(func pid=147051)[0m f1_weighted: 0.21358058291075024
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.105, 0.0, 0.461, 0.0, 0.0, 0.221, 0.0, 0.0, 0.11]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.8890 | Steps: 2 | Val loss: 2.0589 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5457 | Steps: 2 | Val loss: 2.2123 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2392 | Steps: 2 | Val loss: 1.9457 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.4558 | Steps: 2 | Val loss: 8.4481 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 04:34:44 (running for 00:24:58.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.574 |      0.175 |                   39 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.889 |      0.248 |                   17 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.191 |      0.355 |                   14 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.881 |      0.09  |                   13 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.2835820895522388
[2m[36m(func pid=145749)[0m top5: 0.804570895522388
[2m[36m(func pid=145749)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=145749)[0m f1_macro: 0.24764928646382484
[2m[36m(func pid=145749)[0m f1_weighted: 0.3129497518938487
[2m[36m(func pid=145749)[0m f1_per_class: [0.243, 0.255, 0.312, 0.359, 0.05, 0.31, 0.352, 0.296, 0.042, 0.258]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.386660447761194
[2m[36m(func pid=146482)[0m top5: 0.8703358208955224
[2m[36m(func pid=146482)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=146482)[0m f1_macro: 0.3381321306810329
[2m[36m(func pid=146482)[0m f1_weighted: 0.40440640310275694
[2m[36m(func pid=146482)[0m f1_per_class: [0.433, 0.275, 0.706, 0.56, 0.08, 0.314, 0.428, 0.277, 0.189, 0.12]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.21128731343283583
[2m[36m(func pid=140594)[0m top5: 0.6930970149253731
[2m[36m(func pid=140594)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=140594)[0m f1_macro: 0.1782855943917472
[2m[36m(func pid=140594)[0m f1_weighted: 0.2302913155209247
[2m[36m(func pid=140594)[0m f1_per_class: [0.146, 0.216, 0.324, 0.277, 0.025, 0.344, 0.204, 0.156, 0.01, 0.079]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.27052238805970147
[2m[36m(func pid=147051)[0m top5: 0.78125
[2m[36m(func pid=147051)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=147051)[0m f1_macro: 0.10150755671681137
[2m[36m(func pid=147051)[0m f1_weighted: 0.23268779022782607
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.296, 0.0, 0.438, 0.0, 0.0, 0.196, 0.0, 0.0, 0.084]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8835 | Steps: 2 | Val loss: 2.0431 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5342 | Steps: 2 | Val loss: 2.2094 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2117 | Steps: 2 | Val loss: 1.9405 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7375 | Steps: 2 | Val loss: 10.9849 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=145749)[0m top1: 0.291044776119403
[2m[36m(func pid=145749)[0m top5: 0.8078358208955224
[2m[36m(func pid=145749)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=145749)[0m f1_macro: 0.2591179205877629
[2m[36m(func pid=145749)[0m f1_weighted: 0.3186295101570761
[2m[36m(func pid=145749)[0m f1_per_class: [0.264, 0.267, 0.329, 0.383, 0.047, 0.319, 0.329, 0.308, 0.082, 0.263]
[2m[36m(func pid=145749)[0m 
== Status ==
Current time: 2024-01-07 04:34:49 (running for 00:25:04.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.546 |      0.178 |                   40 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.883 |      0.259 |                   18 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.239 |      0.338 |                   15 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.456 |      0.102 |                   14 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.21175373134328357
[2m[36m(func pid=140594)[0m top5: 0.6972947761194029
[2m[36m(func pid=140594)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=140594)[0m f1_macro: 0.17658660813059313
[2m[36m(func pid=140594)[0m f1_weighted: 0.22868782287926614
[2m[36m(func pid=140594)[0m f1_per_class: [0.146, 0.232, 0.297, 0.271, 0.026, 0.349, 0.193, 0.164, 0.011, 0.077]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.37919776119402987
[2m[36m(func pid=146482)[0m top5: 0.9034514925373134
[2m[36m(func pid=146482)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=146482)[0m f1_macro: 0.3743383715870979
[2m[36m(func pid=146482)[0m f1_weighted: 0.3749394476366996
[2m[36m(func pid=146482)[0m f1_per_class: [0.373, 0.312, 0.889, 0.52, 0.081, 0.377, 0.279, 0.45, 0.234, 0.229]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.12313432835820895
[2m[36m(func pid=147051)[0m top5: 0.8073694029850746
[2m[36m(func pid=147051)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=147051)[0m f1_macro: 0.04066753425023843
[2m[36m(func pid=147051)[0m f1_weighted: 0.051829417605259144
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.059, 0.0, 0.071]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.9473 | Steps: 2 | Val loss: 2.0261 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5615 | Steps: 2 | Val loss: 2.2079 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1313 | Steps: 2 | Val loss: 2.0794 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.9830 | Steps: 2 | Val loss: 12.4889 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:34:55 (running for 00:25:09.62)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.534 |      0.177 |                   41 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.947 |      0.262 |                   19 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.212 |      0.374 |                   16 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.737 |      0.041 |                   15 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.2966417910447761
[2m[36m(func pid=145749)[0m top5: 0.8055037313432836
[2m[36m(func pid=145749)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=145749)[0m f1_macro: 0.26166430395083473
[2m[36m(func pid=145749)[0m f1_weighted: 0.3218323115675253
[2m[36m(func pid=145749)[0m f1_per_class: [0.256, 0.273, 0.312, 0.39, 0.05, 0.352, 0.313, 0.339, 0.065, 0.267]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.21595149253731344
[2m[36m(func pid=140594)[0m top5: 0.6986940298507462
[2m[36m(func pid=140594)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=140594)[0m f1_macro: 0.1786827237753606
[2m[36m(func pid=140594)[0m f1_weighted: 0.23509563340497744
[2m[36m(func pid=140594)[0m f1_per_class: [0.146, 0.236, 0.286, 0.279, 0.026, 0.352, 0.203, 0.166, 0.01, 0.082]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3493470149253731
[2m[36m(func pid=146482)[0m top5: 0.909981343283582
[2m[36m(func pid=146482)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=146482)[0m f1_macro: 0.36738123348442003
[2m[36m(func pid=146482)[0m f1_weighted: 0.32457521093762853
[2m[36m(func pid=146482)[0m f1_per_class: [0.463, 0.342, 0.846, 0.483, 0.091, 0.364, 0.126, 0.44, 0.238, 0.281]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.16138059701492538
[2m[36m(func pid=147051)[0m top5: 0.8582089552238806
[2m[36m(func pid=147051)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=147051)[0m f1_macro: 0.06702019681470298
[2m[36m(func pid=147051)[0m f1_weighted: 0.06565261866643149
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.156, 0.0, 0.199]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7465 | Steps: 2 | Val loss: 2.0096 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5680 | Steps: 2 | Val loss: 2.2021 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2598 | Steps: 2 | Val loss: 1.8694 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5573 | Steps: 2 | Val loss: 10.2510 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:35:00 (running for 00:25:14.91)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.561 |      0.179 |                   42 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.747 |      0.269 |                   20 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.131 |      0.367 |                   17 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.983 |      0.067 |                   16 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.30736940298507465
[2m[36m(func pid=145749)[0m top5: 0.8069029850746269
[2m[36m(func pid=145749)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=145749)[0m f1_macro: 0.2690558791404687
[2m[36m(func pid=145749)[0m f1_weighted: 0.3284431410822084
[2m[36m(func pid=145749)[0m f1_per_class: [0.261, 0.29, 0.32, 0.401, 0.054, 0.364, 0.306, 0.378, 0.043, 0.276]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22154850746268656
[2m[36m(func pid=140594)[0m top5: 0.7038246268656716
[2m[36m(func pid=140594)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=140594)[0m f1_macro: 0.1825123504175062
[2m[36m(func pid=140594)[0m f1_weighted: 0.2403898557432733
[2m[36m(func pid=140594)[0m f1_per_class: [0.15, 0.244, 0.286, 0.28, 0.021, 0.362, 0.208, 0.184, 0.01, 0.08]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.41651119402985076
[2m[36m(func pid=146482)[0m top5: 0.914179104477612
[2m[36m(func pid=146482)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=146482)[0m f1_macro: 0.3965606194011032
[2m[36m(func pid=146482)[0m f1_weighted: 0.4374382822842032
[2m[36m(func pid=146482)[0m f1_per_class: [0.485, 0.295, 0.75, 0.542, 0.147, 0.344, 0.484, 0.445, 0.246, 0.228]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.14132462686567165
[2m[36m(func pid=147051)[0m top5: 0.8264925373134329
[2m[36m(func pid=147051)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=147051)[0m f1_macro: 0.058337417619942725
[2m[36m(func pid=147051)[0m f1_weighted: 0.05843728911909998
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.119, 0.0, 0.177]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6997 | Steps: 2 | Val loss: 1.9981 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5442 | Steps: 2 | Val loss: 2.1990 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0864 | Steps: 2 | Val loss: 2.0356 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.2285 | Steps: 2 | Val loss: 5.0195 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:35:05 (running for 00:25:20.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.568 |      0.183 |                   43 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.7   |      0.274 |                   21 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.26  |      0.397 |                   18 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.557 |      0.058 |                   17 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.302705223880597
[2m[36m(func pid=145749)[0m top5: 0.8157649253731343
[2m[36m(func pid=145749)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=145749)[0m f1_macro: 0.27362460134060873
[2m[36m(func pid=145749)[0m f1_weighted: 0.32048895685234924
[2m[36m(func pid=145749)[0m f1_per_class: [0.264, 0.284, 0.393, 0.385, 0.044, 0.379, 0.289, 0.383, 0.04, 0.274]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2248134328358209
[2m[36m(func pid=140594)[0m top5: 0.7061567164179104
[2m[36m(func pid=140594)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=140594)[0m f1_macro: 0.18579750860976305
[2m[36m(func pid=140594)[0m f1_weighted: 0.24298894851923855
[2m[36m(func pid=140594)[0m f1_per_class: [0.155, 0.255, 0.306, 0.284, 0.021, 0.364, 0.206, 0.178, 0.011, 0.078]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.38572761194029853
[2m[36m(func pid=146482)[0m top5: 0.84375
[2m[36m(func pid=146482)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=146482)[0m f1_macro: 0.3311125253231805
[2m[36m(func pid=146482)[0m f1_weighted: 0.40175748704328973
[2m[36m(func pid=146482)[0m f1_per_class: [0.392, 0.227, 0.667, 0.506, 0.156, 0.046, 0.581, 0.36, 0.219, 0.157]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.1609141791044776
[2m[36m(func pid=147051)[0m top5: 0.7938432835820896
[2m[36m(func pid=147051)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=147051)[0m f1_macro: 0.08697258010800948
[2m[36m(func pid=147051)[0m f1_weighted: 0.12149561282401873
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.257, 0.069, 0.056, 0.0, 0.0, 0.16, 0.207, 0.0, 0.12]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.6415 | Steps: 2 | Val loss: 1.9883 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1635 | Steps: 2 | Val loss: 2.1569 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5359 | Steps: 2 | Val loss: 2.1984 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.9830 | Steps: 2 | Val loss: 6.3316 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:35:11 (running for 00:25:25.59)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.544 |      0.186 |                   44 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.641 |      0.276 |                   22 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.086 |      0.331 |                   19 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.228 |      0.087 |                   18 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.29850746268656714
[2m[36m(func pid=145749)[0m top5: 0.8171641791044776
[2m[36m(func pid=145749)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=145749)[0m f1_macro: 0.2760565400083858
[2m[36m(func pid=145749)[0m f1_weighted: 0.31516907308130776
[2m[36m(func pid=145749)[0m f1_per_class: [0.234, 0.278, 0.444, 0.375, 0.044, 0.391, 0.277, 0.406, 0.038, 0.274]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3605410447761194
[2m[36m(func pid=146482)[0m top5: 0.8213619402985075
[2m[36m(func pid=146482)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=146482)[0m f1_macro: 0.27545037473662726
[2m[36m(func pid=146482)[0m f1_weighted: 0.3650169028659638
[2m[36m(func pid=146482)[0m f1_per_class: [0.209, 0.203, 0.667, 0.42, 0.136, 0.008, 0.626, 0.13, 0.206, 0.149]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22061567164179105
[2m[36m(func pid=140594)[0m top5: 0.7084888059701493
[2m[36m(func pid=140594)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=140594)[0m f1_macro: 0.18230055486690383
[2m[36m(func pid=140594)[0m f1_weighted: 0.2370363921750964
[2m[36m(func pid=140594)[0m f1_per_class: [0.158, 0.247, 0.297, 0.28, 0.021, 0.368, 0.195, 0.174, 0.011, 0.073]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.16837686567164178
[2m[36m(func pid=147051)[0m top5: 0.8013059701492538
[2m[36m(func pid=147051)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=147051)[0m f1_macro: 0.1120041113832709
[2m[36m(func pid=147051)[0m f1_weighted: 0.12605393976657145
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.374, 0.063, 0.087, 0.0, 0.039, 0.059, 0.195, 0.0, 0.302]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.5514 | Steps: 2 | Val loss: 1.9817 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1078 | Steps: 2 | Val loss: 2.3390 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5814 | Steps: 2 | Val loss: 2.1989 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5502 | Steps: 2 | Val loss: 5.5784 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:35:16 (running for 00:25:30.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.536 |      0.182 |                   45 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.551 |      0.287 |                   23 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.164 |      0.275 |                   20 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.983 |      0.112 |                   19 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3003731343283582
[2m[36m(func pid=145749)[0m top5: 0.8180970149253731
[2m[36m(func pid=145749)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=145749)[0m f1_macro: 0.2865921187169539
[2m[36m(func pid=145749)[0m f1_weighted: 0.3189883545642642
[2m[36m(func pid=145749)[0m f1_per_class: [0.221, 0.284, 0.511, 0.391, 0.038, 0.393, 0.264, 0.407, 0.087, 0.27]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.31716417910447764
[2m[36m(func pid=146482)[0m top5: 0.8120335820895522
[2m[36m(func pid=146482)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=146482)[0m f1_macro: 0.251691760988931
[2m[36m(func pid=146482)[0m f1_weighted: 0.32848127798794224
[2m[36m(func pid=146482)[0m f1_per_class: [0.153, 0.201, 0.72, 0.342, 0.118, 0.008, 0.603, 0.016, 0.198, 0.157]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2196828358208955
[2m[36m(func pid=140594)[0m top5: 0.7038246268656716
[2m[36m(func pid=140594)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=140594)[0m f1_macro: 0.18373814159957988
[2m[36m(func pid=140594)[0m f1_weighted: 0.2379569029906623
[2m[36m(func pid=140594)[0m f1_per_class: [0.148, 0.256, 0.297, 0.276, 0.021, 0.347, 0.202, 0.181, 0.011, 0.098]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.18796641791044777
[2m[36m(func pid=147051)[0m top5: 0.8600746268656716
[2m[36m(func pid=147051)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=147051)[0m f1_macro: 0.11874120827553458
[2m[36m(func pid=147051)[0m f1_weighted: 0.11976667086252604
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.434, 0.089, 0.085, 0.0, 0.038, 0.006, 0.183, 0.0, 0.353]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4966 | Steps: 2 | Val loss: 1.9758 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0744 | Steps: 2 | Val loss: 2.4276 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5155 | Steps: 2 | Val loss: 2.1949 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2807 | Steps: 2 | Val loss: 8.0453 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:35:22 (running for 00:25:36.44)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.581 |      0.184 |                   46 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.497 |      0.291 |                   24 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.108 |      0.252 |                   21 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.55  |      0.119 |                   20 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3003731343283582
[2m[36m(func pid=145749)[0m top5: 0.8199626865671642
[2m[36m(func pid=145749)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=145749)[0m f1_macro: 0.2910759866782442
[2m[36m(func pid=145749)[0m f1_weighted: 0.31881375859335725
[2m[36m(func pid=145749)[0m f1_per_class: [0.217, 0.293, 0.533, 0.391, 0.038, 0.39, 0.253, 0.42, 0.119, 0.257]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.31296641791044777
[2m[36m(func pid=146482)[0m top5: 0.8171641791044776
[2m[36m(func pid=146482)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=146482)[0m f1_macro: 0.25810888822249384
[2m[36m(func pid=146482)[0m f1_weighted: 0.3298658021919683
[2m[36m(func pid=146482)[0m f1_per_class: [0.153, 0.219, 0.741, 0.357, 0.103, 0.039, 0.574, 0.0, 0.192, 0.203]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22574626865671643
[2m[36m(func pid=140594)[0m top5: 0.7112873134328358
[2m[36m(func pid=140594)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=140594)[0m f1_macro: 0.1888554303006651
[2m[36m(func pid=140594)[0m f1_weighted: 0.24484884484152328
[2m[36m(func pid=140594)[0m f1_per_class: [0.152, 0.263, 0.31, 0.287, 0.021, 0.357, 0.207, 0.177, 0.011, 0.103]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.23694029850746268
[2m[36m(func pid=147051)[0m top5: 0.8973880597014925
[2m[36m(func pid=147051)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=147051)[0m f1_macro: 0.1673527857749007
[2m[36m(func pid=147051)[0m f1_weighted: 0.20431868584219287
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.382, 0.074, 0.128, 0.145, 0.142, 0.215, 0.307, 0.0, 0.28]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.4786 | Steps: 2 | Val loss: 1.9624 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1099 | Steps: 2 | Val loss: 2.4046 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4761 | Steps: 2 | Val loss: 2.1941 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8161 | Steps: 2 | Val loss: 9.2197 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 04:35:27 (running for 00:25:41.68)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.515 |      0.189 |                   47 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.479 |      0.303 |                   25 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.074 |      0.258 |                   22 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.281 |      0.167 |                   21 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3111007462686567
[2m[36m(func pid=145749)[0m top5: 0.8269589552238806
[2m[36m(func pid=145749)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=145749)[0m f1_macro: 0.3030741714427689
[2m[36m(func pid=145749)[0m f1_weighted: 0.33415441543137825
[2m[36m(func pid=145749)[0m f1_per_class: [0.227, 0.286, 0.558, 0.409, 0.036, 0.404, 0.28, 0.439, 0.133, 0.259]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3208955223880597
[2m[36m(func pid=146482)[0m top5: 0.8386194029850746
[2m[36m(func pid=146482)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=146482)[0m f1_macro: 0.2607510851057738
[2m[36m(func pid=146482)[0m f1_weighted: 0.33498113499746773
[2m[36m(func pid=146482)[0m f1_per_class: [0.173, 0.241, 0.667, 0.349, 0.081, 0.081, 0.568, 0.0, 0.2, 0.248]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22388059701492538
[2m[36m(func pid=140594)[0m top5: 0.7126865671641791
[2m[36m(func pid=140594)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=140594)[0m f1_macro: 0.18638407452937591
[2m[36m(func pid=140594)[0m f1_weighted: 0.24287991679713586
[2m[36m(func pid=140594)[0m f1_per_class: [0.157, 0.257, 0.314, 0.284, 0.027, 0.356, 0.208, 0.171, 0.011, 0.078]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.27238805970149255
[2m[36m(func pid=147051)[0m top5: 0.8773320895522388
[2m[36m(func pid=147051)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=147051)[0m f1_macro: 0.17942102086256223
[2m[36m(func pid=147051)[0m f1_weighted: 0.28165317014973085
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.318, 0.092, 0.181, 0.077, 0.153, 0.441, 0.434, 0.0, 0.098]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.4592 | Steps: 2 | Val loss: 1.9426 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0490 | Steps: 2 | Val loss: 2.4349 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.9470 | Steps: 2 | Val loss: 3.8391 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4553 | Steps: 2 | Val loss: 2.1931 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 04:35:32 (running for 00:25:47.05)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.476 |      0.186 |                   48 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.459 |      0.315 |                   26 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.11  |      0.261 |                   23 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.816 |      0.179 |                   22 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3246268656716418
[2m[36m(func pid=145749)[0m top5: 0.8334888059701493
[2m[36m(func pid=145749)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=145749)[0m f1_macro: 0.3146935618750518
[2m[36m(func pid=145749)[0m f1_weighted: 0.34907158677899597
[2m[36m(func pid=145749)[0m f1_per_class: [0.273, 0.278, 0.585, 0.433, 0.042, 0.394, 0.312, 0.446, 0.131, 0.253]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2905783582089552
[2m[36m(func pid=146482)[0m top5: 0.8157649253731343
[2m[36m(func pid=146482)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=146482)[0m f1_macro: 0.2546829621236806
[2m[36m(func pid=146482)[0m f1_weighted: 0.3118807503279252
[2m[36m(func pid=146482)[0m f1_per_class: [0.188, 0.237, 0.632, 0.287, 0.059, 0.07, 0.536, 0.103, 0.185, 0.25]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.33348880597014924
[2m[36m(func pid=147051)[0m top5: 0.9048507462686567
[2m[36m(func pid=147051)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=147051)[0m f1_macro: 0.279456111477586
[2m[36m(func pid=147051)[0m f1_weighted: 0.315561347633292
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.381, 0.88, 0.323, 0.056, 0.179, 0.362, 0.402, 0.0, 0.211]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22621268656716417
[2m[36m(func pid=140594)[0m top5: 0.7122201492537313
[2m[36m(func pid=140594)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=140594)[0m f1_macro: 0.19062526870558383
[2m[36m(func pid=140594)[0m f1_weighted: 0.24512606306394538
[2m[36m(func pid=140594)[0m f1_per_class: [0.155, 0.261, 0.319, 0.285, 0.027, 0.357, 0.211, 0.175, 0.011, 0.105]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3846 | Steps: 2 | Val loss: 1.9338 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.1765 | Steps: 2 | Val loss: 2.4212 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4713 | Steps: 2 | Val loss: 2.1931 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6722 | Steps: 2 | Val loss: 3.8367 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:35:38 (running for 00:25:52.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.455 |      0.191 |                   49 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.385 |      0.316 |                   27 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.049 |      0.255 |                   24 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.947 |      0.279 |                   23 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.32882462686567165
[2m[36m(func pid=145749)[0m top5: 0.8367537313432836
[2m[36m(func pid=145749)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=145749)[0m f1_macro: 0.3164852902056261
[2m[36m(func pid=145749)[0m f1_weighted: 0.3526200520941356
[2m[36m(func pid=145749)[0m f1_per_class: [0.275, 0.289, 0.571, 0.449, 0.042, 0.402, 0.297, 0.451, 0.141, 0.247]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3101679104477612
[2m[36m(func pid=146482)[0m top5: 0.8269589552238806
[2m[36m(func pid=146482)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=146482)[0m f1_macro: 0.28477176332608695
[2m[36m(func pid=146482)[0m f1_weighted: 0.3333529916678282
[2m[36m(func pid=146482)[0m f1_per_class: [0.271, 0.275, 0.5, 0.376, 0.052, 0.03, 0.448, 0.438, 0.209, 0.25]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22201492537313433
[2m[36m(func pid=140594)[0m top5: 0.7098880597014925
[2m[36m(func pid=140594)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=140594)[0m f1_macro: 0.18828067508100907
[2m[36m(func pid=140594)[0m f1_weighted: 0.2394592012457556
[2m[36m(func pid=140594)[0m f1_per_class: [0.165, 0.263, 0.297, 0.263, 0.026, 0.36, 0.208, 0.185, 0.011, 0.104]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3180970149253731
[2m[36m(func pid=147051)[0m top5: 0.9048507462686567
[2m[36m(func pid=147051)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=147051)[0m f1_macro: 0.2618187626843028
[2m[36m(func pid=147051)[0m f1_weighted: 0.2957540670435912
[2m[36m(func pid=147051)[0m f1_per_class: [0.037, 0.38, 0.762, 0.373, 0.0, 0.103, 0.286, 0.337, 0.027, 0.315]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.2774 | Steps: 2 | Val loss: 1.9261 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0612 | Steps: 2 | Val loss: 2.4046 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4931 | Steps: 2 | Val loss: 2.1933 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8265 | Steps: 2 | Val loss: 3.2462 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:35:43 (running for 00:25:57.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.471 |      0.188 |                   50 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.277 |      0.317 |                   28 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.177 |      0.285 |                   25 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.672 |      0.262 |                   24 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3292910447761194
[2m[36m(func pid=145749)[0m top5: 0.8386194029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=145749)[0m f1_macro: 0.3171520035903901
[2m[36m(func pid=145749)[0m f1_weighted: 0.3541520272171437
[2m[36m(func pid=145749)[0m f1_per_class: [0.261, 0.288, 0.571, 0.442, 0.043, 0.415, 0.305, 0.454, 0.133, 0.259]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3199626865671642
[2m[36m(func pid=146482)[0m top5: 0.8222947761194029
[2m[36m(func pid=146482)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=146482)[0m f1_macro: 0.260671705260712
[2m[36m(func pid=146482)[0m f1_weighted: 0.3412947017122757
[2m[36m(func pid=146482)[0m f1_per_class: [0.305, 0.251, 0.444, 0.401, 0.052, 0.008, 0.508, 0.321, 0.096, 0.219]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.4291044776119403
[2m[36m(func pid=147051)[0m top5: 0.9123134328358209
[2m[36m(func pid=147051)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=147051)[0m f1_macro: 0.32671703523950935
[2m[36m(func pid=147051)[0m f1_weighted: 0.4291328622048074
[2m[36m(func pid=147051)[0m f1_per_class: [0.061, 0.424, 0.7, 0.415, 0.044, 0.349, 0.544, 0.506, 0.025, 0.199]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23041044776119404
[2m[36m(func pid=140594)[0m top5: 0.7075559701492538
[2m[36m(func pid=140594)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=140594)[0m f1_macro: 0.1915404977462235
[2m[36m(func pid=140594)[0m f1_weighted: 0.24844978128271752
[2m[36m(func pid=140594)[0m f1_per_class: [0.172, 0.286, 0.282, 0.258, 0.026, 0.369, 0.226, 0.183, 0.012, 0.1]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.2587 | Steps: 2 | Val loss: 1.9202 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1252 | Steps: 2 | Val loss: 2.2716 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8333 | Steps: 2 | Val loss: 3.8860 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4756 | Steps: 2 | Val loss: 2.1946 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:35:48 (running for 00:26:02.95)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.493 |      0.192 |                   51 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.259 |      0.317 |                   29 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.061 |      0.261 |                   26 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.827 |      0.327 |                   25 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.32882462686567165
[2m[36m(func pid=145749)[0m top5: 0.8418843283582089
[2m[36m(func pid=145749)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=145749)[0m f1_macro: 0.31658036147133695
[2m[36m(func pid=145749)[0m f1_weighted: 0.3555444790043339
[2m[36m(func pid=145749)[0m f1_per_class: [0.254, 0.283, 0.571, 0.441, 0.049, 0.412, 0.318, 0.441, 0.139, 0.259]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.34701492537313433
[2m[36m(func pid=146482)[0m top5: 0.84375
[2m[36m(func pid=146482)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=146482)[0m f1_macro: 0.28138525274159154
[2m[36m(func pid=146482)[0m f1_weighted: 0.3635136868896331
[2m[36m(func pid=146482)[0m f1_per_class: [0.282, 0.276, 0.48, 0.447, 0.057, 0.023, 0.508, 0.356, 0.137, 0.246]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.37919776119402987
[2m[36m(func pid=147051)[0m top5: 0.8796641791044776
[2m[36m(func pid=147051)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=147051)[0m f1_macro: 0.31443784953334897
[2m[36m(func pid=147051)[0m f1_weighted: 0.36661448123926166
[2m[36m(func pid=147051)[0m f1_per_class: [0.147, 0.386, 0.762, 0.431, 0.0, 0.072, 0.436, 0.465, 0.095, 0.35]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23041044776119404
[2m[36m(func pid=140594)[0m top5: 0.7028917910447762
[2m[36m(func pid=140594)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=140594)[0m f1_macro: 0.19302366745819233
[2m[36m(func pid=140594)[0m f1_weighted: 0.24828607577056383
[2m[36m(func pid=140594)[0m f1_per_class: [0.174, 0.298, 0.278, 0.252, 0.025, 0.369, 0.222, 0.189, 0.024, 0.098]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.2405 | Steps: 2 | Val loss: 1.9063 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0348 | Steps: 2 | Val loss: 2.1586 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4044 | Steps: 2 | Val loss: 2.1953 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3770 | Steps: 2 | Val loss: 5.1958 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:35:54 (running for 00:26:08.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.476 |      0.193 |                   52 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.241 |      0.326 |                   30 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.125 |      0.281 |                   27 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.833 |      0.314 |                   26 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.33722014925373134
[2m[36m(func pid=145749)[0m top5: 0.8446828358208955
[2m[36m(func pid=145749)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=145749)[0m f1_macro: 0.325774670191584
[2m[36m(func pid=145749)[0m f1_weighted: 0.363145656982824
[2m[36m(func pid=145749)[0m f1_per_class: [0.26, 0.297, 0.585, 0.447, 0.044, 0.412, 0.32, 0.469, 0.16, 0.262]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3829291044776119
[2m[36m(func pid=146482)[0m top5: 0.8624067164179104
[2m[36m(func pid=146482)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=146482)[0m f1_macro: 0.3284634924949436
[2m[36m(func pid=146482)[0m f1_weighted: 0.39782264917297966
[2m[36m(func pid=146482)[0m f1_per_class: [0.357, 0.297, 0.571, 0.51, 0.051, 0.038, 0.521, 0.386, 0.226, 0.327]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.30736940298507465
[2m[36m(func pid=147051)[0m top5: 0.8572761194029851
[2m[36m(func pid=147051)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=147051)[0m f1_macro: 0.2641850830745921
[2m[36m(func pid=147051)[0m f1_weighted: 0.2788161084053659
[2m[36m(func pid=147051)[0m f1_per_class: [0.126, 0.396, 0.923, 0.505, 0.0, 0.037, 0.119, 0.327, 0.027, 0.182]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22807835820895522
[2m[36m(func pid=140594)[0m top5: 0.6996268656716418
[2m[36m(func pid=140594)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=140594)[0m f1_macro: 0.1915827577514757
[2m[36m(func pid=140594)[0m f1_weighted: 0.24335709182182028
[2m[36m(func pid=140594)[0m f1_per_class: [0.172, 0.305, 0.282, 0.24, 0.026, 0.369, 0.212, 0.193, 0.025, 0.092]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2306 | Steps: 2 | Val loss: 1.8933 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.1494 | Steps: 2 | Val loss: 5.3049 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.5087 | Steps: 2 | Val loss: 2.1933 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0663 | Steps: 2 | Val loss: 2.1876 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 04:35:59 (running for 00:26:13.86)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.404 |      0.192 |                   53 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.231 |      0.328 |                   31 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.035 |      0.328 |                   28 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.377 |      0.264 |                   27 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.34095149253731344
[2m[36m(func pid=145749)[0m top5: 0.847481343283582
[2m[36m(func pid=145749)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=145749)[0m f1_macro: 0.3279844668999251
[2m[36m(func pid=145749)[0m f1_weighted: 0.3670633306321588
[2m[36m(func pid=145749)[0m f1_per_class: [0.262, 0.3, 0.585, 0.449, 0.045, 0.416, 0.328, 0.473, 0.161, 0.26]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.384794776119403
[2m[36m(func pid=147051)[0m top5: 0.8680037313432836
[2m[36m(func pid=147051)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=147051)[0m f1_macro: 0.31054304056654636
[2m[36m(func pid=147051)[0m f1_weighted: 0.3408288504100798
[2m[36m(func pid=147051)[0m f1_per_class: [0.236, 0.391, 0.923, 0.554, 0.0, 0.141, 0.226, 0.384, 0.0, 0.25]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.228544776119403
[2m[36m(func pid=140594)[0m top5: 0.7056902985074627
[2m[36m(func pid=140594)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=140594)[0m f1_macro: 0.1913671240797725
[2m[36m(func pid=140594)[0m f1_weighted: 0.2429190779731376
[2m[36m(func pid=140594)[0m f1_per_class: [0.187, 0.319, 0.272, 0.241, 0.026, 0.36, 0.208, 0.169, 0.026, 0.106]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3689365671641791
[2m[36m(func pid=146482)[0m top5: 0.8689365671641791
[2m[36m(func pid=146482)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=146482)[0m f1_macro: 0.3305996607783575
[2m[36m(func pid=146482)[0m f1_weighted: 0.39145197528181475
[2m[36m(func pid=146482)[0m f1_per_class: [0.424, 0.301, 0.615, 0.512, 0.049, 0.096, 0.486, 0.324, 0.167, 0.33]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2120 | Steps: 2 | Val loss: 1.8753 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4136 | Steps: 2 | Val loss: 2.1909 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0875 | Steps: 2 | Val loss: 2.1898 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8027 | Steps: 2 | Val loss: 5.0044 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=145749)[0m top1: 0.34701492537313433
[2m[36m(func pid=145749)[0m top5: 0.8507462686567164
[2m[36m(func pid=145749)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=145749)[0m f1_macro: 0.3335287582921468
[2m[36m(func pid=145749)[0m f1_weighted: 0.37266527514542086
[2m[36m(func pid=145749)[0m f1_per_class: [0.269, 0.305, 0.615, 0.447, 0.049, 0.422, 0.344, 0.455, 0.177, 0.252]
[2m[36m(func pid=145749)[0m 
== Status ==
Current time: 2024-01-07 04:36:04 (running for 00:26:19.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.509 |      0.191 |                   54 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.212 |      0.334 |                   32 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.066 |      0.331 |                   29 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.149 |      0.311 |                   28 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140594)[0m top1: 0.22807835820895522
[2m[36m(func pid=140594)[0m top5: 0.7098880597014925
[2m[36m(func pid=140594)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=140594)[0m f1_macro: 0.1921364546837737
[2m[36m(func pid=140594)[0m f1_weighted: 0.24429365713672857
[2m[36m(func pid=140594)[0m f1_per_class: [0.185, 0.306, 0.289, 0.243, 0.024, 0.368, 0.218, 0.149, 0.027, 0.11]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.4099813432835821
[2m[36m(func pid=147051)[0m top5: 0.8614738805970149
[2m[36m(func pid=147051)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=147051)[0m f1_macro: 0.3414851135655722
[2m[36m(func pid=147051)[0m f1_weighted: 0.39205617316191077
[2m[36m(func pid=147051)[0m f1_per_class: [0.266, 0.423, 0.786, 0.545, 0.048, 0.255, 0.328, 0.435, 0.041, 0.289]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.36473880597014924
[2m[36m(func pid=146482)[0m top5: 0.8745335820895522
[2m[36m(func pid=146482)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=146482)[0m f1_macro: 0.34497010343066975
[2m[36m(func pid=146482)[0m f1_weighted: 0.39616043448270927
[2m[36m(func pid=146482)[0m f1_per_class: [0.459, 0.296, 0.686, 0.521, 0.039, 0.161, 0.465, 0.325, 0.206, 0.292]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.0900 | Steps: 2 | Val loss: 1.8538 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4494 | Steps: 2 | Val loss: 2.1855 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0346 | Steps: 2 | Val loss: 2.2038 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7734 | Steps: 2 | Val loss: 5.9736 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 04:36:10 (running for 00:26:24.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.414 |      0.192 |                   55 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.09  |      0.338 |                   33 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.087 |      0.345 |                   30 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.803 |      0.341 |                   29 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3521455223880597
[2m[36m(func pid=145749)[0m top5: 0.8577425373134329
[2m[36m(func pid=145749)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=145749)[0m f1_macro: 0.3382399835057937
[2m[36m(func pid=145749)[0m f1_weighted: 0.37485098488456275
[2m[36m(func pid=145749)[0m f1_per_class: [0.257, 0.322, 0.649, 0.452, 0.053, 0.429, 0.335, 0.45, 0.169, 0.267]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23274253731343283
[2m[36m(func pid=140594)[0m top5: 0.7173507462686567
[2m[36m(func pid=140594)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=140594)[0m f1_macro: 0.19599500939589273
[2m[36m(func pid=140594)[0m f1_weighted: 0.24790706973467164
[2m[36m(func pid=140594)[0m f1_per_class: [0.189, 0.309, 0.301, 0.246, 0.025, 0.373, 0.223, 0.154, 0.027, 0.112]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3474813432835821
[2m[36m(func pid=146482)[0m top5: 0.8726679104477612
[2m[36m(func pid=146482)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=146482)[0m f1_macro: 0.3298803089317064
[2m[36m(func pid=146482)[0m f1_weighted: 0.3860661593228493
[2m[36m(func pid=146482)[0m f1_per_class: [0.419, 0.273, 0.686, 0.517, 0.033, 0.186, 0.45, 0.267, 0.244, 0.222]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.28125
[2m[36m(func pid=147051)[0m top5: 0.8535447761194029
[2m[36m(func pid=147051)[0m f1_micro: 0.28125
[2m[36m(func pid=147051)[0m f1_macro: 0.2721411810998279
[2m[36m(func pid=147051)[0m f1_weighted: 0.30526348144760557
[2m[36m(func pid=147051)[0m f1_per_class: [0.129, 0.416, 0.606, 0.36, 0.04, 0.258, 0.225, 0.447, 0.074, 0.167]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.2345 | Steps: 2 | Val loss: 1.8467 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3829 | Steps: 2 | Val loss: 2.1850 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0338 | Steps: 2 | Val loss: 2.2456 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.6108 | Steps: 2 | Val loss: 7.0847 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:36:15 (running for 00:26:29.64)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.449 |      0.196 |                   56 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.234 |      0.331 |                   34 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.035 |      0.33  |                   31 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.773 |      0.272 |                   30 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.34421641791044777
[2m[36m(func pid=145749)[0m top5: 0.8605410447761194
[2m[36m(func pid=145749)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=145749)[0m f1_macro: 0.33138152788533093
[2m[36m(func pid=145749)[0m f1_weighted: 0.3661204260477858
[2m[36m(func pid=145749)[0m f1_per_class: [0.238, 0.33, 0.649, 0.441, 0.055, 0.419, 0.32, 0.437, 0.169, 0.256]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.22901119402985073
[2m[36m(func pid=140594)[0m top5: 0.7182835820895522
[2m[36m(func pid=140594)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=140594)[0m f1_macro: 0.19554996353443746
[2m[36m(func pid=140594)[0m f1_weighted: 0.24519277194231137
[2m[36m(func pid=140594)[0m f1_per_class: [0.181, 0.3, 0.314, 0.245, 0.024, 0.377, 0.218, 0.16, 0.028, 0.109]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3353544776119403
[2m[36m(func pid=146482)[0m top5: 0.875
[2m[36m(func pid=146482)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=146482)[0m f1_macro: 0.3169844343137621
[2m[36m(func pid=146482)[0m f1_weighted: 0.37792660848965637
[2m[36m(func pid=146482)[0m f1_per_class: [0.442, 0.271, 0.667, 0.505, 0.028, 0.165, 0.457, 0.214, 0.218, 0.202]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.24486940298507462
[2m[36m(func pid=147051)[0m top5: 0.8180970149253731
[2m[36m(func pid=147051)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=147051)[0m f1_macro: 0.2436759858908364
[2m[36m(func pid=147051)[0m f1_weighted: 0.2544621616024721
[2m[36m(func pid=147051)[0m f1_per_class: [0.063, 0.366, 0.545, 0.297, 0.094, 0.271, 0.156, 0.365, 0.066, 0.213]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0327 | Steps: 2 | Val loss: 1.8416 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3935 | Steps: 2 | Val loss: 2.1826 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1252 | Steps: 2 | Val loss: 2.2940 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6345 | Steps: 2 | Val loss: 8.6544 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 04:36:20 (running for 00:26:34.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.383 |      0.196 |                   57 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.033 |      0.335 |                   35 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.034 |      0.317 |                   32 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.611 |      0.244 |                   31 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3493470149253731
[2m[36m(func pid=145749)[0m top5: 0.8610074626865671
[2m[36m(func pid=145749)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=145749)[0m f1_macro: 0.3348234703832784
[2m[36m(func pid=145749)[0m f1_weighted: 0.37456768470699403
[2m[36m(func pid=145749)[0m f1_per_class: [0.242, 0.325, 0.649, 0.438, 0.046, 0.421, 0.35, 0.449, 0.171, 0.256]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23041044776119404
[2m[36m(func pid=140594)[0m top5: 0.7192164179104478
[2m[36m(func pid=140594)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=140594)[0m f1_macro: 0.1959332687130726
[2m[36m(func pid=140594)[0m f1_weighted: 0.2471027062398202
[2m[36m(func pid=140594)[0m f1_per_class: [0.187, 0.297, 0.306, 0.254, 0.024, 0.373, 0.218, 0.168, 0.028, 0.105]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3204291044776119
[2m[36m(func pid=146482)[0m top5: 0.851679104477612
[2m[36m(func pid=146482)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=146482)[0m f1_macro: 0.3116998369762846
[2m[36m(func pid=146482)[0m f1_weighted: 0.3664580136487343
[2m[36m(func pid=146482)[0m f1_per_class: [0.336, 0.272, 0.6, 0.443, 0.028, 0.188, 0.446, 0.369, 0.213, 0.222]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.22901119402985073
[2m[36m(func pid=147051)[0m top5: 0.8218283582089553
[2m[36m(func pid=147051)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=147051)[0m f1_macro: 0.20307441439601873
[2m[36m(func pid=147051)[0m f1_weighted: 0.2312045910877529
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.366, 0.438, 0.232, 0.041, 0.176, 0.194, 0.304, 0.085, 0.195]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0433 | Steps: 2 | Val loss: 1.8430 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.4015 | Steps: 2 | Val loss: 2.1801 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0538 | Steps: 2 | Val loss: 2.5116 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6071 | Steps: 2 | Val loss: 7.7832 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:36:25 (running for 00:26:40.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.394 |      0.196 |                   58 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.043 |      0.339 |                   36 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.125 |      0.312 |                   33 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.634 |      0.203 |                   32 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3474813432835821
[2m[36m(func pid=145749)[0m top5: 0.8563432835820896
[2m[36m(func pid=145749)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=145749)[0m f1_macro: 0.3392178887540799
[2m[36m(func pid=145749)[0m f1_weighted: 0.3752359849092
[2m[36m(func pid=145749)[0m f1_per_class: [0.253, 0.316, 0.686, 0.435, 0.056, 0.396, 0.366, 0.458, 0.176, 0.25]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23134328358208955
[2m[36m(func pid=140594)[0m top5: 0.7192164179104478
[2m[36m(func pid=140594)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=140594)[0m f1_macro: 0.19408817793698374
[2m[36m(func pid=140594)[0m f1_weighted: 0.24933834443993919
[2m[36m(func pid=140594)[0m f1_per_class: [0.183, 0.291, 0.301, 0.266, 0.025, 0.376, 0.22, 0.145, 0.028, 0.104]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2677238805970149
[2m[36m(func pid=146482)[0m top5: 0.7952425373134329
[2m[36m(func pid=146482)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=146482)[0m f1_macro: 0.2764050431354871
[2m[36m(func pid=146482)[0m f1_weighted: 0.3008764053595024
[2m[36m(func pid=146482)[0m f1_per_class: [0.183, 0.246, 0.585, 0.284, 0.027, 0.209, 0.383, 0.435, 0.174, 0.238]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.2490671641791045
[2m[36m(func pid=147051)[0m top5: 0.8334888059701493
[2m[36m(func pid=147051)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=147051)[0m f1_macro: 0.2104189214330061
[2m[36m(func pid=147051)[0m f1_weighted: 0.2626818460009936
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.355, 0.48, 0.237, 0.083, 0.184, 0.304, 0.297, 0.066, 0.099]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.0187 | Steps: 2 | Val loss: 1.8379 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0261 | Steps: 2 | Val loss: 2.7386 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3608 | Steps: 2 | Val loss: 2.1787 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1282 | Steps: 2 | Val loss: 5.1539 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:36:31 (running for 00:26:45.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.401 |      0.194 |                   59 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.019 |      0.338 |                   37 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.054 |      0.276 |                   34 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.607 |      0.21  |                   33 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3460820895522388
[2m[36m(func pid=145749)[0m top5: 0.8572761194029851
[2m[36m(func pid=145749)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=145749)[0m f1_macro: 0.33789560611510716
[2m[36m(func pid=145749)[0m f1_weighted: 0.37487654618980215
[2m[36m(func pid=145749)[0m f1_per_class: [0.245, 0.312, 0.686, 0.431, 0.063, 0.396, 0.373, 0.459, 0.168, 0.247]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23134328358208955
[2m[36m(func pid=140594)[0m top5: 0.7215485074626866
[2m[36m(func pid=140594)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=140594)[0m f1_macro: 0.19640748083069953
[2m[36m(func pid=140594)[0m f1_weighted: 0.2516631104894189
[2m[36m(func pid=140594)[0m f1_per_class: [0.171, 0.283, 0.314, 0.267, 0.025, 0.371, 0.232, 0.162, 0.026, 0.114]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.23973880597014927
[2m[36m(func pid=146482)[0m top5: 0.7574626865671642
[2m[36m(func pid=146482)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=146482)[0m f1_macro: 0.2551895475346271
[2m[36m(func pid=146482)[0m f1_weighted: 0.2593910761991823
[2m[36m(func pid=146482)[0m f1_per_class: [0.143, 0.222, 0.571, 0.216, 0.034, 0.215, 0.325, 0.426, 0.159, 0.239]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.25886194029850745
[2m[36m(func pid=147051)[0m top5: 0.8507462686567164
[2m[36m(func pid=147051)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=147051)[0m f1_macro: 0.1721134632311038
[2m[36m(func pid=147051)[0m f1_weighted: 0.25885540813917896
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.29, 0.0, 0.378, 0.102, 0.152, 0.217, 0.265, 0.075, 0.241]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0066 | Steps: 2 | Val loss: 1.8420 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3735 | Steps: 2 | Val loss: 2.1789 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0239 | Steps: 2 | Val loss: 2.8333 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8102 | Steps: 2 | Val loss: 4.8247 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 04:36:36 (running for 00:26:51.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.361 |      0.196 |                   60 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  1.007 |      0.336 |                   38 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.026 |      0.255 |                   35 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  2.128 |      0.172 |                   34 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.34095149253731344
[2m[36m(func pid=145749)[0m top5: 0.8605410447761194
[2m[36m(func pid=145749)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=145749)[0m f1_macro: 0.33617795548023593
[2m[36m(func pid=145749)[0m f1_weighted: 0.37153927854473534
[2m[36m(func pid=145749)[0m f1_per_class: [0.21, 0.33, 0.686, 0.41, 0.07, 0.41, 0.367, 0.469, 0.151, 0.26]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2332089552238806
[2m[36m(func pid=140594)[0m top5: 0.7229477611940298
[2m[36m(func pid=140594)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=140594)[0m f1_macro: 0.19685695600730052
[2m[36m(func pid=140594)[0m f1_weighted: 0.254251352129309
[2m[36m(func pid=140594)[0m f1_per_class: [0.162, 0.288, 0.301, 0.269, 0.03, 0.375, 0.233, 0.175, 0.026, 0.11]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3358208955223881
[2m[36m(func pid=147051)[0m top5: 0.8638059701492538
[2m[36m(func pid=147051)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=147051)[0m f1_macro: 0.18985673258790986
[2m[36m(func pid=147051)[0m f1_weighted: 0.3132288088192601
[2m[36m(func pid=147051)[0m f1_per_class: [0.038, 0.472, 0.0, 0.409, 0.0, 0.127, 0.272, 0.297, 0.048, 0.235]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.23087686567164178
[2m[36m(func pid=146482)[0m top5: 0.7518656716417911
[2m[36m(func pid=146482)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=146482)[0m f1_macro: 0.2547148964500724
[2m[36m(func pid=146482)[0m f1_weighted: 0.2491628557981562
[2m[36m(func pid=146482)[0m f1_per_class: [0.119, 0.214, 0.6, 0.199, 0.045, 0.254, 0.301, 0.416, 0.146, 0.254]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8380 | Steps: 2 | Val loss: 1.8476 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3348 | Steps: 2 | Val loss: 2.1749 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.1138 | Steps: 2 | Val loss: 2.7260 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0837 | Steps: 2 | Val loss: 4.9354 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:36:42 (running for 00:26:56.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.373 |      0.197 |                   61 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.838 |      0.336 |                   39 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.024 |      0.255 |                   36 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.81  |      0.19  |                   35 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3353544776119403
[2m[36m(func pid=145749)[0m top5: 0.8610074626865671
[2m[36m(func pid=145749)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=145749)[0m f1_macro: 0.33554354402487746
[2m[36m(func pid=145749)[0m f1_weighted: 0.36923342828642935
[2m[36m(func pid=145749)[0m f1_per_class: [0.2, 0.313, 0.686, 0.39, 0.081, 0.402, 0.39, 0.471, 0.162, 0.261]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23647388059701493
[2m[36m(func pid=140594)[0m top5: 0.7290111940298507
[2m[36m(func pid=140594)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=140594)[0m f1_macro: 0.20081631679725062
[2m[36m(func pid=140594)[0m f1_weighted: 0.2580904445615901
[2m[36m(func pid=140594)[0m f1_per_class: [0.158, 0.284, 0.306, 0.277, 0.031, 0.384, 0.231, 0.215, 0.013, 0.111]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.24347014925373134
[2m[36m(func pid=146482)[0m top5: 0.78125
[2m[36m(func pid=146482)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=146482)[0m f1_macro: 0.27973576571590236
[2m[36m(func pid=146482)[0m f1_weighted: 0.2664076165381098
[2m[36m(func pid=146482)[0m f1_per_class: [0.148, 0.237, 0.686, 0.219, 0.041, 0.213, 0.329, 0.414, 0.216, 0.294]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.40718283582089554
[2m[36m(func pid=147051)[0m top5: 0.8964552238805971
[2m[36m(func pid=147051)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=147051)[0m f1_macro: 0.2180434224477798
[2m[36m(func pid=147051)[0m f1_weighted: 0.38008033607568775
[2m[36m(func pid=147051)[0m f1_per_class: [0.073, 0.468, 0.0, 0.359, 0.0, 0.134, 0.533, 0.339, 0.039, 0.235]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7986 | Steps: 2 | Val loss: 1.8494 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3421 | Steps: 2 | Val loss: 2.1710 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3759 | Steps: 2 | Val loss: 4.6989 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0220 | Steps: 2 | Val loss: 2.8791 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=145749)[0m top1: 0.33488805970149255
[2m[36m(func pid=145749)[0m top5: 0.8610074626865671
[2m[36m(func pid=145749)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=145749)[0m f1_macro: 0.3364526581858245
[2m[36m(func pid=145749)[0m f1_weighted: 0.3710208990661006
[2m[36m(func pid=145749)[0m f1_per_class: [0.197, 0.313, 0.686, 0.389, 0.075, 0.399, 0.395, 0.478, 0.173, 0.258]
[2m[36m(func pid=145749)[0m 
== Status ==
Current time: 2024-01-07 04:36:47 (running for 00:27:01.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.335 |      0.201 |                   62 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.799 |      0.336 |                   40 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.114 |      0.28  |                   37 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.084 |      0.218 |                   36 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.30923507462686567
[2m[36m(func pid=147051)[0m top5: 0.8899253731343284
[2m[36m(func pid=147051)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=147051)[0m f1_macro: 0.1741784441773364
[2m[36m(func pid=147051)[0m f1_weighted: 0.2781135071537379
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.429, 0.0, 0.246, 0.0, 0.162, 0.324, 0.3, 0.0, 0.281]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.23740671641791045
[2m[36m(func pid=140594)[0m top5: 0.7322761194029851
[2m[36m(func pid=140594)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=140594)[0m f1_macro: 0.2048592033502498
[2m[36m(func pid=140594)[0m f1_weighted: 0.25936155498873514
[2m[36m(func pid=140594)[0m f1_per_class: [0.158, 0.273, 0.333, 0.279, 0.031, 0.379, 0.24, 0.208, 0.013, 0.133]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.21128731343283583
[2m[36m(func pid=146482)[0m top5: 0.7434701492537313
[2m[36m(func pid=146482)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=146482)[0m f1_macro: 0.234597321695646
[2m[36m(func pid=146482)[0m f1_weighted: 0.23509875008597556
[2m[36m(func pid=146482)[0m f1_per_class: [0.151, 0.205, 0.522, 0.221, 0.038, 0.079, 0.305, 0.385, 0.195, 0.244]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8899 | Steps: 2 | Val loss: 1.8448 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.5308 | Steps: 2 | Val loss: 4.6035 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3855 | Steps: 2 | Val loss: 2.1680 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1054 | Steps: 2 | Val loss: 2.6293 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:36:52 (running for 00:27:07.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.342 |      0.205 |                   63 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.89  |      0.337 |                   41 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.022 |      0.235 |                   38 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.376 |      0.174 |                   37 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.33722014925373134
[2m[36m(func pid=145749)[0m top5: 0.8591417910447762
[2m[36m(func pid=145749)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=145749)[0m f1_macro: 0.3370069489650449
[2m[36m(func pid=145749)[0m f1_weighted: 0.374710494511064
[2m[36m(func pid=145749)[0m f1_per_class: [0.217, 0.311, 0.686, 0.39, 0.072, 0.4, 0.409, 0.477, 0.16, 0.248]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.26072761194029853
[2m[36m(func pid=147051)[0m top5: 0.8460820895522388
[2m[36m(func pid=147051)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=147051)[0m f1_macro: 0.1542622827131314
[2m[36m(func pid=147051)[0m f1_weighted: 0.23638312056996735
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.311, 0.157, 0.402, 0.0, 0.211, 0.099, 0.257, 0.0, 0.105]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2439365671641791
[2m[36m(func pid=140594)[0m top5: 0.7360074626865671
[2m[36m(func pid=140594)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=140594)[0m f1_macro: 0.21165685232395223
[2m[36m(func pid=140594)[0m f1_weighted: 0.2658901572619384
[2m[36m(func pid=140594)[0m f1_per_class: [0.154, 0.282, 0.361, 0.293, 0.032, 0.382, 0.242, 0.207, 0.013, 0.151]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.25326492537313433
[2m[36m(func pid=146482)[0m top5: 0.789179104477612
[2m[36m(func pid=146482)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=146482)[0m f1_macro: 0.2219424965917009
[2m[36m(func pid=146482)[0m f1_weighted: 0.28349657836459896
[2m[36m(func pid=146482)[0m f1_per_class: [0.14, 0.214, 0.48, 0.231, 0.045, 0.059, 0.504, 0.19, 0.188, 0.169]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8031 | Steps: 2 | Val loss: 1.8328 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8944 | Steps: 2 | Val loss: 5.9463 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3260 | Steps: 2 | Val loss: 2.1675 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1047 | Steps: 2 | Val loss: 2.4698 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 04:36:58 (running for 00:27:12.67)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.385 |      0.212 |                   64 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.803 |      0.343 |                   42 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.105 |      0.222 |                   39 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.531 |      0.154 |                   38 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3451492537313433
[2m[36m(func pid=145749)[0m top5: 0.8596082089552238
[2m[36m(func pid=145749)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=145749)[0m f1_macro: 0.3428932603968571
[2m[36m(func pid=145749)[0m f1_weighted: 0.3817340900303897
[2m[36m(func pid=145749)[0m f1_per_class: [0.248, 0.308, 0.686, 0.418, 0.077, 0.391, 0.409, 0.474, 0.16, 0.258]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.2653917910447761
[2m[36m(func pid=147051)[0m top5: 0.8381529850746269
[2m[36m(func pid=147051)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=147051)[0m f1_macro: 0.14277592231093442
[2m[36m(func pid=147051)[0m f1_weighted: 0.22683336356127737
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.35, 0.134, 0.432, 0.0, 0.277, 0.0, 0.234, 0.0, 0.0]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24347014925373134
[2m[36m(func pid=140594)[0m top5: 0.7378731343283582
[2m[36m(func pid=140594)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=140594)[0m f1_macro: 0.2103492854938834
[2m[36m(func pid=140594)[0m f1_weighted: 0.26554175955934955
[2m[36m(func pid=140594)[0m f1_per_class: [0.162, 0.283, 0.344, 0.293, 0.032, 0.382, 0.242, 0.199, 0.013, 0.154]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.310634328358209
[2m[36m(func pid=146482)[0m top5: 0.8446828358208955
[2m[36m(func pid=146482)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=146482)[0m f1_macro: 0.25229731635238445
[2m[36m(func pid=146482)[0m f1_weighted: 0.3261400367748739
[2m[36m(func pid=146482)[0m f1_per_class: [0.151, 0.249, 0.632, 0.308, 0.051, 0.111, 0.56, 0.0, 0.245, 0.217]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7125 | Steps: 2 | Val loss: 1.8221 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9421 | Steps: 2 | Val loss: 4.9721 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.3398 | Steps: 2 | Val loss: 2.1668 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0648 | Steps: 2 | Val loss: 2.2913 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 04:37:03 (running for 00:27:17.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.326 |      0.21  |                   65 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.712 |      0.347 |                   43 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.105 |      0.252 |                   40 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.894 |      0.143 |                   39 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3493470149253731
[2m[36m(func pid=145749)[0m top5: 0.8642723880597015
[2m[36m(func pid=145749)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=145749)[0m f1_macro: 0.3468683045127089
[2m[36m(func pid=145749)[0m f1_weighted: 0.3864804446347031
[2m[36m(func pid=145749)[0m f1_per_class: [0.264, 0.306, 0.706, 0.43, 0.076, 0.392, 0.416, 0.458, 0.156, 0.263]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.29244402985074625
[2m[36m(func pid=147051)[0m top5: 0.8530783582089553
[2m[36m(func pid=147051)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=147051)[0m f1_macro: 0.17616913590437056
[2m[36m(func pid=147051)[0m f1_weighted: 0.28050088314907506
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.427, 0.218, 0.419, 0.0, 0.328, 0.126, 0.219, 0.024, 0.0]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24347014925373134
[2m[36m(func pid=140594)[0m top5: 0.7364738805970149
[2m[36m(func pid=140594)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=140594)[0m f1_macro: 0.2088113226168094
[2m[36m(func pid=140594)[0m f1_weighted: 0.26527430144289793
[2m[36m(func pid=140594)[0m f1_per_class: [0.16, 0.289, 0.319, 0.286, 0.031, 0.391, 0.238, 0.215, 0.013, 0.146]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3218283582089552
[2m[36m(func pid=146482)[0m top5: 0.8708022388059702
[2m[36m(func pid=146482)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=146482)[0m f1_macro: 0.2960980905237644
[2m[36m(func pid=146482)[0m f1_weighted: 0.34622792925830953
[2m[36m(func pid=146482)[0m f1_per_class: [0.273, 0.237, 0.774, 0.344, 0.035, 0.148, 0.559, 0.06, 0.266, 0.264]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7809 | Steps: 2 | Val loss: 1.8125 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9316 | Steps: 2 | Val loss: 4.0592 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3536 | Steps: 2 | Val loss: 2.1656 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0204 | Steps: 2 | Val loss: 2.2361 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:37:09 (running for 00:27:23.40)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.34  |      0.209 |                   66 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.781 |      0.346 |                   44 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.065 |      0.296 |                   41 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.942 |      0.176 |                   40 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.35261194029850745
[2m[36m(func pid=145749)[0m top5: 0.8698694029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=145749)[0m f1_macro: 0.34631967913997935
[2m[36m(func pid=145749)[0m f1_weighted: 0.39067570945060387
[2m[36m(func pid=145749)[0m f1_per_class: [0.271, 0.315, 0.686, 0.433, 0.07, 0.385, 0.427, 0.448, 0.156, 0.274]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.31949626865671643
[2m[36m(func pid=147051)[0m top5: 0.8619402985074627
[2m[36m(func pid=147051)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=147051)[0m f1_macro: 0.20166801615258606
[2m[36m(func pid=147051)[0m f1_weighted: 0.3086920521160159
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.484, 0.0, 0.439, 0.165, 0.352, 0.148, 0.247, 0.046, 0.136]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2439365671641791
[2m[36m(func pid=140594)[0m top5: 0.7336753731343284
[2m[36m(func pid=140594)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=140594)[0m f1_macro: 0.21057852768283522
[2m[36m(func pid=140594)[0m f1_weighted: 0.2648342320875783
[2m[36m(func pid=140594)[0m f1_per_class: [0.18, 0.288, 0.31, 0.286, 0.03, 0.397, 0.233, 0.218, 0.013, 0.151]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3138992537313433
[2m[36m(func pid=146482)[0m top5: 0.8768656716417911
[2m[36m(func pid=146482)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=146482)[0m f1_macro: 0.3206146303483576
[2m[36m(func pid=146482)[0m f1_weighted: 0.3554546215091712
[2m[36m(func pid=146482)[0m f1_per_class: [0.34, 0.216, 0.828, 0.355, 0.034, 0.166, 0.541, 0.276, 0.259, 0.191]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7886 | Steps: 2 | Val loss: 1.8162 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6819 | Steps: 2 | Val loss: 4.7942 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4055 | Steps: 2 | Val loss: 2.1639 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0253 | Steps: 2 | Val loss: 2.3066 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:37:14 (running for 00:27:28.80)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.354 |      0.211 |                   67 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.789 |      0.35  |                   45 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.02  |      0.321 |                   42 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.932 |      0.202 |                   41 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3516791044776119
[2m[36m(func pid=145749)[0m top5: 0.8698694029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=145749)[0m f1_macro: 0.3496062843927852
[2m[36m(func pid=145749)[0m f1_weighted: 0.3892070749618414
[2m[36m(func pid=145749)[0m f1_per_class: [0.278, 0.305, 0.706, 0.426, 0.074, 0.384, 0.43, 0.469, 0.149, 0.276]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.27798507462686567
[2m[36m(func pid=147051)[0m top5: 0.863339552238806
[2m[36m(func pid=147051)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=147051)[0m f1_macro: 0.16650654180215796
[2m[36m(func pid=147051)[0m f1_weighted: 0.24697781785052517
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.475, 0.0, 0.37, 0.047, 0.218, 0.063, 0.23, 0.071, 0.19]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24253731343283583
[2m[36m(func pid=140594)[0m top5: 0.7327425373134329
[2m[36m(func pid=140594)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=140594)[0m f1_macro: 0.21179606867459913
[2m[36m(func pid=140594)[0m f1_weighted: 0.2633321638367898
[2m[36m(func pid=140594)[0m f1_per_class: [0.186, 0.286, 0.333, 0.284, 0.034, 0.393, 0.231, 0.224, 0.013, 0.133]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.29151119402985076
[2m[36m(func pid=146482)[0m top5: 0.863339552238806
[2m[36m(func pid=146482)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=146482)[0m f1_macro: 0.30438461514028753
[2m[36m(func pid=146482)[0m f1_weighted: 0.33754964626044937
[2m[36m(func pid=146482)[0m f1_per_class: [0.265, 0.207, 0.857, 0.366, 0.031, 0.11, 0.489, 0.377, 0.221, 0.12]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7454 | Steps: 2 | Val loss: 1.8172 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.3773 | Steps: 2 | Val loss: 4.1574 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.3265 | Steps: 2 | Val loss: 2.1606 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0262 | Steps: 2 | Val loss: 2.3952 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:37:19 (running for 00:27:34.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.405 |      0.212 |                   68 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.745 |      0.349 |                   46 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.025 |      0.304 |                   43 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.682 |      0.167 |                   42 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.35027985074626866
[2m[36m(func pid=145749)[0m top5: 0.8675373134328358
[2m[36m(func pid=145749)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=145749)[0m f1_macro: 0.34943384563142343
[2m[36m(func pid=145749)[0m f1_weighted: 0.3870532942163726
[2m[36m(func pid=145749)[0m f1_per_class: [0.288, 0.308, 0.706, 0.419, 0.074, 0.378, 0.431, 0.46, 0.149, 0.282]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.28591417910447764
[2m[36m(func pid=147051)[0m top5: 0.8666044776119403
[2m[36m(func pid=147051)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=147051)[0m f1_macro: 0.17527967125897112
[2m[36m(func pid=147051)[0m f1_weighted: 0.2608439237521994
[2m[36m(func pid=147051)[0m f1_per_class: [0.0, 0.454, 0.0, 0.318, 0.044, 0.272, 0.141, 0.259, 0.107, 0.157]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2453358208955224
[2m[36m(func pid=140594)[0m top5: 0.7350746268656716
[2m[36m(func pid=140594)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=140594)[0m f1_macro: 0.2141370484206932
[2m[36m(func pid=140594)[0m f1_weighted: 0.2648095045598654
[2m[36m(func pid=140594)[0m f1_per_class: [0.189, 0.294, 0.324, 0.281, 0.035, 0.399, 0.23, 0.232, 0.013, 0.145]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2896455223880597
[2m[36m(func pid=146482)[0m top5: 0.8376865671641791
[2m[36m(func pid=146482)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=146482)[0m f1_macro: 0.313478374534207
[2m[36m(func pid=146482)[0m f1_weighted: 0.33166166539776815
[2m[36m(func pid=146482)[0m f1_per_class: [0.311, 0.226, 0.889, 0.403, 0.04, 0.113, 0.414, 0.387, 0.252, 0.1]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7125 | Steps: 2 | Val loss: 1.8184 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2578 | Steps: 2 | Val loss: 3.6973 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3437 | Steps: 2 | Val loss: 2.1577 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0405 | Steps: 2 | Val loss: 2.4666 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:37:25 (running for 00:27:39.56)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.326 |      0.214 |                   69 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.712 |      0.343 |                   47 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.026 |      0.313 |                   44 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.377 |      0.175 |                   43 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3474813432835821
[2m[36m(func pid=145749)[0m top5: 0.8680037313432836
[2m[36m(func pid=145749)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=145749)[0m f1_macro: 0.34284085588515373
[2m[36m(func pid=145749)[0m f1_weighted: 0.3853848128210833
[2m[36m(func pid=145749)[0m f1_per_class: [0.286, 0.308, 0.686, 0.422, 0.073, 0.355, 0.436, 0.442, 0.141, 0.28]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.2751865671641791
[2m[36m(func pid=147051)[0m top5: 0.8600746268656716
[2m[36m(func pid=147051)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=147051)[0m f1_macro: 0.20491803411409468
[2m[36m(func pid=147051)[0m f1_weighted: 0.28016151726030436
[2m[36m(func pid=147051)[0m f1_per_class: [0.032, 0.383, 0.186, 0.25, 0.085, 0.233, 0.312, 0.303, 0.086, 0.179]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24766791044776118
[2m[36m(func pid=140594)[0m top5: 0.7364738805970149
[2m[36m(func pid=140594)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=140594)[0m f1_macro: 0.21575617798744373
[2m[36m(func pid=140594)[0m f1_weighted: 0.2681601404442471
[2m[36m(func pid=140594)[0m f1_per_class: [0.193, 0.293, 0.333, 0.28, 0.03, 0.396, 0.243, 0.236, 0.014, 0.14]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2933768656716418
[2m[36m(func pid=146482)[0m top5: 0.8180970149253731
[2m[36m(func pid=146482)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=146482)[0m f1_macro: 0.3222879769256224
[2m[36m(func pid=146482)[0m f1_weighted: 0.32701621461091573
[2m[36m(func pid=146482)[0m f1_per_class: [0.374, 0.232, 0.889, 0.429, 0.043, 0.155, 0.348, 0.395, 0.247, 0.11]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7080 | Steps: 2 | Val loss: 1.8014 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6839 | Steps: 2 | Val loss: 3.8003 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2494 | Steps: 2 | Val loss: 2.1579 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0898 | Steps: 2 | Val loss: 2.6865 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:37:30 (running for 00:27:44.89)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.344 |      0.216 |                   70 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.708 |      0.349 |                   48 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.04  |      0.322 |                   45 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.258 |      0.205 |                   44 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.355410447761194
[2m[36m(func pid=145749)[0m top5: 0.875
[2m[36m(func pid=145749)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=145749)[0m f1_macro: 0.34872301423178065
[2m[36m(func pid=145749)[0m f1_weighted: 0.3941855487375834
[2m[36m(func pid=145749)[0m f1_per_class: [0.3, 0.309, 0.686, 0.449, 0.072, 0.359, 0.436, 0.447, 0.141, 0.288]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.22761194029850745
[2m[36m(func pid=147051)[0m top5: 0.8493470149253731
[2m[36m(func pid=147051)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=147051)[0m f1_macro: 0.18788662311562954
[2m[36m(func pid=147051)[0m f1_weighted: 0.255232894224457
[2m[36m(func pid=147051)[0m f1_per_class: [0.091, 0.189, 0.195, 0.252, 0.071, 0.174, 0.355, 0.319, 0.093, 0.14]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24347014925373134
[2m[36m(func pid=140594)[0m top5: 0.738339552238806
[2m[36m(func pid=140594)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=140594)[0m f1_macro: 0.21256115313018512
[2m[36m(func pid=140594)[0m f1_weighted: 0.263478004540885
[2m[36m(func pid=140594)[0m f1_per_class: [0.197, 0.289, 0.32, 0.271, 0.029, 0.402, 0.237, 0.234, 0.014, 0.133]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.27238805970149255
[2m[36m(func pid=146482)[0m top5: 0.7961753731343284
[2m[36m(func pid=146482)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=146482)[0m f1_macro: 0.3110127515032121
[2m[36m(func pid=146482)[0m f1_weighted: 0.2812423368760027
[2m[36m(func pid=146482)[0m f1_per_class: [0.38, 0.25, 0.889, 0.408, 0.057, 0.202, 0.19, 0.366, 0.253, 0.114]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7107 | Steps: 2 | Val loss: 1.7996 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3333 | Steps: 2 | Val loss: 4.4741 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3479 | Steps: 2 | Val loss: 2.1630 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0123 | Steps: 2 | Val loss: 2.9062 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:37:35 (running for 00:27:50.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.249 |      0.213 |                   71 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.711 |      0.343 |                   49 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.09  |      0.311 |                   46 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.684 |      0.188 |                   45 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.35634328358208955
[2m[36m(func pid=145749)[0m top5: 0.8777985074626866
[2m[36m(func pid=145749)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=145749)[0m f1_macro: 0.3433941630564566
[2m[36m(func pid=145749)[0m f1_weighted: 0.39550901580265196
[2m[36m(func pid=145749)[0m f1_per_class: [0.295, 0.319, 0.649, 0.448, 0.073, 0.351, 0.442, 0.435, 0.144, 0.278]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.20102611940298507
[2m[36m(func pid=147051)[0m top5: 0.8190298507462687
[2m[36m(func pid=147051)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=147051)[0m f1_macro: 0.1655333897989615
[2m[36m(func pid=147051)[0m f1_weighted: 0.20683706037931054
[2m[36m(func pid=147051)[0m f1_per_class: [0.113, 0.077, 0.176, 0.204, 0.09, 0.128, 0.316, 0.342, 0.078, 0.131]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24113805970149255
[2m[36m(func pid=140594)[0m top5: 0.730410447761194
[2m[36m(func pid=140594)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=140594)[0m f1_macro: 0.21204006213848867
[2m[36m(func pid=140594)[0m f1_weighted: 0.26069320669173673
[2m[36m(func pid=140594)[0m f1_per_class: [0.194, 0.306, 0.324, 0.247, 0.029, 0.395, 0.241, 0.244, 0.013, 0.128]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2555970149253731
[2m[36m(func pid=146482)[0m top5: 0.7719216417910447
[2m[36m(func pid=146482)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=146482)[0m f1_macro: 0.2948174370950488
[2m[36m(func pid=146482)[0m f1_weighted: 0.25055611859374477
[2m[36m(func pid=146482)[0m f1_per_class: [0.314, 0.254, 0.8, 0.364, 0.06, 0.231, 0.11, 0.402, 0.289, 0.124]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5833 | Steps: 2 | Val loss: 1.7941 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1806 | Steps: 2 | Val loss: 5.4339 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2433 | Steps: 2 | Val loss: 2.1604 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0159 | Steps: 2 | Val loss: 2.9946 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 04:37:41 (running for 00:27:55.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.348 |      0.212 |                   72 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.583 |      0.347 |                   50 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.012 |      0.295 |                   47 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.333 |      0.166 |                   46 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.35634328358208955
[2m[36m(func pid=145749)[0m top5: 0.8773320895522388
[2m[36m(func pid=145749)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=145749)[0m f1_macro: 0.34744693208017463
[2m[36m(func pid=145749)[0m f1_weighted: 0.39386815568263783
[2m[36m(func pid=145749)[0m f1_per_class: [0.306, 0.313, 0.667, 0.452, 0.072, 0.36, 0.429, 0.453, 0.141, 0.282]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.20988805970149255
[2m[36m(func pid=147051)[0m top5: 0.7756529850746269
[2m[36m(func pid=147051)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=147051)[0m f1_macro: 0.16993027420741974
[2m[36m(func pid=147051)[0m f1_weighted: 0.21874343540669267
[2m[36m(func pid=147051)[0m f1_per_class: [0.106, 0.229, 0.2, 0.165, 0.0, 0.163, 0.295, 0.347, 0.052, 0.143]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24440298507462688
[2m[36m(func pid=140594)[0m top5: 0.7318097014925373
[2m[36m(func pid=140594)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=140594)[0m f1_macro: 0.2168637311217713
[2m[36m(func pid=140594)[0m f1_weighted: 0.2641134848482473
[2m[36m(func pid=140594)[0m f1_per_class: [0.197, 0.309, 0.324, 0.255, 0.029, 0.399, 0.239, 0.255, 0.013, 0.149]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2416044776119403
[2m[36m(func pid=146482)[0m top5: 0.7583955223880597
[2m[36m(func pid=146482)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=146482)[0m f1_macro: 0.2678675149911833
[2m[36m(func pid=146482)[0m f1_weighted: 0.22946869725630578
[2m[36m(func pid=146482)[0m f1_per_class: [0.234, 0.268, 0.615, 0.312, 0.066, 0.252, 0.077, 0.434, 0.272, 0.149]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6652 | Steps: 2 | Val loss: 1.7857 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7449 | Steps: 2 | Val loss: 4.8571 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2909 | Steps: 2 | Val loss: 2.1559 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0298 | Steps: 2 | Val loss: 2.9362 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:37:46 (running for 00:28:00.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.243 |      0.217 |                   73 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.665 |      0.355 |                   51 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.016 |      0.268 |                   48 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.181 |      0.17  |                   47 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3614738805970149
[2m[36m(func pid=145749)[0m top5: 0.8815298507462687
[2m[36m(func pid=145749)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=145749)[0m f1_macro: 0.3551379439069354
[2m[36m(func pid=145749)[0m f1_weighted: 0.3976735171217451
[2m[36m(func pid=145749)[0m f1_per_class: [0.316, 0.325, 0.686, 0.445, 0.073, 0.384, 0.427, 0.472, 0.138, 0.286]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.29011194029850745
[2m[36m(func pid=147051)[0m top5: 0.7719216417910447
[2m[36m(func pid=147051)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=147051)[0m f1_macro: 0.22806818284189342
[2m[36m(func pid=147051)[0m f1_weighted: 0.3100919087403847
[2m[36m(func pid=147051)[0m f1_per_class: [0.125, 0.3, 0.255, 0.249, 0.083, 0.211, 0.444, 0.417, 0.058, 0.137]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.2453358208955224
[2m[36m(func pid=140594)[0m top5: 0.742070895522388
[2m[36m(func pid=140594)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=140594)[0m f1_macro: 0.21444023079082006
[2m[36m(func pid=140594)[0m f1_weighted: 0.26648832806372297
[2m[36m(func pid=140594)[0m f1_per_class: [0.19, 0.294, 0.293, 0.26, 0.03, 0.41, 0.248, 0.248, 0.013, 0.158]
[2m[36m(func pid=140594)[0m 
[2m[36m(func pid=146482)[0m top1: 0.259794776119403
[2m[36m(func pid=146482)[0m top5: 0.7966417910447762
[2m[36m(func pid=146482)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=146482)[0m f1_macro: 0.2780150805405438
[2m[36m(func pid=146482)[0m f1_weighted: 0.2523960569592577
[2m[36m(func pid=146482)[0m f1_per_class: [0.236, 0.274, 0.533, 0.308, 0.068, 0.246, 0.148, 0.469, 0.256, 0.241]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6411 | Steps: 2 | Val loss: 1.8013 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4993 | Steps: 2 | Val loss: 4.0422 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=140594)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2456 | Steps: 2 | Val loss: 2.1525 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0349 | Steps: 2 | Val loss: 2.7812 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:37:51 (running for 00:28:06.00)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.332
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00012 | RUNNING    | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.291 |      0.214 |                   74 |
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.641 |      0.348 |                   52 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.03  |      0.278 |                   49 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.745 |      0.228 |                   48 |
| train_35a0b_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3512126865671642
[2m[36m(func pid=145749)[0m top5: 0.8773320895522388
[2m[36m(func pid=145749)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=145749)[0m f1_macro: 0.34809609674364517
[2m[36m(func pid=145749)[0m f1_weighted: 0.38981535123069494
[2m[36m(func pid=145749)[0m f1_per_class: [0.272, 0.316, 0.706, 0.432, 0.073, 0.377, 0.428, 0.441, 0.155, 0.281]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3451492537313433
[2m[36m(func pid=147051)[0m top5: 0.7989738805970149
[2m[36m(func pid=147051)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=147051)[0m f1_macro: 0.24027488818846918
[2m[36m(func pid=147051)[0m f1_weighted: 0.37070881182691495
[2m[36m(func pid=147051)[0m f1_per_class: [0.139, 0.31, 0.444, 0.386, 0.121, 0.242, 0.574, 0.029, 0.033, 0.124]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=140594)[0m top1: 0.24207089552238806
[2m[36m(func pid=140594)[0m top5: 0.7458022388059702
[2m[36m(func pid=140594)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=140594)[0m f1_macro: 0.21374686463711515
[2m[36m(func pid=140594)[0m f1_weighted: 0.2641736872526991
[2m[36m(func pid=140594)[0m f1_per_class: [0.189, 0.281, 0.293, 0.267, 0.03, 0.398, 0.244, 0.249, 0.026, 0.16]
[2m[36m(func pid=146482)[0m top1: 0.27845149253731344
[2m[36m(func pid=146482)[0m top5: 0.8390858208955224
[2m[36m(func pid=146482)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=146482)[0m f1_macro: 0.28142136135944856
[2m[36m(func pid=146482)[0m f1_weighted: 0.29122829612924545
[2m[36m(func pid=146482)[0m f1_per_class: [0.233, 0.263, 0.421, 0.307, 0.051, 0.238, 0.296, 0.417, 0.29, 0.298]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5674 | Steps: 2 | Val loss: 1.8231 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5656 | Steps: 2 | Val loss: 4.0683 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0145 | Steps: 2 | Val loss: 2.7189 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=145749)[0m top1: 0.3376865671641791
[2m[36m(func pid=145749)[0m top5: 0.8717350746268657
[2m[36m(func pid=145749)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=145749)[0m f1_macro: 0.3402187627816836
[2m[36m(func pid=145749)[0m f1_weighted: 0.37692160190504476
[2m[36m(func pid=145749)[0m f1_per_class: [0.236, 0.317, 0.706, 0.396, 0.067, 0.371, 0.42, 0.449, 0.158, 0.281]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3302238805970149
[2m[36m(func pid=147051)[0m top5: 0.8180970149253731
[2m[36m(func pid=147051)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=147051)[0m f1_macro: 0.25693553423665505
[2m[36m(func pid=147051)[0m f1_weighted: 0.3698278736742605
[2m[36m(func pid=147051)[0m f1_per_class: [0.124, 0.256, 0.444, 0.463, 0.073, 0.261, 0.467, 0.337, 0.039, 0.105]
[2m[36m(func pid=146482)[0m top1: 0.283115671641791
[2m[36m(func pid=146482)[0m top5: 0.8451492537313433
[2m[36m(func pid=146482)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=146482)[0m f1_macro: 0.282620939634025
[2m[36m(func pid=146482)[0m f1_weighted: 0.3062061383033152
[2m[36m(func pid=146482)[0m f1_per_class: [0.242, 0.248, 0.414, 0.31, 0.048, 0.212, 0.373, 0.347, 0.306, 0.327]
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5178 | Steps: 2 | Val loss: 1.8290 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=145749)[0m top1: 0.34281716417910446
[2m[36m(func pid=145749)[0m top5: 0.8684701492537313
[2m[36m(func pid=145749)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=145749)[0m f1_macro: 0.3440045392629948
[2m[36m(func pid=145749)[0m f1_weighted: 0.38060615591413877
[2m[36m(func pid=145749)[0m f1_per_class: [0.229, 0.316, 0.686, 0.404, 0.07, 0.389, 0.414, 0.47, 0.174, 0.288]
== Status ==
Current time: 2024-01-07 04:37:56 (running for 00:28:11.22)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.567 |      0.34  |                   53 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.035 |      0.281 |                   50 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.499 |      0.24  |                   49 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 04:38:02 (running for 00:28:16.82)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.567 |      0.34  |                   53 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.035 |      0.281 |                   50 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.566 |      0.257 |                   50 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=158211)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=158211)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=158211)[0m Configuration completed!
[2m[36m(func pid=158211)[0m New optimizer parameters:
[2m[36m(func pid=158211)[0m SGD (
[2m[36m(func pid=158211)[0m Parameter Group 0
[2m[36m(func pid=158211)[0m     dampening: 0
[2m[36m(func pid=158211)[0m     differentiable: False
[2m[36m(func pid=158211)[0m     foreach: None
[2m[36m(func pid=158211)[0m     lr: 0.0001
[2m[36m(func pid=158211)[0m     maximize: False
[2m[36m(func pid=158211)[0m     momentum: 0.99
[2m[36m(func pid=158211)[0m     nesterov: False
[2m[36m(func pid=158211)[0m     weight_decay: 1e-05
[2m[36m(func pid=158211)[0m )
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5160 | Steps: 2 | Val loss: 1.8384 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0654 | Steps: 2 | Val loss: 4.5000 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0286 | Steps: 2 | Val loss: 2.6627 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9500 | Steps: 2 | Val loss: 2.3184 | Batch size: 32 | lr: 0.0001 | Duration: 4.95s
== Status ==
Current time: 2024-01-07 04:38:07 (running for 00:28:21.83)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.518 |      0.344 |                   54 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.015 |      0.283 |                   51 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.566 |      0.257 |                   50 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3376865671641791
[2m[36m(func pid=145749)[0m top5: 0.863339552238806
[2m[36m(func pid=145749)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=145749)[0m f1_macro: 0.3431503670878562
[2m[36m(func pid=145749)[0m f1_weighted: 0.37375303868460236
[2m[36m(func pid=145749)[0m f1_per_class: [0.223, 0.318, 0.706, 0.397, 0.069, 0.387, 0.394, 0.491, 0.167, 0.279]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.35401119402985076
[2m[36m(func pid=147051)[0m top5: 0.8414179104477612
[2m[36m(func pid=147051)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=147051)[0m f1_macro: 0.2678471701097008
[2m[36m(func pid=147051)[0m f1_weighted: 0.3734523173572067
[2m[36m(func pid=147051)[0m f1_per_class: [0.267, 0.192, 0.237, 0.513, 0.074, 0.334, 0.409, 0.461, 0.048, 0.145]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.28591417910447764
[2m[36m(func pid=146482)[0m top5: 0.8456156716417911
[2m[36m(func pid=146482)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=146482)[0m f1_macro: 0.27547797630644444
[2m[36m(func pid=146482)[0m f1_weighted: 0.31952153475677886
[2m[36m(func pid=146482)[0m f1_per_class: [0.232, 0.225, 0.453, 0.34, 0.043, 0.133, 0.452, 0.258, 0.289, 0.33]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.17490671641791045
[2m[36m(func pid=158211)[0m top5: 0.5335820895522388
[2m[36m(func pid=158211)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=158211)[0m f1_macro: 0.11850063585891077
[2m[36m(func pid=158211)[0m f1_weighted: 0.12491304849297984
[2m[36m(func pid=158211)[0m f1_per_class: [0.312, 0.345, 0.0, 0.092, 0.0, 0.208, 0.024, 0.0, 0.0, 0.203]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6089 | Steps: 2 | Val loss: 1.8375 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7867 | Steps: 2 | Val loss: 5.4138 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0194 | Steps: 2 | Val loss: 2.6955 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9762 | Steps: 2 | Val loss: 2.3215 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 04:38:13 (running for 00:28:27.76)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.609 |      0.342 |                   56 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.029 |      0.275 |                   52 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  1.065 |      0.268 |                   51 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.95  |      0.119 |                    1 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3362873134328358
[2m[36m(func pid=145749)[0m top5: 0.8628731343283582
[2m[36m(func pid=145749)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=145749)[0m f1_macro: 0.34187577368904043
[2m[36m(func pid=145749)[0m f1_weighted: 0.37170412633391453
[2m[36m(func pid=145749)[0m f1_per_class: [0.221, 0.318, 0.706, 0.397, 0.074, 0.392, 0.389, 0.48, 0.159, 0.284]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.32742537313432835
[2m[36m(func pid=147051)[0m top5: 0.8232276119402985
[2m[36m(func pid=147051)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=147051)[0m f1_macro: 0.2449727809223546
[2m[36m(func pid=147051)[0m f1_weighted: 0.348170719609655
[2m[36m(func pid=147051)[0m f1_per_class: [0.24, 0.149, 0.204, 0.472, 0.065, 0.277, 0.413, 0.451, 0.063, 0.116]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2667910447761194
[2m[36m(func pid=146482)[0m top5: 0.8507462686567164
[2m[36m(func pid=146482)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=146482)[0m f1_macro: 0.26906627885965395
[2m[36m(func pid=146482)[0m f1_weighted: 0.3060465013079895
[2m[36m(func pid=146482)[0m f1_per_class: [0.252, 0.219, 0.444, 0.357, 0.03, 0.114, 0.389, 0.337, 0.268, 0.279]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.18236940298507462
[2m[36m(func pid=158211)[0m top5: 0.5331156716417911
[2m[36m(func pid=158211)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=158211)[0m f1_macro: 0.10825478954219903
[2m[36m(func pid=158211)[0m f1_weighted: 0.1285980261831506
[2m[36m(func pid=158211)[0m f1_per_class: [0.194, 0.332, 0.0, 0.101, 0.01, 0.275, 0.015, 0.035, 0.0, 0.12]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5259 | Steps: 2 | Val loss: 1.8346 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3656 | Steps: 2 | Val loss: 5.8644 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0091 | Steps: 2 | Val loss: 2.7892 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:38:18 (running for 00:28:33.03)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.526 |      0.348 |                   57 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.019 |      0.269 |                   53 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.787 |      0.245 |                   52 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.976 |      0.108 |                    2 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3316231343283582
[2m[36m(func pid=145749)[0m top5: 0.8684701492537313
[2m[36m(func pid=145749)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=145749)[0m f1_macro: 0.3484674770233141
[2m[36m(func pid=145749)[0m f1_weighted: 0.36553323134306026
[2m[36m(func pid=145749)[0m f1_per_class: [0.207, 0.314, 0.774, 0.4, 0.072, 0.38, 0.367, 0.483, 0.192, 0.296]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.26725746268656714
[2m[36m(func pid=146482)[0m top5: 0.8544776119402985
[2m[36m(func pid=146482)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=146482)[0m f1_macro: 0.282378785298902
[2m[36m(func pid=146482)[0m f1_weighted: 0.31315087702238653
[2m[36m(func pid=146482)[0m f1_per_class: [0.261, 0.229, 0.462, 0.374, 0.028, 0.133, 0.355, 0.477, 0.299, 0.204]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3292910447761194
[2m[36m(func pid=147051)[0m top5: 0.7985074626865671
[2m[36m(func pid=147051)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=147051)[0m f1_macro: 0.2394792300738106
[2m[36m(func pid=147051)[0m f1_weighted: 0.3533055521037824
[2m[36m(func pid=147051)[0m f1_per_class: [0.185, 0.091, 0.215, 0.458, 0.072, 0.314, 0.47, 0.451, 0.034, 0.105]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0019 | Steps: 2 | Val loss: 2.3286 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=158211)[0m top1: 0.17397388059701493
[2m[36m(func pid=158211)[0m top5: 0.5223880597014925
[2m[36m(func pid=158211)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=158211)[0m f1_macro: 0.09815297970760327
[2m[36m(func pid=158211)[0m f1_weighted: 0.12195028862205147
[2m[36m(func pid=158211)[0m f1_per_class: [0.156, 0.299, 0.0, 0.106, 0.01, 0.299, 0.006, 0.011, 0.0, 0.095]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5628 | Steps: 2 | Val loss: 1.8368 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0389 | Steps: 2 | Val loss: 2.8365 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2990 | Steps: 2 | Val loss: 6.4815 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:38:24 (running for 00:28:38.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.563 |      0.348 |                   58 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.009 |      0.282 |                   54 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.366 |      0.239 |                   53 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  3.002 |      0.098 |                    3 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.32975746268656714
[2m[36m(func pid=145749)[0m top5: 0.8684701492537313
[2m[36m(func pid=145749)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=145749)[0m f1_macro: 0.34750100431810227
[2m[36m(func pid=145749)[0m f1_weighted: 0.36271944776269954
[2m[36m(func pid=145749)[0m f1_per_class: [0.201, 0.321, 0.774, 0.389, 0.073, 0.38, 0.364, 0.486, 0.189, 0.299]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.26632462686567165
[2m[36m(func pid=146482)[0m top5: 0.8512126865671642
[2m[36m(func pid=146482)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=146482)[0m f1_macro: 0.2757580698833063
[2m[36m(func pid=146482)[0m f1_weighted: 0.31278736405556307
[2m[36m(func pid=146482)[0m f1_per_class: [0.286, 0.243, 0.49, 0.415, 0.029, 0.102, 0.332, 0.415, 0.292, 0.154]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m top1: 0.31343283582089554
[2m[36m(func pid=147051)[0m top5: 0.7709888059701493
[2m[36m(func pid=147051)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=147051)[0m f1_macro: 0.24237512702540093
[2m[36m(func pid=147051)[0m f1_weighted: 0.343377887837305
[2m[36m(func pid=147051)[0m f1_per_class: [0.177, 0.114, 0.233, 0.39, 0.096, 0.383, 0.458, 0.461, 0.042, 0.07]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9539 | Steps: 2 | Val loss: 2.3349 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4508 | Steps: 2 | Val loss: 1.8145 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2442 | Steps: 2 | Val loss: 7.4225 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=158211)[0m top1: 0.1767723880597015
[2m[36m(func pid=158211)[0m top5: 0.5167910447761194
[2m[36m(func pid=158211)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=158211)[0m f1_macro: 0.09907666856511296
[2m[36m(func pid=158211)[0m f1_weighted: 0.12525435247410016
[2m[36m(func pid=158211)[0m f1_per_class: [0.139, 0.291, 0.0, 0.108, 0.01, 0.324, 0.009, 0.02, 0.0, 0.089]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0694 | Steps: 2 | Val loss: 2.8932 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:38:29 (running for 00:28:44.06)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.451 |      0.348 |                   59 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.039 |      0.276 |                   55 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.299 |      0.242 |                   54 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.954 |      0.099 |                    4 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.33861940298507465
[2m[36m(func pid=145749)[0m top5: 0.8698694029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=145749)[0m f1_macro: 0.34775040113625577
[2m[36m(func pid=145749)[0m f1_weighted: 0.3677574636984254
[2m[36m(func pid=145749)[0m f1_per_class: [0.238, 0.323, 0.75, 0.414, 0.074, 0.387, 0.356, 0.466, 0.185, 0.284]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3050373134328358
[2m[36m(func pid=147051)[0m top5: 0.7765858208955224
[2m[36m(func pid=147051)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=147051)[0m f1_macro: 0.225163768576382
[2m[36m(func pid=147051)[0m f1_weighted: 0.34513503183928
[2m[36m(func pid=147051)[0m f1_per_class: [0.143, 0.235, 0.121, 0.295, 0.091, 0.424, 0.499, 0.314, 0.057, 0.072]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.271455223880597
[2m[36m(func pid=146482)[0m top5: 0.8381529850746269
[2m[36m(func pid=146482)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=146482)[0m f1_macro: 0.29723406019091847
[2m[36m(func pid=146482)[0m f1_weighted: 0.3099447028834467
[2m[36m(func pid=146482)[0m f1_per_class: [0.311, 0.247, 0.6, 0.331, 0.032, 0.149, 0.365, 0.49, 0.25, 0.196]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9793 | Steps: 2 | Val loss: 2.3323 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4961 | Steps: 2 | Val loss: 8.0719 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4417 | Steps: 2 | Val loss: 1.8114 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0412 | Steps: 2 | Val loss: 2.9417 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=158211)[0m top1: 0.17630597014925373
[2m[36m(func pid=158211)[0m top5: 0.519589552238806
[2m[36m(func pid=158211)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=158211)[0m f1_macro: 0.09113215653882387
[2m[36m(func pid=158211)[0m f1_weighted: 0.12826000563080045
[2m[36m(func pid=158211)[0m f1_per_class: [0.103, 0.284, 0.0, 0.116, 0.0, 0.333, 0.018, 0.019, 0.0, 0.038]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:38:35 (running for 00:28:49.54)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.451 |      0.348 |                   59 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.069 |      0.297 |                   56 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.496 |      0.184 |                   56 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.979 |      0.091 |                    5 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.29151119402985076
[2m[36m(func pid=147051)[0m top5: 0.7887126865671642
[2m[36m(func pid=147051)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=147051)[0m f1_macro: 0.18437381977140954
[2m[36m(func pid=147051)[0m f1_weighted: 0.3273870540750785
[2m[36m(func pid=147051)[0m f1_per_class: [0.131, 0.327, 0.0, 0.204, 0.088, 0.403, 0.538, 0.039, 0.036, 0.076]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m top1: 0.34328358208955223
[2m[36m(func pid=145749)[0m top5: 0.8675373134328358
[2m[36m(func pid=145749)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=145749)[0m f1_macro: 0.3450417492446137
[2m[36m(func pid=145749)[0m f1_weighted: 0.37189461532887647
[2m[36m(func pid=145749)[0m f1_per_class: [0.263, 0.323, 0.727, 0.428, 0.077, 0.358, 0.371, 0.456, 0.17, 0.277]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2849813432835821
[2m[36m(func pid=146482)[0m top5: 0.8101679104477612
[2m[36m(func pid=146482)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=146482)[0m f1_macro: 0.3154946192765694
[2m[36m(func pid=146482)[0m f1_weighted: 0.3133650571135874
[2m[36m(func pid=146482)[0m f1_per_class: [0.34, 0.252, 0.706, 0.251, 0.037, 0.176, 0.437, 0.458, 0.265, 0.233]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9372 | Steps: 2 | Val loss: 2.3276 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4121 | Steps: 2 | Val loss: 1.8175 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5586 | Steps: 2 | Val loss: 8.6874 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0149 | Steps: 2 | Val loss: 2.8907 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=158211)[0m top1: 0.16977611940298507
[2m[36m(func pid=158211)[0m top5: 0.5167910447761194
[2m[36m(func pid=158211)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=158211)[0m f1_macro: 0.09417728169339816
[2m[36m(func pid=158211)[0m f1_weighted: 0.1294049342618519
[2m[36m(func pid=158211)[0m f1_per_class: [0.065, 0.277, 0.053, 0.106, 0.0, 0.318, 0.037, 0.046, 0.0, 0.04]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:38:40 (running for 00:28:54.95)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.442 |      0.345 |                   60 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.041 |      0.315 |                   57 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.559 |      0.164 |                   57 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.937 |      0.094 |                    6 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.2691231343283582
[2m[36m(func pid=147051)[0m top5: 0.7971082089552238
[2m[36m(func pid=147051)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=147051)[0m f1_macro: 0.16398460476321086
[2m[36m(func pid=147051)[0m f1_weighted: 0.3020643550268884
[2m[36m(func pid=147051)[0m f1_per_class: [0.127, 0.355, 0.0, 0.134, 0.052, 0.29, 0.551, 0.027, 0.02, 0.083]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3423507462686567
[2m[36m(func pid=145749)[0m top5: 0.8698694029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=145749)[0m f1_macro: 0.3384416258077903
[2m[36m(func pid=145749)[0m f1_weighted: 0.3698171448017297
[2m[36m(func pid=145749)[0m f1_per_class: [0.271, 0.321, 0.686, 0.431, 0.076, 0.348, 0.369, 0.453, 0.143, 0.286]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3087686567164179
[2m[36m(func pid=146482)[0m top5: 0.7985074626865671
[2m[36m(func pid=146482)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=146482)[0m f1_macro: 0.3107992118535691
[2m[36m(func pid=146482)[0m f1_weighted: 0.33416894826130045
[2m[36m(func pid=146482)[0m f1_per_class: [0.329, 0.27, 0.71, 0.256, 0.04, 0.151, 0.526, 0.341, 0.248, 0.236]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9229 | Steps: 2 | Val loss: 2.3183 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5300 | Steps: 2 | Val loss: 8.2676 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4288 | Steps: 2 | Val loss: 1.8169 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0123 | Steps: 2 | Val loss: 2.8447 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=158211)[0m top1: 0.17397388059701493
[2m[36m(func pid=158211)[0m top5: 0.527518656716418
[2m[36m(func pid=158211)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=158211)[0m f1_macro: 0.09977780599633342
[2m[36m(func pid=158211)[0m f1_weighted: 0.13714561219994761
[2m[36m(func pid=158211)[0m f1_per_class: [0.061, 0.282, 0.048, 0.101, 0.0, 0.328, 0.054, 0.085, 0.0, 0.039]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:38:45 (running for 00:29:00.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.412 |      0.338 |                   61 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.015 |      0.311 |                   58 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.53  |      0.168 |                   58 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.923 |      0.1   |                    7 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.22574626865671643
[2m[36m(func pid=147051)[0m top5: 0.8041044776119403
[2m[36m(func pid=147051)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=147051)[0m f1_macro: 0.16820553969211113
[2m[36m(func pid=147051)[0m f1_weighted: 0.27662510452724715
[2m[36m(func pid=147051)[0m f1_per_class: [0.125, 0.391, 0.0, 0.141, 0.04, 0.122, 0.456, 0.257, 0.039, 0.112]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3516791044776119
[2m[36m(func pid=145749)[0m top5: 0.8675373134328358
[2m[36m(func pid=145749)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=145749)[0m f1_macro: 0.34499385438623176
[2m[36m(func pid=145749)[0m f1_weighted: 0.38164455734189806
[2m[36m(func pid=145749)[0m f1_per_class: [0.287, 0.323, 0.686, 0.438, 0.076, 0.345, 0.399, 0.458, 0.159, 0.28]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3199626865671642
[2m[36m(func pid=146482)[0m top5: 0.7877798507462687
[2m[36m(func pid=146482)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=146482)[0m f1_macro: 0.30780718017132014
[2m[36m(func pid=146482)[0m f1_weighted: 0.3365542368525066
[2m[36m(func pid=146482)[0m f1_per_class: [0.308, 0.281, 0.733, 0.254, 0.044, 0.11, 0.557, 0.283, 0.246, 0.262]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9190 | Steps: 2 | Val loss: 2.3143 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5743 | Steps: 2 | Val loss: 1.8117 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3421 | Steps: 2 | Val loss: 9.1440 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0097 | Steps: 2 | Val loss: 2.8271 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=158211)[0m top1: 0.17490671641791045
[2m[36m(func pid=158211)[0m top5: 0.5345149253731343
[2m[36m(func pid=158211)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=158211)[0m f1_macro: 0.100024619526295
[2m[36m(func pid=158211)[0m f1_weighted: 0.14120203020712283
[2m[36m(func pid=158211)[0m f1_per_class: [0.069, 0.282, 0.047, 0.095, 0.013, 0.341, 0.069, 0.085, 0.0, 0.0]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:38:51 (running for 00:29:05.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.574 |      0.351 |                   63 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.012 |      0.308 |                   59 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.53  |      0.168 |                   58 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.919 |      0.1   |                    8 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3572761194029851
[2m[36m(func pid=145749)[0m top5: 0.8666044776119403
[2m[36m(func pid=145749)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=145749)[0m f1_macro: 0.35057917299758967
[2m[36m(func pid=145749)[0m f1_weighted: 0.3895454087707745
[2m[36m(func pid=145749)[0m f1_per_class: [0.298, 0.326, 0.686, 0.455, 0.073, 0.353, 0.399, 0.484, 0.164, 0.268]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m top1: 0.15671641791044777
[2m[36m(func pid=147051)[0m top5: 0.7290111940298507
[2m[36m(func pid=147051)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=147051)[0m f1_macro: 0.14667172973892023
[2m[36m(func pid=147051)[0m f1_weighted: 0.1922069974524893
[2m[36m(func pid=147051)[0m f1_per_class: [0.138, 0.379, 0.0, 0.107, 0.061, 0.101, 0.2, 0.351, 0.049, 0.081]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3204291044776119
[2m[36m(func pid=146482)[0m top5: 0.7840485074626866
[2m[36m(func pid=146482)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=146482)[0m f1_macro: 0.3016223861566928
[2m[36m(func pid=146482)[0m f1_weighted: 0.332994572519304
[2m[36m(func pid=146482)[0m f1_per_class: [0.295, 0.287, 0.71, 0.258, 0.044, 0.072, 0.553, 0.286, 0.246, 0.266]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8762 | Steps: 2 | Val loss: 2.3085 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3044 | Steps: 2 | Val loss: 6.5227 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4405 | Steps: 2 | Val loss: 1.8145 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0122 | Steps: 2 | Val loss: 2.7842 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=147051)[0m top1: 0.18889925373134328
[2m[36m(func pid=147051)[0m top5: 0.7765858208955224
[2m[36m(func pid=147051)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=147051)[0m f1_macro: 0.17459862628103123
[2m[36m(func pid=147051)[0m f1_weighted: 0.227179737025268
[2m[36m(func pid=147051)[0m f1_per_class: [0.263, 0.111, 0.0, 0.162, 0.052, 0.2, 0.363, 0.37, 0.098, 0.126]
[2m[36m(func pid=147051)[0m 
== Status ==
Current time: 2024-01-07 04:38:56 (running for 00:29:11.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.574 |      0.351 |                   63 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.01  |      0.302 |                   60 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.304 |      0.175 |                   60 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.919 |      0.1   |                    8 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.35494402985074625
[2m[36m(func pid=145749)[0m top5: 0.8731343283582089
[2m[36m(func pid=145749)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=145749)[0m f1_macro: 0.34201257344130065
[2m[36m(func pid=145749)[0m f1_weighted: 0.39055968937238117
[2m[36m(func pid=145749)[0m f1_per_class: [0.288, 0.324, 0.632, 0.463, 0.071, 0.34, 0.404, 0.477, 0.166, 0.255]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m top1: 0.17630597014925373
[2m[36m(func pid=158211)[0m top5: 0.5419776119402985
[2m[36m(func pid=158211)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=158211)[0m f1_macro: 0.1035472506597324
[2m[36m(func pid=158211)[0m f1_weighted: 0.1471977780900754
[2m[36m(func pid=158211)[0m f1_per_class: [0.068, 0.279, 0.041, 0.099, 0.018, 0.352, 0.081, 0.081, 0.016, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3162313432835821
[2m[36m(func pid=146482)[0m top5: 0.7877798507462687
[2m[36m(func pid=146482)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=146482)[0m f1_macro: 0.29851472277688557
[2m[36m(func pid=146482)[0m f1_weighted: 0.32972304282256987
[2m[36m(func pid=146482)[0m f1_per_class: [0.29, 0.285, 0.71, 0.251, 0.044, 0.079, 0.55, 0.275, 0.242, 0.258]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2597 | Steps: 2 | Val loss: 5.6497 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4130 | Steps: 2 | Val loss: 1.8169 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9040 | Steps: 2 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0181 | Steps: 2 | Val loss: 2.8670 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:39:02 (running for 00:29:16.61)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.44  |      0.342 |                   64 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.012 |      0.299 |                   61 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.26  |      0.175 |                   61 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.876 |      0.104 |                    9 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.2733208955223881
[2m[36m(func pid=147051)[0m top5: 0.7989738805970149
[2m[36m(func pid=147051)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=147051)[0m f1_macro: 0.17535672377895303
[2m[36m(func pid=147051)[0m f1_weighted: 0.3008385725236116
[2m[36m(func pid=147051)[0m f1_per_class: [0.063, 0.081, 0.078, 0.26, 0.057, 0.306, 0.563, 0.069, 0.099, 0.176]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3512126865671642
[2m[36m(func pid=145749)[0m top5: 0.8717350746268657
[2m[36m(func pid=145749)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=145749)[0m f1_macro: 0.3406095582740433
[2m[36m(func pid=145749)[0m f1_weighted: 0.3886402258483297
[2m[36m(func pid=145749)[0m f1_per_class: [0.27, 0.318, 0.632, 0.462, 0.069, 0.346, 0.398, 0.498, 0.163, 0.252]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m top1: 0.17257462686567165
[2m[36m(func pid=158211)[0m top5: 0.5611007462686567
[2m[36m(func pid=158211)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=158211)[0m f1_macro: 0.11921512603100681
[2m[36m(func pid=158211)[0m f1_weighted: 0.1461476211348881
[2m[36m(func pid=158211)[0m f1_per_class: [0.094, 0.265, 0.185, 0.101, 0.017, 0.345, 0.08, 0.089, 0.016, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2971082089552239
[2m[36m(func pid=146482)[0m top5: 0.7910447761194029
[2m[36m(func pid=146482)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=146482)[0m f1_macro: 0.290485059791333
[2m[36m(func pid=146482)[0m f1_weighted: 0.30326837446302735
[2m[36m(func pid=146482)[0m f1_per_class: [0.276, 0.282, 0.667, 0.23, 0.052, 0.045, 0.481, 0.349, 0.262, 0.262]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4358 | Steps: 2 | Val loss: 4.4952 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3433 | Steps: 2 | Val loss: 1.8191 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8404 | Steps: 2 | Val loss: 2.2997 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0125 | Steps: 2 | Val loss: 2.9462 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:39:07 (running for 00:29:21.82)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.413 |      0.341 |                   65 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.018 |      0.29  |                   62 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.436 |      0.235 |                   62 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.904 |      0.119 |                   10 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.29850746268656714
[2m[36m(func pid=147051)[0m top5: 0.7985074626865671
[2m[36m(func pid=147051)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=147051)[0m f1_macro: 0.2348613739575675
[2m[36m(func pid=147051)[0m f1_weighted: 0.33860280090488526
[2m[36m(func pid=147051)[0m f1_per_class: [0.222, 0.143, 0.139, 0.345, 0.058, 0.353, 0.498, 0.33, 0.045, 0.215]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3512126865671642
[2m[36m(func pid=145749)[0m top5: 0.8717350746268657
[2m[36m(func pid=145749)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=145749)[0m f1_macro: 0.3388998383253834
[2m[36m(func pid=145749)[0m f1_weighted: 0.3889996660668072
[2m[36m(func pid=145749)[0m f1_per_class: [0.271, 0.321, 0.632, 0.461, 0.069, 0.349, 0.4, 0.488, 0.163, 0.236]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.283115671641791
[2m[36m(func pid=146482)[0m top5: 0.7971082089552238
[2m[36m(func pid=146482)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=146482)[0m f1_macro: 0.2816424569629716
[2m[36m(func pid=146482)[0m f1_weighted: 0.28584874679587996
[2m[36m(func pid=146482)[0m f1_per_class: [0.272, 0.281, 0.611, 0.215, 0.049, 0.052, 0.435, 0.347, 0.264, 0.29]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.17257462686567165
[2m[36m(func pid=158211)[0m top5: 0.5690298507462687
[2m[36m(func pid=158211)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=158211)[0m f1_macro: 0.12541430991987124
[2m[36m(func pid=158211)[0m f1_weighted: 0.14835659862410044
[2m[36m(func pid=158211)[0m f1_per_class: [0.093, 0.264, 0.23, 0.097, 0.016, 0.35, 0.088, 0.103, 0.015, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2679 | Steps: 2 | Val loss: 4.8502 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3856 | Steps: 2 | Val loss: 1.8167 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0370 | Steps: 2 | Val loss: 2.9284 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8346 | Steps: 2 | Val loss: 2.2923 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 04:39:12 (running for 00:29:27.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.343 |      0.339 |                   66 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.013 |      0.282 |                   63 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.268 |      0.233 |                   63 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.84  |      0.125 |                   11 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.2621268656716418
[2m[36m(func pid=147051)[0m top5: 0.7845149253731343
[2m[36m(func pid=147051)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=147051)[0m f1_macro: 0.23330919501326716
[2m[36m(func pid=147051)[0m f1_weighted: 0.2721869931073945
[2m[36m(func pid=147051)[0m f1_per_class: [0.23, 0.279, 0.182, 0.337, 0.05, 0.347, 0.182, 0.453, 0.034, 0.239]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=145749)[0m top1: 0.35261194029850745
[2m[36m(func pid=145749)[0m top5: 0.8736007462686567
[2m[36m(func pid=145749)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=145749)[0m f1_macro: 0.3383987097262575
[2m[36m(func pid=145749)[0m f1_weighted: 0.3919360206789146
[2m[36m(func pid=145749)[0m f1_per_class: [0.244, 0.323, 0.649, 0.461, 0.077, 0.358, 0.413, 0.453, 0.162, 0.245]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.28404850746268656
[2m[36m(func pid=146482)[0m top5: 0.8078358208955224
[2m[36m(func pid=146482)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=146482)[0m f1_macro: 0.2860776427902279
[2m[36m(func pid=146482)[0m f1_weighted: 0.28786734726793506
[2m[36m(func pid=146482)[0m f1_per_class: [0.264, 0.285, 0.629, 0.213, 0.05, 0.078, 0.43, 0.353, 0.269, 0.29]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.1707089552238806
[2m[36m(func pid=158211)[0m top5: 0.574160447761194
[2m[36m(func pid=158211)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=158211)[0m f1_macro: 0.1251538437527362
[2m[36m(func pid=158211)[0m f1_weighted: 0.14792467095099956
[2m[36m(func pid=158211)[0m f1_per_class: [0.09, 0.268, 0.222, 0.094, 0.01, 0.353, 0.083, 0.117, 0.015, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1715 | Steps: 2 | Val loss: 4.9341 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3494 | Steps: 2 | Val loss: 1.8172 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0567 | Steps: 2 | Val loss: 2.7285 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7870 | Steps: 2 | Val loss: 2.2857 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 04:39:18 (running for 00:29:32.43)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.386 |      0.338 |                   67 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.037 |      0.286 |                   64 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.172 |      0.229 |                   64 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.835 |      0.125 |                   12 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.2560634328358209
[2m[36m(func pid=147051)[0m top5: 0.7747201492537313
[2m[36m(func pid=147051)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=147051)[0m f1_macro: 0.2293503339395638
[2m[36m(func pid=147051)[0m f1_weighted: 0.29000891539440943
[2m[36m(func pid=147051)[0m f1_per_class: [0.204, 0.341, 0.254, 0.309, 0.049, 0.286, 0.279, 0.332, 0.054, 0.185]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.2980410447761194
[2m[36m(func pid=146482)[0m top5: 0.8558768656716418
[2m[36m(func pid=146482)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=146482)[0m f1_macro: 0.2897586533000195
[2m[36m(func pid=146482)[0m f1_weighted: 0.31366759940679567
[2m[36m(func pid=146482)[0m f1_per_class: [0.236, 0.293, 0.595, 0.263, 0.051, 0.115, 0.452, 0.372, 0.266, 0.255]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.35867537313432835
[2m[36m(func pid=145749)[0m top5: 0.8698694029850746
[2m[36m(func pid=145749)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=145749)[0m f1_macro: 0.3398924090731685
[2m[36m(func pid=145749)[0m f1_weighted: 0.398653723321599
[2m[36m(func pid=145749)[0m f1_per_class: [0.249, 0.327, 0.615, 0.462, 0.078, 0.35, 0.434, 0.448, 0.183, 0.253]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m top1: 0.17444029850746268
[2m[36m(func pid=158211)[0m top5: 0.5792910447761194
[2m[36m(func pid=158211)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=158211)[0m f1_macro: 0.13229859467186184
[2m[36m(func pid=158211)[0m f1_weighted: 0.15663894033617065
[2m[36m(func pid=158211)[0m f1_per_class: [0.113, 0.269, 0.233, 0.102, 0.014, 0.36, 0.1, 0.116, 0.016, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3194 | Steps: 2 | Val loss: 6.7771 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0148 | Steps: 2 | Val loss: 2.5705 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4567 | Steps: 2 | Val loss: 1.8152 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7694 | Steps: 2 | Val loss: 2.2782 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=147051)[0m top1: 0.18516791044776118
[2m[36m(func pid=147051)[0m top5: 0.6716417910447762
[2m[36m(func pid=147051)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=147051)[0m f1_macro: 0.1661592790603476
[2m[36m(func pid=147051)[0m f1_weighted: 0.2181515754322239
[2m[36m(func pid=147051)[0m f1_per_class: [0.187, 0.335, 0.364, 0.287, 0.043, 0.124, 0.186, 0.055, 0.021, 0.06]
== Status ==
Current time: 2024-01-07 04:39:23 (running for 00:29:37.65)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.349 |      0.34  |                   68 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.057 |      0.29  |                   65 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.319 |      0.166 |                   65 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.787 |      0.132 |                   13 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3255597014925373
[2m[36m(func pid=146482)[0m top5: 0.8899253731343284
[2m[36m(func pid=146482)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=146482)[0m f1_macro: 0.3200691909158182
[2m[36m(func pid=146482)[0m f1_weighted: 0.3662938558389692
[2m[36m(func pid=146482)[0m f1_per_class: [0.208, 0.285, 0.741, 0.398, 0.037, 0.167, 0.478, 0.439, 0.233, 0.215]
[2m[36m(func pid=145749)[0m top1: 0.36427238805970147
[2m[36m(func pid=145749)[0m top5: 0.8694029850746269
[2m[36m(func pid=145749)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=145749)[0m f1_macro: 0.344170028569724
[2m[36m(func pid=145749)[0m f1_weighted: 0.405887122339133
[2m[36m(func pid=145749)[0m f1_per_class: [0.22, 0.329, 0.632, 0.455, 0.077, 0.355, 0.458, 0.477, 0.18, 0.26]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.18003731343283583
[2m[36m(func pid=158211)[0m top5: 0.5867537313432836
[2m[36m(func pid=158211)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=158211)[0m f1_macro: 0.14203475106659996
[2m[36m(func pid=158211)[0m f1_weighted: 0.16528621325707776
[2m[36m(func pid=158211)[0m f1_per_class: [0.153, 0.266, 0.246, 0.11, 0.018, 0.369, 0.114, 0.129, 0.016, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6514 | Steps: 2 | Val loss: 6.3428 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3502 | Steps: 2 | Val loss: 1.8065 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0080 | Steps: 2 | Val loss: 2.5688 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=147051)[0m top1: 0.21128731343283583
[2m[36m(func pid=147051)[0m top5: 0.7154850746268657
[2m[36m(func pid=147051)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=147051)[0m f1_macro: 0.2102865950465084
[2m[36m(func pid=147051)[0m f1_weighted: 0.25217535582836426
[2m[36m(func pid=147051)[0m f1_per_class: [0.186, 0.365, 0.643, 0.304, 0.047, 0.113, 0.26, 0.07, 0.034, 0.082]
[2m[36m(func pid=147051)[0m 
== Status ==
Current time: 2024-01-07 04:39:28 (running for 00:29:43.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.457 |      0.344 |                   69 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.015 |      0.32  |                   66 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.651 |      0.21  |                   66 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.769 |      0.142 |                   14 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7417 | Steps: 2 | Val loss: 2.2699 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=145749)[0m top1: 0.3666044776119403
[2m[36m(func pid=145749)[0m top5: 0.8694029850746269
[2m[36m(func pid=145749)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=145749)[0m f1_macro: 0.3490291315556139
[2m[36m(func pid=145749)[0m f1_weighted: 0.40799190815609127
[2m[36m(func pid=145749)[0m f1_per_class: [0.209, 0.329, 0.667, 0.441, 0.081, 0.365, 0.473, 0.475, 0.185, 0.265]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.32975746268656714
[2m[36m(func pid=146482)[0m top5: 0.9015858208955224
[2m[36m(func pid=146482)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=146482)[0m f1_macro: 0.31675456037575545
[2m[36m(func pid=146482)[0m f1_weighted: 0.3716919092273115
[2m[36m(func pid=146482)[0m f1_per_class: [0.275, 0.288, 0.72, 0.463, 0.033, 0.15, 0.452, 0.354, 0.243, 0.188]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.17723880597014927
[2m[36m(func pid=158211)[0m top5: 0.5951492537313433
[2m[36m(func pid=158211)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=158211)[0m f1_macro: 0.14383947840743222
[2m[36m(func pid=158211)[0m f1_weighted: 0.1681160762993711
[2m[36m(func pid=158211)[0m f1_per_class: [0.149, 0.266, 0.254, 0.119, 0.017, 0.354, 0.117, 0.148, 0.016, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5403 | Steps: 2 | Val loss: 4.4069 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0168 | Steps: 2 | Val loss: 2.5969 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3267 | Steps: 2 | Val loss: 1.7850 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:39:33 (running for 00:29:48.21)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.35  |      0.349 |                   70 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.008 |      0.317 |                   67 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.54  |      0.31  |                   67 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.742 |      0.144 |                   15 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.35027985074626866
[2m[36m(func pid=147051)[0m top5: 0.8236940298507462
[2m[36m(func pid=147051)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=147051)[0m f1_macro: 0.30977961844794294
[2m[36m(func pid=147051)[0m f1_weighted: 0.3864669157915127
[2m[36m(func pid=147051)[0m f1_per_class: [0.13, 0.328, 0.7, 0.463, 0.056, 0.215, 0.485, 0.299, 0.098, 0.324]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7130 | Steps: 2 | Val loss: 2.2627 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=146482)[0m top1: 0.33255597014925375
[2m[36m(func pid=146482)[0m top5: 0.9053171641791045
[2m[36m(func pid=146482)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=146482)[0m f1_macro: 0.315826504932854
[2m[36m(func pid=146482)[0m f1_weighted: 0.37173899403546123
[2m[36m(func pid=146482)[0m f1_per_class: [0.314, 0.268, 0.72, 0.503, 0.044, 0.138, 0.437, 0.324, 0.216, 0.195]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.37453358208955223
[2m[36m(func pid=145749)[0m top5: 0.8782649253731343
[2m[36m(func pid=145749)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=145749)[0m f1_macro: 0.35363188737701395
[2m[36m(func pid=145749)[0m f1_weighted: 0.4141874796731937
[2m[36m(func pid=145749)[0m f1_per_class: [0.227, 0.333, 0.686, 0.462, 0.082, 0.353, 0.476, 0.471, 0.18, 0.267]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8830 | Steps: 2 | Val loss: 5.3175 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=158211)[0m top1: 0.1814365671641791
[2m[36m(func pid=158211)[0m top5: 0.6124067164179104
[2m[36m(func pid=158211)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=158211)[0m f1_macro: 0.1482340989408623
[2m[36m(func pid=158211)[0m f1_weighted: 0.1729730314709488
[2m[36m(func pid=158211)[0m f1_per_class: [0.144, 0.273, 0.25, 0.126, 0.017, 0.361, 0.114, 0.182, 0.015, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0200 | Steps: 2 | Val loss: 2.6114 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3548 | Steps: 2 | Val loss: 1.7908 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:39:39 (running for 00:29:53.73)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.327 |      0.354 |                   71 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.017 |      0.316 |                   68 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.883 |      0.251 |                   68 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.713 |      0.148 |                   16 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.324160447761194
[2m[36m(func pid=147051)[0m top5: 0.7803171641791045
[2m[36m(func pid=147051)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=147051)[0m f1_macro: 0.2505848815077968
[2m[36m(func pid=147051)[0m f1_weighted: 0.35001568059463956
[2m[36m(func pid=147051)[0m f1_per_class: [0.046, 0.279, 0.267, 0.37, 0.071, 0.096, 0.509, 0.46, 0.08, 0.329]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6642 | Steps: 2 | Val loss: 2.2560 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=146482)[0m top1: 0.33861940298507465
[2m[36m(func pid=146482)[0m top5: 0.9113805970149254
[2m[36m(func pid=146482)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=146482)[0m f1_macro: 0.3288047562415723
[2m[36m(func pid=146482)[0m f1_weighted: 0.37815721013795084
[2m[36m(func pid=146482)[0m f1_per_class: [0.355, 0.268, 0.769, 0.519, 0.042, 0.14, 0.44, 0.296, 0.25, 0.208]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3689365671641791
[2m[36m(func pid=145749)[0m top5: 0.8791977611940298
[2m[36m(func pid=145749)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=145749)[0m f1_macro: 0.35176105515465506
[2m[36m(func pid=145749)[0m f1_weighted: 0.40933009698679057
[2m[36m(func pid=145749)[0m f1_per_class: [0.231, 0.327, 0.706, 0.463, 0.082, 0.334, 0.472, 0.455, 0.184, 0.265]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2636 | Steps: 2 | Val loss: 7.1912 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=158211)[0m top1: 0.18097014925373134
[2m[36m(func pid=158211)[0m top5: 0.6203358208955224
[2m[36m(func pid=158211)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=158211)[0m f1_macro: 0.15173840180699774
[2m[36m(func pid=158211)[0m f1_weighted: 0.1767312409827623
[2m[36m(func pid=158211)[0m f1_per_class: [0.169, 0.27, 0.256, 0.139, 0.017, 0.352, 0.118, 0.181, 0.015, 0.0]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0071 | Steps: 2 | Val loss: 2.6325 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3874 | Steps: 2 | Val loss: 1.7937 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 04:39:44 (running for 00:29:59.11)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.355 |      0.352 |                   72 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.02  |      0.329 |                   69 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.264 |      0.21  |                   69 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.664 |      0.152 |                   17 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.27705223880597013
[2m[36m(func pid=147051)[0m top5: 0.757929104477612
[2m[36m(func pid=147051)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=147051)[0m f1_macro: 0.20965737849073038
[2m[36m(func pid=147051)[0m f1_weighted: 0.2785293013314987
[2m[36m(func pid=147051)[0m f1_per_class: [0.042, 0.271, 0.267, 0.081, 0.05, 0.08, 0.564, 0.406, 0.083, 0.253]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.333955223880597
[2m[36m(func pid=146482)[0m top5: 0.9123134328358209
[2m[36m(func pid=146482)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=146482)[0m f1_macro: 0.3386376891771349
[2m[36m(func pid=146482)[0m f1_weighted: 0.3728725607597713
[2m[36m(func pid=146482)[0m f1_per_class: [0.378, 0.25, 0.857, 0.522, 0.044, 0.148, 0.425, 0.286, 0.25, 0.226]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6197 | Steps: 2 | Val loss: 2.2471 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=145749)[0m top1: 0.36100746268656714
[2m[36m(func pid=145749)[0m top5: 0.886660447761194
[2m[36m(func pid=145749)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=145749)[0m f1_macro: 0.35057365433732013
[2m[36m(func pid=145749)[0m f1_weighted: 0.3982560973422823
[2m[36m(func pid=145749)[0m f1_per_class: [0.245, 0.329, 0.75, 0.455, 0.08, 0.337, 0.444, 0.435, 0.157, 0.272]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6112 | Steps: 2 | Val loss: 5.6150 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=158211)[0m top1: 0.1828358208955224
[2m[36m(func pid=158211)[0m top5: 0.6333955223880597
[2m[36m(func pid=158211)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=158211)[0m f1_macro: 0.16150195124788744
[2m[36m(func pid=158211)[0m f1_weighted: 0.18209014072002327
[2m[36m(func pid=158211)[0m f1_per_class: [0.171, 0.268, 0.275, 0.153, 0.013, 0.359, 0.118, 0.181, 0.016, 0.062]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0114 | Steps: 2 | Val loss: 2.6488 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2808 | Steps: 2 | Val loss: 1.8097 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=147051)[0m top1: 0.32882462686567165
[2m[36m(func pid=147051)[0m top5: 0.8269589552238806
[2m[36m(func pid=147051)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=147051)[0m f1_macro: 0.28350535120673337
[2m[36m(func pid=147051)[0m f1_weighted: 0.3484609664647013
[2m[36m(func pid=147051)[0m f1_per_class: [0.107, 0.371, 0.632, 0.27, 0.045, 0.169, 0.548, 0.234, 0.066, 0.393]
[2m[36m(func pid=147051)[0m 
== Status ==
Current time: 2024-01-07 04:39:50 (running for 00:30:04.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.387 |      0.351 |                   73 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.007 |      0.339 |                   70 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.611 |      0.284 |                   70 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.62  |      0.162 |                   18 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=146482)[0m top1: 0.3306902985074627
[2m[36m(func pid=146482)[0m top5: 0.9132462686567164
[2m[36m(func pid=146482)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=146482)[0m f1_macro: 0.3370549881200679
[2m[36m(func pid=146482)[0m f1_weighted: 0.3676095535068093
[2m[36m(func pid=146482)[0m f1_per_class: [0.4, 0.25, 0.828, 0.524, 0.044, 0.15, 0.404, 0.281, 0.243, 0.247]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3516791044776119
[2m[36m(func pid=145749)[0m top5: 0.8815298507462687
[2m[36m(func pid=145749)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=145749)[0m f1_macro: 0.3481510595826303
[2m[36m(func pid=145749)[0m f1_weighted: 0.3892364861936675
[2m[36m(func pid=145749)[0m f1_per_class: [0.247, 0.324, 0.75, 0.431, 0.078, 0.33, 0.439, 0.447, 0.168, 0.268]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6057 | Steps: 2 | Val loss: 2.2328 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6879 | Steps: 2 | Val loss: 4.6570 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0034 | Steps: 2 | Val loss: 2.5929 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=158211)[0m top1: 0.18516791044776118
[2m[36m(func pid=158211)[0m top5: 0.6618470149253731
[2m[36m(func pid=158211)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=158211)[0m f1_macro: 0.16517002498276634
[2m[36m(func pid=158211)[0m f1_weighted: 0.1916157815766461
[2m[36m(func pid=158211)[0m f1_per_class: [0.167, 0.264, 0.289, 0.167, 0.012, 0.346, 0.144, 0.184, 0.015, 0.063]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3396 | Steps: 2 | Val loss: 1.8296 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:39:55 (running for 00:30:09.75)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.281 |      0.348 |                   74 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.011 |      0.337 |                   71 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.688 |      0.347 |                   71 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.606 |      0.165 |                   19 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147051)[0m top1: 0.37453358208955223
[2m[36m(func pid=147051)[0m top5: 0.8847947761194029
[2m[36m(func pid=147051)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=147051)[0m f1_macro: 0.34661476991232903
[2m[36m(func pid=147051)[0m f1_weighted: 0.3803806748199637
[2m[36m(func pid=147051)[0m f1_per_class: [0.152, 0.483, 0.727, 0.448, 0.094, 0.417, 0.282, 0.435, 0.094, 0.333]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.33675373134328357
[2m[36m(func pid=146482)[0m top5: 0.9155783582089553
[2m[36m(func pid=146482)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=146482)[0m f1_macro: 0.34107716349968376
[2m[36m(func pid=146482)[0m f1_weighted: 0.3716151216680967
[2m[36m(func pid=146482)[0m f1_per_class: [0.427, 0.243, 0.774, 0.521, 0.051, 0.161, 0.413, 0.305, 0.255, 0.26]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3493470149253731
[2m[36m(func pid=145749)[0m top5: 0.8824626865671642
[2m[36m(func pid=145749)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=145749)[0m f1_macro: 0.3485654359290456
[2m[36m(func pid=145749)[0m f1_weighted: 0.38735353281052676
[2m[36m(func pid=145749)[0m f1_per_class: [0.225, 0.33, 0.75, 0.416, 0.083, 0.35, 0.434, 0.46, 0.171, 0.267]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6312 | Steps: 2 | Val loss: 2.2233 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4202 | Steps: 2 | Val loss: 5.0425 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0268 | Steps: 2 | Val loss: 2.5259 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3082 | Steps: 2 | Val loss: 1.8534 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=158211)[0m top1: 0.19309701492537312
[2m[36m(func pid=158211)[0m top5: 0.6753731343283582
[2m[36m(func pid=158211)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=158211)[0m f1_macro: 0.1784322866872277
[2m[36m(func pid=158211)[0m f1_weighted: 0.2026203123879288
[2m[36m(func pid=158211)[0m f1_per_class: [0.188, 0.269, 0.319, 0.184, 0.017, 0.339, 0.158, 0.195, 0.029, 0.087]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m top1: 0.34794776119402987
[2m[36m(func pid=147051)[0m top5: 0.9001865671641791
[2m[36m(func pid=147051)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=147051)[0m f1_macro: 0.3106542664058568
[2m[36m(func pid=147051)[0m f1_weighted: 0.32833756986779844
[2m[36m(func pid=147051)[0m f1_per_class: [0.157, 0.425, 0.815, 0.516, 0.124, 0.319, 0.132, 0.387, 0.068, 0.164]
[2m[36m(func pid=147051)[0m 
== Status ==
Current time: 2024-01-07 04:40:00 (running for 00:30:15.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.334
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.34  |      0.349 |                   75 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.003 |      0.341 |                   72 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.42  |      0.311 |                   72 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.631 |      0.178 |                   20 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=146482)[0m top1: 0.34468283582089554
[2m[36m(func pid=146482)[0m top5: 0.9127798507462687
[2m[36m(func pid=146482)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=146482)[0m f1_macro: 0.3422638108264272
[2m[36m(func pid=146482)[0m f1_weighted: 0.3781934464900183
[2m[36m(func pid=146482)[0m f1_per_class: [0.402, 0.245, 0.75, 0.518, 0.059, 0.213, 0.421, 0.291, 0.258, 0.265]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.34794776119402987
[2m[36m(func pid=145749)[0m top5: 0.8731343283582089
[2m[36m(func pid=145749)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=145749)[0m f1_macro: 0.34427546685881144
[2m[36m(func pid=145749)[0m f1_weighted: 0.38949162817446126
[2m[36m(func pid=145749)[0m f1_per_class: [0.189, 0.341, 0.686, 0.39, 0.078, 0.369, 0.45, 0.467, 0.211, 0.261]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5325 | Steps: 2 | Val loss: 2.2127 | Batch size: 32 | lr: 0.0001 | Duration: 3.25s
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1667 | Steps: 2 | Val loss: 4.9984 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0056 | Steps: 2 | Val loss: 2.4644 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3426 | Steps: 2 | Val loss: 1.8684 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:40:05 (running for 00:30:20.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.334
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.308 |      0.344 |                   76 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.027 |      0.342 |                   73 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.42  |      0.311 |                   72 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.533 |      0.182 |                   21 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.197294776119403
[2m[36m(func pid=158211)[0m top5: 0.6916977611940298
[2m[36m(func pid=158211)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=158211)[0m f1_macro: 0.1818306616388057
[2m[36m(func pid=158211)[0m f1_weighted: 0.2104769560706108
[2m[36m(func pid=158211)[0m f1_per_class: [0.202, 0.258, 0.301, 0.201, 0.017, 0.355, 0.166, 0.208, 0.029, 0.081]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=147051)[0m top1: 0.3512126865671642
[2m[36m(func pid=147051)[0m top5: 0.8852611940298507
[2m[36m(func pid=147051)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=147051)[0m f1_macro: 0.2657518809948205
[2m[36m(func pid=147051)[0m f1_weighted: 0.33183493199066066
[2m[36m(func pid=147051)[0m f1_per_class: [0.143, 0.352, 0.558, 0.559, 0.078, 0.298, 0.18, 0.28, 0.102, 0.107]
[2m[36m(func pid=147051)[0m 
[2m[36m(func pid=146482)[0m top1: 0.34888059701492535
[2m[36m(func pid=146482)[0m top5: 0.9081156716417911
[2m[36m(func pid=146482)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=146482)[0m f1_macro: 0.34733354827953844
[2m[36m(func pid=146482)[0m f1_weighted: 0.37978113030549737
[2m[36m(func pid=146482)[0m f1_per_class: [0.368, 0.251, 0.727, 0.509, 0.067, 0.257, 0.406, 0.326, 0.294, 0.269]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.33861940298507465
[2m[36m(func pid=145749)[0m top5: 0.8708022388059702
[2m[36m(func pid=145749)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=145749)[0m f1_macro: 0.3404720711519381
[2m[36m(func pid=145749)[0m f1_weighted: 0.3787454459722266
[2m[36m(func pid=145749)[0m f1_per_class: [0.181, 0.343, 0.706, 0.366, 0.078, 0.373, 0.439, 0.447, 0.203, 0.268]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2209 | Steps: 2 | Val loss: 4.9383 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5477 | Steps: 2 | Val loss: 2.2042 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0085 | Steps: 2 | Val loss: 2.4254 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2989 | Steps: 2 | Val loss: 1.8624 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=147051)[0m top1: 0.34375
[2m[36m(func pid=147051)[0m top5: 0.8619402985074627
[2m[36m(func pid=147051)[0m f1_micro: 0.34375
[2m[36m(func pid=147051)[0m f1_macro: 0.25513296543093744
[2m[36m(func pid=147051)[0m f1_weighted: 0.33804799057986124
[2m[36m(func pid=147051)[0m f1_per_class: [0.121, 0.282, 0.407, 0.544, 0.057, 0.296, 0.248, 0.378, 0.02, 0.2]
[2m[36m(func pid=147051)[0m 
== Status ==
Current time: 2024-01-07 04:40:11 (running for 00:30:25.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.334
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.343 |      0.34  |                   77 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.006 |      0.347 |                   74 |
| train_35a0b_00015 | RUNNING    | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.221 |      0.255 |                   74 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.533 |      0.182 |                   21 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.20662313432835822
[2m[36m(func pid=158211)[0m top5: 0.6986940298507462
[2m[36m(func pid=158211)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=158211)[0m f1_macro: 0.19023904123084578
[2m[36m(func pid=158211)[0m f1_weighted: 0.22359190817708063
[2m[36m(func pid=158211)[0m f1_per_class: [0.209, 0.259, 0.324, 0.222, 0.017, 0.346, 0.189, 0.225, 0.03, 0.081]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3596082089552239
[2m[36m(func pid=146482)[0m top5: 0.902518656716418
[2m[36m(func pid=146482)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=146482)[0m f1_macro: 0.35273142614816577
[2m[36m(func pid=146482)[0m f1_weighted: 0.3889690865040392
[2m[36m(func pid=146482)[0m f1_per_class: [0.359, 0.251, 0.727, 0.508, 0.079, 0.279, 0.422, 0.376, 0.284, 0.243]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.341884328358209
[2m[36m(func pid=145749)[0m top5: 0.8745335820895522
[2m[36m(func pid=145749)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=145749)[0m f1_macro: 0.34365017955225574
[2m[36m(func pid=145749)[0m f1_weighted: 0.38184496859706546
[2m[36m(func pid=145749)[0m f1_per_class: [0.206, 0.339, 0.727, 0.392, 0.075, 0.366, 0.426, 0.466, 0.188, 0.252]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=147051)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1438 | Steps: 2 | Val loss: 4.9861 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4974 | Steps: 2 | Val loss: 2.1916 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0158 | Steps: 2 | Val loss: 2.4308 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2626 | Steps: 2 | Val loss: 1.8550 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=147051)[0m top1: 0.3306902985074627
[2m[36m(func pid=147051)[0m top5: 0.847481343283582
[2m[36m(func pid=147051)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=147051)[0m f1_macro: 0.2506041533015046
[2m[36m(func pid=147051)[0m f1_weighted: 0.34687952070369743
[2m[36m(func pid=147051)[0m f1_per_class: [0.154, 0.262, 0.244, 0.491, 0.055, 0.278, 0.339, 0.392, 0.043, 0.248]
== Status ==
Current time: 2024-01-07 04:40:16 (running for 00:30:31.19)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 3 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.299 |      0.344 |                   78 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.009 |      0.353 |                   75 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.548 |      0.19  |                   22 |
| train_35a0b_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.21548507462686567
[2m[36m(func pid=158211)[0m top5: 0.715018656716418
[2m[36m(func pid=158211)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=158211)[0m f1_macro: 0.19871473164482384
[2m[36m(func pid=158211)[0m f1_weighted: 0.23323655674381405
[2m[36m(func pid=158211)[0m f1_per_class: [0.218, 0.26, 0.289, 0.236, 0.017, 0.347, 0.197, 0.268, 0.031, 0.123]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.365205223880597
[2m[36m(func pid=146482)[0m top5: 0.8964552238805971
[2m[36m(func pid=146482)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=146482)[0m f1_macro: 0.3527750760902987
[2m[36m(func pid=146482)[0m f1_weighted: 0.39006216306592756
[2m[36m(func pid=146482)[0m f1_per_class: [0.355, 0.264, 0.706, 0.49, 0.092, 0.343, 0.422, 0.31, 0.305, 0.242]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.33908582089552236
[2m[36m(func pid=145749)[0m top5: 0.8759328358208955
[2m[36m(func pid=145749)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=145749)[0m f1_macro: 0.3382669581185459
[2m[36m(func pid=145749)[0m f1_weighted: 0.3752354139600582
[2m[36m(func pid=145749)[0m f1_per_class: [0.242, 0.33, 0.706, 0.4, 0.074, 0.33, 0.415, 0.477, 0.164, 0.247]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4544 | Steps: 2 | Val loss: 2.1806 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0068 | Steps: 2 | Val loss: 2.5291 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2399 | Steps: 2 | Val loss: 1.8524 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=158211)[0m top1: 0.21875
[2m[36m(func pid=158211)[0m top5: 0.7290111940298507
[2m[36m(func pid=158211)[0m f1_micro: 0.21875
[2m[36m(func pid=158211)[0m f1_macro: 0.20210113168410357
[2m[36m(func pid=158211)[0m f1_weighted: 0.23683749665420076
[2m[36m(func pid=158211)[0m f1_per_class: [0.235, 0.251, 0.276, 0.26, 0.022, 0.335, 0.194, 0.27, 0.032, 0.146]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3516791044776119
[2m[36m(func pid=146482)[0m top5: 0.8857276119402985
[2m[36m(func pid=146482)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=146482)[0m f1_macro: 0.33652101541070023
[2m[36m(func pid=146482)[0m f1_weighted: 0.3719828270131474
[2m[36m(func pid=146482)[0m f1_per_class: [0.332, 0.27, 0.706, 0.445, 0.106, 0.351, 0.418, 0.206, 0.308, 0.224]
[2m[36m(func pid=145749)[0m top1: 0.33861940298507465
[2m[36m(func pid=145749)[0m top5: 0.8777985074626866
[2m[36m(func pid=145749)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=145749)[0m f1_macro: 0.33697343143538633
[2m[36m(func pid=145749)[0m f1_weighted: 0.37541306022050996
[2m[36m(func pid=145749)[0m f1_per_class: [0.262, 0.326, 0.727, 0.404, 0.071, 0.312, 0.424, 0.464, 0.147, 0.232]
== Status ==
Current time: 2024-01-07 04:40:23 (running for 00:30:37.37)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.263 |      0.338 |                   79 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.016 |      0.353 |                   76 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.454 |      0.202 |                   24 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=163905)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=163905)[0m Configuration completed!
[2m[36m(func pid=163905)[0m New optimizer parameters:
[2m[36m(func pid=163905)[0m SGD (
[2m[36m(func pid=163905)[0m Parameter Group 0
[2m[36m(func pid=163905)[0m     dampening: 0
[2m[36m(func pid=163905)[0m     differentiable: False
[2m[36m(func pid=163905)[0m     foreach: None
[2m[36m(func pid=163905)[0m     lr: 0.001
[2m[36m(func pid=163905)[0m     maximize: False
[2m[36m(func pid=163905)[0m     momentum: 0.99
[2m[36m(func pid=163905)[0m     nesterov: False
[2m[36m(func pid=163905)[0m     weight_decay: 1e-05
[2m[36m(func pid=163905)[0m )
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4230 | Steps: 2 | Val loss: 2.1699 | Batch size: 32 | lr: 0.0001 | Duration: 3.32s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3003 | Steps: 2 | Val loss: 1.8513 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0029 | Steps: 2 | Val loss: 2.6113 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:40:29 (running for 00:30:43.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.24  |      0.337 |                   80 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.007 |      0.337 |                   77 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.423 |      0.209 |                   25 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.22434701492537312
[2m[36m(func pid=158211)[0m top5: 0.7388059701492538
[2m[36m(func pid=158211)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=158211)[0m f1_macro: 0.20858931444232942
[2m[36m(func pid=158211)[0m f1_weighted: 0.24332486582183113
[2m[36m(func pid=158211)[0m f1_per_class: [0.233, 0.246, 0.267, 0.275, 0.022, 0.326, 0.203, 0.287, 0.034, 0.193]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9549 | Steps: 2 | Val loss: 2.3212 | Batch size: 32 | lr: 0.001 | Duration: 4.65s
[2m[36m(func pid=145749)[0m top1: 0.34048507462686567
[2m[36m(func pid=145749)[0m top5: 0.8763992537313433
[2m[36m(func pid=145749)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=145749)[0m f1_macro: 0.33839762310939275
[2m[36m(func pid=145749)[0m f1_weighted: 0.37693944928208645
[2m[36m(func pid=145749)[0m f1_per_class: [0.269, 0.326, 0.727, 0.409, 0.076, 0.316, 0.425, 0.443, 0.155, 0.239]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.33302238805970147
[2m[36m(func pid=146482)[0m top5: 0.8694029850746269
[2m[36m(func pid=146482)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=146482)[0m f1_macro: 0.3200767917365125
[2m[36m(func pid=146482)[0m f1_weighted: 0.35071656207972457
[2m[36m(func pid=146482)[0m f1_per_class: [0.303, 0.273, 0.706, 0.408, 0.115, 0.323, 0.405, 0.134, 0.324, 0.21]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3945 | Steps: 2 | Val loss: 2.1588 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=163905)[0m top1: 0.1767723880597015
[2m[36m(func pid=163905)[0m top5: 0.5303171641791045
[2m[36m(func pid=163905)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=163905)[0m f1_macro: 0.11761229144335601
[2m[36m(func pid=163905)[0m f1_weighted: 0.12657736323747942
[2m[36m(func pid=163905)[0m f1_per_class: [0.31, 0.345, 0.0, 0.097, 0.0, 0.218, 0.018, 0.027, 0.0, 0.161]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2385 | Steps: 2 | Val loss: 1.8426 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0074 | Steps: 2 | Val loss: 2.6670 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:40:34 (running for 00:30:49.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.3   |      0.338 |                   81 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.003 |      0.32  |                   78 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.395 |      0.217 |                   26 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.955 |      0.118 |                    1 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.23180970149253732
[2m[36m(func pid=158211)[0m top5: 0.75
[2m[36m(func pid=158211)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=158211)[0m f1_macro: 0.21651572580474104
[2m[36m(func pid=158211)[0m f1_weighted: 0.2512694038073676
[2m[36m(func pid=158211)[0m f1_per_class: [0.25, 0.242, 0.264, 0.296, 0.023, 0.33, 0.207, 0.295, 0.034, 0.225]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9120 | Steps: 2 | Val loss: 2.2961 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=145749)[0m top1: 0.34701492537313433
[2m[36m(func pid=145749)[0m top5: 0.8768656716417911
[2m[36m(func pid=145749)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=145749)[0m f1_macro: 0.3392420011942166
[2m[36m(func pid=145749)[0m f1_weighted: 0.3839505337033611
[2m[36m(func pid=145749)[0m f1_per_class: [0.244, 0.333, 0.686, 0.413, 0.08, 0.327, 0.436, 0.453, 0.167, 0.255]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.32509328358208955
[2m[36m(func pid=146482)[0m top5: 0.8572761194029851
[2m[36m(func pid=146482)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=146482)[0m f1_macro: 0.3164211777924799
[2m[36m(func pid=146482)[0m f1_weighted: 0.34100281618821854
[2m[36m(func pid=146482)[0m f1_per_class: [0.295, 0.272, 0.706, 0.4, 0.118, 0.32, 0.381, 0.137, 0.322, 0.213]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3700 | Steps: 2 | Val loss: 2.1490 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=163905)[0m top1: 0.18703358208955223
[2m[36m(func pid=163905)[0m top5: 0.558768656716418
[2m[36m(func pid=163905)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=163905)[0m f1_macro: 0.12358826682647292
[2m[36m(func pid=163905)[0m f1_weighted: 0.13584971983685357
[2m[36m(func pid=163905)[0m f1_per_class: [0.245, 0.332, 0.0, 0.118, 0.012, 0.277, 0.009, 0.076, 0.0, 0.167]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2523 | Steps: 2 | Val loss: 1.8677 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0106 | Steps: 2 | Val loss: 2.6767 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 04:40:40 (running for 00:30:55.09)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.238 |      0.339 |                   82 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.007 |      0.316 |                   79 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.37  |      0.221 |                   27 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.912 |      0.124 |                    2 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.24113805970149255
[2m[36m(func pid=158211)[0m top5: 0.761660447761194
[2m[36m(func pid=158211)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=158211)[0m f1_macro: 0.22059245417126175
[2m[36m(func pid=158211)[0m f1_weighted: 0.2618863179724614
[2m[36m(func pid=158211)[0m f1_per_class: [0.274, 0.242, 0.25, 0.315, 0.028, 0.327, 0.224, 0.301, 0.034, 0.211]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9316 | Steps: 2 | Val loss: 2.2911 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=145749)[0m top1: 0.34794776119402987
[2m[36m(func pid=145749)[0m top5: 0.8740671641791045
[2m[36m(func pid=145749)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=145749)[0m f1_macro: 0.33946129296182936
[2m[36m(func pid=145749)[0m f1_weighted: 0.3880477985373618
[2m[36m(func pid=145749)[0m f1_per_class: [0.218, 0.337, 0.667, 0.409, 0.078, 0.341, 0.444, 0.452, 0.197, 0.252]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.32975746268656714
[2m[36m(func pid=146482)[0m top5: 0.8526119402985075
[2m[36m(func pid=146482)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=146482)[0m f1_macro: 0.3195821749881626
[2m[36m(func pid=146482)[0m f1_weighted: 0.3475130387118434
[2m[36m(func pid=146482)[0m f1_per_class: [0.298, 0.283, 0.706, 0.398, 0.119, 0.319, 0.396, 0.151, 0.33, 0.196]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2957 | Steps: 2 | Val loss: 2.1410 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=163905)[0m top1: 0.16837686567164178
[2m[36m(func pid=163905)[0m top5: 0.590018656716418
[2m[36m(func pid=163905)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=163905)[0m f1_macro: 0.10540264570976143
[2m[36m(func pid=163905)[0m f1_weighted: 0.14155241708921834
[2m[36m(func pid=163905)[0m f1_per_class: [0.097, 0.308, 0.0, 0.12, 0.0, 0.22, 0.067, 0.108, 0.0, 0.133]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2717 | Steps: 2 | Val loss: 1.8934 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0044 | Steps: 2 | Val loss: 2.6439 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 04:40:46 (running for 00:31:00.85)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.252 |      0.339 |                   83 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.011 |      0.32  |                   80 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.296 |      0.224 |                   28 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.932 |      0.105 |                    3 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.24440298507462688
[2m[36m(func pid=158211)[0m top5: 0.7681902985074627
[2m[36m(func pid=158211)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=158211)[0m f1_macro: 0.2244360981638874
[2m[36m(func pid=158211)[0m f1_weighted: 0.26438117161169966
[2m[36m(func pid=158211)[0m f1_per_class: [0.298, 0.238, 0.25, 0.325, 0.029, 0.323, 0.225, 0.3, 0.035, 0.222]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7954 | Steps: 2 | Val loss: 2.2901 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=145749)[0m top1: 0.33908582089552236
[2m[36m(func pid=145749)[0m top5: 0.8675373134328358
[2m[36m(func pid=145749)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=145749)[0m f1_macro: 0.33579946662507737
[2m[36m(func pid=145749)[0m f1_weighted: 0.37964797860143296
[2m[36m(func pid=145749)[0m f1_per_class: [0.195, 0.339, 0.667, 0.391, 0.08, 0.33, 0.434, 0.454, 0.216, 0.252]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3381529850746269
[2m[36m(func pid=146482)[0m top5: 0.8530783582089553
[2m[36m(func pid=146482)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=146482)[0m f1_macro: 0.3240019255093751
[2m[36m(func pid=146482)[0m f1_weighted: 0.3584713806341019
[2m[36m(func pid=146482)[0m f1_per_class: [0.313, 0.28, 0.706, 0.424, 0.112, 0.32, 0.408, 0.16, 0.33, 0.187]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m top1: 0.15625
[2m[36m(func pid=163905)[0m top5: 0.5853544776119403
[2m[36m(func pid=163905)[0m f1_micro: 0.15625
[2m[36m(func pid=163905)[0m f1_macro: 0.10775228511201185
[2m[36m(func pid=163905)[0m f1_weighted: 0.1444238810512744
[2m[36m(func pid=163905)[0m f1_per_class: [0.089, 0.288, 0.103, 0.119, 0.009, 0.153, 0.124, 0.053, 0.0, 0.14]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3056 | Steps: 2 | Val loss: 2.1355 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2320 | Steps: 2 | Val loss: 1.8768 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0076 | Steps: 2 | Val loss: 2.6204 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7151 | Steps: 2 | Val loss: 2.2759 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 04:40:52 (running for 00:31:06.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.232 |      0.342 |                   85 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.324 |                   81 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.296 |      0.224 |                   28 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.795 |      0.108 |                    4 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3376865671641791
[2m[36m(func pid=145749)[0m top5: 0.8722014925373134
[2m[36m(func pid=145749)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=145749)[0m f1_macro: 0.34157900239259065
[2m[36m(func pid=145749)[0m f1_weighted: 0.3786345930200014
[2m[36m(func pid=145749)[0m f1_per_class: [0.192, 0.335, 0.727, 0.387, 0.078, 0.33, 0.434, 0.469, 0.219, 0.245]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m top1: 0.2453358208955224
[2m[36m(func pid=158211)[0m top5: 0.7644589552238806
[2m[36m(func pid=158211)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=158211)[0m f1_macro: 0.2226563138455952
[2m[36m(func pid=158211)[0m f1_weighted: 0.26383306713256943
[2m[36m(func pid=158211)[0m f1_per_class: [0.298, 0.237, 0.222, 0.329, 0.03, 0.327, 0.217, 0.307, 0.036, 0.224]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3460820895522388
[2m[36m(func pid=146482)[0m top5: 0.8563432835820896
[2m[36m(func pid=146482)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=146482)[0m f1_macro: 0.33129860851044385
[2m[36m(func pid=146482)[0m f1_weighted: 0.36931527650512525
[2m[36m(func pid=146482)[0m f1_per_class: [0.324, 0.28, 0.706, 0.439, 0.099, 0.327, 0.419, 0.203, 0.327, 0.19]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m top1: 0.17210820895522388
[2m[36m(func pid=163905)[0m top5: 0.5993470149253731
[2m[36m(func pid=163905)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=163905)[0m f1_macro: 0.1480006833649166
[2m[36m(func pid=163905)[0m f1_weighted: 0.17172531679276717
[2m[36m(func pid=163905)[0m f1_per_class: [0.105, 0.279, 0.298, 0.128, 0.008, 0.163, 0.195, 0.082, 0.0, 0.222]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3469 | Steps: 2 | Val loss: 1.8447 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3211 | Steps: 2 | Val loss: 2.1254 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0164 | Steps: 2 | Val loss: 2.5690 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=145749)[0m top1: 0.3493470149253731
[2m[36m(func pid=145749)[0m top5: 0.878731343283582
[2m[36m(func pid=145749)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=145749)[0m f1_macro: 0.3476140106857244
[2m[36m(func pid=145749)[0m f1_weighted: 0.38610153899246025
[2m[36m(func pid=145749)[0m f1_per_class: [0.229, 0.362, 0.75, 0.412, 0.077, 0.314, 0.429, 0.452, 0.185, 0.267]
[2m[36m(func pid=145749)[0m 
== Status ==
Current time: 2024-01-07 04:40:57 (running for 00:31:12.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.347 |      0.348 |                   86 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.008 |      0.331 |                   82 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.306 |      0.223 |                   29 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.715 |      0.148 |                    5 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6248 | Steps: 2 | Val loss: 2.2492 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=146482)[0m top1: 0.3605410447761194
[2m[36m(func pid=146482)[0m top5: 0.8614738805970149
[2m[36m(func pid=146482)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=146482)[0m f1_macro: 0.3394152726277705
[2m[36m(func pid=146482)[0m f1_weighted: 0.38428963619886664
[2m[36m(func pid=146482)[0m f1_per_class: [0.347, 0.285, 0.727, 0.475, 0.097, 0.303, 0.44, 0.203, 0.326, 0.192]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.25419776119402987
[2m[36m(func pid=158211)[0m top5: 0.7765858208955224
[2m[36m(func pid=158211)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=158211)[0m f1_macro: 0.2285650317075314
[2m[36m(func pid=158211)[0m f1_weighted: 0.27257467572238075
[2m[36m(func pid=158211)[0m f1_per_class: [0.294, 0.238, 0.224, 0.349, 0.031, 0.317, 0.228, 0.313, 0.036, 0.255]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.18796641791044777
[2m[36m(func pid=163905)[0m top5: 0.6273320895522388
[2m[36m(func pid=163905)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=163905)[0m f1_macro: 0.1680391727476815
[2m[36m(func pid=163905)[0m f1_weighted: 0.19375162375677368
[2m[36m(func pid=163905)[0m f1_per_class: [0.132, 0.276, 0.373, 0.148, 0.007, 0.181, 0.233, 0.126, 0.0, 0.204]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2016 | Steps: 2 | Val loss: 1.8246 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0031 | Steps: 2 | Val loss: 2.5201 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2348 | Steps: 2 | Val loss: 2.1195 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 04:41:03 (running for 00:31:17.47)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.202 |      0.347 |                   87 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.016 |      0.339 |                   83 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.321 |      0.229 |                   30 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.625 |      0.168 |                    6 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.353544776119403
[2m[36m(func pid=145749)[0m top5: 0.8819962686567164
[2m[36m(func pid=145749)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=145749)[0m f1_macro: 0.3471345260028858
[2m[36m(func pid=145749)[0m f1_weighted: 0.38791719840439587
[2m[36m(func pid=145749)[0m f1_per_class: [0.26, 0.356, 0.727, 0.428, 0.081, 0.304, 0.424, 0.472, 0.163, 0.256]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5230 | Steps: 2 | Val loss: 2.2115 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=146482)[0m top1: 0.36800373134328357
[2m[36m(func pid=146482)[0m top5: 0.8675373134328358
[2m[36m(func pid=146482)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=146482)[0m f1_macro: 0.3471820761387071
[2m[36m(func pid=146482)[0m f1_weighted: 0.392156260631421
[2m[36m(func pid=146482)[0m f1_per_class: [0.395, 0.275, 0.727, 0.489, 0.092, 0.302, 0.453, 0.221, 0.314, 0.203]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.2630597014925373
[2m[36m(func pid=158211)[0m top5: 0.7770522388059702
[2m[36m(func pid=158211)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=158211)[0m f1_macro: 0.23519238825644617
[2m[36m(func pid=158211)[0m f1_weighted: 0.28159859034957574
[2m[36m(func pid=158211)[0m f1_per_class: [0.305, 0.243, 0.211, 0.36, 0.037, 0.324, 0.24, 0.321, 0.036, 0.275]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.2140858208955224
[2m[36m(func pid=163905)[0m top5: 0.6814365671641791
[2m[36m(func pid=163905)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=163905)[0m f1_macro: 0.18452667296504824
[2m[36m(func pid=163905)[0m f1_weighted: 0.22461210807262288
[2m[36m(func pid=163905)[0m f1_per_class: [0.177, 0.279, 0.324, 0.182, 0.02, 0.206, 0.28, 0.185, 0.0, 0.192]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2217 | Steps: 2 | Val loss: 1.8067 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.4996 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2455 | Steps: 2 | Val loss: 2.1114 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 04:41:08 (running for 00:31:23.00)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.222 |      0.351 |                   88 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.003 |      0.347 |                   84 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.235 |      0.235 |                   31 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.523 |      0.185 |                    7 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3596082089552239
[2m[36m(func pid=145749)[0m top5: 0.8833955223880597
[2m[36m(func pid=145749)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=145749)[0m f1_macro: 0.35070284870511076
[2m[36m(func pid=145749)[0m f1_weighted: 0.3943947179084877
[2m[36m(func pid=145749)[0m f1_per_class: [0.268, 0.344, 0.727, 0.443, 0.081, 0.307, 0.436, 0.474, 0.169, 0.256]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3675373134328358
[2m[36m(func pid=146482)[0m top5: 0.8717350746268657
[2m[36m(func pid=146482)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=146482)[0m f1_macro: 0.3457664623800435
[2m[36m(func pid=146482)[0m f1_weighted: 0.3933685364304135
[2m[36m(func pid=146482)[0m f1_per_class: [0.418, 0.274, 0.727, 0.497, 0.084, 0.287, 0.457, 0.229, 0.294, 0.191]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4620 | Steps: 2 | Val loss: 2.1636 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=158211)[0m top1: 0.2630597014925373
[2m[36m(func pid=158211)[0m top5: 0.7798507462686567
[2m[36m(func pid=158211)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=158211)[0m f1_macro: 0.23548956627202866
[2m[36m(func pid=158211)[0m f1_weighted: 0.28147160568532
[2m[36m(func pid=158211)[0m f1_per_class: [0.309, 0.241, 0.203, 0.365, 0.032, 0.323, 0.237, 0.315, 0.037, 0.294]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.24207089552238806
[2m[36m(func pid=163905)[0m top5: 0.7467350746268657
[2m[36m(func pid=163905)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=163905)[0m f1_macro: 0.214714417104883
[2m[36m(func pid=163905)[0m f1_weighted: 0.2576363644061373
[2m[36m(func pid=163905)[0m f1_per_class: [0.207, 0.272, 0.324, 0.242, 0.026, 0.257, 0.302, 0.247, 0.0, 0.269]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.1777 | Steps: 2 | Val loss: 1.8172 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0080 | Steps: 2 | Val loss: 2.4948 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2197 | Steps: 2 | Val loss: 2.1064 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:41:14 (running for 00:31:28.38)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.178 |      0.347 |                   89 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.002 |      0.346 |                   85 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.246 |      0.235 |                   32 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.462 |      0.215 |                    8 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.35867537313432835
[2m[36m(func pid=145749)[0m top5: 0.8805970149253731
[2m[36m(func pid=145749)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=145749)[0m f1_macro: 0.3468623525285106
[2m[36m(func pid=145749)[0m f1_weighted: 0.3950519543033046
[2m[36m(func pid=145749)[0m f1_per_class: [0.264, 0.346, 0.727, 0.447, 0.075, 0.303, 0.44, 0.454, 0.168, 0.244]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.36473880597014924
[2m[36m(func pid=146482)[0m top5: 0.8768656716417911
[2m[36m(func pid=146482)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=146482)[0m f1_macro: 0.34663249017232417
[2m[36m(func pid=146482)[0m f1_weighted: 0.39209332244789324
[2m[36m(func pid=146482)[0m f1_per_class: [0.437, 0.26, 0.727, 0.5, 0.085, 0.284, 0.454, 0.256, 0.275, 0.188]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3173 | Steps: 2 | Val loss: 2.1220 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=158211)[0m top1: 0.2621268656716418
[2m[36m(func pid=158211)[0m top5: 0.7863805970149254
[2m[36m(func pid=158211)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=158211)[0m f1_macro: 0.2316683375404452
[2m[36m(func pid=158211)[0m f1_weighted: 0.27991255866620907
[2m[36m(func pid=158211)[0m f1_per_class: [0.302, 0.239, 0.195, 0.373, 0.032, 0.311, 0.23, 0.32, 0.037, 0.278]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.2560634328358209
[2m[36m(func pid=163905)[0m top5: 0.7831156716417911
[2m[36m(func pid=163905)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=163905)[0m f1_macro: 0.22380328482606662
[2m[36m(func pid=163905)[0m f1_weighted: 0.2727976803506355
[2m[36m(func pid=163905)[0m f1_per_class: [0.26, 0.256, 0.273, 0.295, 0.033, 0.283, 0.298, 0.259, 0.0, 0.282]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.1818 | Steps: 2 | Val loss: 1.8327 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0020 | Steps: 2 | Val loss: 2.4829 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1825 | Steps: 2 | Val loss: 2.1024 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 04:41:19 (running for 00:31:33.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.178 |      0.347 |                   89 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.002 |      0.348 |                   87 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.22  |      0.232 |                   33 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.317 |      0.224 |                    9 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3675373134328358
[2m[36m(func pid=145749)[0m top5: 0.8777985074626866
[2m[36m(func pid=145749)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=145749)[0m f1_macro: 0.3486852208940696
[2m[36m(func pid=145749)[0m f1_weighted: 0.4044839614752087
[2m[36m(func pid=145749)[0m f1_per_class: [0.258, 0.349, 0.686, 0.448, 0.079, 0.321, 0.462, 0.452, 0.184, 0.248]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.36800373134328357
[2m[36m(func pid=146482)[0m top5: 0.8801305970149254
[2m[36m(func pid=146482)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=146482)[0m f1_macro: 0.34808109439391177
[2m[36m(func pid=146482)[0m f1_weighted: 0.39680027232856074
[2m[36m(func pid=146482)[0m f1_per_class: [0.431, 0.263, 0.706, 0.505, 0.083, 0.276, 0.461, 0.286, 0.279, 0.191]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2351 | Steps: 2 | Val loss: 2.0870 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=158211)[0m top1: 0.2681902985074627
[2m[36m(func pid=158211)[0m top5: 0.7826492537313433
[2m[36m(func pid=158211)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=158211)[0m f1_macro: 0.23445630063825593
[2m[36m(func pid=158211)[0m f1_weighted: 0.28689503847808145
[2m[36m(func pid=158211)[0m f1_per_class: [0.29, 0.239, 0.192, 0.387, 0.036, 0.327, 0.235, 0.33, 0.019, 0.291]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.2789179104477612
[2m[36m(func pid=163905)[0m top5: 0.7961753731343284
[2m[36m(func pid=163905)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=163905)[0m f1_macro: 0.2335554606577646
[2m[36m(func pid=163905)[0m f1_weighted: 0.30016357241355807
[2m[36m(func pid=163905)[0m f1_per_class: [0.27, 0.227, 0.212, 0.362, 0.026, 0.291, 0.33, 0.307, 0.0, 0.309]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0068 | Steps: 2 | Val loss: 2.4832 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2472 | Steps: 2 | Val loss: 1.8388 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.1297 | Steps: 2 | Val loss: 2.0926 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 04:41:24 (running for 00:31:39.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.247 |      0.352 |                   91 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.002 |      0.348 |                   87 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.182 |      0.234 |                   34 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.235 |      0.234 |                   10 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3666044776119403
[2m[36m(func pid=145749)[0m top5: 0.8796641791044776
[2m[36m(func pid=145749)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=145749)[0m f1_macro: 0.3516326487893122
[2m[36m(func pid=145749)[0m f1_weighted: 0.4034493928754405
[2m[36m(func pid=145749)[0m f1_per_class: [0.251, 0.355, 0.706, 0.442, 0.079, 0.318, 0.459, 0.458, 0.193, 0.255]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3712686567164179
[2m[36m(func pid=146482)[0m top5: 0.8843283582089553
[2m[36m(func pid=146482)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=146482)[0m f1_macro: 0.3565017920673152
[2m[36m(func pid=146482)[0m f1_weighted: 0.4028708704669374
[2m[36m(func pid=146482)[0m f1_per_class: [0.434, 0.266, 0.706, 0.502, 0.075, 0.28, 0.467, 0.352, 0.288, 0.196]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0877 | Steps: 2 | Val loss: 2.0613 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=158211)[0m top1: 0.271455223880597
[2m[36m(func pid=158211)[0m top5: 0.7905783582089553
[2m[36m(func pid=158211)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=158211)[0m f1_macro: 0.23757230558835268
[2m[36m(func pid=158211)[0m f1_weighted: 0.288560168219665
[2m[36m(func pid=158211)[0m f1_per_class: [0.322, 0.238, 0.198, 0.389, 0.032, 0.324, 0.236, 0.341, 0.02, 0.276]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.2933768656716418
[2m[36m(func pid=163905)[0m top5: 0.8031716417910447
[2m[36m(func pid=163905)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=163905)[0m f1_macro: 0.24313422832387968
[2m[36m(func pid=163905)[0m f1_weighted: 0.3132550266645397
[2m[36m(func pid=163905)[0m f1_per_class: [0.293, 0.234, 0.189, 0.384, 0.039, 0.279, 0.345, 0.323, 0.04, 0.304]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2095 | Steps: 2 | Val loss: 1.8409 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0041 | Steps: 2 | Val loss: 2.4876 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.0851 | Steps: 2 | Val loss: 2.0846 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 04:41:30 (running for 00:31:44.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.209 |      0.343 |                   92 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.007 |      0.357 |                   88 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.13  |      0.238 |                   35 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  2.088 |      0.243 |                   11 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3628731343283582
[2m[36m(func pid=145749)[0m top5: 0.8768656716417911
[2m[36m(func pid=145749)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=145749)[0m f1_macro: 0.3432993721868088
[2m[36m(func pid=145749)[0m f1_weighted: 0.39994167823768384
[2m[36m(func pid=145749)[0m f1_per_class: [0.241, 0.354, 0.649, 0.434, 0.08, 0.34, 0.453, 0.441, 0.183, 0.258]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.36847014925373134
[2m[36m(func pid=146482)[0m top5: 0.8833955223880597
[2m[36m(func pid=146482)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=146482)[0m f1_macro: 0.3556771756191782
[2m[36m(func pid=146482)[0m f1_weighted: 0.4013487245526291
[2m[36m(func pid=146482)[0m f1_per_class: [0.423, 0.266, 0.686, 0.497, 0.071, 0.269, 0.465, 0.381, 0.3, 0.199]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9693 | Steps: 2 | Val loss: 2.0310 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=158211)[0m top1: 0.27472014925373134
[2m[36m(func pid=158211)[0m top5: 0.7933768656716418
[2m[36m(func pid=158211)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=158211)[0m f1_macro: 0.23768413811691316
[2m[36m(func pid=158211)[0m f1_weighted: 0.29191841227499504
[2m[36m(func pid=158211)[0m f1_per_class: [0.314, 0.239, 0.2, 0.393, 0.032, 0.323, 0.244, 0.343, 0.02, 0.268]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3180970149253731
[2m[36m(func pid=163905)[0m top5: 0.8138992537313433
[2m[36m(func pid=163905)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=163905)[0m f1_macro: 0.2550315917251504
[2m[36m(func pid=163905)[0m f1_weighted: 0.3374317896436236
[2m[36m(func pid=163905)[0m f1_per_class: [0.31, 0.231, 0.198, 0.438, 0.061, 0.278, 0.377, 0.314, 0.062, 0.281]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.1732 | Steps: 2 | Val loss: 1.8323 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0037 | Steps: 2 | Val loss: 2.4856 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.0373 | Steps: 2 | Val loss: 2.0784 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 04:41:35 (running for 00:31:50.13)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.209 |      0.343 |                   92 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.353 |                   90 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.085 |      0.238 |                   36 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.969 |      0.255 |                   12 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=146482)[0m top1: 0.36427238805970147
[2m[36m(func pid=146482)[0m top5: 0.8815298507462687
[2m[36m(func pid=146482)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=146482)[0m f1_macro: 0.3526023699960777
[2m[36m(func pid=146482)[0m f1_weighted: 0.39653480044167877
[2m[36m(func pid=146482)[0m f1_per_class: [0.405, 0.263, 0.686, 0.493, 0.071, 0.263, 0.456, 0.395, 0.295, 0.2]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.8760 | Steps: 2 | Val loss: 2.0049 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=145749)[0m top1: 0.36240671641791045
[2m[36m(func pid=145749)[0m top5: 0.8777985074626866
[2m[36m(func pid=145749)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=145749)[0m f1_macro: 0.346273580673413
[2m[36m(func pid=145749)[0m f1_weighted: 0.3994574915648399
[2m[36m(func pid=145749)[0m f1_per_class: [0.237, 0.351, 0.667, 0.429, 0.078, 0.325, 0.458, 0.464, 0.19, 0.263]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3353544776119403
[2m[36m(func pid=163905)[0m top5: 0.8213619402985075
[2m[36m(func pid=163905)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=163905)[0m f1_macro: 0.262150083419982
[2m[36m(func pid=163905)[0m f1_weighted: 0.35633309903377436
[2m[36m(func pid=163905)[0m f1_per_class: [0.302, 0.233, 0.198, 0.455, 0.064, 0.281, 0.415, 0.35, 0.067, 0.256]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.28171641791044777
[2m[36m(func pid=158211)[0m top5: 0.7957089552238806
[2m[36m(func pid=158211)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=158211)[0m f1_macro: 0.24265722170280846
[2m[36m(func pid=158211)[0m f1_weighted: 0.29914376071785054
[2m[36m(func pid=158211)[0m f1_per_class: [0.31, 0.246, 0.207, 0.395, 0.038, 0.325, 0.26, 0.353, 0.021, 0.273]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2235 | Steps: 2 | Val loss: 1.8240 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0077 | Steps: 2 | Val loss: 2.4903 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.7499 | Steps: 2 | Val loss: 1.9887 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:41:41 (running for 00:31:55.64)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.173 |      0.346 |                   93 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.008 |      0.36  |                   91 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.037 |      0.243 |                   37 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.876 |      0.262 |                   13 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3670708955223881
[2m[36m(func pid=145749)[0m top5: 0.8810634328358209
[2m[36m(func pid=145749)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=145749)[0m f1_macro: 0.35421146896782374
[2m[36m(func pid=145749)[0m f1_weighted: 0.40346792275748583
[2m[36m(func pid=145749)[0m f1_per_class: [0.262, 0.343, 0.727, 0.444, 0.08, 0.331, 0.458, 0.465, 0.184, 0.248]
[2m[36m(func pid=146482)[0m top1: 0.36847014925373134
[2m[36m(func pid=146482)[0m top5: 0.8796641791044776
[2m[36m(func pid=146482)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=146482)[0m f1_macro: 0.3604499406272318
[2m[36m(func pid=146482)[0m f1_weighted: 0.40011361746595225
[2m[36m(func pid=146482)[0m f1_per_class: [0.415, 0.269, 0.686, 0.492, 0.076, 0.239, 0.465, 0.428, 0.305, 0.23]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.0462 | Steps: 2 | Val loss: 2.0732 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=163905)[0m top1: 0.33955223880597013
[2m[36m(func pid=163905)[0m top5: 0.8250932835820896
[2m[36m(func pid=163905)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=163905)[0m f1_macro: 0.26541099408435226
[2m[36m(func pid=163905)[0m f1_weighted: 0.36252616493217305
[2m[36m(func pid=163905)[0m f1_per_class: [0.292, 0.245, 0.176, 0.462, 0.079, 0.264, 0.426, 0.352, 0.104, 0.254]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.28451492537313433
[2m[36m(func pid=158211)[0m top5: 0.7985074626865671
[2m[36m(func pid=158211)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=158211)[0m f1_macro: 0.24276803707346634
[2m[36m(func pid=158211)[0m f1_weighted: 0.3030947371994747
[2m[36m(func pid=158211)[0m f1_per_class: [0.288, 0.245, 0.2, 0.399, 0.038, 0.314, 0.274, 0.359, 0.021, 0.29]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0021 | Steps: 2 | Val loss: 2.4924 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.1623 | Steps: 2 | Val loss: 1.8235 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6033 | Steps: 2 | Val loss: 1.9671 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:41:46 (running for 00:32:01.12)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.224 |      0.354 |                   94 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.002 |      0.365 |                   92 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.046 |      0.243 |                   38 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.75  |      0.265 |                   14 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3694029850746269
[2m[36m(func pid=145749)[0m top5: 0.8847947761194029
[2m[36m(func pid=145749)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=145749)[0m f1_macro: 0.3557068156694986
[2m[36m(func pid=145749)[0m f1_weighted: 0.4078088447152787
[2m[36m(func pid=145749)[0m f1_per_class: [0.254, 0.348, 0.727, 0.44, 0.078, 0.327, 0.473, 0.481, 0.189, 0.241]
[2m[36m(func pid=146482)[0m top1: 0.3694029850746269
[2m[36m(func pid=146482)[0m top5: 0.8801305970149254
[2m[36m(func pid=146482)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=146482)[0m f1_macro: 0.364650107388189
[2m[36m(func pid=146482)[0m f1_weighted: 0.40166644174559285
[2m[36m(func pid=146482)[0m f1_per_class: [0.415, 0.268, 0.686, 0.487, 0.073, 0.237, 0.469, 0.463, 0.306, 0.244]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.0113 | Steps: 2 | Val loss: 2.0712 | Batch size: 32 | lr: 0.0001 | Duration: 3.31s
[2m[36m(func pid=163905)[0m top1: 0.34328358208955223
[2m[36m(func pid=163905)[0m top5: 0.8297574626865671
[2m[36m(func pid=163905)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=163905)[0m f1_macro: 0.26078999271095943
[2m[36m(func pid=163905)[0m f1_weighted: 0.3688719799354272
[2m[36m(func pid=163905)[0m f1_per_class: [0.281, 0.264, 0.162, 0.465, 0.074, 0.238, 0.446, 0.35, 0.099, 0.229]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0054 | Steps: 2 | Val loss: 2.5322 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.1508 | Steps: 2 | Val loss: 1.8196 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=158211)[0m top1: 0.28451492537313433
[2m[36m(func pid=158211)[0m top5: 0.8013059701492538
[2m[36m(func pid=158211)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=158211)[0m f1_macro: 0.24054159528017588
[2m[36m(func pid=158211)[0m f1_weighted: 0.30326205755703667
[2m[36m(func pid=158211)[0m f1_per_class: [0.284, 0.248, 0.186, 0.401, 0.039, 0.311, 0.272, 0.359, 0.021, 0.284]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5253 | Steps: 2 | Val loss: 1.9362 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:41:52 (running for 00:32:06.66)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.162 |      0.356 |                   95 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.005 |      0.357 |                   93 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  2.011 |      0.241 |                   39 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.603 |      0.261 |                   15 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3694029850746269
[2m[36m(func pid=145749)[0m top5: 0.882929104477612
[2m[36m(func pid=145749)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=145749)[0m f1_macro: 0.353083066093071
[2m[36m(func pid=145749)[0m f1_weighted: 0.4082795453312073
[2m[36m(func pid=145749)[0m f1_per_class: [0.25, 0.348, 0.706, 0.443, 0.084, 0.326, 0.475, 0.466, 0.196, 0.238]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3591417910447761
[2m[36m(func pid=146482)[0m top5: 0.8801305970149254
[2m[36m(func pid=146482)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=146482)[0m f1_macro: 0.35688297316644507
[2m[36m(func pid=146482)[0m f1_weighted: 0.3914213126051189
[2m[36m(func pid=146482)[0m f1_per_class: [0.4, 0.272, 0.686, 0.476, 0.071, 0.238, 0.445, 0.461, 0.304, 0.217]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.9321 | Steps: 2 | Val loss: 2.0644 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=163905)[0m top1: 0.3493470149253731
[2m[36m(func pid=163905)[0m top5: 0.8292910447761194
[2m[36m(func pid=163905)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=163905)[0m f1_macro: 0.2607481873514047
[2m[36m(func pid=163905)[0m f1_weighted: 0.3754812268527367
[2m[36m(func pid=163905)[0m f1_per_class: [0.28, 0.251, 0.163, 0.479, 0.082, 0.262, 0.46, 0.326, 0.093, 0.212]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.5444 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2924 | Steps: 2 | Val loss: 1.8120 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=158211)[0m top1: 0.29151119402985076
[2m[36m(func pid=158211)[0m top5: 0.8017723880597015
[2m[36m(func pid=158211)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=158211)[0m f1_macro: 0.2452914776215363
[2m[36m(func pid=158211)[0m f1_weighted: 0.31049931651297774
[2m[36m(func pid=158211)[0m f1_per_class: [0.296, 0.251, 0.19, 0.411, 0.045, 0.311, 0.283, 0.365, 0.022, 0.278]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.4437 | Steps: 2 | Val loss: 1.8986 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:41:57 (running for 00:32:12.00)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.151 |      0.353 |                   96 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.003 |      0.357 |                   94 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.932 |      0.245 |                   40 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.525 |      0.261 |                   16 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=146482)[0m top1: 0.36100746268656714
[2m[36m(func pid=146482)[0m top5: 0.8768656716417911
[2m[36m(func pid=146482)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=146482)[0m f1_macro: 0.35737557415993704
[2m[36m(func pid=146482)[0m f1_weighted: 0.39371998564486876
[2m[36m(func pid=146482)[0m f1_per_class: [0.38, 0.275, 0.686, 0.472, 0.071, 0.227, 0.459, 0.457, 0.31, 0.237]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.36986940298507465
[2m[36m(func pid=145749)[0m top5: 0.8847947761194029
[2m[36m(func pid=145749)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=145749)[0m f1_macro: 0.3519705325240065
[2m[36m(func pid=145749)[0m f1_weighted: 0.4096912409741157
[2m[36m(func pid=145749)[0m f1_per_class: [0.25, 0.348, 0.706, 0.446, 0.082, 0.326, 0.48, 0.449, 0.195, 0.238]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9370 | Steps: 2 | Val loss: 2.0577 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=163905)[0m top1: 0.363339552238806
[2m[36m(func pid=163905)[0m top5: 0.8381529850746269
[2m[36m(func pid=163905)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=163905)[0m f1_macro: 0.26987073094226816
[2m[36m(func pid=163905)[0m f1_weighted: 0.3876316943561848
[2m[36m(func pid=163905)[0m f1_per_class: [0.316, 0.245, 0.174, 0.501, 0.088, 0.255, 0.477, 0.361, 0.093, 0.188]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0104 | Steps: 2 | Val loss: 2.5473 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2504 | Steps: 2 | Val loss: 1.8325 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=158211)[0m top1: 0.29197761194029853
[2m[36m(func pid=158211)[0m top5: 0.8050373134328358
[2m[36m(func pid=158211)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=158211)[0m f1_macro: 0.24593009048722161
[2m[36m(func pid=158211)[0m f1_weighted: 0.3122216285936159
[2m[36m(func pid=158211)[0m f1_per_class: [0.287, 0.249, 0.202, 0.41, 0.045, 0.308, 0.292, 0.365, 0.042, 0.26]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.2212 | Steps: 2 | Val loss: 1.8563 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=146482)[0m top1: 0.36613805970149255
[2m[36m(func pid=146482)[0m top5: 0.8763992537313433
[2m[36m(func pid=146482)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=146482)[0m f1_macro: 0.3611650304895153
[2m[36m(func pid=146482)[0m f1_weighted: 0.39932676065117606
[2m[36m(func pid=146482)[0m f1_per_class: [0.384, 0.282, 0.686, 0.471, 0.072, 0.223, 0.473, 0.466, 0.323, 0.233]
[2m[36m(func pid=146482)[0m 
== Status ==
Current time: 2024-01-07 04:42:02 (running for 00:32:17.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.292 |      0.352 |                   97 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.01  |      0.361 |                   95 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.937 |      0.246 |                   41 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.444 |      0.27  |                   17 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145749)[0m top1: 0.3670708955223881
[2m[36m(func pid=145749)[0m top5: 0.8805970149253731
[2m[36m(func pid=145749)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=145749)[0m f1_macro: 0.3484525819699205
[2m[36m(func pid=145749)[0m f1_weighted: 0.4094359118846587
[2m[36m(func pid=145749)[0m f1_per_class: [0.245, 0.348, 0.706, 0.448, 0.078, 0.303, 0.486, 0.442, 0.205, 0.222]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8637 | Steps: 2 | Val loss: 2.0502 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=163905)[0m top1: 0.373134328358209
[2m[36m(func pid=163905)[0m top5: 0.8404850746268657
[2m[36m(func pid=163905)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=163905)[0m f1_macro: 0.2821296112721546
[2m[36m(func pid=163905)[0m f1_weighted: 0.3966550045579661
[2m[36m(func pid=163905)[0m f1_per_class: [0.322, 0.242, 0.212, 0.511, 0.081, 0.301, 0.473, 0.402, 0.098, 0.178]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0339 | Steps: 2 | Val loss: 2.6235 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.1600 | Steps: 2 | Val loss: 1.8764 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=158211)[0m top1: 0.29197761194029853
[2m[36m(func pid=158211)[0m top5: 0.8083022388059702
[2m[36m(func pid=158211)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=158211)[0m f1_macro: 0.24557319431033114
[2m[36m(func pid=158211)[0m f1_weighted: 0.3114382669785972
[2m[36m(func pid=158211)[0m f1_per_class: [0.29, 0.25, 0.209, 0.412, 0.046, 0.304, 0.29, 0.358, 0.041, 0.257]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.0970 | Steps: 2 | Val loss: 1.8083 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:42:08 (running for 00:32:22.44)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.25  |      0.348 |                   98 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.034 |      0.356 |                   96 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.864 |      0.246 |                   42 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.221 |      0.282 |                   18 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=146482)[0m top1: 0.3530783582089552
[2m[36m(func pid=146482)[0m top5: 0.8731343283582089
[2m[36m(func pid=146482)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=146482)[0m f1_macro: 0.3556913109117939
[2m[36m(func pid=146482)[0m f1_weighted: 0.38608893105396164
[2m[36m(func pid=146482)[0m f1_per_class: [0.354, 0.288, 0.706, 0.442, 0.07, 0.224, 0.454, 0.464, 0.329, 0.226]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=145749)[0m top1: 0.3516791044776119
[2m[36m(func pid=145749)[0m top5: 0.8726679104477612
[2m[36m(func pid=145749)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=145749)[0m f1_macro: 0.31995964001972016
[2m[36m(func pid=145749)[0m f1_weighted: 0.39498694549984614
[2m[36m(func pid=145749)[0m f1_per_class: [0.229, 0.339, 0.632, 0.443, 0.071, 0.274, 0.489, 0.307, 0.202, 0.213]
[2m[36m(func pid=145749)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3787313432835821
[2m[36m(func pid=163905)[0m top5: 0.8484141791044776
[2m[36m(func pid=163905)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=163905)[0m f1_macro: 0.29187495426598264
[2m[36m(func pid=163905)[0m f1_weighted: 0.39594447133456845
[2m[36m(func pid=163905)[0m f1_per_class: [0.34, 0.243, 0.267, 0.521, 0.088, 0.33, 0.447, 0.413, 0.082, 0.188]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.8234 | Steps: 2 | Val loss: 2.0431 | Batch size: 32 | lr: 0.0001 | Duration: 3.27s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.7047 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145749)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.1870 | Steps: 2 | Val loss: 1.9021 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:42:13 (running for 00:32:27.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00013 | RUNNING    | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.16  |      0.32  |                   99 |
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.034 |      0.356 |                   96 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.823 |      0.249 |                   43 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  1.097 |      0.292 |                   19 |
| train_35a0b_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m top1: 0.29757462686567165
[2m[36m(func pid=158211)[0m top5: 0.8097014925373134
[2m[36m(func pid=158211)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=158211)[0m f1_macro: 0.24867899222986786
[2m[36m(func pid=158211)[0m f1_weighted: 0.31802410587952906
[2m[36m(func pid=158211)[0m f1_per_class: [0.286, 0.251, 0.209, 0.417, 0.047, 0.301, 0.304, 0.375, 0.04, 0.257]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=146482)[0m top1: 0.33722014925373134
[2m[36m(func pid=146482)[0m top5: 0.8652052238805971
[2m[36m(func pid=146482)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=146482)[0m f1_macro: 0.3476758198756114
[2m[36m(func pid=146482)[0m f1_weighted: 0.3702935611074476
[2m[36m(func pid=146482)[0m f1_per_class: [0.312, 0.291, 0.727, 0.378, 0.063, 0.217, 0.464, 0.464, 0.321, 0.239]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9929 | Steps: 2 | Val loss: 1.7696 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=145749)[0m top1: 0.3512126865671642
[2m[36m(func pid=145749)[0m top5: 0.8656716417910447
[2m[36m(func pid=145749)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=145749)[0m f1_macro: 0.31158567140593973
[2m[36m(func pid=145749)[0m f1_weighted: 0.39347043106820856
[2m[36m(func pid=145749)[0m f1_per_class: [0.226, 0.349, 0.6, 0.448, 0.069, 0.268, 0.488, 0.255, 0.196, 0.219]
[2m[36m(func pid=163905)[0m top1: 0.3843283582089552
[2m[36m(func pid=163905)[0m top5: 0.8610074626865671
[2m[36m(func pid=163905)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=163905)[0m f1_macro: 0.29982657361201254
[2m[36m(func pid=163905)[0m f1_weighted: 0.39619108355575655
[2m[36m(func pid=163905)[0m f1_per_class: [0.354, 0.255, 0.308, 0.527, 0.108, 0.338, 0.428, 0.43, 0.061, 0.189]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0058 | Steps: 2 | Val loss: 2.7497 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.8009 | Steps: 2 | Val loss: 2.0386 | Batch size: 32 | lr: 0.0001 | Duration: 3.33s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9364 | Steps: 2 | Val loss: 1.7396 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=146482)[0m top1: 0.3162313432835821
[2m[36m(func pid=146482)[0m top5: 0.8619402985074627
[2m[36m(func pid=146482)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=146482)[0m f1_macro: 0.34134591900276257
[2m[36m(func pid=146482)[0m f1_weighted: 0.3482910073211731
[2m[36m(func pid=146482)[0m f1_per_class: [0.293, 0.277, 0.8, 0.311, 0.059, 0.226, 0.463, 0.452, 0.296, 0.236]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=158211)[0m top1: 0.2989738805970149
[2m[36m(func pid=158211)[0m top5: 0.8092350746268657
[2m[36m(func pid=158211)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=158211)[0m f1_macro: 0.25168008785961005
[2m[36m(func pid=158211)[0m f1_weighted: 0.32063218625967715
[2m[36m(func pid=158211)[0m f1_per_class: [0.28, 0.256, 0.203, 0.423, 0.047, 0.313, 0.297, 0.369, 0.081, 0.248]
[2m[36m(func pid=163905)[0m top1: 0.39132462686567165
[2m[36m(func pid=163905)[0m top5: 0.8684701492537313
[2m[36m(func pid=163905)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=163905)[0m f1_macro: 0.3257623506522439
[2m[36m(func pid=163905)[0m f1_weighted: 0.4002173482254655
[2m[36m(func pid=163905)[0m f1_per_class: [0.384, 0.262, 0.444, 0.532, 0.124, 0.367, 0.411, 0.434, 0.1, 0.199]
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0107 | Steps: 2 | Val loss: 2.7645 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:42:18 (running for 00:32:33.15)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.006 |      0.341 |                   98 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.823 |      0.249 |                   43 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.993 |      0.3   |                   20 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=168639)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=168639)[0m Configuration completed!
[2m[36m(func pid=168639)[0m New optimizer parameters:
[2m[36m(func pid=168639)[0m SGD (
[2m[36m(func pid=168639)[0m Parameter Group 0
[2m[36m(func pid=168639)[0m     dampening: 0
[2m[36m(func pid=168639)[0m     differentiable: False
[2m[36m(func pid=168639)[0m     foreach: None
[2m[36m(func pid=168639)[0m     lr: 0.01
[2m[36m(func pid=168639)[0m     maximize: False
[2m[36m(func pid=168639)[0m     momentum: 0.99
[2m[36m(func pid=168639)[0m     nesterov: False
[2m[36m(func pid=168639)[0m     weight_decay: 1e-05
[2m[36m(func pid=168639)[0m )
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:42:24 (running for 00:32:38.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.011 |      0.343 |                   99 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.801 |      0.252 |                   44 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.936 |      0.326 |                   21 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=146482)[0m top1: 0.3125
[2m[36m(func pid=146482)[0m top5: 0.8647388059701493
[2m[36m(func pid=146482)[0m f1_micro: 0.3125
[2m[36m(func pid=146482)[0m f1_macro: 0.34314298127414317
[2m[36m(func pid=146482)[0m f1_weighted: 0.3417834542035075
[2m[36m(func pid=146482)[0m f1_per_class: [0.311, 0.279, 0.828, 0.298, 0.059, 0.212, 0.456, 0.451, 0.301, 0.238]
[2m[36m(func pid=146482)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.9612 | Steps: 2 | Val loss: 1.7207 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.9183 | Steps: 2 | Val loss: 2.0207 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9877 | Steps: 2 | Val loss: 2.3190 | Batch size: 32 | lr: 0.01 | Duration: 4.91s
[2m[36m(func pid=146482)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0036 | Steps: 2 | Val loss: 2.7573 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=163905)[0m top1: 0.39132462686567165
[2m[36m(func pid=163905)[0m top5: 0.875
[2m[36m(func pid=163905)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=163905)[0m f1_macro: 0.33633498689438485
[2m[36m(func pid=163905)[0m f1_weighted: 0.3986202569387481
[2m[36m(func pid=163905)[0m f1_per_class: [0.405, 0.281, 0.522, 0.531, 0.133, 0.365, 0.395, 0.424, 0.101, 0.207]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.31203358208955223
[2m[36m(func pid=158211)[0m top5: 0.8134328358208955
[2m[36m(func pid=158211)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=158211)[0m f1_macro: 0.261713259534504
[2m[36m(func pid=158211)[0m f1_weighted: 0.331905704066914
[2m[36m(func pid=158211)[0m f1_per_class: [0.29, 0.255, 0.22, 0.443, 0.049, 0.348, 0.299, 0.395, 0.061, 0.257]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:42:29 (running for 00:32:43.97)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00014 | RUNNING    | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.011 |      0.343 |                   99 |
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.918 |      0.262 |                   45 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.961 |      0.336 |                   22 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  2.988 |      0.108 |                    1 |
| train_35a0b_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.15858208955223882
[2m[36m(func pid=168639)[0m top5: 0.5177238805970149
[2m[36m(func pid=168639)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=168639)[0m f1_macro: 0.10770228742057689
[2m[36m(func pid=168639)[0m f1_weighted: 0.10136844433546001
[2m[36m(func pid=168639)[0m f1_per_class: [0.225, 0.268, 0.038, 0.07, 0.0, 0.236, 0.0, 0.013, 0.025, 0.203]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=146482)[0m top1: 0.3199626865671642
[2m[36m(func pid=146482)[0m top5: 0.8666044776119403
[2m[36m(func pid=146482)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=146482)[0m f1_macro: 0.3452330250970318
[2m[36m(func pid=146482)[0m f1_weighted: 0.34956889692627613
[2m[36m(func pid=146482)[0m f1_per_class: [0.319, 0.287, 0.828, 0.311, 0.062, 0.219, 0.465, 0.443, 0.289, 0.229]
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8040 | Steps: 2 | Val loss: 1.7044 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7363 | Steps: 2 | Val loss: 2.0139 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7720 | Steps: 2 | Val loss: 2.2753 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=163905)[0m top1: 0.39552238805970147
[2m[36m(func pid=163905)[0m top5: 0.8819962686567164
[2m[36m(func pid=163905)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=163905)[0m f1_macro: 0.346781453121957
[2m[36m(func pid=163905)[0m f1_weighted: 0.40255359306577254
[2m[36m(func pid=163905)[0m f1_per_class: [0.402, 0.285, 0.6, 0.536, 0.137, 0.379, 0.395, 0.424, 0.097, 0.213]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.31763059701492535
[2m[36m(func pid=158211)[0m top5: 0.8134328358208955
[2m[36m(func pid=158211)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=158211)[0m f1_macro: 0.26773662228430417
[2m[36m(func pid=158211)[0m f1_weighted: 0.3383944652576696
[2m[36m(func pid=158211)[0m f1_per_class: [0.284, 0.255, 0.226, 0.456, 0.049, 0.352, 0.304, 0.395, 0.1, 0.257]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=168639)[0m top1: 0.15578358208955223
[2m[36m(func pid=168639)[0m top5: 0.6044776119402985
[2m[36m(func pid=168639)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=168639)[0m f1_macro: 0.11781046496311394
[2m[36m(func pid=168639)[0m f1_weighted: 0.17106903979442545
[2m[36m(func pid=168639)[0m f1_per_class: [0.067, 0.268, 0.0, 0.059, 0.0, 0.013, 0.287, 0.308, 0.011, 0.165]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6945 | Steps: 2 | Val loss: 1.6957 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7320 | Steps: 2 | Val loss: 2.0026 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3983 | Steps: 2 | Val loss: 2.2120 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 04:42:34 (running for 00:32:49.33)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.736 |      0.268 |                   46 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.804 |      0.347 |                   23 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  2.772 |      0.118 |                    2 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=169477)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=169477)[0m Configuration completed!
[2m[36m(func pid=169477)[0m New optimizer parameters:
[2m[36m(func pid=169477)[0m SGD (
[2m[36m(func pid=169477)[0m Parameter Group 0
[2m[36m(func pid=169477)[0m     dampening: 0
[2m[36m(func pid=169477)[0m     differentiable: False
[2m[36m(func pid=169477)[0m     foreach: None
[2m[36m(func pid=169477)[0m     lr: 0.1
[2m[36m(func pid=169477)[0m     maximize: False
[2m[36m(func pid=169477)[0m     momentum: 0.99
[2m[36m(func pid=169477)[0m     nesterov: False
[2m[36m(func pid=169477)[0m     weight_decay: 1e-05
[2m[36m(func pid=169477)[0m )
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.39225746268656714
[2m[36m(func pid=163905)[0m top5: 0.8847947761194029
[2m[36m(func pid=163905)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=163905)[0m f1_macro: 0.3507047416826864
[2m[36m(func pid=163905)[0m f1_weighted: 0.4009630037549571
[2m[36m(func pid=163905)[0m f1_per_class: [0.398, 0.287, 0.6, 0.529, 0.133, 0.388, 0.385, 0.429, 0.144, 0.214]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.32136194029850745
[2m[36m(func pid=158211)[0m top5: 0.816231343283582
[2m[36m(func pid=158211)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=158211)[0m f1_macro: 0.2662454050936599
[2m[36m(func pid=158211)[0m f1_weighted: 0.3419747826602973
[2m[36m(func pid=158211)[0m f1_per_class: [0.283, 0.261, 0.231, 0.459, 0.052, 0.345, 0.316, 0.387, 0.08, 0.248]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:42:40 (running for 00:32:54.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.732 |      0.266 |                   47 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.695 |      0.351 |                   24 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  2.398 |      0.133 |                    3 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.19309701492537312
[2m[36m(func pid=168639)[0m top5: 0.6669776119402985
[2m[36m(func pid=168639)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=168639)[0m f1_macro: 0.13342588770463606
[2m[36m(func pid=168639)[0m f1_weighted: 0.21209019913615995
[2m[36m(func pid=168639)[0m f1_per_class: [0.103, 0.139, 0.017, 0.213, 0.085, 0.008, 0.363, 0.244, 0.027, 0.135]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5655 | Steps: 2 | Val loss: 1.7043 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.6821 | Steps: 2 | Val loss: 1.9946 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9442 | Steps: 2 | Val loss: 2.3659 | Batch size: 32 | lr: 0.1 | Duration: 4.53s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0281 | Steps: 2 | Val loss: 2.1189 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=163905)[0m top1: 0.3908582089552239
[2m[36m(func pid=163905)[0m top5: 0.8871268656716418
[2m[36m(func pid=163905)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=163905)[0m f1_macro: 0.34829924438530757
[2m[36m(func pid=163905)[0m f1_weighted: 0.40207869747909913
[2m[36m(func pid=163905)[0m f1_per_class: [0.388, 0.279, 0.545, 0.531, 0.13, 0.398, 0.387, 0.421, 0.189, 0.216]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.32649253731343286
[2m[36m(func pid=158211)[0m top5: 0.8190298507462687
[2m[36m(func pid=158211)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=158211)[0m f1_macro: 0.2716513286546506
[2m[36m(func pid=158211)[0m f1_weighted: 0.3484488363764024
[2m[36m(func pid=158211)[0m f1_per_class: [0.275, 0.27, 0.242, 0.464, 0.052, 0.349, 0.32, 0.408, 0.1, 0.235]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=169477)[0m top1: 0.027052238805970148
[2m[36m(func pid=169477)[0m top5: 0.5806902985074627
[2m[36m(func pid=169477)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=169477)[0m f1_macro: 0.019099766720847074
[2m[36m(func pid=169477)[0m f1_weighted: 0.008466170650083
[2m[36m(func pid=169477)[0m f1_per_class: [0.068, 0.015, 0.028, 0.003, 0.012, 0.024, 0.0, 0.0, 0.0, 0.04]
[2m[36m(func pid=169477)[0m 
== Status ==
Current time: 2024-01-07 04:42:46 (running for 00:33:00.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.682 |      0.272 |                   48 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.566 |      0.348 |                   25 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  2.028 |      0.171 |                    4 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.944 |      0.019 |                    1 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.24766791044776118
[2m[36m(func pid=168639)[0m top5: 0.7024253731343284
[2m[36m(func pid=168639)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=168639)[0m f1_macro: 0.1713196312237978
[2m[36m(func pid=168639)[0m f1_weighted: 0.24808233979016528
[2m[36m(func pid=168639)[0m f1_per_class: [0.167, 0.089, 0.131, 0.416, 0.103, 0.024, 0.299, 0.289, 0.043, 0.152]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5166 | Steps: 2 | Val loss: 1.7202 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6860 | Steps: 2 | Val loss: 1.9883 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.0595 | Steps: 2 | Val loss: 2.7895 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8396 | Steps: 2 | Val loss: 2.1504 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=163905)[0m top1: 0.38386194029850745
[2m[36m(func pid=163905)[0m top5: 0.8894589552238806
[2m[36m(func pid=163905)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=163905)[0m f1_macro: 0.34453559188738103
[2m[36m(func pid=163905)[0m f1_weighted: 0.3965584352163064
[2m[36m(func pid=163905)[0m f1_per_class: [0.392, 0.275, 0.545, 0.528, 0.117, 0.394, 0.375, 0.428, 0.17, 0.221]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.07136194029850747
[2m[36m(func pid=169477)[0m top5: 0.5886194029850746
[2m[36m(func pid=169477)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=169477)[0m f1_macro: 0.05228190727605379
[2m[36m(func pid=169477)[0m f1_weighted: 0.048563155635672456
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.016, 0.019, 0.007, 0.0, 0.0, 0.095, 0.238, 0.0, 0.148]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.33115671641791045
[2m[36m(func pid=158211)[0m top5: 0.8227611940298507
[2m[36m(func pid=158211)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=158211)[0m f1_macro: 0.2754211242399315
[2m[36m(func pid=158211)[0m f1_weighted: 0.3539961022741376
[2m[36m(func pid=158211)[0m f1_per_class: [0.276, 0.273, 0.25, 0.47, 0.051, 0.359, 0.327, 0.413, 0.098, 0.238]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:42:51 (running for 00:33:05.84)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.686 |      0.275 |                   49 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.517 |      0.345 |                   26 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.84  |      0.156 |                    5 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.06  |      0.052 |                    2 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.23087686567164178
[2m[36m(func pid=168639)[0m top5: 0.6791044776119403
[2m[36m(func pid=168639)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=168639)[0m f1_macro: 0.15584312767706146
[2m[36m(func pid=168639)[0m f1_weighted: 0.20813650207353046
[2m[36m(func pid=168639)[0m f1_per_class: [0.242, 0.071, 0.144, 0.498, 0.042, 0.031, 0.114, 0.149, 0.067, 0.2]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5604 | Steps: 2 | Val loss: 1.7639 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.2245 | Steps: 2 | Val loss: 2.6277 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6029 | Steps: 2 | Val loss: 1.9807 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3914 | Steps: 2 | Val loss: 1.9457 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=163905)[0m top1: 0.3712686567164179
[2m[36m(func pid=163905)[0m top5: 0.8885261194029851
[2m[36m(func pid=163905)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=163905)[0m f1_macro: 0.33517674056374347
[2m[36m(func pid=163905)[0m f1_weighted: 0.38800404969942276
[2m[36m(func pid=163905)[0m f1_per_class: [0.358, 0.274, 0.471, 0.505, 0.107, 0.401, 0.363, 0.447, 0.193, 0.234]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1553171641791045
[2m[36m(func pid=169477)[0m top5: 0.7971082089552238
[2m[36m(func pid=169477)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.10922297510973511
[2m[36m(func pid=169477)[0m f1_weighted: 0.10896144646132042
[2m[36m(func pid=169477)[0m f1_per_class: [0.115, 0.0, 0.0, 0.006, 0.19, 0.0, 0.295, 0.189, 0.054, 0.242]
[2m[36m(func pid=169477)[0m 
== Status ==
Current time: 2024-01-07 04:42:56 (running for 00:33:11.02)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.686 |      0.275 |                   49 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.56  |      0.335 |                   27 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.391 |      0.235 |                    6 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.225 |      0.109 |                    3 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.3185634328358209
[2m[36m(func pid=168639)[0m top5: 0.7952425373134329
[2m[36m(func pid=168639)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=168639)[0m f1_macro: 0.23524494819676808
[2m[36m(func pid=168639)[0m f1_weighted: 0.2859540600022239
[2m[36m(func pid=168639)[0m f1_per_class: [0.304, 0.071, 0.429, 0.552, 0.09, 0.126, 0.261, 0.241, 0.054, 0.226]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m top1: 0.33675373134328357
[2m[36m(func pid=158211)[0m top5: 0.8227611940298507
[2m[36m(func pid=158211)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=158211)[0m f1_macro: 0.27577747085970417
[2m[36m(func pid=158211)[0m f1_weighted: 0.36007794383024694
[2m[36m(func pid=158211)[0m f1_per_class: [0.278, 0.278, 0.24, 0.479, 0.054, 0.357, 0.338, 0.407, 0.098, 0.228]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4242 | Steps: 2 | Val loss: 1.7934 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9749 | Steps: 2 | Val loss: 3.0534 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9968 | Steps: 2 | Val loss: 1.7918 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.5648 | Steps: 2 | Val loss: 1.9693 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=163905)[0m top1: 0.3666044776119403
[2m[36m(func pid=163905)[0m top5: 0.8843283582089553
[2m[36m(func pid=163905)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=163905)[0m f1_macro: 0.3290578699080512
[2m[36m(func pid=163905)[0m f1_weighted: 0.3875546343635076
[2m[36m(func pid=163905)[0m f1_per_class: [0.344, 0.267, 0.444, 0.499, 0.097, 0.399, 0.375, 0.448, 0.176, 0.24]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:43:01 (running for 00:33:16.33)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.603 |      0.276 |                   50 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.424 |      0.329 |                   28 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.391 |      0.235 |                    6 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  0.975 |      0.12  |                    4 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.21455223880597016
[2m[36m(func pid=169477)[0m top5: 0.7863805970149254
[2m[36m(func pid=169477)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=169477)[0m f1_macro: 0.12017845322516621
[2m[36m(func pid=169477)[0m f1_weighted: 0.15801863759013934
[2m[36m(func pid=169477)[0m f1_per_class: [0.032, 0.105, 0.0, 0.427, 0.0, 0.0, 0.0, 0.247, 0.064, 0.328]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.3512126865671642
[2m[36m(func pid=168639)[0m top5: 0.8414179104477612
[2m[36m(func pid=168639)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=168639)[0m f1_macro: 0.28779401622615935
[2m[36m(func pid=168639)[0m f1_weighted: 0.33694928806217733
[2m[36m(func pid=168639)[0m f1_per_class: [0.263, 0.131, 0.571, 0.525, 0.143, 0.183, 0.372, 0.363, 0.068, 0.259]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34328358208955223
[2m[36m(func pid=158211)[0m top5: 0.8288246268656716
[2m[36m(func pid=158211)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=158211)[0m f1_macro: 0.27978733835400643
[2m[36m(func pid=158211)[0m f1_weighted: 0.36816023032384837
[2m[36m(func pid=158211)[0m f1_per_class: [0.278, 0.278, 0.245, 0.488, 0.047, 0.359, 0.354, 0.413, 0.101, 0.234]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3897 | Steps: 2 | Val loss: 1.8195 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8052 | Steps: 2 | Val loss: 1.7049 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8101 | Steps: 2 | Val loss: 41.0108 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6378 | Steps: 2 | Val loss: 1.9568 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=163905)[0m top1: 0.35867537313432835
[2m[36m(func pid=163905)[0m top5: 0.8889925373134329
[2m[36m(func pid=163905)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=163905)[0m f1_macro: 0.3235056291453551
[2m[36m(func pid=163905)[0m f1_weighted: 0.38343305412107276
[2m[36m(func pid=163905)[0m f1_per_class: [0.326, 0.279, 0.436, 0.473, 0.087, 0.381, 0.388, 0.443, 0.169, 0.252]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:43:07 (running for 00:33:21.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.565 |      0.28  |                   51 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.39  |      0.324 |                   29 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.805 |      0.326 |                    8 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  0.975 |      0.12  |                    4 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.3824626865671642
[2m[36m(func pid=168639)[0m top5: 0.867070895522388
[2m[36m(func pid=168639)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=168639)[0m f1_macro: 0.32606184874295996
[2m[36m(func pid=168639)[0m f1_weighted: 0.36396034603182825
[2m[36m(func pid=168639)[0m f1_per_class: [0.382, 0.11, 0.667, 0.558, 0.135, 0.21, 0.416, 0.384, 0.086, 0.315]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.006063432835820896
[2m[36m(func pid=169477)[0m top5: 0.37173507462686567
[2m[36m(func pid=169477)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=169477)[0m f1_macro: 0.0012639766650461835
[2m[36m(func pid=169477)[0m f1_weighted: 7.664037614552419e-05
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.341884328358209
[2m[36m(func pid=158211)[0m top5: 0.8334888059701493
[2m[36m(func pid=158211)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=158211)[0m f1_macro: 0.2792554026883735
[2m[36m(func pid=158211)[0m f1_weighted: 0.3655681764035274
[2m[36m(func pid=158211)[0m f1_per_class: [0.276, 0.271, 0.253, 0.48, 0.049, 0.369, 0.354, 0.41, 0.101, 0.229]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4714 | Steps: 2 | Val loss: 1.8517 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4819 | Steps: 2 | Val loss: 1.6752 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.2036 | Steps: 2 | Val loss: 2834.4109 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=163905)[0m top1: 0.35074626865671643
[2m[36m(func pid=163905)[0m top5: 0.886660447761194
[2m[36m(func pid=163905)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=163905)[0m f1_macro: 0.3221840455344527
[2m[36m(func pid=163905)[0m f1_weighted: 0.3772943821198578
[2m[36m(func pid=163905)[0m f1_per_class: [0.296, 0.278, 0.453, 0.464, 0.087, 0.364, 0.381, 0.455, 0.186, 0.259]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5734 | Steps: 2 | Val loss: 1.9495 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:43:12 (running for 00:33:27.01)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.638 |      0.279 |                   52 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.471 |      0.322 |                   30 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.482 |      0.357 |                    9 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.81  |      0.001 |                    5 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.40298507462686567
[2m[36m(func pid=168639)[0m top5: 0.8857276119402985
[2m[36m(func pid=168639)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=168639)[0m f1_macro: 0.3569345037471022
[2m[36m(func pid=168639)[0m f1_weighted: 0.40064645198620824
[2m[36m(func pid=168639)[0m f1_per_class: [0.535, 0.164, 0.686, 0.565, 0.107, 0.304, 0.459, 0.362, 0.099, 0.289]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.00792910447761194
[2m[36m(func pid=169477)[0m top5: 0.3003731343283582
[2m[36m(func pid=169477)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=169477)[0m f1_macro: 0.008937521837181972
[2m[36m(func pid=169477)[0m f1_weighted: 0.0018004307149574366
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.007, 0.016, 0.0, 0.067, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34375
[2m[36m(func pid=158211)[0m top5: 0.8344216417910447
[2m[36m(func pid=158211)[0m f1_micro: 0.34375
[2m[36m(func pid=158211)[0m f1_macro: 0.27828194133900974
[2m[36m(func pid=158211)[0m f1_weighted: 0.36964423388874956
[2m[36m(func pid=158211)[0m f1_per_class: [0.258, 0.279, 0.24, 0.48, 0.051, 0.369, 0.366, 0.404, 0.101, 0.235]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2832 | Steps: 2 | Val loss: 1.8928 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3483 | Steps: 2 | Val loss: 1.6777 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.5188 | Steps: 2 | Val loss: 12478.7559 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=163905)[0m top1: 0.34888059701492535
[2m[36m(func pid=163905)[0m top5: 0.886660447761194
[2m[36m(func pid=163905)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=163905)[0m f1_macro: 0.3232777837775888
[2m[36m(func pid=163905)[0m f1_weighted: 0.3736656225050717
[2m[36m(func pid=163905)[0m f1_per_class: [0.305, 0.292, 0.453, 0.464, 0.086, 0.37, 0.356, 0.464, 0.179, 0.263]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4682 | Steps: 2 | Val loss: 1.9383 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=168639)[0m top1: 0.40718283582089554
[2m[36m(func pid=168639)[0m top5: 0.9057835820895522
[2m[36m(func pid=168639)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=168639)[0m f1_macro: 0.34072291513270925
[2m[36m(func pid=168639)[0m f1_weighted: 0.41525391458069055
[2m[36m(func pid=168639)[0m f1_per_class: [0.413, 0.239, 0.49, 0.559, 0.118, 0.352, 0.458, 0.393, 0.084, 0.301]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:43:18 (running for 00:33:32.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.573 |      0.278 |                   53 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.283 |      0.323 |                   31 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.348 |      0.341 |                   10 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  3.204 |      0.009 |                    6 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.006063432835820896
[2m[36m(func pid=169477)[0m top5: 0.5093283582089553
[2m[36m(func pid=169477)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=169477)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=169477)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2808 | Steps: 2 | Val loss: 1.9075 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=158211)[0m top1: 0.34095149253731344
[2m[36m(func pid=158211)[0m top5: 0.8344216417910447
[2m[36m(func pid=158211)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=158211)[0m f1_macro: 0.2794573504244674
[2m[36m(func pid=158211)[0m f1_weighted: 0.3650060760526565
[2m[36m(func pid=158211)[0m f1_per_class: [0.26, 0.277, 0.264, 0.473, 0.052, 0.369, 0.357, 0.41, 0.1, 0.234]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.0363 | Steps: 2 | Val loss: 14303.7100 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2562 | Steps: 2 | Val loss: 1.7891 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163905)[0m top1: 0.34794776119402987
[2m[36m(func pid=163905)[0m top5: 0.8913246268656716
[2m[36m(func pid=163905)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=163905)[0m f1_macro: 0.3247436180591971
[2m[36m(func pid=163905)[0m f1_weighted: 0.3698755069246167
[2m[36m(func pid=163905)[0m f1_per_class: [0.318, 0.294, 0.462, 0.465, 0.088, 0.372, 0.338, 0.475, 0.18, 0.255]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4604 | Steps: 2 | Val loss: 1.9340 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:43:23 (running for 00:33:37.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.468 |      0.279 |                   54 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.281 |      0.325 |                   32 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.348 |      0.341 |                   10 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.036 |      0.001 |                    8 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.006063432835820896
[2m[36m(func pid=169477)[0m top5: 0.5093283582089553
[2m[36m(func pid=169477)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=169477)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=169477)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.39505597014925375
[2m[36m(func pid=168639)[0m top5: 0.9043843283582089
[2m[36m(func pid=168639)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=168639)[0m f1_macro: 0.3172904496402496
[2m[36m(func pid=168639)[0m f1_weighted: 0.40157335056779325
[2m[36m(func pid=168639)[0m f1_per_class: [0.345, 0.244, 0.421, 0.563, 0.124, 0.29, 0.436, 0.382, 0.118, 0.25]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1987 | Steps: 2 | Val loss: 1.8984 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=158211)[0m top1: 0.341884328358209
[2m[36m(func pid=158211)[0m top5: 0.8339552238805971
[2m[36m(func pid=158211)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=158211)[0m f1_macro: 0.28012331240775395
[2m[36m(func pid=158211)[0m f1_weighted: 0.3662086757181902
[2m[36m(func pid=158211)[0m f1_per_class: [0.263, 0.274, 0.258, 0.478, 0.051, 0.373, 0.354, 0.418, 0.1, 0.232]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2987 | Steps: 2 | Val loss: 1.9442 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.0099 | Steps: 2 | Val loss: 14330.9160 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=163905)[0m top1: 0.36986940298507465
[2m[36m(func pid=163905)[0m top5: 0.8950559701492538
[2m[36m(func pid=163905)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=163905)[0m f1_macro: 0.34238392501921744
[2m[36m(func pid=163905)[0m f1_weighted: 0.39107992175769274
[2m[36m(func pid=163905)[0m f1_per_class: [0.358, 0.323, 0.511, 0.49, 0.104, 0.373, 0.364, 0.475, 0.181, 0.245]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.5505 | Steps: 2 | Val loss: 1.9284 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:43:28 (running for 00:33:42.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.46  |      0.28  |                   55 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.199 |      0.342 |                   33 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.256 |      0.317 |                   11 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.01  |      0.001 |                    9 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.3787313432835821
[2m[36m(func pid=168639)[0m top5: 0.9067164179104478
[2m[36m(func pid=168639)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=168639)[0m f1_macro: 0.2848612209316213
[2m[36m(func pid=168639)[0m f1_weighted: 0.37965423943009624
[2m[36m(func pid=168639)[0m f1_per_class: [0.231, 0.222, 0.407, 0.582, 0.126, 0.21, 0.418, 0.259, 0.133, 0.262]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.006063432835820896
[2m[36m(func pid=169477)[0m top5: 0.5093283582089553
[2m[36m(func pid=169477)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=169477)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=169477)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1584 | Steps: 2 | Val loss: 1.9006 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=158211)[0m top1: 0.34095149253731344
[2m[36m(func pid=158211)[0m top5: 0.8325559701492538
[2m[36m(func pid=158211)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=158211)[0m f1_macro: 0.28230367360107717
[2m[36m(func pid=158211)[0m f1_weighted: 0.36409152185550037
[2m[36m(func pid=158211)[0m f1_per_class: [0.256, 0.283, 0.276, 0.466, 0.053, 0.383, 0.348, 0.425, 0.1, 0.234]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.8596 | Steps: 2 | Val loss: 15031.1787 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1005 | Steps: 2 | Val loss: 2.2339 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163905)[0m top1: 0.3805970149253731
[2m[36m(func pid=163905)[0m top5: 0.8964552238805971
[2m[36m(func pid=163905)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=163905)[0m f1_macro: 0.3565282824815479
[2m[36m(func pid=163905)[0m f1_weighted: 0.40146642651342856
[2m[36m(func pid=163905)[0m f1_per_class: [0.388, 0.349, 0.585, 0.494, 0.091, 0.37, 0.377, 0.487, 0.179, 0.245]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:43:33 (running for 00:33:48.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.55  |      0.282 |                   56 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.158 |      0.357 |                   34 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.299 |      0.285 |                   12 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.86  |      0.001 |                   10 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.006063432835820896
[2m[36m(func pid=169477)[0m top5: 0.5093283582089553
[2m[36m(func pid=169477)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=169477)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=169477)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.3995 | Steps: 2 | Val loss: 1.9179 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=168639)[0m top1: 0.3656716417910448
[2m[36m(func pid=168639)[0m top5: 0.9123134328358209
[2m[36m(func pid=168639)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=168639)[0m f1_macro: 0.292489865923856
[2m[36m(func pid=168639)[0m f1_weighted: 0.3571109615822606
[2m[36m(func pid=168639)[0m f1_per_class: [0.231, 0.17, 0.649, 0.581, 0.156, 0.175, 0.405, 0.133, 0.124, 0.301]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1729 | Steps: 2 | Val loss: 1.9126 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.2008 | Steps: 2 | Val loss: 3739.3965 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=158211)[0m top1: 0.34421641791044777
[2m[36m(func pid=158211)[0m top5: 0.8362873134328358
[2m[36m(func pid=158211)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=158211)[0m f1_macro: 0.28653186514900975
[2m[36m(func pid=158211)[0m f1_weighted: 0.36610847151565257
[2m[36m(func pid=158211)[0m f1_per_class: [0.259, 0.291, 0.293, 0.465, 0.055, 0.378, 0.351, 0.434, 0.101, 0.24]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1014 | Steps: 2 | Val loss: 2.4273 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=163905)[0m top1: 0.3880597014925373
[2m[36m(func pid=163905)[0m top5: 0.8964552238805971
[2m[36m(func pid=163905)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=163905)[0m f1_macro: 0.3669992146998991
[2m[36m(func pid=163905)[0m f1_weighted: 0.40859516380585403
[2m[36m(func pid=163905)[0m f1_per_class: [0.407, 0.366, 0.632, 0.497, 0.086, 0.374, 0.381, 0.5, 0.183, 0.245]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:43:39 (running for 00:33:53.50)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.399 |      0.287 |                   57 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.173 |      0.367 |                   35 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.1   |      0.292 |                   13 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  3.201 |      0.032 |                   11 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.09701492537313433
[2m[36m(func pid=169477)[0m top5: 0.5867537313432836
[2m[36m(func pid=169477)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=169477)[0m f1_macro: 0.03182861514919663
[2m[36m(func pid=169477)[0m f1_weighted: 0.05477965946853339
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.394589552238806
[2m[36m(func pid=168639)[0m top5: 0.9305037313432836
[2m[36m(func pid=168639)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=168639)[0m f1_macro: 0.33459080244594297
[2m[36m(func pid=168639)[0m f1_weighted: 0.3789217039101294
[2m[36m(func pid=168639)[0m f1_per_class: [0.25, 0.157, 0.727, 0.584, 0.2, 0.216, 0.424, 0.3, 0.185, 0.304]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3944 | Steps: 2 | Val loss: 1.9089 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1202 | Steps: 2 | Val loss: 1.9141 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0756 | Steps: 2 | Val loss: 2.5791 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 6.0474 | Steps: 2 | Val loss: 79883.1953 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=158211)[0m top1: 0.34654850746268656
[2m[36m(func pid=158211)[0m top5: 0.8390858208955224
[2m[36m(func pid=158211)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=158211)[0m f1_macro: 0.2907812205247002
[2m[36m(func pid=158211)[0m f1_weighted: 0.3698415375442668
[2m[36m(func pid=158211)[0m f1_per_class: [0.243, 0.287, 0.32, 0.464, 0.055, 0.382, 0.362, 0.449, 0.102, 0.243]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3969216417910448
[2m[36m(func pid=163905)[0m top5: 0.8969216417910447
[2m[36m(func pid=163905)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=163905)[0m f1_macro: 0.381574202107788
[2m[36m(func pid=163905)[0m f1_weighted: 0.41715305276239567
[2m[36m(func pid=163905)[0m f1_per_class: [0.446, 0.38, 0.686, 0.504, 0.088, 0.369, 0.391, 0.5, 0.199, 0.252]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:43:44 (running for 00:33:58.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.394 |      0.291 |                   58 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.12  |      0.382 |                   36 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.101 |      0.335 |                   14 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  6.047 |      0.005 |                   12 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.4193097014925373
[2m[36m(func pid=168639)[0m top5: 0.9267723880597015
[2m[36m(func pid=168639)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=168639)[0m f1_macro: 0.3550528402752028
[2m[36m(func pid=168639)[0m f1_weighted: 0.40441992944547983
[2m[36m(func pid=168639)[0m f1_per_class: [0.395, 0.148, 0.649, 0.592, 0.212, 0.26, 0.47, 0.362, 0.186, 0.276]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.010261194029850746
[2m[36m(func pid=169477)[0m top5: 0.5
[2m[36m(func pid=169477)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=169477)[0m f1_macro: 0.005491648202668933
[2m[36m(func pid=169477)[0m f1_weighted: 0.007398717554365544
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.043, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.3463 | Steps: 2 | Val loss: 1.8976 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.1346 | Steps: 2 | Val loss: 1.9360 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1584 | Steps: 2 | Val loss: 2.9286 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5305 | Steps: 2 | Val loss: 26488.5293 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=158211)[0m top1: 0.3460820895522388
[2m[36m(func pid=158211)[0m top5: 0.8423507462686567
[2m[36m(func pid=158211)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=158211)[0m f1_macro: 0.29343028228370066
[2m[36m(func pid=158211)[0m f1_weighted: 0.36937963728995715
[2m[36m(func pid=158211)[0m f1_per_class: [0.248, 0.287, 0.338, 0.46, 0.061, 0.38, 0.364, 0.452, 0.102, 0.243]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3987873134328358
[2m[36m(func pid=163905)[0m top5: 0.8959888059701493
[2m[36m(func pid=163905)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=163905)[0m f1_macro: 0.3835969320968037
[2m[36m(func pid=163905)[0m f1_weighted: 0.4175466648901452
[2m[36m(func pid=163905)[0m f1_per_class: [0.459, 0.39, 0.686, 0.502, 0.09, 0.366, 0.389, 0.489, 0.216, 0.248]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.40158582089552236
[2m[36m(func pid=168639)[0m top5: 0.90625
[2m[36m(func pid=168639)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=168639)[0m f1_macro: 0.3205732231978886
[2m[36m(func pid=168639)[0m f1_weighted: 0.3882138483283775
[2m[36m(func pid=168639)[0m f1_per_class: [0.361, 0.148, 0.615, 0.568, 0.157, 0.135, 0.504, 0.351, 0.094, 0.273]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:43:49 (running for 00:34:04.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.346 |      0.293 |                   59 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.135 |      0.384 |                   37 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.158 |      0.321 |                   16 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  6.047 |      0.005 |                   12 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.025186567164179104
[2m[36m(func pid=169477)[0m top5: 0.44449626865671643
[2m[36m(func pid=169477)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=169477)[0m f1_macro: 0.01370387312976542
[2m[36m(func pid=169477)[0m f1_weighted: 0.01309149599059547
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.072, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3979 | Steps: 2 | Val loss: 1.8877 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0890 | Steps: 2 | Val loss: 1.9624 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1866 | Steps: 2 | Val loss: 3.3143 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9666 | Steps: 2 | Val loss: 10845.8496 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=158211)[0m top1: 0.34654850746268656
[2m[36m(func pid=158211)[0m top5: 0.8428171641791045
[2m[36m(func pid=158211)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=158211)[0m f1_macro: 0.29594371174145234
[2m[36m(func pid=158211)[0m f1_weighted: 0.3696184614033406
[2m[36m(func pid=158211)[0m f1_per_class: [0.252, 0.29, 0.358, 0.454, 0.062, 0.376, 0.369, 0.442, 0.12, 0.237]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.39598880597014924
[2m[36m(func pid=163905)[0m top5: 0.8950559701492538
[2m[36m(func pid=163905)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=163905)[0m f1_macro: 0.3826199147477353
[2m[36m(func pid=163905)[0m f1_weighted: 0.41641591775777925
[2m[36m(func pid=163905)[0m f1_per_class: [0.448, 0.382, 0.706, 0.499, 0.091, 0.353, 0.398, 0.491, 0.22, 0.238]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:43:55 (running for 00:34:09.79)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.398 |      0.296 |                   60 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.089 |      0.383 |                   38 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.187 |      0.305 |                   17 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.531 |      0.014 |                   13 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.36986940298507465
[2m[36m(func pid=168639)[0m top5: 0.8917910447761194
[2m[36m(func pid=168639)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=168639)[0m f1_macro: 0.30469319561573827
[2m[36m(func pid=168639)[0m f1_weighted: 0.3612226405928703
[2m[36m(func pid=168639)[0m f1_per_class: [0.395, 0.17, 0.545, 0.578, 0.123, 0.034, 0.432, 0.321, 0.098, 0.351]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.044309701492537316
[2m[36m(func pid=169477)[0m top5: 0.45988805970149255
[2m[36m(func pid=169477)[0m f1_micro: 0.044309701492537316
[2m[36m(func pid=169477)[0m f1_macro: 0.015185645409811621
[2m[36m(func pid=169477)[0m f1_weighted: 0.021438504332060068
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2827 | Steps: 2 | Val loss: 1.8808 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1616 | Steps: 2 | Val loss: 2.0127 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1939 | Steps: 2 | Val loss: 3.7953 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3683 | Steps: 2 | Val loss: 6438.9072 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=158211)[0m top1: 0.345615671641791
[2m[36m(func pid=158211)[0m top5: 0.8418843283582089
[2m[36m(func pid=158211)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=158211)[0m f1_macro: 0.29680256439449826
[2m[36m(func pid=158211)[0m f1_weighted: 0.36771666276913806
[2m[36m(func pid=158211)[0m f1_per_class: [0.252, 0.294, 0.375, 0.45, 0.064, 0.391, 0.36, 0.433, 0.117, 0.232]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3927238805970149
[2m[36m(func pid=163905)[0m top5: 0.8964552238805971
[2m[36m(func pid=163905)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=163905)[0m f1_macro: 0.38247367880225963
[2m[36m(func pid=163905)[0m f1_weighted: 0.4141148684767746
[2m[36m(func pid=163905)[0m f1_per_class: [0.452, 0.383, 0.706, 0.49, 0.09, 0.334, 0.403, 0.491, 0.24, 0.237]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:44:00 (running for 00:34:15.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.283 |      0.297 |                   61 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.162 |      0.382 |                   39 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.194 |      0.251 |                   18 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.967 |      0.015 |                   14 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.324160447761194
[2m[36m(func pid=168639)[0m top5: 0.8861940298507462
[2m[36m(func pid=168639)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=168639)[0m f1_macro: 0.2506724756194208
[2m[36m(func pid=168639)[0m f1_weighted: 0.31830572738025353
[2m[36m(func pid=168639)[0m f1_per_class: [0.158, 0.204, 0.417, 0.545, 0.104, 0.027, 0.323, 0.314, 0.103, 0.312]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.05783582089552239
[2m[36m(func pid=169477)[0m top5: 0.4612873134328358
[2m[36m(func pid=169477)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=169477)[0m f1_macro: 0.018269440819404632
[2m[36m(func pid=169477)[0m f1_weighted: 0.027646229210575062
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.16, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2430 | Steps: 2 | Val loss: 1.8775 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1000 | Steps: 2 | Val loss: 2.0641 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1160 | Steps: 2 | Val loss: 3.8558 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9484 | Steps: 2 | Val loss: 4053.0391 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=163905)[0m top1: 0.3885261194029851
[2m[36m(func pid=163905)[0m top5: 0.8927238805970149
[2m[36m(func pid=163905)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=163905)[0m f1_macro: 0.37854326350767376
[2m[36m(func pid=163905)[0m f1_weighted: 0.40892744395738007
[2m[36m(func pid=163905)[0m f1_per_class: [0.431, 0.385, 0.706, 0.482, 0.081, 0.332, 0.394, 0.495, 0.239, 0.242]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.345615671641791
[2m[36m(func pid=158211)[0m top5: 0.8414179104477612
[2m[36m(func pid=158211)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=158211)[0m f1_macro: 0.2966792672747091
[2m[36m(func pid=158211)[0m f1_weighted: 0.36794165698184067
[2m[36m(func pid=158211)[0m f1_per_class: [0.25, 0.292, 0.369, 0.448, 0.072, 0.39, 0.365, 0.431, 0.118, 0.232]
[2m[36m(func pid=158211)[0m 
== Status ==
Current time: 2024-01-07 04:44:06 (running for 00:34:20.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.243 |      0.297 |                   62 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.1   |      0.379 |                   40 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.116 |      0.216 |                   19 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.368 |      0.018 |                   15 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.30177238805970147
[2m[36m(func pid=168639)[0m top5: 0.8847947761194029
[2m[36m(func pid=168639)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=168639)[0m f1_macro: 0.21603113120032874
[2m[36m(func pid=168639)[0m f1_weighted: 0.30950835711862285
[2m[36m(func pid=168639)[0m f1_per_class: [0.038, 0.198, 0.125, 0.522, 0.08, 0.014, 0.324, 0.319, 0.215, 0.325]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.08115671641791045
[2m[36m(func pid=169477)[0m top5: 0.48134328358208955
[2m[36m(func pid=169477)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.023406211099559322
[2m[36m(func pid=169477)[0m f1_weighted: 0.03584704855037735
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.207, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1127 | Steps: 2 | Val loss: 2.1366 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3064 | Steps: 2 | Val loss: 1.8773 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5503 | Steps: 2 | Val loss: 4.7645 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7952 | Steps: 2 | Val loss: 1808.2213 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=163905)[0m top1: 0.38619402985074625
[2m[36m(func pid=163905)[0m top5: 0.8889925373134329
[2m[36m(func pid=163905)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=163905)[0m f1_macro: 0.37765868559411714
[2m[36m(func pid=163905)[0m f1_weighted: 0.4045722566598717
[2m[36m(func pid=163905)[0m f1_per_class: [0.412, 0.394, 0.706, 0.473, 0.083, 0.33, 0.383, 0.498, 0.24, 0.259]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m top1: 0.3414179104477612
[2m[36m(func pid=158211)[0m top5: 0.8432835820895522
[2m[36m(func pid=158211)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=158211)[0m f1_macro: 0.29615138863870666
[2m[36m(func pid=158211)[0m f1_weighted: 0.3640507948151192
== Status ==
Current time: 2024-01-07 04:44:11 (running for 00:34:26.00)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.306 |      0.296 |                   63 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.113 |      0.378 |                   41 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.116 |      0.216 |                   19 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.948 |      0.023 |                   16 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=158211)[0m f1_per_class: [0.245, 0.285, 0.381, 0.435, 0.064, 0.405, 0.363, 0.427, 0.119, 0.238]

[2m[36m(func pid=168639)[0m top1: 0.16044776119402984
[2m[36m(func pid=168639)[0m top5: 0.8013059701492538
[2m[36m(func pid=168639)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=168639)[0m f1_macro: 0.15446404125912075
[2m[36m(func pid=168639)[0m f1_weighted: 0.20638267477828626
[2m[36m(func pid=168639)[0m f1_per_class: [0.061, 0.269, 0.052, 0.232, 0.031, 0.0, 0.253, 0.132, 0.207, 0.308]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.12546641791044777
[2m[36m(func pid=169477)[0m top5: 0.5242537313432836
[2m[36m(func pid=169477)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=169477)[0m f1_macro: 0.030413418762816474
[2m[36m(func pid=169477)[0m f1_weighted: 0.04518945777099097
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.261, 0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1815 | Steps: 2 | Val loss: 2.2043 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8450 | Steps: 2 | Val loss: 1010.1030 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2101 | Steps: 2 | Val loss: 7.3866 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3272 | Steps: 2 | Val loss: 1.8779 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=163905)[0m top1: 0.3805970149253731
[2m[36m(func pid=163905)[0m top5: 0.8857276119402985
[2m[36m(func pid=163905)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=163905)[0m f1_macro: 0.37780752127652556
[2m[36m(func pid=163905)[0m f1_weighted: 0.398090470562435
[2m[36m(func pid=163905)[0m f1_per_class: [0.392, 0.399, 0.774, 0.459, 0.084, 0.33, 0.374, 0.489, 0.224, 0.252]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:44:17 (running for 00:34:31.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.306 |      0.296 |                   63 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.181 |      0.378 |                   42 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.55  |      0.154 |                   20 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.845 |      0.054 |                   18 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.16930970149253732
[2m[36m(func pid=169477)[0m top5: 0.5676305970149254
[2m[36m(func pid=169477)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=169477)[0m f1_macro: 0.05415204678362573
[2m[36m(func pid=169477)[0m f1_weighted: 0.05125196386488609
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.289, 0.253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.10867537313432836
[2m[36m(func pid=168639)[0m top5: 0.6646455223880597
[2m[36m(func pid=168639)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=168639)[0m f1_macro: 0.15448549130141717
[2m[36m(func pid=168639)[0m f1_weighted: 0.096375443530583
[2m[36m(func pid=168639)[0m f1_per_class: [0.308, 0.263, 0.312, 0.063, 0.025, 0.0, 0.048, 0.0, 0.216, 0.31]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34421641791044777
[2m[36m(func pid=158211)[0m top5: 0.84375
[2m[36m(func pid=158211)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=158211)[0m f1_macro: 0.2955468360280892
[2m[36m(func pid=158211)[0m f1_weighted: 0.36709575952627743
[2m[36m(func pid=158211)[0m f1_per_class: [0.244, 0.299, 0.364, 0.433, 0.068, 0.41, 0.367, 0.421, 0.118, 0.232]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0542 | Steps: 2 | Val loss: 2.2711 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.9026 | Steps: 2 | Val loss: 741.8131 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3474 | Steps: 2 | Val loss: 6.5864 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2310 | Steps: 2 | Val loss: 1.8683 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 04:44:22 (running for 00:34:36.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.327 |      0.296 |                   64 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.054 |      0.374 |                   43 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.21  |      0.154 |                   21 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.845 |      0.054 |                   18 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37220149253731344
[2m[36m(func pid=163905)[0m top5: 0.8801305970149254
[2m[36m(func pid=163905)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=163905)[0m f1_macro: 0.3740417251740555
[2m[36m(func pid=163905)[0m f1_weighted: 0.38790264016911075
[2m[36m(func pid=163905)[0m f1_per_class: [0.394, 0.396, 0.774, 0.448, 0.081, 0.334, 0.349, 0.502, 0.22, 0.242]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.17117537313432835
[2m[36m(func pid=169477)[0m top5: 0.5648320895522388
[2m[36m(func pid=169477)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=169477)[0m f1_macro: 0.02926634768740032
[2m[36m(func pid=169477)[0m f1_weighted: 0.05036978683139327
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.11287313432835822
[2m[36m(func pid=168639)[0m top5: 0.625
[2m[36m(func pid=168639)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=168639)[0m f1_macro: 0.13660739654060222
[2m[36m(func pid=168639)[0m f1_weighted: 0.12233888671651641
[2m[36m(func pid=168639)[0m f1_per_class: [0.196, 0.275, 0.146, 0.085, 0.025, 0.0, 0.115, 0.0, 0.257, 0.265]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34421641791044777
[2m[36m(func pid=158211)[0m top5: 0.8465485074626866
[2m[36m(func pid=158211)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=158211)[0m f1_macro: 0.29934711222126115
[2m[36m(func pid=158211)[0m f1_weighted: 0.36553911880565376
[2m[36m(func pid=158211)[0m f1_per_class: [0.261, 0.301, 0.387, 0.433, 0.064, 0.42, 0.357, 0.412, 0.118, 0.242]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0909 | Steps: 2 | Val loss: 2.3170 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1223 | Steps: 2 | Val loss: 9.0701 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.8027 | Steps: 2 | Val loss: 595.2837 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1426 | Steps: 2 | Val loss: 1.8639 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:44:27 (running for 00:34:42.03)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.231 |      0.299 |                   65 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.091 |      0.372 |                   44 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.347 |      0.137 |                   22 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.903 |      0.029 |                   19 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3670708955223881
[2m[36m(func pid=163905)[0m top5: 0.8768656716417911
[2m[36m(func pid=163905)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=163905)[0m f1_macro: 0.372005705568512
[2m[36m(func pid=163905)[0m f1_weighted: 0.38304915220896296
[2m[36m(func pid=163905)[0m f1_per_class: [0.392, 0.387, 0.774, 0.448, 0.083, 0.338, 0.338, 0.488, 0.229, 0.242]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.16977611940298507
[2m[36m(func pid=169477)[0m top5: 0.534981343283582
[2m[36m(func pid=169477)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=169477)[0m f1_macro: 0.029131652661064426
[2m[36m(func pid=169477)[0m f1_weighted: 0.0501379656340148
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.26725746268656714
[2m[36m(func pid=168639)[0m top5: 0.613339552238806
[2m[36m(func pid=168639)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=168639)[0m f1_macro: 0.1444297249720492
[2m[36m(func pid=168639)[0m f1_weighted: 0.23289181159936606
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.172, 0.031, 0.003, 0.098, 0.0, 0.607, 0.214, 0.214, 0.105]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34095149253731344
[2m[36m(func pid=158211)[0m top5: 0.8470149253731343
[2m[36m(func pid=158211)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=158211)[0m f1_macro: 0.2969923041262149
[2m[36m(func pid=158211)[0m f1_weighted: 0.3620040004283998
[2m[36m(func pid=158211)[0m f1_per_class: [0.259, 0.293, 0.387, 0.434, 0.072, 0.408, 0.353, 0.412, 0.117, 0.235]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0588 | Steps: 2 | Val loss: 2.3530 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8232 | Steps: 2 | Val loss: 12.2140 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.9903 | Steps: 2 | Val loss: 431.4732 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.1161 | Steps: 2 | Val loss: 1.8522 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 04:44:33 (running for 00:34:47.45)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.143 |      0.297 |                   66 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.059 |      0.372 |                   45 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.122 |      0.144 |                   23 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.803 |      0.029 |                   20 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.36613805970149255
[2m[36m(func pid=163905)[0m top5: 0.8801305970149254
[2m[36m(func pid=163905)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=163905)[0m f1_macro: 0.37202953937161426
[2m[36m(func pid=163905)[0m f1_weighted: 0.3831220742847995
[2m[36m(func pid=163905)[0m f1_per_class: [0.386, 0.38, 0.774, 0.437, 0.081, 0.342, 0.35, 0.495, 0.232, 0.243]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2574626865671642
[2m[36m(func pid=168639)[0m top5: 0.5769589552238806
[2m[36m(func pid=168639)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=168639)[0m f1_macro: 0.09121286881330296
[2m[36m(func pid=168639)[0m f1_weighted: 0.2095906106748668
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.237, 0.028, 0.0, 0.083, 0.0, 0.564, 0.0, 0.0, 0.0]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.16417910447761194
[2m[36m(func pid=169477)[0m top5: 0.4818097014925373
[2m[36m(func pid=169477)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=169477)[0m f1_macro: 0.02869955156950673
[2m[36m(func pid=169477)[0m f1_weighted: 0.04939428418445888
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34421641791044777
[2m[36m(func pid=158211)[0m top5: 0.8507462686567164
[2m[36m(func pid=158211)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=158211)[0m f1_macro: 0.3030097463514227
[2m[36m(func pid=158211)[0m f1_weighted: 0.3641080893904514
[2m[36m(func pid=158211)[0m f1_per_class: [0.273, 0.294, 0.429, 0.442, 0.071, 0.407, 0.351, 0.412, 0.117, 0.235]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0921 | Steps: 2 | Val loss: 2.3509 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4276 | Steps: 2 | Val loss: 14.0101 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 11.2363 | Steps: 2 | Val loss: 432.9614 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.1621 | Steps: 2 | Val loss: 1.8466 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 04:44:38 (running for 00:34:53.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.116 |      0.303 |                   67 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.092 |      0.381 |                   46 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.823 |      0.091 |                   24 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.99  |      0.029 |                   21 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.376865671641791
[2m[36m(func pid=163905)[0m top5: 0.8913246268656716
[2m[36m(func pid=163905)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=163905)[0m f1_macro: 0.3811661520800853
[2m[36m(func pid=163905)[0m f1_weighted: 0.3990158376522769
[2m[36m(func pid=163905)[0m f1_per_class: [0.386, 0.383, 0.8, 0.442, 0.077, 0.323, 0.403, 0.494, 0.233, 0.271]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.14878731343283583
[2m[36m(func pid=168639)[0m top5: 0.6310634328358209
[2m[36m(func pid=168639)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=168639)[0m f1_macro: 0.0912596781237938
[2m[36m(func pid=168639)[0m f1_weighted: 0.14342056688985996
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.3, 0.026, 0.023, 0.0, 0.0, 0.23, 0.278, 0.0, 0.056]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.16138059701492538
[2m[36m(func pid=169477)[0m top5: 0.46595149253731344
[2m[36m(func pid=169477)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=169477)[0m f1_macro: 0.029071359155907268
[2m[36m(func pid=169477)[0m f1_weighted: 0.05039022790048094
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.287, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.3460820895522388
[2m[36m(func pid=158211)[0m top5: 0.8521455223880597
[2m[36m(func pid=158211)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=158211)[0m f1_macro: 0.30762266799194615
[2m[36m(func pid=158211)[0m f1_weighted: 0.36680161199299555
[2m[36m(func pid=158211)[0m f1_per_class: [0.273, 0.296, 0.453, 0.443, 0.071, 0.406, 0.355, 0.415, 0.133, 0.231]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0722 | Steps: 2 | Val loss: 2.3691 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.9818 | Steps: 2 | Val loss: 357.6483 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3285 | Steps: 2 | Val loss: 9.3073 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1185 | Steps: 2 | Val loss: 1.8414 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:44:44 (running for 00:34:58.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.162 |      0.308 |                   68 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.092 |      0.381 |                   46 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  2.428 |      0.091 |                   25 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.982 |      0.031 |                   23 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.1553171641791045
[2m[36m(func pid=169477)[0m top5: 0.4832089552238806
[2m[36m(func pid=169477)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.03135526238639695
[2m[36m(func pid=169477)[0m f1_weighted: 0.05155289843657701
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.37593283582089554
[2m[36m(func pid=163905)[0m top5: 0.8917910447761194
[2m[36m(func pid=163905)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=163905)[0m f1_macro: 0.3791566071582132
[2m[36m(func pid=163905)[0m f1_weighted: 0.40151922733216744
[2m[36m(func pid=163905)[0m f1_per_class: [0.376, 0.386, 0.8, 0.435, 0.081, 0.277, 0.435, 0.488, 0.243, 0.272]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.146455223880597
[2m[36m(func pid=168639)[0m top5: 0.6324626865671642
[2m[36m(func pid=168639)[0m f1_micro: 0.146455223880597
[2m[36m(func pid=168639)[0m f1_macro: 0.12783817402890957
[2m[36m(func pid=168639)[0m f1_weighted: 0.1627771940145997
[2m[36m(func pid=168639)[0m f1_per_class: [0.069, 0.359, 0.056, 0.053, 0.296, 0.0, 0.257, 0.0, 0.163, 0.025]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34654850746268656
[2m[36m(func pid=158211)[0m top5: 0.8535447761194029
[2m[36m(func pid=158211)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=158211)[0m f1_macro: 0.31434663970917526
[2m[36m(func pid=158211)[0m f1_weighted: 0.36977269690980413
[2m[36m(func pid=158211)[0m f1_per_class: [0.251, 0.301, 0.533, 0.449, 0.071, 0.393, 0.362, 0.406, 0.151, 0.226]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0326 | Steps: 2 | Val loss: 2.3713 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.2235 | Steps: 2 | Val loss: 247.7108 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4396 | Steps: 2 | Val loss: 9.8773 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1507 | Steps: 2 | Val loss: 1.8313 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:44:49 (running for 00:35:03.77)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.119 |      0.314 |                   69 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.033 |      0.375 |                   48 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.328 |      0.128 |                   26 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.982 |      0.031 |                   23 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37593283582089554
[2m[36m(func pid=163905)[0m top5: 0.894589552238806
[2m[36m(func pid=163905)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=163905)[0m f1_macro: 0.3751415405475247
[2m[36m(func pid=163905)[0m f1_weighted: 0.4005254328740371
[2m[36m(func pid=163905)[0m f1_per_class: [0.373, 0.387, 0.8, 0.421, 0.084, 0.229, 0.463, 0.49, 0.218, 0.286]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.12220149253731344
[2m[36m(func pid=168639)[0m top5: 0.6343283582089553
[2m[36m(func pid=168639)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=168639)[0m f1_macro: 0.08992267822955982
[2m[36m(func pid=168639)[0m f1_weighted: 0.11804115142948989
[2m[36m(func pid=168639)[0m f1_per_class: [0.134, 0.27, 0.0, 0.068, 0.151, 0.0, 0.151, 0.0, 0.101, 0.025]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.15625
[2m[36m(func pid=169477)[0m top5: 0.5256529850746269
[2m[36m(func pid=169477)[0m f1_micro: 0.15625
[2m[36m(func pid=169477)[0m f1_macro: 0.04515005761624186
[2m[36m(func pid=169477)[0m f1_weighted: 0.06043788130185014
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.31, 0.0, 0.01, 0.0, 0.0, 0.0, 0.06, 0.0, 0.071]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.34841417910447764
[2m[36m(func pid=158211)[0m top5: 0.8582089552238806
[2m[36m(func pid=158211)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=158211)[0m f1_macro: 0.31910653881323137
[2m[36m(func pid=158211)[0m f1_weighted: 0.37451888944595446
[2m[36m(func pid=158211)[0m f1_per_class: [0.238, 0.309, 0.558, 0.444, 0.072, 0.398, 0.374, 0.413, 0.162, 0.224]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0237 | Steps: 2 | Val loss: 2.4108 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.5161 | Steps: 2 | Val loss: 9.6455 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.6073 | Steps: 2 | Val loss: 174.0282 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:44:54 (running for 00:35:09.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.151 |      0.319 |                   70 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.024 |      0.368 |                   49 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.44  |      0.09  |                   27 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.224 |      0.045 |                   24 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37919776119402987
[2m[36m(func pid=163905)[0m top5: 0.8908582089552238
[2m[36m(func pid=163905)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=163905)[0m f1_macro: 0.36813707288148856
[2m[36m(func pid=163905)[0m f1_weighted: 0.4009390727734905
[2m[36m(func pid=163905)[0m f1_per_class: [0.371, 0.39, 0.774, 0.416, 0.078, 0.138, 0.504, 0.484, 0.23, 0.298]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.0174 | Steps: 2 | Val loss: 1.8225 | Batch size: 32 | lr: 0.0001 | Duration: 3.25s
[2m[36m(func pid=168639)[0m top1: 0.08488805970149253
[2m[36m(func pid=168639)[0m top5: 0.5890858208955224
[2m[36m(func pid=168639)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=168639)[0m f1_macro: 0.08100714143061197
[2m[36m(func pid=168639)[0m f1_weighted: 0.06659520729282449
[2m[36m(func pid=168639)[0m f1_per_class: [0.221, 0.222, 0.0, 0.007, 0.081, 0.02, 0.036, 0.064, 0.129, 0.03]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1478544776119403
[2m[36m(func pid=169477)[0m top5: 0.5834888059701493
[2m[36m(func pid=169477)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=169477)[0m f1_macro: 0.05672185382643589
[2m[36m(func pid=169477)[0m f1_weighted: 0.07166106714649613
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.317, 0.085, 0.031, 0.0, 0.0, 0.0, 0.134, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m top1: 0.3521455223880597
[2m[36m(func pid=158211)[0m top5: 0.8614738805970149
[2m[36m(func pid=158211)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=158211)[0m f1_macro: 0.33060082241379324
[2m[36m(func pid=158211)[0m f1_weighted: 0.3787571657514112
[2m[36m(func pid=158211)[0m f1_per_class: [0.232, 0.325, 0.649, 0.439, 0.074, 0.395, 0.38, 0.429, 0.161, 0.222]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0122 | Steps: 2 | Val loss: 2.4724 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9303 | Steps: 2 | Val loss: 12.0265 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5755 | Steps: 2 | Val loss: 197.4695 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:45:00 (running for 00:35:14.75)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.017 |      0.331 |                   71 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.012 |      0.359 |                   50 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.516 |      0.081 |                   28 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.607 |      0.057 |                   25 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37919776119402987
[2m[36m(func pid=163905)[0m top5: 0.8833955223880597
[2m[36m(func pid=163905)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=163905)[0m f1_macro: 0.358707367819374
[2m[36m(func pid=163905)[0m f1_weighted: 0.3961619809302066
[2m[36m(func pid=163905)[0m f1_per_class: [0.375, 0.395, 0.774, 0.408, 0.074, 0.072, 0.529, 0.426, 0.233, 0.303]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.11240671641791045
[2m[36m(func pid=168639)[0m top5: 0.6175373134328358
[2m[36m(func pid=168639)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=168639)[0m f1_macro: 0.06272537263433225
[2m[36m(func pid=168639)[0m f1_weighted: 0.06995152477013468
[2m[36m(func pid=168639)[0m f1_per_class: [0.026, 0.352, 0.0, 0.0, 0.067, 0.0, 0.012, 0.0, 0.125, 0.045]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.13759328358208955
[2m[36m(func pid=169477)[0m top5: 0.6217350746268657
[2m[36m(func pid=169477)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=169477)[0m f1_macro: 0.06049333768134495
[2m[36m(func pid=169477)[0m f1_weighted: 0.08248507859814912
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.324, 0.045, 0.058, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.0462 | Steps: 2 | Val loss: 1.8212 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=158211)[0m top1: 0.3512126865671642
[2m[36m(func pid=158211)[0m top5: 0.8666044776119403
[2m[36m(func pid=158211)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=158211)[0m f1_macro: 0.3293317553462006
[2m[36m(func pid=158211)[0m f1_weighted: 0.37921102620914543
[2m[36m(func pid=158211)[0m f1_per_class: [0.224, 0.339, 0.649, 0.435, 0.077, 0.381, 0.383, 0.425, 0.167, 0.213]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5168 | Steps: 2 | Val loss: 11.4033 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0278 | Steps: 2 | Val loss: 2.5276 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7591 | Steps: 2 | Val loss: 207.4551 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=168639)[0m top1: 0.16651119402985073
[2m[36m(func pid=168639)[0m top5: 0.5517723880597015
[2m[36m(func pid=168639)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=168639)[0m f1_macro: 0.10172685803088784
[2m[36m(func pid=168639)[0m f1_weighted: 0.124406309616424
[2m[36m(func pid=168639)[0m f1_per_class: [0.062, 0.394, 0.167, 0.026, 0.062, 0.0, 0.141, 0.0, 0.125, 0.041]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:45:06 (running for 00:35:20.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  1.046 |      0.329 |                   72 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.012 |      0.359 |                   50 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.517 |      0.102 |                   30 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.575 |      0.06  |                   26 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37966417910447764
[2m[36m(func pid=163905)[0m top5: 0.8745335820895522
[2m[36m(func pid=163905)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=163905)[0m f1_macro: 0.34741943304524253
[2m[36m(func pid=163905)[0m f1_weighted: 0.3967665987332469
[2m[36m(func pid=163905)[0m f1_per_class: [0.359, 0.395, 0.733, 0.405, 0.073, 0.059, 0.548, 0.386, 0.242, 0.274]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.14132462686567165
[2m[36m(func pid=169477)[0m top5: 0.6590485074626866
[2m[36m(func pid=169477)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=169477)[0m f1_macro: 0.07159695730885224
[2m[36m(func pid=169477)[0m f1_weighted: 0.0882907567871333
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.339, 0.04, 0.06, 0.0, 0.0, 0.0, 0.215, 0.0, 0.062]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9930 | Steps: 2 | Val loss: 1.8171 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4999 | Steps: 2 | Val loss: 14.3097 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=158211)[0m top1: 0.3512126865671642
[2m[36m(func pid=158211)[0m top5: 0.867070895522388
[2m[36m(func pid=158211)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=158211)[0m f1_macro: 0.3298003710118328
[2m[36m(func pid=158211)[0m f1_weighted: 0.3803105736276744
[2m[36m(func pid=158211)[0m f1_per_class: [0.221, 0.338, 0.667, 0.442, 0.074, 0.376, 0.386, 0.411, 0.164, 0.22]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0123 | Steps: 2 | Val loss: 2.5594 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6190 | Steps: 2 | Val loss: 189.2519 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 04:45:11 (running for 00:35:25.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.993 |      0.33  |                   73 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.028 |      0.347 |                   51 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.5   |      0.078 |                   31 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.759 |      0.072 |                   27 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.16324626865671643
[2m[36m(func pid=168639)[0m top5: 0.5550373134328358
[2m[36m(func pid=168639)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=168639)[0m f1_macro: 0.07823896908009849
[2m[36m(func pid=168639)[0m f1_weighted: 0.09897379604468855
[2m[36m(func pid=168639)[0m f1_per_class: [0.046, 0.376, 0.18, 0.108, 0.0, 0.0, 0.003, 0.0, 0.02, 0.05]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.14365671641791045
[2m[36m(func pid=169477)[0m top5: 0.6660447761194029
[2m[36m(func pid=169477)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.0744363625004774
[2m[36m(func pid=169477)[0m f1_weighted: 0.09400203178166647
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.358, 0.039, 0.068, 0.0, 0.0, 0.0, 0.214, 0.0, 0.065]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.37453358208955223
[2m[36m(func pid=163905)[0m top5: 0.8759328358208955
[2m[36m(func pid=163905)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=163905)[0m f1_macro: 0.3481491574246528
[2m[36m(func pid=163905)[0m f1_weighted: 0.3923554970356905
[2m[36m(func pid=163905)[0m f1_per_class: [0.347, 0.398, 0.786, 0.396, 0.072, 0.059, 0.537, 0.408, 0.225, 0.254]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9880 | Steps: 2 | Val loss: 1.8137 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=158211)[0m top1: 0.34468283582089554
[2m[36m(func pid=158211)[0m top5: 0.871268656716418
[2m[36m(func pid=158211)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=158211)[0m f1_macro: 0.3311721546688696
[2m[36m(func pid=158211)[0m f1_weighted: 0.3759195307363069
[2m[36m(func pid=158211)[0m f1_per_class: [0.206, 0.32, 0.686, 0.421, 0.078, 0.377, 0.396, 0.442, 0.161, 0.225]
[2m[36m(func pid=158211)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.7556 | Steps: 2 | Val loss: 184.0923 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0386 | Steps: 2 | Val loss: 2.5629 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.0255 | Steps: 2 | Val loss: 28.8344 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 04:45:17 (running for 00:35:31.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.336
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00016 | RUNNING    | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.988 |      0.331 |                   74 |
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.012 |      0.348 |                   52 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.5   |      0.078 |                   31 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.756 |      0.079 |                   29 |
| train_35a0b_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.14692164179104478
[2m[36m(func pid=169477)[0m top5: 0.6646455223880597
[2m[36m(func pid=169477)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=169477)[0m f1_macro: 0.0792192283855561
[2m[36m(func pid=169477)[0m f1_weighted: 0.10273485048563745
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.363, 0.039, 0.086, 0.0, 0.024, 0.0, 0.216, 0.0, 0.065]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3726679104477612
[2m[36m(func pid=163905)[0m top5: 0.8759328358208955
[2m[36m(func pid=163905)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=163905)[0m f1_macro: 0.34781125037545413
[2m[36m(func pid=163905)[0m f1_weighted: 0.3913372861221041
[2m[36m(func pid=163905)[0m f1_per_class: [0.346, 0.394, 0.786, 0.403, 0.074, 0.058, 0.529, 0.419, 0.218, 0.25]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.16138059701492538
[2m[36m(func pid=168639)[0m top5: 0.7215485074626866
[2m[36m(func pid=168639)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=168639)[0m f1_macro: 0.07946073652895241
[2m[36m(func pid=168639)[0m f1_weighted: 0.07903156278507412
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.371, 0.211, 0.007, 0.0, 0.0, 0.003, 0.175, 0.028, 0.0]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=158211)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9627 | Steps: 2 | Val loss: 1.8069 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.5694 | Steps: 2 | Val loss: 69.5077 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0140 | Steps: 2 | Val loss: 2.5569 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.0494 | Steps: 2 | Val loss: 29.2381 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=158211)[0m top1: 0.345615671641791
[2m[36m(func pid=158211)[0m top5: 0.8759328358208955
[2m[36m(func pid=158211)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=158211)[0m f1_macro: 0.3349273180780769
[2m[36m(func pid=158211)[0m f1_weighted: 0.3777978870327582
[2m[36m(func pid=158211)[0m f1_per_class: [0.209, 0.314, 0.706, 0.417, 0.082, 0.379, 0.406, 0.449, 0.159, 0.229]
[2m[36m(func pid=169477)[0m top1: 0.18610074626865672
[2m[36m(func pid=169477)[0m top5: 0.7243470149253731
[2m[36m(func pid=169477)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=169477)[0m f1_macro: 0.09997310343932175
[2m[36m(func pid=169477)[0m f1_weighted: 0.12913018428341033
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.384, 0.064, 0.163, 0.0, 0.024, 0.0, 0.222, 0.0, 0.143]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.37033582089552236
[2m[36m(func pid=163905)[0m top5: 0.8745335820895522
[2m[36m(func pid=163905)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=163905)[0m f1_macro: 0.3520086653213301
[2m[36m(func pid=163905)[0m f1_weighted: 0.39067697015526415
[2m[36m(func pid=163905)[0m f1_per_class: [0.346, 0.389, 0.786, 0.407, 0.078, 0.083, 0.511, 0.444, 0.226, 0.25]
[2m[36m(func pid=168639)[0m top1: 0.1525186567164179
[2m[36m(func pid=168639)[0m top5: 0.8535447761194029
[2m[36m(func pid=168639)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=168639)[0m f1_macro: 0.07376361327559941
[2m[36m(func pid=168639)[0m f1_weighted: 0.0771353557122801
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.349, 0.125, 0.016, 0.0, 0.0, 0.003, 0.173, 0.0, 0.071]
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.7435 | Steps: 2 | Val loss: 53.5233 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=169477)[0m top1: 0.20615671641791045
[2m[36m(func pid=169477)[0m top5: 0.738339552238806
[2m[36m(func pid=169477)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.11252391232569732
[2m[36m(func pid=169477)[0m f1_weighted: 0.1638569601364693
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.382, 0.07, 0.283, 0.0, 0.039, 0.0, 0.222, 0.0, 0.129]
== Status ==
Current time: 2024-01-07 04:45:22 (running for 00:35:36.58)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.039 |      0.348 |                   53 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.025 |      0.079 |                   32 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  3.569 |      0.1   |                   30 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 04:45:29 (running for 00:35:43.81)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.039 |      0.348 |                   53 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.025 |      0.079 |                   32 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.744 |      0.113 |                   31 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=176361)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=176361)[0m Configuration completed!
[2m[36m(func pid=176361)[0m New optimizer parameters:
[2m[36m(func pid=176361)[0m SGD (
[2m[36m(func pid=176361)[0m Parameter Group 0
[2m[36m(func pid=176361)[0m     dampening: 0
[2m[36m(func pid=176361)[0m     differentiable: False
[2m[36m(func pid=176361)[0m     foreach: None
[2m[36m(func pid=176361)[0m     lr: 0.0001
[2m[36m(func pid=176361)[0m     maximize: False
[2m[36m(func pid=176361)[0m     momentum: 0.9
[2m[36m(func pid=176361)[0m     nesterov: False
[2m[36m(func pid=176361)[0m     weight_decay: 1e-05
[2m[36m(func pid=176361)[0m )
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0500 | Steps: 2 | Val loss: 2.5499 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.1272 | Steps: 2 | Val loss: 28.5314 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.6885 | Steps: 2 | Val loss: 41.1928 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9702 | Steps: 2 | Val loss: 2.3183 | Batch size: 32 | lr: 0.0001 | Duration: 4.82s
== Status ==
Current time: 2024-01-07 04:45:34 (running for 00:35:48.83)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.014 |      0.352 |                   54 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.049 |      0.074 |                   33 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.744 |      0.113 |                   31 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37546641791044777
[2m[36m(func pid=163905)[0m top5: 0.8810634328358209
[2m[36m(func pid=163905)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=163905)[0m f1_macro: 0.3579238039467297
[2m[36m(func pid=163905)[0m f1_weighted: 0.39929519408836667
[2m[36m(func pid=163905)[0m f1_per_class: [0.323, 0.396, 0.759, 0.422, 0.087, 0.167, 0.487, 0.469, 0.229, 0.242]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.14458955223880596
[2m[36m(func pid=168639)[0m top5: 0.8339552238805971
[2m[36m(func pid=168639)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=168639)[0m f1_macro: 0.06100119456298505
[2m[36m(func pid=168639)[0m f1_weighted: 0.07759023157368235
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.315, 0.0, 0.041, 0.0, 0.0, 0.003, 0.178, 0.0, 0.074]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.21875
[2m[36m(func pid=169477)[0m top5: 0.7467350746268657
[2m[36m(func pid=169477)[0m f1_micro: 0.21875
[2m[36m(func pid=169477)[0m f1_macro: 0.1216564261875924
[2m[36m(func pid=169477)[0m f1_weighted: 0.18147523296315243
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.373, 0.083, 0.327, 0.0, 0.09, 0.003, 0.226, 0.0, 0.114]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1767723880597015
[2m[36m(func pid=176361)[0m top5: 0.5382462686567164
[2m[36m(func pid=176361)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=176361)[0m f1_macro: 0.11961656392229832
[2m[36m(func pid=176361)[0m f1_weighted: 0.12695119589705195
[2m[36m(func pid=176361)[0m f1_per_class: [0.3, 0.348, 0.0, 0.092, 0.0, 0.219, 0.024, 0.013, 0.0, 0.2]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0230 | Steps: 2 | Val loss: 2.6066 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6112 | Steps: 2 | Val loss: 24.8263 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.6629 | Steps: 2 | Val loss: 34.4348 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9651 | Steps: 2 | Val loss: 2.3218 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 04:45:40 (running for 00:35:54.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.05  |      0.358 |                   55 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.127 |      0.061 |                   34 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.663 |      0.125 |                   33 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.97  |      0.12  |                    1 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.22434701492537312
[2m[36m(func pid=169477)[0m top5: 0.7527985074626866
[2m[36m(func pid=169477)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=169477)[0m f1_macro: 0.12503321236838605
[2m[36m(func pid=169477)[0m f1_weighted: 0.1858514579246202
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.373, 0.102, 0.345, 0.0, 0.096, 0.0, 0.217, 0.0, 0.118]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.36100746268656714
[2m[36m(func pid=163905)[0m top5: 0.8791977611940298
[2m[36m(func pid=163905)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=163905)[0m f1_macro: 0.35382082561668793
[2m[36m(func pid=163905)[0m f1_weighted: 0.38263426457464683
[2m[36m(func pid=163905)[0m f1_per_class: [0.312, 0.382, 0.759, 0.421, 0.091, 0.215, 0.417, 0.505, 0.223, 0.214]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.14878731343283583
[2m[36m(func pid=168639)[0m top5: 0.820429104477612
[2m[36m(func pid=168639)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=168639)[0m f1_macro: 0.07535679055270053
[2m[36m(func pid=168639)[0m f1_weighted: 0.08350270908319125
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.311, 0.0, 0.031, 0.095, 0.0, 0.03, 0.179, 0.0, 0.107]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=176361)[0m top1: 0.18330223880597016
[2m[36m(func pid=176361)[0m top5: 0.5303171641791045
[2m[36m(func pid=176361)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=176361)[0m f1_macro: 0.11259748215692915
[2m[36m(func pid=176361)[0m f1_weighted: 0.12921826427058214
[2m[36m(func pid=176361)[0m f1_per_class: [0.243, 0.33, 0.0, 0.1, 0.01, 0.283, 0.012, 0.035, 0.0, 0.111]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0168 | Steps: 2 | Val loss: 2.6940 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 11.5920 | Steps: 2 | Val loss: 32.7148 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.3631 | Steps: 2 | Val loss: 20.7438 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9930 | Steps: 2 | Val loss: 2.3347 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:45:45 (running for 00:36:00.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.023 |      0.354 |                   56 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.611 |      0.075 |                   35 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  | 11.592 |      0.117 |                   34 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.965 |      0.113 |                    2 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3474813432835821
[2m[36m(func pid=163905)[0m top5: 0.8722014925373134
[2m[36m(func pid=163905)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=163905)[0m f1_macro: 0.3471011700468909
[2m[36m(func pid=163905)[0m f1_weighted: 0.36452628154694666
[2m[36m(func pid=163905)[0m f1_per_class: [0.3, 0.376, 0.733, 0.413, 0.105, 0.241, 0.355, 0.52, 0.23, 0.199]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.18983208955223882
[2m[36m(func pid=169477)[0m top5: 0.7532649253731343
[2m[36m(func pid=169477)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=169477)[0m f1_macro: 0.1174369657016557
[2m[36m(func pid=169477)[0m f1_weighted: 0.14183813941605078
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.341, 0.162, 0.221, 0.059, 0.062, 0.0, 0.199, 0.0, 0.131]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.14272388059701493
[2m[36m(func pid=168639)[0m top5: 0.8022388059701493
[2m[36m(func pid=168639)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=168639)[0m f1_macro: 0.09304007384483987
[2m[36m(func pid=168639)[0m f1_weighted: 0.0963727246884362
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.327, 0.067, 0.006, 0.0, 0.0, 0.08, 0.182, 0.026, 0.243]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=176361)[0m top1: 0.17164179104477612
[2m[36m(func pid=176361)[0m top5: 0.5144589552238806
[2m[36m(func pid=176361)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=176361)[0m f1_macro: 0.09431369894666164
[2m[36m(func pid=176361)[0m f1_weighted: 0.12056994267272844
[2m[36m(func pid=176361)[0m f1_per_class: [0.133, 0.292, 0.0, 0.106, 0.011, 0.297, 0.006, 0.021, 0.0, 0.077]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0099 | Steps: 2 | Val loss: 2.7674 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.5909 | Steps: 2 | Val loss: 42.1667 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7517 | Steps: 2 | Val loss: 18.1013 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9470 | Steps: 2 | Val loss: 2.3502 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:45:51 (running for 00:36:05.49)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.01  |      0.341 |                   58 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.363 |      0.093 |                   36 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  | 11.592 |      0.117 |                   34 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.993 |      0.094 |                    3 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3376865671641791
[2m[36m(func pid=163905)[0m top5: 0.8624067164179104
[2m[36m(func pid=163905)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=163905)[0m f1_macro: 0.3414019141974479
[2m[36m(func pid=163905)[0m f1_weighted: 0.35546333447090095
[2m[36m(func pid=163905)[0m f1_per_class: [0.279, 0.354, 0.71, 0.392, 0.102, 0.302, 0.335, 0.527, 0.231, 0.184]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.16417910447761194
[2m[36m(func pid=169477)[0m top5: 0.726679104477612
[2m[36m(func pid=169477)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=169477)[0m f1_macro: 0.10024021337175233
[2m[36m(func pid=169477)[0m f1_weighted: 0.11983962541575024
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.333, 0.107, 0.15, 0.057, 0.066, 0.0, 0.186, 0.0, 0.103]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.12546641791044777
[2m[36m(func pid=168639)[0m top5: 0.7807835820895522
[2m[36m(func pid=168639)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=168639)[0m f1_macro: 0.0807824804628464
[2m[36m(func pid=168639)[0m f1_weighted: 0.09321961323680991
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.319, 0.041, 0.003, 0.0, 0.0, 0.081, 0.191, 0.0, 0.173]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=176361)[0m top1: 0.16324626865671643
[2m[36m(func pid=176361)[0m top5: 0.5004664179104478
[2m[36m(func pid=176361)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=176361)[0m f1_macro: 0.09603401789618064
[2m[36m(func pid=176361)[0m f1_weighted: 0.11960348320141168
[2m[36m(func pid=176361)[0m f1_per_class: [0.177, 0.269, 0.0, 0.103, 0.01, 0.293, 0.018, 0.018, 0.0, 0.071]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0338 | Steps: 2 | Val loss: 2.8401 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.9759 | Steps: 2 | Val loss: 20.0197 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.5246 | Steps: 2 | Val loss: 42.8679 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9430 | Steps: 2 | Val loss: 2.3622 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:45:56 (running for 00:36:10.76)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.034 |      0.342 |                   59 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.752 |      0.081 |                   37 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.591 |      0.1   |                   35 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.947 |      0.096 |                    4 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3292910447761194
[2m[36m(func pid=163905)[0m top5: 0.8544776119402985
[2m[36m(func pid=163905)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=163905)[0m f1_macro: 0.3420193512819432
[2m[36m(func pid=163905)[0m f1_weighted: 0.34470837983768216
[2m[36m(func pid=163905)[0m f1_per_class: [0.263, 0.353, 0.759, 0.369, 0.109, 0.312, 0.315, 0.535, 0.227, 0.178]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m top1: 0.16837686567164178
[2m[36m(func pid=169477)[0m top5: 0.7257462686567164
[2m[36m(func pid=169477)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=169477)[0m f1_macro: 0.11408732409940225
[2m[36m(func pid=169477)[0m f1_weighted: 0.13481198900255345
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.333, 0.1, 0.202, 0.129, 0.066, 0.0, 0.182, 0.0, 0.128]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.10494402985074627
[2m[36m(func pid=168639)[0m top5: 0.761660447761194
[2m[36m(func pid=168639)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=168639)[0m f1_macro: 0.09128145768656058
[2m[36m(func pid=168639)[0m f1_weighted: 0.0888013399281494
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.304, 0.038, 0.003, 0.2, 0.0, 0.078, 0.14, 0.026, 0.123]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=176361)[0m top1: 0.15438432835820895
[2m[36m(func pid=176361)[0m top5: 0.4920708955223881
[2m[36m(func pid=176361)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=176361)[0m f1_macro: 0.07854545718417913
[2m[36m(func pid=176361)[0m f1_weighted: 0.11464866132046034
[2m[36m(func pid=176361)[0m f1_per_class: [0.091, 0.25, 0.0, 0.103, 0.009, 0.294, 0.021, 0.017, 0.0, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0066 | Steps: 2 | Val loss: 2.8922 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6876 | Steps: 2 | Val loss: 21.9203 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7428 | Steps: 2 | Val loss: 36.4478 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9625 | Steps: 2 | Val loss: 2.3644 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:46:01 (running for 00:36:16.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.007 |      0.348 |                   60 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.976 |      0.091 |                   38 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.525 |      0.114 |                   36 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.943 |      0.079 |                    5 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3278917910447761
[2m[36m(func pid=163905)[0m top5: 0.847481343283582
[2m[36m(func pid=163905)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=163905)[0m f1_macro: 0.3480212533319048
[2m[36m(func pid=163905)[0m f1_weighted: 0.3430938695470207
[2m[36m(func pid=163905)[0m f1_per_class: [0.277, 0.343, 0.8, 0.364, 0.117, 0.308, 0.316, 0.556, 0.238, 0.162]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.10494402985074627
[2m[36m(func pid=168639)[0m top5: 0.7280783582089553
[2m[36m(func pid=168639)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=168639)[0m f1_macro: 0.10698580789334669
[2m[36m(func pid=168639)[0m f1_weighted: 0.07503431341688133
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.295, 0.038, 0.013, 0.385, 0.0, 0.018, 0.158, 0.046, 0.117]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.20009328358208955
[2m[36m(func pid=169477)[0m top5: 0.7434701492537313
[2m[36m(func pid=169477)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=169477)[0m f1_macro: 0.12999015478601145
[2m[36m(func pid=169477)[0m f1_weighted: 0.17716931634473881
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.336, 0.15, 0.332, 0.083, 0.065, 0.017, 0.194, 0.0, 0.122]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1478544776119403
[2m[36m(func pid=176361)[0m top5: 0.48507462686567165
[2m[36m(func pid=176361)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=176361)[0m f1_macro: 0.0782322793989906
[2m[36m(func pid=176361)[0m f1_weighted: 0.11680134253409488
[2m[36m(func pid=176361)[0m f1_per_class: [0.082, 0.223, 0.0, 0.106, 0.009, 0.289, 0.041, 0.032, 0.0, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4570 | Steps: 2 | Val loss: 22.9157 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0295 | Steps: 2 | Val loss: 2.8826 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.2831 | Steps: 2 | Val loss: 30.5830 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9698 | Steps: 2 | Val loss: 2.3616 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:46:07 (running for 00:36:21.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.029 |      0.358 |                   61 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.688 |      0.107 |                   39 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.743 |      0.13  |                   37 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.962 |      0.078 |                    6 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3400186567164179
[2m[36m(func pid=163905)[0m top5: 0.8526119402985075
[2m[36m(func pid=163905)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=163905)[0m f1_macro: 0.35803032372014254
[2m[36m(func pid=163905)[0m f1_weighted: 0.35909892643049945
[2m[36m(func pid=163905)[0m f1_per_class: [0.308, 0.34, 0.8, 0.389, 0.115, 0.3, 0.344, 0.566, 0.251, 0.165]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.11287313432835822
[2m[36m(func pid=168639)[0m top5: 0.7173507462686567
[2m[36m(func pid=168639)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=168639)[0m f1_macro: 0.08998146963033901
[2m[36m(func pid=168639)[0m f1_weighted: 0.07015784049551828
[2m[36m(func pid=168639)[0m f1_per_class: [0.068, 0.293, 0.05, 0.01, 0.148, 0.0, 0.003, 0.183, 0.045, 0.099]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.19449626865671643
[2m[36m(func pid=169477)[0m top5: 0.7444029850746269
[2m[36m(func pid=169477)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=169477)[0m f1_macro: 0.1295777130585833
[2m[36m(func pid=169477)[0m f1_weighted: 0.16861771834737138
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.352, 0.14, 0.269, 0.1, 0.016, 0.049, 0.217, 0.031, 0.121]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.14598880597014927
[2m[36m(func pid=176361)[0m top5: 0.4939365671641791
[2m[36m(func pid=176361)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=176361)[0m f1_macro: 0.08672540242567671
[2m[36m(func pid=176361)[0m f1_weighted: 0.11717899362622439
[2m[36m(func pid=176361)[0m f1_per_class: [0.077, 0.208, 0.062, 0.108, 0.033, 0.287, 0.046, 0.046, 0.0, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0101 | Steps: 2 | Val loss: 2.8506 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6724 | Steps: 2 | Val loss: 39.5474 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4879 | Steps: 2 | Val loss: 21.5588 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9066 | Steps: 2 | Val loss: 2.3540 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 04:46:12 (running for 00:36:27.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.01  |      0.367 |                   62 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.457 |      0.09  |                   40 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.283 |      0.13  |                   38 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.97  |      0.087 |                    7 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3591417910447761
[2m[36m(func pid=163905)[0m top5: 0.8610074626865671
[2m[36m(func pid=163905)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=163905)[0m f1_macro: 0.36699821049260817
[2m[36m(func pid=163905)[0m f1_weighted: 0.3849245782355492
[2m[36m(func pid=163905)[0m f1_per_class: [0.35, 0.349, 0.8, 0.433, 0.128, 0.296, 0.395, 0.506, 0.254, 0.159]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.1394589552238806
[2m[36m(func pid=168639)[0m top5: 0.7705223880597015
[2m[36m(func pid=168639)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=168639)[0m f1_macro: 0.10611885945269024
[2m[36m(func pid=168639)[0m f1_weighted: 0.07831842798690156
[2m[36m(func pid=168639)[0m f1_per_class: [0.14, 0.291, 0.085, 0.032, 0.154, 0.0, 0.0, 0.191, 0.076, 0.092]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1333955223880597
[2m[36m(func pid=169477)[0m top5: 0.7238805970149254
[2m[36m(func pid=169477)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=169477)[0m f1_macro: 0.08924108397541426
[2m[36m(func pid=169477)[0m f1_weighted: 0.07852964675321814
[2m[36m(func pid=169477)[0m f1_per_class: [0.023, 0.357, 0.06, 0.0, 0.08, 0.0, 0.003, 0.208, 0.035, 0.126]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.14132462686567165
[2m[36m(func pid=176361)[0m top5: 0.5004664179104478
[2m[36m(func pid=176361)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=176361)[0m f1_macro: 0.08332066842142696
[2m[36m(func pid=176361)[0m f1_weighted: 0.11609615481114346
[2m[36m(func pid=176361)[0m f1_per_class: [0.053, 0.186, 0.065, 0.102, 0.024, 0.29, 0.059, 0.054, 0.0, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0106 | Steps: 2 | Val loss: 2.8674 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2093 | Steps: 2 | Val loss: 42.9609 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5332 | Steps: 2 | Val loss: 21.5831 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9353 | Steps: 2 | Val loss: 2.3506 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 04:46:18 (running for 00:36:32.55)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.011 |      0.37  |                   63 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.488 |      0.106 |                   41 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.672 |      0.089 |                   39 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.907 |      0.083 |                    8 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.3675373134328358
[2m[36m(func pid=163905)[0m top5: 0.8661380597014925
[2m[36m(func pid=163905)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=163905)[0m f1_macro: 0.3696280187011581
[2m[36m(func pid=163905)[0m f1_weighted: 0.40026924029057587
[2m[36m(func pid=163905)[0m f1_per_class: [0.387, 0.354, 0.8, 0.438, 0.107, 0.265, 0.455, 0.464, 0.267, 0.159]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.12453358208955224
[2m[36m(func pid=168639)[0m top5: 0.7625932835820896
[2m[36m(func pid=168639)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=168639)[0m f1_macro: 0.07932831507801705
[2m[36m(func pid=168639)[0m f1_weighted: 0.07709465565750141
[2m[36m(func pid=168639)[0m f1_per_class: [0.136, 0.259, 0.0, 0.044, 0.0, 0.0, 0.012, 0.167, 0.097, 0.078]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.11986940298507463
[2m[36m(func pid=169477)[0m top5: 0.7005597014925373
[2m[36m(func pid=169477)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=169477)[0m f1_macro: 0.08680126141691145
[2m[36m(func pid=169477)[0m f1_weighted: 0.07456383779691689
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.342, 0.046, 0.0, 0.077, 0.0, 0.0, 0.201, 0.04, 0.162]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.14412313432835822
[2m[36m(func pid=176361)[0m top5: 0.5046641791044776
[2m[36m(func pid=176361)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=176361)[0m f1_macro: 0.08464941109887854
[2m[36m(func pid=176361)[0m f1_weighted: 0.12342246719173797
[2m[36m(func pid=176361)[0m f1_per_class: [0.052, 0.181, 0.057, 0.126, 0.016, 0.289, 0.064, 0.061, 0.0, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0067 | Steps: 2 | Val loss: 2.8773 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0292 | Steps: 2 | Val loss: 21.2338 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5090 | Steps: 2 | Val loss: 46.3083 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9059 | Steps: 2 | Val loss: 2.3453 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=163905)[0m top1: 0.3773320895522388
[2m[36m(func pid=163905)[0m top5: 0.8689365671641791
[2m[36m(func pid=163905)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=163905)[0m f1_macro: 0.3712363405964791
[2m[36m(func pid=163905)[0m f1_weighted: 0.41231501555142264
[2m[36m(func pid=163905)[0m f1_per_class: [0.372, 0.355, 0.8, 0.445, 0.101, 0.264, 0.493, 0.452, 0.264, 0.167]
[2m[36m(func pid=163905)[0m 
== Status ==
Current time: 2024-01-07 04:46:23 (running for 00:36:38.09)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.007 |      0.371 |                   64 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.533 |      0.079 |                   42 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.209 |      0.087 |                   40 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.935 |      0.085 |                    9 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.2140858208955224
[2m[36m(func pid=168639)[0m top5: 0.7644589552238806
[2m[36m(func pid=168639)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=168639)[0m f1_macro: 0.13208193626986653
[2m[36m(func pid=168639)[0m f1_weighted: 0.21440619216999202
[2m[36m(func pid=168639)[0m f1_per_class: [0.115, 0.223, 0.0, 0.078, 0.0, 0.0, 0.433, 0.34, 0.081, 0.052]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.09328358208955224
[2m[36m(func pid=169477)[0m top5: 0.6613805970149254
[2m[36m(func pid=169477)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=169477)[0m f1_macro: 0.07325221945782256
[2m[36m(func pid=169477)[0m f1_weighted: 0.049347145233098015
[2m[36m(func pid=169477)[0m f1_per_class: [0.024, 0.189, 0.046, 0.0, 0.085, 0.0, 0.009, 0.18, 0.0, 0.2]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.14365671641791045
[2m[36m(func pid=176361)[0m top5: 0.5083955223880597
[2m[36m(func pid=176361)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=176361)[0m f1_macro: 0.09229523824875818
[2m[36m(func pid=176361)[0m f1_weighted: 0.12479471631293122
[2m[36m(func pid=176361)[0m f1_per_class: [0.061, 0.174, 0.114, 0.132, 0.016, 0.29, 0.064, 0.062, 0.01, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0031 | Steps: 2 | Val loss: 2.9145 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.4523 | Steps: 2 | Val loss: 22.2652 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.5946 | Steps: 2 | Val loss: 43.6729 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8822 | Steps: 2 | Val loss: 2.3363 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 04:46:29 (running for 00:36:43.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.007 |      0.371 |                   64 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.452 |      0.121 |                   44 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.509 |      0.073 |                   41 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.906 |      0.092 |                   10 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163905)[0m top1: 0.37966417910447764
[2m[36m(func pid=163905)[0m top5: 0.8731343283582089
[2m[36m(func pid=163905)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=163905)[0m f1_macro: 0.3689422256600409
[2m[36m(func pid=163905)[0m f1_weighted: 0.4134445261228529
[2m[36m(func pid=163905)[0m f1_per_class: [0.4, 0.36, 0.828, 0.437, 0.102, 0.237, 0.524, 0.392, 0.245, 0.167]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m top1: 0.166044776119403
[2m[36m(func pid=168639)[0m top5: 0.7681902985074627
[2m[36m(func pid=168639)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=168639)[0m f1_macro: 0.12054764217093825
[2m[36m(func pid=168639)[0m f1_weighted: 0.18139768306128398
[2m[36m(func pid=168639)[0m f1_per_class: [0.165, 0.21, 0.0, 0.158, 0.0, 0.0, 0.258, 0.328, 0.051, 0.035]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.08768656716417911
[2m[36m(func pid=169477)[0m top5: 0.636660447761194
[2m[36m(func pid=169477)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=169477)[0m f1_macro: 0.06159593651981195
[2m[36m(func pid=169477)[0m f1_weighted: 0.042773205773225145
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.159, 0.054, 0.0, 0.074, 0.0, 0.009, 0.178, 0.0, 0.143]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.14365671641791045
[2m[36m(func pid=176361)[0m top5: 0.5153917910447762
[2m[36m(func pid=176361)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=176361)[0m f1_macro: 0.09255906560415508
[2m[36m(func pid=176361)[0m f1_weighted: 0.12677972136455712
[2m[36m(func pid=176361)[0m f1_per_class: [0.06, 0.172, 0.105, 0.135, 0.022, 0.29, 0.069, 0.063, 0.01, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0585 | Steps: 2 | Val loss: 25.5773 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0126 | Steps: 2 | Val loss: 2.9517 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.7837 | Steps: 2 | Val loss: 44.4198 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8885 | Steps: 2 | Val loss: 2.3283 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:46:34 (running for 00:36:49.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.003 |      0.369 |                   65 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.058 |      0.072 |                   45 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.595 |      0.062 |                   42 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.882 |      0.093 |                   11 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.07929104477611941
[2m[36m(func pid=168639)[0m top5: 0.7346082089552238
[2m[36m(func pid=168639)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=168639)[0m f1_macro: 0.0717134089062237
[2m[36m(func pid=168639)[0m f1_weighted: 0.09905106812530233
[2m[36m(func pid=168639)[0m f1_per_class: [0.111, 0.191, 0.047, 0.136, 0.0, 0.0, 0.062, 0.087, 0.052, 0.031]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.10261194029850747
[2m[36m(func pid=169477)[0m top5: 0.6478544776119403
[2m[36m(func pid=169477)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=169477)[0m f1_macro: 0.08023753934971466
[2m[36m(func pid=169477)[0m f1_weighted: 0.06567698679854639
[2m[36m(func pid=169477)[0m f1_per_class: [0.01, 0.182, 0.059, 0.003, 0.099, 0.045, 0.045, 0.2, 0.0, 0.159]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.376865671641791
[2m[36m(func pid=163905)[0m top5: 0.8791977611940298
[2m[36m(func pid=163905)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=163905)[0m f1_macro: 0.355539983818979
[2m[36m(func pid=163905)[0m f1_weighted: 0.4052189570901805
[2m[36m(func pid=163905)[0m f1_per_class: [0.381, 0.357, 0.815, 0.419, 0.105, 0.189, 0.545, 0.331, 0.239, 0.173]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=176361)[0m top1: 0.14319029850746268
[2m[36m(func pid=176361)[0m top5: 0.5317164179104478
[2m[36m(func pid=176361)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=176361)[0m f1_macro: 0.09506902779034497
[2m[36m(func pid=176361)[0m f1_weighted: 0.12759327765551406
[2m[36m(func pid=176361)[0m f1_per_class: [0.045, 0.179, 0.146, 0.136, 0.021, 0.289, 0.068, 0.056, 0.01, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1977 | Steps: 2 | Val loss: 27.2175 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6953 | Steps: 2 | Val loss: 38.5358 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0343 | Steps: 2 | Val loss: 3.0273 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8459 | Steps: 2 | Val loss: 2.3222 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:46:40 (running for 00:36:54.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.013 |      0.356 |                   66 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.198 |      0.056 |                   46 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.784 |      0.08  |                   43 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.888 |      0.095 |                   12 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.05783582089552239
[2m[36m(func pid=168639)[0m top5: 0.715018656716418
[2m[36m(func pid=168639)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=168639)[0m f1_macro: 0.055686554254306755
[2m[36m(func pid=168639)[0m f1_weighted: 0.06889819607896028
[2m[36m(func pid=168639)[0m f1_per_class: [0.086, 0.139, 0.031, 0.116, 0.0, 0.0, 0.012, 0.074, 0.065, 0.033]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.16184701492537312
[2m[36m(func pid=169477)[0m top5: 0.7024253731343284
[2m[36m(func pid=169477)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=169477)[0m f1_macro: 0.12006440816715543
[2m[36m(func pid=169477)[0m f1_weighted: 0.1414701779195416
[2m[36m(func pid=169477)[0m f1_per_class: [0.024, 0.31, 0.069, 0.04, 0.069, 0.059, 0.175, 0.242, 0.0, 0.212]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.36613805970149255
[2m[36m(func pid=163905)[0m top5: 0.8680037313432836
[2m[36m(func pid=163905)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=163905)[0m f1_macro: 0.3514076670234393
[2m[36m(func pid=163905)[0m f1_weighted: 0.3974382788925638
[2m[36m(func pid=163905)[0m f1_per_class: [0.389, 0.35, 0.828, 0.408, 0.1, 0.216, 0.529, 0.305, 0.236, 0.153]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1455223880597015
[2m[36m(func pid=176361)[0m top5: 0.5335820895522388
[2m[36m(func pid=176361)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=176361)[0m f1_macro: 0.10534528256939509
[2m[36m(func pid=176361)[0m f1_weighted: 0.13125599236490995
[2m[36m(func pid=176361)[0m f1_per_class: [0.043, 0.185, 0.233, 0.129, 0.019, 0.299, 0.078, 0.057, 0.011, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4387 | Steps: 2 | Val loss: 23.4638 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.6148 | Steps: 2 | Val loss: 36.9521 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0187 | Steps: 2 | Val loss: 3.1800 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8803 | Steps: 2 | Val loss: 2.3135 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:46:45 (running for 00:36:60.00)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.034 |      0.351 |                   67 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.198 |      0.056 |                   46 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.695 |      0.12  |                   44 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.846 |      0.105 |                   13 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.08208955223880597
[2m[36m(func pid=168639)[0m top5: 0.6986940298507462
[2m[36m(func pid=168639)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=168639)[0m f1_macro: 0.06237917588313423
[2m[36m(func pid=168639)[0m f1_weighted: 0.08132101772688861
[2m[36m(func pid=168639)[0m f1_per_class: [0.033, 0.119, 0.035, 0.149, 0.0, 0.0, 0.021, 0.17, 0.06, 0.037]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1912313432835821
[2m[36m(func pid=169477)[0m top5: 0.7234141791044776
[2m[36m(func pid=169477)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=169477)[0m f1_macro: 0.11314809785178212
[2m[36m(func pid=169477)[0m f1_weighted: 0.17166092499139382
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.366, 0.08, 0.137, 0.0, 0.046, 0.166, 0.24, 0.0, 0.095]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=163905)[0m top1: 0.34701492537313433
[2m[36m(func pid=163905)[0m top5: 0.851679104477612
[2m[36m(func pid=163905)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=163905)[0m f1_macro: 0.3392972430855941
[2m[36m(func pid=163905)[0m f1_weighted: 0.384987963569162
[2m[36m(func pid=163905)[0m f1_per_class: [0.388, 0.328, 0.8, 0.414, 0.122, 0.239, 0.496, 0.268, 0.229, 0.11]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=176361)[0m top1: 0.15391791044776118
[2m[36m(func pid=176361)[0m top5: 0.5405783582089553
[2m[36m(func pid=176361)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=176361)[0m f1_macro: 0.10729376083224602
[2m[36m(func pid=176361)[0m f1_weighted: 0.1393003545034134
[2m[36m(func pid=176361)[0m f1_per_class: [0.045, 0.208, 0.196, 0.128, 0.018, 0.313, 0.085, 0.069, 0.011, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0226 | Steps: 2 | Val loss: 22.2409 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.9339 | Steps: 2 | Val loss: 29.3718 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8405 | Steps: 2 | Val loss: 2.3032 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0217 | Steps: 2 | Val loss: 3.3092 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:46:51 (running for 00:37:05.48)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.019 |      0.339 |                   68 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.439 |      0.062 |                   47 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.615 |      0.113 |                   45 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.88  |      0.107 |                   14 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.13805970149253732
[2m[36m(func pid=168639)[0m top5: 0.7336753731343284
[2m[36m(func pid=168639)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=168639)[0m f1_macro: 0.1003539353059345
[2m[36m(func pid=168639)[0m f1_weighted: 0.12647110983734544
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.172, 0.0, 0.228, 0.138, 0.0, 0.04, 0.283, 0.102, 0.041]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.20522388059701493
[2m[36m(func pid=169477)[0m top5: 0.7131529850746269
[2m[36m(func pid=169477)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=169477)[0m f1_macro: 0.12956202987563573
[2m[36m(func pid=169477)[0m f1_weighted: 0.17186254233660375
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.376, 0.13, 0.181, 0.0, 0.08, 0.104, 0.219, 0.026, 0.179]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1646455223880597
[2m[36m(func pid=176361)[0m top5: 0.5499067164179104
[2m[36m(func pid=176361)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=176361)[0m f1_macro: 0.11332286860319736
[2m[36m(func pid=176361)[0m f1_weighted: 0.14879126369077084
[2m[36m(func pid=176361)[0m f1_per_class: [0.047, 0.234, 0.192, 0.139, 0.017, 0.327, 0.084, 0.081, 0.012, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=163905)[0m top1: 0.333955223880597
[2m[36m(func pid=163905)[0m top5: 0.8432835820895522
[2m[36m(func pid=163905)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=163905)[0m f1_macro: 0.3293156687509574
[2m[36m(func pid=163905)[0m f1_weighted: 0.37284780132933387
[2m[36m(func pid=163905)[0m f1_per_class: [0.388, 0.318, 0.8, 0.416, 0.146, 0.24, 0.471, 0.221, 0.201, 0.092]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6239 | Steps: 2 | Val loss: 22.4258 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.4100 | Steps: 2 | Val loss: 26.4156 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8427 | Steps: 2 | Val loss: 2.2952 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0036 | Steps: 2 | Val loss: 3.3921 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 04:46:56 (running for 00:37:10.96)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.022 |      0.329 |                   69 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.023 |      0.1   |                   48 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.934 |      0.13  |                   46 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.841 |      0.113 |                   15 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.1884328358208955
[2m[36m(func pid=169477)[0m top5: 0.6856343283582089
[2m[36m(func pid=169477)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=169477)[0m f1_macro: 0.13971102977708577
[2m[36m(func pid=169477)[0m f1_weighted: 0.1509220687756931
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.32, 0.194, 0.149, 0.111, 0.135, 0.072, 0.213, 0.027, 0.175]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.17444029850746268
[2m[36m(func pid=176361)[0m top5: 0.5564365671641791
[2m[36m(func pid=176361)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=176361)[0m f1_macro: 0.12354315772740847
[2m[36m(func pid=176361)[0m f1_weighted: 0.15643531147267808
[2m[36m(func pid=176361)[0m f1_per_class: [0.051, 0.248, 0.222, 0.143, 0.027, 0.348, 0.087, 0.08, 0.028, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.13992537313432835
[2m[36m(func pid=168639)[0m top5: 0.7322761194029851
[2m[36m(func pid=168639)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=168639)[0m f1_macro: 0.10018273108354694
[2m[36m(func pid=168639)[0m f1_weighted: 0.12586645661532703
[2m[36m(func pid=168639)[0m f1_per_class: [0.0, 0.095, 0.0, 0.239, 0.185, 0.0, 0.074, 0.28, 0.085, 0.043]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m top1: 0.31576492537313433
[2m[36m(func pid=163905)[0m top5: 0.8367537313432836
[2m[36m(func pid=163905)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=163905)[0m f1_macro: 0.31114711574387377
[2m[36m(func pid=163905)[0m f1_weighted: 0.3547867387381397
[2m[36m(func pid=163905)[0m f1_per_class: [0.346, 0.3, 0.8, 0.407, 0.168, 0.23, 0.452, 0.151, 0.182, 0.077]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.3843 | Steps: 2 | Val loss: 24.8332 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8329 | Steps: 2 | Val loss: 2.2901 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0254 | Steps: 2 | Val loss: 22.9050 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0101 | Steps: 2 | Val loss: 3.4604 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:47:02 (running for 00:37:16.45)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.004 |      0.311 |                   70 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.624 |      0.1   |                   49 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.384 |      0.137 |                   48 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.843 |      0.124 |                   16 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.16744402985074627
[2m[36m(func pid=169477)[0m top5: 0.675839552238806
[2m[36m(func pid=169477)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=169477)[0m f1_macro: 0.1367269089759371
[2m[36m(func pid=169477)[0m f1_weighted: 0.12602764057479435
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.281, 0.3, 0.099, 0.135, 0.093, 0.074, 0.216, 0.0, 0.168]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.17257462686567165
[2m[36m(func pid=176361)[0m top5: 0.5643656716417911
[2m[36m(func pid=176361)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=176361)[0m f1_macro: 0.12424117692628447
[2m[36m(func pid=176361)[0m f1_weighted: 0.1545554893877584
[2m[36m(func pid=176361)[0m f1_per_class: [0.049, 0.256, 0.241, 0.13, 0.021, 0.349, 0.088, 0.078, 0.03, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.11753731343283583
[2m[36m(func pid=168639)[0m top5: 0.7080223880597015
[2m[36m(func pid=168639)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=168639)[0m f1_macro: 0.07722002867697339
[2m[36m(func pid=168639)[0m f1_weighted: 0.11432926895126591
[2m[36m(func pid=168639)[0m f1_per_class: [0.043, 0.015, 0.0, 0.225, 0.096, 0.0, 0.111, 0.239, 0.0, 0.044]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3101679104477612
[2m[36m(func pid=163905)[0m top5: 0.8288246268656716
[2m[36m(func pid=163905)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=163905)[0m f1_macro: 0.30844043442783886
[2m[36m(func pid=163905)[0m f1_weighted: 0.3505059042798683
[2m[36m(func pid=163905)[0m f1_per_class: [0.344, 0.281, 0.8, 0.404, 0.165, 0.237, 0.45, 0.139, 0.192, 0.074]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.4759 | Steps: 2 | Val loss: 23.2957 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7890 | Steps: 2 | Val loss: 2.2872 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5116 | Steps: 2 | Val loss: 22.5828 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0051 | Steps: 2 | Val loss: 3.4647 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 04:47:07 (running for 00:37:21.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.01  |      0.308 |                   71 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.025 |      0.077 |                   50 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.476 |      0.121 |                   49 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.833 |      0.124 |                   17 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.15485074626865672
[2m[36m(func pid=169477)[0m top5: 0.6856343283582089
[2m[36m(func pid=169477)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=169477)[0m f1_macro: 0.12141664819037254
[2m[36m(func pid=169477)[0m f1_weighted: 0.11024094687173161
[2m[36m(func pid=169477)[0m f1_per_class: [0.015, 0.262, 0.305, 0.019, 0.066, 0.008, 0.137, 0.232, 0.0, 0.17]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.17350746268656717
[2m[36m(func pid=176361)[0m top5: 0.5690298507462687
[2m[36m(func pid=176361)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=176361)[0m f1_macro: 0.1267100766080688
[2m[36m(func pid=176361)[0m f1_weighted: 0.15488731792407187
[2m[36m(func pid=176361)[0m f1_per_class: [0.066, 0.263, 0.246, 0.12, 0.015, 0.35, 0.09, 0.102, 0.016, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.10727611940298508
[2m[36m(func pid=168639)[0m top5: 0.7024253731343284
[2m[36m(func pid=168639)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=168639)[0m f1_macro: 0.07250277074650742
[2m[36m(func pid=168639)[0m f1_weighted: 0.12623692341200604
[2m[36m(func pid=168639)[0m f1_per_class: [0.043, 0.01, 0.0, 0.255, 0.074, 0.0, 0.143, 0.146, 0.014, 0.04]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m top1: 0.30970149253731344
[2m[36m(func pid=163905)[0m top5: 0.8250932835820896
[2m[36m(func pid=163905)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=163905)[0m f1_macro: 0.30664891473358097
[2m[36m(func pid=163905)[0m f1_weighted: 0.3494189453065797
[2m[36m(func pid=163905)[0m f1_per_class: [0.33, 0.277, 0.8, 0.399, 0.165, 0.238, 0.454, 0.137, 0.191, 0.076]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.9939 | Steps: 2 | Val loss: 21.0647 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8074 | Steps: 2 | Val loss: 2.2812 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.9039 | Steps: 2 | Val loss: 20.1944 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0138 | Steps: 2 | Val loss: 3.4415 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:47:12 (running for 00:37:26.89)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.005 |      0.307 |                   72 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.512 |      0.073 |                   51 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.476 |      0.121 |                   49 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.807 |      0.129 |                   19 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.17257462686567165
[2m[36m(func pid=176361)[0m top5: 0.582089552238806
[2m[36m(func pid=176361)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=176361)[0m f1_macro: 0.1292346621464302
[2m[36m(func pid=176361)[0m f1_weighted: 0.15437358302332474
[2m[36m(func pid=176361)[0m f1_per_class: [0.081, 0.262, 0.259, 0.12, 0.015, 0.353, 0.087, 0.1, 0.015, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.18423507462686567
[2m[36m(func pid=169477)[0m top5: 0.7047574626865671
[2m[36m(func pid=169477)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=169477)[0m f1_macro: 0.1399935761400491
[2m[36m(func pid=169477)[0m f1_weighted: 0.1437552393698048
[2m[36m(func pid=169477)[0m f1_per_class: [0.015, 0.245, 0.409, 0.022, 0.03, 0.0, 0.25, 0.278, 0.0, 0.152]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.15671641791044777
[2m[36m(func pid=168639)[0m top5: 0.7173507462686567
[2m[36m(func pid=168639)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=168639)[0m f1_macro: 0.10142050763236019
[2m[36m(func pid=168639)[0m f1_weighted: 0.18640229153506757
[2m[36m(func pid=168639)[0m f1_per_class: [0.043, 0.061, 0.0, 0.31, 0.058, 0.0, 0.247, 0.227, 0.025, 0.043]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3087686567164179
[2m[36m(func pid=163905)[0m top5: 0.832089552238806
[2m[36m(func pid=163905)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=163905)[0m f1_macro: 0.30354308195817997
[2m[36m(func pid=163905)[0m f1_weighted: 0.3471748535471114
[2m[36m(func pid=163905)[0m f1_per_class: [0.322, 0.266, 0.8, 0.39, 0.156, 0.242, 0.461, 0.135, 0.187, 0.076]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8522 | Steps: 2 | Val loss: 18.4658 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7903 | Steps: 2 | Val loss: 2.2778 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0490 | Steps: 2 | Val loss: 19.5861 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0764 | Steps: 2 | Val loss: 3.4599 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:47:18 (running for 00:37:32.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.014 |      0.304 |                   73 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.904 |      0.101 |                   52 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.994 |      0.14  |                   50 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.79  |      0.129 |                   20 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.17350746268656717
[2m[36m(func pid=176361)[0m top5: 0.5890858208955224
[2m[36m(func pid=176361)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=176361)[0m f1_macro: 0.1291531676159696
[2m[36m(func pid=176361)[0m f1_weighted: 0.1589773675002867
[2m[36m(func pid=176361)[0m f1_per_class: [0.081, 0.258, 0.246, 0.135, 0.02, 0.346, 0.094, 0.098, 0.015, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.21921641791044777
[2m[36m(func pid=169477)[0m top5: 0.7168843283582089
[2m[36m(func pid=169477)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=169477)[0m f1_macro: 0.16874396396999286
[2m[36m(func pid=169477)[0m f1_weighted: 0.18714782556181997
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.267, 0.474, 0.138, 0.102, 0.0, 0.271, 0.287, 0.0, 0.149]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.22434701492537312
[2m[36m(func pid=168639)[0m top5: 0.7691231343283582
[2m[36m(func pid=168639)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=168639)[0m f1_macro: 0.13441770201415762
[2m[36m(func pid=168639)[0m f1_weighted: 0.2424353556005122
[2m[36m(func pid=168639)[0m f1_per_class: [0.082, 0.078, 0.0, 0.304, 0.0, 0.008, 0.389, 0.425, 0.015, 0.043]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m top1: 0.3101679104477612
[2m[36m(func pid=163905)[0m top5: 0.8292910447761194
[2m[36m(func pid=163905)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=163905)[0m f1_macro: 0.3086377235772876
[2m[36m(func pid=163905)[0m f1_weighted: 0.3491070000903031
[2m[36m(func pid=163905)[0m f1_per_class: [0.316, 0.267, 0.828, 0.383, 0.151, 0.249, 0.464, 0.17, 0.179, 0.079]
[2m[36m(func pid=163905)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6272 | Steps: 2 | Val loss: 17.4347 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7722 | Steps: 2 | Val loss: 2.2724 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4961 | Steps: 2 | Val loss: 20.6684 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=163905)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0209 | Steps: 2 | Val loss: 3.3632 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 04:47:23 (running for 00:37:37.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00017 | RUNNING    | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.076 |      0.309 |                   74 |
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  1.049 |      0.134 |                   53 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.852 |      0.169 |                   51 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.772 |      0.131 |                   21 |
| train_35a0b_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.1767723880597015
[2m[36m(func pid=169477)[0m top5: 0.7201492537313433
[2m[36m(func pid=169477)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=169477)[0m f1_macro: 0.142714854147949
[2m[36m(func pid=169477)[0m f1_weighted: 0.13385239626015916
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.306, 0.457, 0.072, 0.076, 0.0, 0.14, 0.233, 0.044, 0.099]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.17723880597014927
[2m[36m(func pid=176361)[0m top5: 0.5956156716417911
[2m[36m(func pid=176361)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=176361)[0m f1_macro: 0.13149024314459581
[2m[36m(func pid=176361)[0m f1_weighted: 0.16582643831568347
[2m[36m(func pid=176361)[0m f1_per_class: [0.076, 0.26, 0.259, 0.148, 0.015, 0.349, 0.104, 0.089, 0.014, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.240205223880597
[2m[36m(func pid=168639)[0m top5: 0.777518656716418
[2m[36m(func pid=168639)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=168639)[0m f1_macro: 0.1397148746149619
[2m[36m(func pid=168639)[0m f1_weighted: 0.26076204458132807
[2m[36m(func pid=168639)[0m f1_per_class: [0.049, 0.05, 0.0, 0.257, 0.0, 0.04, 0.496, 0.463, 0.0, 0.043]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=163905)[0m top1: 0.31949626865671643
[2m[36m(func pid=163905)[0m top5: 0.8372201492537313
[2m[36m(func pid=163905)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=163905)[0m f1_macro: 0.3094959531576842
[2m[36m(func pid=163905)[0m f1_weighted: 0.3566499138538153
[2m[36m(func pid=163905)[0m f1_per_class: [0.313, 0.26, 0.8, 0.386, 0.132, 0.249, 0.488, 0.186, 0.186, 0.094]
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7796 | Steps: 2 | Val loss: 2.2679 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.7600 | Steps: 2 | Val loss: 16.0749 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3074 | Steps: 2 | Val loss: 22.0542 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=176361)[0m top1: 0.17537313432835822
[2m[36m(func pid=176361)[0m top5: 0.6086753731343284
[2m[36m(func pid=176361)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=176361)[0m f1_macro: 0.13574981756059207
[2m[36m(func pid=176361)[0m f1_weighted: 0.16707186743536884
[2m[36m(func pid=176361)[0m f1_per_class: [0.083, 0.247, 0.291, 0.152, 0.02, 0.346, 0.111, 0.095, 0.012, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.18236940298507462
[2m[36m(func pid=169477)[0m top5: 0.7486007462686567
[2m[36m(func pid=169477)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=169477)[0m f1_macro: 0.1415029170118142
[2m[36m(func pid=169477)[0m f1_weighted: 0.12551214905015243
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.371, 0.485, 0.121, 0.045, 0.037, 0.025, 0.199, 0.0, 0.133]
[2m[36m(func pid=168639)[0m top1: 0.21875
[2m[36m(func pid=168639)[0m top5: 0.7728544776119403
[2m[36m(func pid=168639)[0m f1_micro: 0.21875
[2m[36m(func pid=168639)[0m f1_macro: 0.11397522615697124
[2m[36m(func pid=168639)[0m f1_weighted: 0.2393311770534014
[2m[36m(func pid=168639)[0m f1_per_class: [0.035, 0.052, 0.0, 0.216, 0.0, 0.047, 0.501, 0.245, 0.0, 0.043]
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7257 | Steps: 2 | Val loss: 2.2670 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:47:29 (running for 00:37:43.48)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.496 |      0.14  |                   54 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.627 |      0.143 |                   52 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.78  |      0.136 |                   22 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=181607)[0m Dataloader to compute accuracy: val

[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=181607)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=181607)[0m Configuration completed!
[2m[36m(func pid=181607)[0m New optimizer parameters:
[2m[36m(func pid=181607)[0m SGD (
[2m[36m(func pid=181607)[0m Parameter Group 0
[2m[36m(func pid=181607)[0m     dampening: 0
[2m[36m(func pid=181607)[0m     differentiable: False
[2m[36m(func pid=181607)[0m     foreach: None
[2m[36m(func pid=181607)[0m     lr: 0.001
[2m[36m(func pid=181607)[0m     maximize: False
[2m[36m(func pid=181607)[0m     momentum: 0.9
[2m[36m(func pid=181607)[0m     nesterov: False
[2m[36m(func pid=181607)[0m     weight_decay: 1e-05
[2m[36m(func pid=181607)[0m )
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m 
== Status ==
Current time: 2024-01-07 04:47:34 (running for 00:37:49.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.307 |      0.114 |                   55 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.76  |      0.142 |                   53 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.726 |      0.137 |                   23 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.17490671641791045
[2m[36m(func pid=176361)[0m top5: 0.6128731343283582
[2m[36m(func pid=176361)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=176361)[0m f1_macro: 0.13667489990616627
[2m[36m(func pid=176361)[0m f1_weighted: 0.1729899127840801
[2m[36m(func pid=176361)[0m f1_per_class: [0.075, 0.227, 0.296, 0.17, 0.02, 0.341, 0.128, 0.098, 0.012, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3997 | Steps: 2 | Val loss: 21.8694 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 7.7805 | Steps: 2 | Val loss: 15.7609 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9787 | Steps: 2 | Val loss: 2.3345 | Batch size: 32 | lr: 0.001 | Duration: 4.40s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7662 | Steps: 2 | Val loss: 2.2665 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=168639)[0m top1: 0.22994402985074627
[2m[36m(func pid=168639)[0m top5: 0.7933768656716418
[2m[36m(func pid=168639)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=168639)[0m f1_macro: 0.12198936736223039
[2m[36m(func pid=168639)[0m f1_weighted: 0.245205610017085
[2m[36m(func pid=168639)[0m f1_per_class: [0.038, 0.086, 0.0, 0.17, 0.0, 0.061, 0.529, 0.291, 0.0, 0.043]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.20615671641791045
[2m[36m(func pid=169477)[0m top5: 0.7821828358208955
[2m[36m(func pid=169477)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.16353079100256324
[2m[36m(func pid=169477)[0m f1_weighted: 0.16149870990590864
[2m[36m(func pid=169477)[0m f1_per_class: [0.0, 0.388, 0.485, 0.22, 0.0, 0.132, 0.006, 0.192, 0.0, 0.213]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1730410447761194
[2m[36m(func pid=176361)[0m top5: 0.621268656716418
[2m[36m(func pid=176361)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=176361)[0m f1_macro: 0.1343445343669366
[2m[36m(func pid=176361)[0m f1_weighted: 0.17564738837241828
[2m[36m(func pid=176361)[0m f1_per_class: [0.069, 0.22, 0.276, 0.184, 0.021, 0.331, 0.132, 0.101, 0.01, 0.0]
[2m[36m(func pid=181607)[0m top1: 0.1599813432835821
[2m[36m(func pid=181607)[0m top5: 0.511660447761194
[2m[36m(func pid=181607)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=181607)[0m f1_macro: 0.10889286292937435
[2m[36m(func pid=181607)[0m f1_weighted: 0.11514721631257688
[2m[36m(func pid=181607)[0m f1_per_class: [0.299, 0.307, 0.0, 0.085, 0.0, 0.225, 0.015, 0.011, 0.0, 0.147]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:47:40 (running for 00:37:54.36)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.4   |      0.122 |                   56 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  7.78  |      0.164 |                   54 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.726 |      0.137 |                   23 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.979 |      0.109 |                    1 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1594 | Steps: 2 | Val loss: 20.4169 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 4.6636 | Steps: 2 | Val loss: 12.9462 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7477 | Steps: 2 | Val loss: 2.2688 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9679 | Steps: 2 | Val loss: 2.3858 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=169477)[0m top1: 0.19449626865671643
[2m[36m(func pid=169477)[0m top5: 0.808768656716418
[2m[36m(func pid=169477)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=169477)[0m f1_macro: 0.1718777358524501
[2m[36m(func pid=169477)[0m f1_weighted: 0.1666115475499699
[2m[36m(func pid=169477)[0m f1_per_class: [0.04, 0.343, 0.522, 0.265, 0.0, 0.132, 0.003, 0.191, 0.0, 0.222]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.24720149253731344
[2m[36m(func pid=168639)[0m top5: 0.8115671641791045
[2m[36m(func pid=168639)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=168639)[0m f1_macro: 0.1470059906832492
[2m[36m(func pid=168639)[0m f1_weighted: 0.2626329786163086
[2m[36m(func pid=168639)[0m f1_per_class: [0.04, 0.132, 0.0, 0.162, 0.0, 0.088, 0.522, 0.481, 0.0, 0.045]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:47:45 (running for 00:37:59.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.159 |      0.147 |                   57 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  4.664 |      0.172 |                   55 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.748 |      0.134 |                   25 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.979 |      0.109 |                    1 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.17024253731343283
[2m[36m(func pid=176361)[0m top5: 0.6203358208955224
[2m[36m(func pid=176361)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=176361)[0m f1_macro: 0.13433924898613914
[2m[36m(func pid=176361)[0m f1_weighted: 0.17477566764845018
[2m[36m(func pid=176361)[0m f1_per_class: [0.067, 0.211, 0.291, 0.18, 0.021, 0.329, 0.139, 0.097, 0.009, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m top1: 0.13386194029850745
[2m[36m(func pid=181607)[0m top5: 0.4626865671641791
[2m[36m(func pid=181607)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=181607)[0m f1_macro: 0.08161237149880687
[2m[36m(func pid=181607)[0m f1_weighted: 0.09626738899435545
[2m[36m(func pid=181607)[0m f1_per_class: [0.142, 0.233, 0.0, 0.071, 0.0, 0.248, 0.009, 0.016, 0.014, 0.083]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7764 | Steps: 2 | Val loss: 11.5879 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.9167 | Steps: 2 | Val loss: 19.3348 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9129 | Steps: 2 | Val loss: 2.4055 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7171 | Steps: 2 | Val loss: 2.2665 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=169477)[0m top1: 0.1623134328358209
[2m[36m(func pid=169477)[0m top5: 0.8176305970149254
[2m[36m(func pid=169477)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=169477)[0m f1_macro: 0.12221828309150107
[2m[36m(func pid=169477)[0m f1_weighted: 0.1492269428797841
[2m[36m(func pid=169477)[0m f1_per_class: [0.045, 0.286, 0.0, 0.137, 0.086, 0.152, 0.094, 0.211, 0.0, 0.211]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2574626865671642
[2m[36m(func pid=168639)[0m top5: 0.808768656716418
[2m[36m(func pid=168639)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=168639)[0m f1_macro: 0.17038830308903327
[2m[36m(func pid=168639)[0m f1_weighted: 0.28406986178166554
[2m[36m(func pid=168639)[0m f1_per_class: [0.062, 0.159, 0.0, 0.167, 0.0, 0.197, 0.521, 0.5, 0.047, 0.052]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:47:50 (running for 00:38:04.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.917 |      0.17  |                   58 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.776 |      0.122 |                   56 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.748 |      0.134 |                   25 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.913 |      0.071 |                    3 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.12126865671641791
[2m[36m(func pid=181607)[0m top5: 0.4454291044776119
[2m[36m(func pid=181607)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=181607)[0m f1_macro: 0.07140806006596173
[2m[36m(func pid=181607)[0m f1_weighted: 0.08913412036116794
[2m[36m(func pid=181607)[0m f1_per_class: [0.126, 0.179, 0.0, 0.076, 0.0, 0.244, 0.015, 0.021, 0.01, 0.043]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1707089552238806
[2m[36m(func pid=176361)[0m top5: 0.6240671641791045
[2m[36m(func pid=176361)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=176361)[0m f1_macro: 0.13597273099476753
[2m[36m(func pid=176361)[0m f1_weighted: 0.17439256974589765
[2m[36m(func pid=176361)[0m f1_per_class: [0.067, 0.218, 0.302, 0.18, 0.027, 0.327, 0.135, 0.097, 0.009, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.0545 | Steps: 2 | Val loss: 10.1393 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4717 | Steps: 2 | Val loss: 16.0266 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8538 | Steps: 2 | Val loss: 2.3772 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6702 | Steps: 2 | Val loss: 2.2594 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=169477)[0m top1: 0.1394589552238806
[2m[36m(func pid=169477)[0m top5: 0.8125
[2m[36m(func pid=169477)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=169477)[0m f1_macro: 0.11642696253143214
[2m[36m(func pid=169477)[0m f1_weighted: 0.12189170921047546
[2m[36m(func pid=169477)[0m f1_per_class: [0.056, 0.206, 0.0, 0.05, 0.063, 0.282, 0.078, 0.216, 0.022, 0.19]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2490671641791045
[2m[36m(func pid=168639)[0m top5: 0.8022388059701493
[2m[36m(func pid=168639)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=168639)[0m f1_macro: 0.18046299793832024
[2m[36m(func pid=168639)[0m f1_weighted: 0.2828165347151354
[2m[36m(func pid=168639)[0m f1_per_class: [0.107, 0.213, 0.0, 0.174, 0.0, 0.223, 0.463, 0.504, 0.064, 0.056]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:47:55 (running for 00:38:10.08)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.472 |      0.18  |                   59 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  3.055 |      0.116 |                   57 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.717 |      0.136 |                   26 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.854 |      0.081 |                    4 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.12126865671641791
[2m[36m(func pid=181607)[0m top5: 0.4869402985074627
[2m[36m(func pid=181607)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=181607)[0m f1_macro: 0.08070059941556515
[2m[36m(func pid=181607)[0m f1_weighted: 0.09522294624149562
[2m[36m(func pid=181607)[0m f1_per_class: [0.092, 0.156, 0.081, 0.102, 0.019, 0.243, 0.018, 0.058, 0.019, 0.019]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1730410447761194
[2m[36m(func pid=176361)[0m top5: 0.632929104477612
[2m[36m(func pid=176361)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=176361)[0m f1_macro: 0.13908977500392486
[2m[36m(func pid=176361)[0m f1_weighted: 0.17869785000236296
[2m[36m(func pid=176361)[0m f1_per_class: [0.067, 0.215, 0.314, 0.187, 0.027, 0.323, 0.144, 0.106, 0.009, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 4.2519 | Steps: 2 | Val loss: 8.0759 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7595 | Steps: 2 | Val loss: 14.5517 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7854 | Steps: 2 | Val loss: 2.3429 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7044 | Steps: 2 | Val loss: 2.2551 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=169477)[0m top1: 0.11333955223880597
[2m[36m(func pid=169477)[0m top5: 0.7980410447761194
[2m[36m(func pid=169477)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=169477)[0m f1_macro: 0.10488590500446385
[2m[36m(func pid=169477)[0m f1_weighted: 0.08095030908752247
[2m[36m(func pid=169477)[0m f1_per_class: [0.066, 0.181, 0.0, 0.0, 0.154, 0.255, 0.009, 0.204, 0.067, 0.112]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.21875
[2m[36m(func pid=168639)[0m top5: 0.8227611940298507
[2m[36m(func pid=168639)[0m f1_micro: 0.21875
[2m[36m(func pid=168639)[0m f1_macro: 0.16821563998413175
[2m[36m(func pid=168639)[0m f1_weighted: 0.2438450466862272
[2m[36m(func pid=168639)[0m f1_per_class: [0.075, 0.217, 0.0, 0.142, 0.0, 0.218, 0.359, 0.522, 0.069, 0.08]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:48:00 (running for 00:38:15.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.76  |      0.168 |                   60 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  4.252 |      0.105 |                   58 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.67  |      0.139 |                   27 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.785 |      0.091 |                    5 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.13013059701492538
[2m[36m(func pid=181607)[0m top5: 0.5340485074626866
[2m[36m(func pid=181607)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=181607)[0m f1_macro: 0.09129919389527483
[2m[36m(func pid=181607)[0m f1_weighted: 0.11767933717465064
[2m[36m(func pid=181607)[0m f1_per_class: [0.094, 0.159, 0.111, 0.136, 0.018, 0.249, 0.058, 0.052, 0.019, 0.017]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.17583955223880596
[2m[36m(func pid=176361)[0m top5: 0.6422574626865671
[2m[36m(func pid=176361)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=176361)[0m f1_macro: 0.1391921618325341
[2m[36m(func pid=176361)[0m f1_weighted: 0.17919401860696169
[2m[36m(func pid=176361)[0m f1_per_class: [0.067, 0.223, 0.296, 0.184, 0.026, 0.338, 0.137, 0.11, 0.01, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.8771 | Steps: 2 | Val loss: 6.6180 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4228 | Steps: 2 | Val loss: 14.3567 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7830 | Steps: 2 | Val loss: 2.3077 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7467 | Steps: 2 | Val loss: 2.2501 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=169477)[0m top1: 0.09001865671641791
[2m[36m(func pid=169477)[0m top5: 0.7840485074626866
[2m[36m(func pid=169477)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=169477)[0m f1_macro: 0.10291960650468801
[2m[36m(func pid=169477)[0m f1_weighted: 0.05545281514434668
[2m[36m(func pid=169477)[0m f1_per_class: [0.068, 0.14, 0.333, 0.0, 0.057, 0.123, 0.0, 0.204, 0.022, 0.082]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2234141791044776
[2m[36m(func pid=168639)[0m top5: 0.8465485074626866
[2m[36m(func pid=168639)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=168639)[0m f1_macro: 0.17912014833845893
[2m[36m(func pid=168639)[0m f1_weighted: 0.239672654434951
[2m[36m(func pid=168639)[0m f1_per_class: [0.191, 0.231, 0.0, 0.136, 0.0, 0.197, 0.344, 0.506, 0.07, 0.115]
[2m[36m(func pid=168639)[0m 
== Status ==
Current time: 2024-01-07 04:48:06 (running for 00:38:20.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.423 |      0.179 |                   61 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.877 |      0.103 |                   59 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.704 |      0.139 |                   28 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.783 |      0.118 |                    6 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.16324626865671643
[2m[36m(func pid=181607)[0m top5: 0.5774253731343284
[2m[36m(func pid=181607)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=181607)[0m f1_macro: 0.11762488618182557
[2m[36m(func pid=181607)[0m f1_weighted: 0.17066443352872027
[2m[36m(func pid=181607)[0m f1_per_class: [0.118, 0.16, 0.123, 0.176, 0.042, 0.284, 0.18, 0.069, 0.009, 0.015]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.18423507462686567
[2m[36m(func pid=176361)[0m top5: 0.644589552238806
[2m[36m(func pid=176361)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=176361)[0m f1_macro: 0.14460602893804486
[2m[36m(func pid=176361)[0m f1_weighted: 0.1878125910771199
[2m[36m(func pid=176361)[0m f1_per_class: [0.068, 0.26, 0.302, 0.194, 0.026, 0.335, 0.134, 0.115, 0.011, 0.0]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6985 | Steps: 2 | Val loss: 14.8071 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.6708 | Steps: 2 | Val loss: 5.6895 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6768 | Steps: 2 | Val loss: 2.2753 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7026 | Steps: 2 | Val loss: 2.2459 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=168639)[0m top1: 0.24720149253731344
[2m[36m(func pid=168639)[0m top5: 0.8507462686567164
[2m[36m(func pid=168639)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=168639)[0m f1_macro: 0.18782914945033052
[2m[36m(func pid=168639)[0m f1_weighted: 0.2654371509617534
[2m[36m(func pid=168639)[0m f1_per_class: [0.132, 0.222, 0.0, 0.134, 0.0, 0.291, 0.408, 0.493, 0.069, 0.13]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.09235074626865672
[2m[36m(func pid=169477)[0m top5: 0.7826492537313433
[2m[36m(func pid=169477)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=169477)[0m f1_macro: 0.11937580965032846
[2m[36m(func pid=169477)[0m f1_weighted: 0.06450100880682146
[2m[36m(func pid=169477)[0m f1_per_class: [0.072, 0.197, 0.467, 0.0, 0.048, 0.116, 0.0, 0.183, 0.035, 0.076]
[2m[36m(func pid=169477)[0m 
== Status ==
Current time: 2024-01-07 04:48:11 (running for 00:38:25.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.699 |      0.188 |                   62 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.671 |      0.119 |                   60 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.747 |      0.145 |                   29 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.677 |      0.136 |                    7 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.1828358208955224
[2m[36m(func pid=181607)[0m top5: 0.6338619402985075
[2m[36m(func pid=181607)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=181607)[0m f1_macro: 0.13594670486383775
[2m[36m(func pid=181607)[0m f1_weighted: 0.20007662386228164
[2m[36m(func pid=181607)[0m f1_per_class: [0.122, 0.158, 0.161, 0.191, 0.051, 0.33, 0.246, 0.066, 0.018, 0.017]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.18703358208955223
[2m[36m(func pid=176361)[0m top5: 0.6501865671641791
[2m[36m(func pid=176361)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=176361)[0m f1_macro: 0.15075720529116993
[2m[36m(func pid=176361)[0m f1_weighted: 0.1907404458432605
[2m[36m(func pid=176361)[0m f1_per_class: [0.09, 0.261, 0.286, 0.192, 0.03, 0.333, 0.142, 0.125, 0.024, 0.025]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.7399 | Steps: 2 | Val loss: 4.9509 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3297 | Steps: 2 | Val loss: 15.1499 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5767 | Steps: 2 | Val loss: 2.2397 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6748 | Steps: 2 | Val loss: 2.2457 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=168639)[0m top1: 0.2667910447761194
[2m[36m(func pid=168639)[0m top5: 0.8498134328358209
[2m[36m(func pid=168639)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=168639)[0m f1_macro: 0.19550858433760537
[2m[36m(func pid=168639)[0m f1_weighted: 0.2835814625742309
[2m[36m(func pid=168639)[0m f1_per_class: [0.176, 0.202, 0.0, 0.123, 0.0, 0.281, 0.491, 0.507, 0.063, 0.113]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.11007462686567164
[2m[36m(func pid=169477)[0m top5: 0.7961753731343284
[2m[36m(func pid=169477)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=169477)[0m f1_macro: 0.1204647408768692
[2m[36m(func pid=169477)[0m f1_weighted: 0.086950487826563
[2m[36m(func pid=169477)[0m f1_per_class: [0.075, 0.3, 0.279, 0.003, 0.145, 0.091, 0.024, 0.182, 0.043, 0.063]
[2m[36m(func pid=169477)[0m 
== Status ==
Current time: 2024-01-07 04:48:16 (running for 00:38:31.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.33  |      0.196 |                   63 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.74  |      0.12  |                   61 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.703 |      0.151 |                   30 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.577 |      0.152 |                    8 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.19916044776119404
[2m[36m(func pid=181607)[0m top5: 0.6688432835820896
[2m[36m(func pid=181607)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=181607)[0m f1_macro: 0.1520794104310516
[2m[36m(func pid=181607)[0m f1_weighted: 0.22329081729459802
[2m[36m(func pid=181607)[0m f1_per_class: [0.126, 0.17, 0.185, 0.218, 0.049, 0.33, 0.287, 0.079, 0.018, 0.058]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1921641791044776
[2m[36m(func pid=176361)[0m top5: 0.644589552238806
[2m[36m(func pid=176361)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=176361)[0m f1_macro: 0.1567596270259163
[2m[36m(func pid=176361)[0m f1_weighted: 0.19281081903628458
[2m[36m(func pid=176361)[0m f1_per_class: [0.085, 0.275, 0.31, 0.185, 0.031, 0.341, 0.142, 0.124, 0.026, 0.047]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2524 | Steps: 2 | Val loss: 16.0979 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.9678 | Steps: 2 | Val loss: 4.1396 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5129 | Steps: 2 | Val loss: 2.2066 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6411 | Steps: 2 | Val loss: 2.2458 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 04:48:22 (running for 00:38:36.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.33  |      0.196 |                   63 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.74  |      0.12  |                   61 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.675 |      0.157 |                   31 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.513 |      0.166 |                    9 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.21735074626865672
[2m[36m(func pid=181607)[0m top5: 0.7056902985074627
[2m[36m(func pid=181607)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=181607)[0m f1_macro: 0.16583735324477297
[2m[36m(func pid=181607)[0m f1_weighted: 0.2465721900333563
[2m[36m(func pid=181607)[0m f1_per_class: [0.136, 0.167, 0.21, 0.271, 0.037, 0.361, 0.303, 0.08, 0.029, 0.065]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2737873134328358
[2m[36m(func pid=168639)[0m top5: 0.8465485074626866
[2m[36m(func pid=168639)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=168639)[0m f1_macro: 0.17822019260043293
[2m[36m(func pid=168639)[0m f1_weighted: 0.28080644539273647
[2m[36m(func pid=168639)[0m f1_per_class: [0.247, 0.168, 0.0, 0.125, 0.0, 0.273, 0.55, 0.23, 0.064, 0.126]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1646455223880597
[2m[36m(func pid=169477)[0m top5: 0.835820895522388
[2m[36m(func pid=169477)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=169477)[0m f1_macro: 0.13099150357791942
[2m[36m(func pid=169477)[0m f1_weighted: 0.16102990384555615
[2m[36m(func pid=169477)[0m f1_per_class: [0.063, 0.4, 0.097, 0.065, 0.143, 0.039, 0.176, 0.212, 0.039, 0.076]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1982276119402985
[2m[36m(func pid=176361)[0m top5: 0.6417910447761194
[2m[36m(func pid=176361)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=176361)[0m f1_macro: 0.1667128109133654
[2m[36m(func pid=176361)[0m f1_weighted: 0.19773157016478246
[2m[36m(func pid=176361)[0m f1_per_class: [0.072, 0.285, 0.364, 0.176, 0.03, 0.367, 0.15, 0.127, 0.028, 0.069]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4629 | Steps: 2 | Val loss: 2.1789 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5241 | Steps: 2 | Val loss: 17.7480 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.5339 | Steps: 2 | Val loss: 4.1686 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6266 | Steps: 2 | Val loss: 2.2450 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:48:27 (running for 00:38:41.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.252 |      0.178 |                   64 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.968 |      0.131 |                   62 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.641 |      0.167 |                   32 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.463 |      0.186 |                   10 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.2462686567164179
[2m[36m(func pid=181607)[0m top5: 0.7308768656716418
[2m[36m(func pid=181607)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=181607)[0m f1_macro: 0.18586137886599194
[2m[36m(func pid=181607)[0m f1_weighted: 0.28057824415418237
[2m[36m(func pid=181607)[0m f1_per_class: [0.144, 0.205, 0.214, 0.326, 0.04, 0.385, 0.326, 0.114, 0.041, 0.065]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1875
[2m[36m(func pid=169477)[0m top5: 0.8512126865671642
[2m[36m(func pid=169477)[0m f1_micro: 0.1875
[2m[36m(func pid=169477)[0m f1_macro: 0.13832426973679882
[2m[36m(func pid=169477)[0m f1_weighted: 0.17992619253735462
[2m[36m(func pid=169477)[0m f1_per_class: [0.068, 0.458, 0.085, 0.125, 0.213, 0.0, 0.166, 0.216, 0.0, 0.051]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.26119402985074625
[2m[36m(func pid=168639)[0m top5: 0.8260261194029851
[2m[36m(func pid=168639)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=168639)[0m f1_macro: 0.1543874280591532
[2m[36m(func pid=168639)[0m f1_weighted: 0.2648218826932573
[2m[36m(func pid=168639)[0m f1_per_class: [0.267, 0.126, 0.0, 0.126, 0.0, 0.287, 0.553, 0.031, 0.063, 0.091]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=176361)[0m top1: 0.19542910447761194
[2m[36m(func pid=176361)[0m top5: 0.6427238805970149
[2m[36m(func pid=176361)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=176361)[0m f1_macro: 0.16220221445397906
[2m[36m(func pid=176361)[0m f1_weighted: 0.19408446531233117
[2m[36m(func pid=176361)[0m f1_per_class: [0.085, 0.291, 0.345, 0.169, 0.029, 0.369, 0.142, 0.116, 0.029, 0.047]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3484 | Steps: 2 | Val loss: 2.1527 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.5569 | Steps: 2 | Val loss: 4.6487 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6146 | Steps: 2 | Val loss: 2.2442 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5512 | Steps: 2 | Val loss: 18.6907 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:48:32 (running for 00:38:46.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.524 |      0.154 |                   65 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.534 |      0.138 |                   63 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.627 |      0.162 |                   33 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.348 |      0.201 |                   11 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.2630597014925373
[2m[36m(func pid=181607)[0m top5: 0.7509328358208955
[2m[36m(func pid=181607)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=181607)[0m f1_macro: 0.20090437030395053
[2m[36m(func pid=181607)[0m f1_weighted: 0.29725491756030176
[2m[36m(func pid=181607)[0m f1_per_class: [0.158, 0.2, 0.25, 0.369, 0.045, 0.369, 0.344, 0.108, 0.078, 0.087]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m top1: 0.17817164179104478
[2m[36m(func pid=169477)[0m top5: 0.8395522388059702
[2m[36m(func pid=169477)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=169477)[0m f1_macro: 0.1329765816452146
[2m[36m(func pid=169477)[0m f1_weighted: 0.13292149433510803
[2m[36m(func pid=169477)[0m f1_per_class: [0.062, 0.496, 0.158, 0.096, 0.24, 0.0, 0.015, 0.199, 0.0, 0.063]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1935634328358209
[2m[36m(func pid=176361)[0m top5: 0.6427238805970149
[2m[36m(func pid=176361)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=176361)[0m f1_macro: 0.16048485206452093
[2m[36m(func pid=176361)[0m f1_weighted: 0.19170851149139356
[2m[36m(func pid=176361)[0m f1_per_class: [0.089, 0.29, 0.333, 0.151, 0.024, 0.364, 0.152, 0.124, 0.032, 0.046]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.23367537313432835
[2m[36m(func pid=168639)[0m top5: 0.8143656716417911
[2m[36m(func pid=168639)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=168639)[0m f1_macro: 0.15628282351392545
[2m[36m(func pid=168639)[0m f1_weighted: 0.2550895522581626
[2m[36m(func pid=168639)[0m f1_per_class: [0.217, 0.081, 0.0, 0.134, 0.0, 0.317, 0.506, 0.16, 0.07, 0.078]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3320 | Steps: 2 | Val loss: 2.1315 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6507 | Steps: 2 | Val loss: 4.9857 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6184 | Steps: 2 | Val loss: 2.2411 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4545 | Steps: 2 | Val loss: 18.1093 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=181607)[0m top1: 0.2737873134328358
[2m[36m(func pid=181607)[0m top5: 0.7625932835820896
[2m[36m(func pid=181607)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=181607)[0m f1_macro: 0.2127383762905775
[2m[36m(func pid=181607)[0m f1_weighted: 0.30951508096688646
[2m[36m(func pid=181607)[0m f1_per_class: [0.158, 0.196, 0.273, 0.38, 0.046, 0.361, 0.371, 0.131, 0.113, 0.098]
== Status ==
Current time: 2024-01-07 04:48:37 (running for 00:38:51.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.551 |      0.156 |                   66 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.557 |      0.133 |                   64 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.615 |      0.16  |                   34 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.332 |      0.213 |                   12 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m top1: 0.18003731343283583
[2m[36m(func pid=169477)[0m top5: 0.8288246268656716
[2m[36m(func pid=169477)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=169477)[0m f1_macro: 0.13718860967052338
[2m[36m(func pid=169477)[0m f1_weighted: 0.11797455691769788
[2m[36m(func pid=169477)[0m f1_per_class: [0.066, 0.482, 0.308, 0.056, 0.154, 0.0, 0.006, 0.201, 0.026, 0.074]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.18983208955223882
[2m[36m(func pid=176361)[0m top5: 0.6441231343283582
[2m[36m(func pid=176361)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=176361)[0m f1_macro: 0.16011132449004484
[2m[36m(func pid=176361)[0m f1_weighted: 0.18692983877939442
[2m[36m(func pid=176361)[0m f1_per_class: [0.091, 0.284, 0.351, 0.14, 0.023, 0.363, 0.15, 0.123, 0.031, 0.044]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.17210820895522388
[2m[36m(func pid=168639)[0m top5: 0.7877798507462687
[2m[36m(func pid=168639)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=168639)[0m f1_macro: 0.1398085402133305
[2m[36m(func pid=168639)[0m f1_weighted: 0.2058310308991598
[2m[36m(func pid=168639)[0m f1_per_class: [0.098, 0.068, 0.0, 0.167, 0.0, 0.316, 0.306, 0.25, 0.063, 0.13]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.2275 | Steps: 2 | Val loss: 2.1094 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4574 | Steps: 2 | Val loss: 5.0298 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6385 | Steps: 2 | Val loss: 2.2377 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.9649 | Steps: 2 | Val loss: 16.2868 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:48:42 (running for 00:38:57.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.454 |      0.14  |                   67 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.651 |      0.137 |                   65 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.618 |      0.16  |                   35 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.227 |      0.225 |                   13 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.28777985074626866
[2m[36m(func pid=181607)[0m top5: 0.7714552238805971
[2m[36m(func pid=181607)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=181607)[0m f1_macro: 0.2247422190229833
[2m[36m(func pid=181607)[0m f1_weighted: 0.3223273707416203
[2m[36m(func pid=181607)[0m f1_per_class: [0.181, 0.202, 0.267, 0.424, 0.05, 0.37, 0.362, 0.147, 0.107, 0.14]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m top1: 0.1935634328358209
[2m[36m(func pid=169477)[0m top5: 0.8227611940298507
[2m[36m(func pid=169477)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=169477)[0m f1_macro: 0.14245808238968374
[2m[36m(func pid=169477)[0m f1_weighted: 0.1466064296198548
[2m[36m(func pid=169477)[0m f1_per_class: [0.063, 0.465, 0.333, 0.077, 0.082, 0.008, 0.088, 0.223, 0.0, 0.084]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.19682835820895522
[2m[36m(func pid=176361)[0m top5: 0.6478544776119403
[2m[36m(func pid=176361)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=176361)[0m f1_macro: 0.16632484930676972
[2m[36m(func pid=176361)[0m f1_weighted: 0.19505024159067602
[2m[36m(func pid=176361)[0m f1_per_class: [0.085, 0.291, 0.361, 0.148, 0.019, 0.379, 0.159, 0.128, 0.03, 0.064]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.18423507462686567
[2m[36m(func pid=168639)[0m top5: 0.7845149253731343
[2m[36m(func pid=168639)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=168639)[0m f1_macro: 0.22508679800563608
[2m[36m(func pid=168639)[0m f1_weighted: 0.1827356856507292
[2m[36m(func pid=168639)[0m f1_per_class: [0.114, 0.089, 0.692, 0.286, 0.0, 0.272, 0.083, 0.34, 0.055, 0.319]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.1767 | Steps: 2 | Val loss: 2.0894 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 9.5474 | Steps: 2 | Val loss: 5.3481 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6493 | Steps: 2 | Val loss: 2.2368 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4959 | Steps: 2 | Val loss: 16.0166 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:48:48 (running for 00:39:02.44)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.965 |      0.225 |                   68 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.457 |      0.142 |                   66 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.639 |      0.166 |                   36 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.177 |      0.243 |                   14 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.2989738805970149
[2m[36m(func pid=181607)[0m top5: 0.7803171641791045
[2m[36m(func pid=181607)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=181607)[0m f1_macro: 0.24296989833800714
[2m[36m(func pid=181607)[0m f1_weighted: 0.3311257837008508
[2m[36m(func pid=181607)[0m f1_per_class: [0.191, 0.188, 0.279, 0.431, 0.049, 0.373, 0.374, 0.196, 0.141, 0.207]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m top1: 0.21361940298507462
[2m[36m(func pid=169477)[0m top5: 0.8180970149253731
[2m[36m(func pid=169477)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=169477)[0m f1_macro: 0.1606283654272493
[2m[36m(func pid=169477)[0m f1_weighted: 0.17865545840493743
[2m[36m(func pid=169477)[0m f1_per_class: [0.055, 0.446, 0.375, 0.082, 0.082, 0.046, 0.181, 0.259, 0.0, 0.08]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.19776119402985073
[2m[36m(func pid=176361)[0m top5: 0.6501865671641791
[2m[36m(func pid=176361)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=176361)[0m f1_macro: 0.1674594714996438
[2m[36m(func pid=176361)[0m f1_weighted: 0.19800740496966268
[2m[36m(func pid=176361)[0m f1_per_class: [0.095, 0.288, 0.349, 0.158, 0.019, 0.37, 0.161, 0.139, 0.03, 0.064]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2234141791044776
[2m[36m(func pid=168639)[0m top5: 0.7854477611940298
[2m[36m(func pid=168639)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=168639)[0m f1_macro: 0.21764660593952323
[2m[36m(func pid=168639)[0m f1_weighted: 0.17541754048170788
[2m[36m(func pid=168639)[0m f1_per_class: [0.195, 0.102, 0.714, 0.39, 0.0, 0.113, 0.012, 0.329, 0.06, 0.26]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0763 | Steps: 2 | Val loss: 2.0714 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4310 | Steps: 2 | Val loss: 5.4002 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5312 | Steps: 2 | Val loss: 2.2347 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1675 | Steps: 2 | Val loss: 16.0250 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 04:48:53 (running for 00:39:07.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.496 |      0.218 |                   69 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  9.547 |      0.161 |                   67 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.649 |      0.167 |                   37 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.076 |      0.262 |                   15 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.31203358208955223
[2m[36m(func pid=181607)[0m top5: 0.7882462686567164
[2m[36m(func pid=181607)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=181607)[0m f1_macro: 0.26228155848899043
[2m[36m(func pid=181607)[0m f1_weighted: 0.34505269156321394
[2m[36m(func pid=181607)[0m f1_per_class: [0.205, 0.204, 0.338, 0.436, 0.049, 0.376, 0.397, 0.218, 0.15, 0.25]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m top1: 0.21875
[2m[36m(func pid=169477)[0m top5: 0.8199626865671642
[2m[36m(func pid=169477)[0m f1_micro: 0.21875
[2m[36m(func pid=169477)[0m f1_macro: 0.1497673476297992
[2m[36m(func pid=169477)[0m f1_weighted: 0.18236185776768207
[2m[36m(func pid=169477)[0m f1_per_class: [0.059, 0.443, 0.2, 0.067, 0.081, 0.103, 0.189, 0.264, 0.0, 0.092]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1958955223880597
[2m[36m(func pid=176361)[0m top5: 0.6567164179104478
[2m[36m(func pid=176361)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=176361)[0m f1_macro: 0.1675016494498211
[2m[36m(func pid=176361)[0m f1_weighted: 0.19833475715530935
[2m[36m(func pid=176361)[0m f1_per_class: [0.11, 0.284, 0.344, 0.165, 0.024, 0.355, 0.163, 0.138, 0.029, 0.062]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.26072761194029853
[2m[36m(func pid=168639)[0m top5: 0.7835820895522388
[2m[36m(func pid=168639)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=168639)[0m f1_macro: 0.2177776580821799
[2m[36m(func pid=168639)[0m f1_weighted: 0.19052501792957022
[2m[36m(func pid=168639)[0m f1_per_class: [0.171, 0.126, 0.667, 0.448, 0.0, 0.093, 0.003, 0.334, 0.07, 0.265]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.1347 | Steps: 2 | Val loss: 2.0589 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6046 | Steps: 2 | Val loss: 2.2340 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 9.4341 | Steps: 2 | Val loss: 5.8891 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5433 | Steps: 2 | Val loss: 15.9400 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=181607)[0m top1: 0.3208955223880597
[2m[36m(func pid=181607)[0m top5: 0.7901119402985075
[2m[36m(func pid=181607)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=181607)[0m f1_macro: 0.27389535136063026
[2m[36m(func pid=181607)[0m f1_weighted: 0.35571535013314887
[2m[36m(func pid=181607)[0m f1_per_class: [0.223, 0.22, 0.324, 0.433, 0.049, 0.387, 0.406, 0.288, 0.149, 0.258]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:48:59 (running for 00:39:14.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.167 |      0.218 |                   70 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  9.434 |      0.139 |                   69 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.531 |      0.168 |                   38 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  2.135 |      0.274 |                   16 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m top1: 0.22994402985074627
[2m[36m(func pid=169477)[0m top5: 0.8288246268656716
[2m[36m(func pid=169477)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=169477)[0m f1_macro: 0.13935264764185634
[2m[36m(func pid=169477)[0m f1_weighted: 0.1916809931975641
[2m[36m(func pid=169477)[0m f1_per_class: [0.049, 0.437, 0.0, 0.069, 0.1, 0.154, 0.202, 0.274, 0.023, 0.085]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=176361)[0m top1: 0.19682835820895522
[2m[36m(func pid=176361)[0m top5: 0.6567164179104478
[2m[36m(func pid=176361)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=176361)[0m f1_macro: 0.16842668377494005
[2m[36m(func pid=176361)[0m f1_weighted: 0.20150473004970246
[2m[36m(func pid=176361)[0m f1_per_class: [0.13, 0.291, 0.338, 0.169, 0.023, 0.349, 0.17, 0.123, 0.028, 0.062]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=168639)[0m top1: 0.2691231343283582
[2m[36m(func pid=168639)[0m top5: 0.7915111940298507
[2m[36m(func pid=168639)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=168639)[0m f1_macro: 0.21650944846758224
[2m[36m(func pid=168639)[0m f1_weighted: 0.19308122355139992
[2m[36m(func pid=168639)[0m f1_per_class: [0.198, 0.154, 0.588, 0.435, 0.0, 0.097, 0.0, 0.363, 0.075, 0.254]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9740 | Steps: 2 | Val loss: 2.0409 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5779 | Steps: 2 | Val loss: 2.2291 | Batch size: 32 | lr: 0.0001 | Duration: 3.30s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6814 | Steps: 2 | Val loss: 6.6737 | Batch size: 32 | lr: 0.1 | Duration: 3.36s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4232 | Steps: 2 | Val loss: 15.9275 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=181607)[0m top1: 0.3306902985074627
[2m[36m(func pid=181607)[0m top5: 0.8022388059701493
[2m[36m(func pid=181607)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=181607)[0m f1_macro: 0.2909736712777649
[2m[36m(func pid=181607)[0m f1_weighted: 0.36444255948885906
[2m[36m(func pid=181607)[0m f1_per_class: [0.266, 0.26, 0.375, 0.427, 0.044, 0.381, 0.412, 0.318, 0.122, 0.303]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:49:05 (running for 00:39:20.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.543 |      0.217 |                   71 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  9.434 |      0.139 |                   69 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.578 |      0.172 |                   40 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.974 |      0.291 |                   17 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.20009328358208955
[2m[36m(func pid=176361)[0m top5: 0.6637126865671642
[2m[36m(func pid=176361)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=176361)[0m f1_macro: 0.17223797164751
[2m[36m(func pid=176361)[0m f1_weighted: 0.20575168884364822
[2m[36m(func pid=176361)[0m f1_per_class: [0.126, 0.297, 0.349, 0.17, 0.028, 0.351, 0.177, 0.135, 0.029, 0.06]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.2178171641791045
[2m[36m(func pid=169477)[0m top5: 0.8330223880597015
[2m[36m(func pid=169477)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=169477)[0m f1_macro: 0.19033599012612396
[2m[36m(func pid=169477)[0m f1_weighted: 0.17110692356225465
[2m[36m(func pid=169477)[0m f1_per_class: [0.043, 0.439, 0.571, 0.079, 0.119, 0.141, 0.117, 0.259, 0.036, 0.099]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.25279850746268656
[2m[36m(func pid=168639)[0m top5: 0.7943097014925373
[2m[36m(func pid=168639)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=168639)[0m f1_macro: 0.20446848184173158
[2m[36m(func pid=168639)[0m f1_weighted: 0.18530772394499784
[2m[36m(func pid=168639)[0m f1_per_class: [0.188, 0.166, 0.486, 0.392, 0.0, 0.121, 0.0, 0.38, 0.056, 0.256]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.9923 | Steps: 2 | Val loss: 2.0232 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8943 | Steps: 2 | Val loss: 15.2415 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.5346 | Steps: 2 | Val loss: 7.7047 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5810 | Steps: 2 | Val loss: 2.2262 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=181607)[0m top1: 0.3358208955223881
[2m[36m(func pid=181607)[0m top5: 0.8069029850746269
[2m[36m(func pid=181607)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=181607)[0m f1_macro: 0.2936315220579974
[2m[36m(func pid=181607)[0m f1_weighted: 0.36694888177816853
[2m[36m(func pid=181607)[0m f1_per_class: [0.265, 0.278, 0.381, 0.426, 0.061, 0.37, 0.416, 0.31, 0.125, 0.304]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:49:11 (running for 00:39:25.57)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.894 |      0.174 |                   73 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.681 |      0.19  |                   70 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.578 |      0.172 |                   40 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.992 |      0.294 |                   18 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168639)[0m top1: 0.24347014925373134
[2m[36m(func pid=168639)[0m top5: 0.816231343283582
[2m[36m(func pid=168639)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=168639)[0m f1_macro: 0.17405717235328777
[2m[36m(func pid=168639)[0m f1_weighted: 0.18369502742720523
[2m[36m(func pid=168639)[0m f1_per_class: [0.181, 0.196, 0.182, 0.371, 0.0, 0.15, 0.0, 0.354, 0.019, 0.288]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=176361)[0m top1: 0.1982276119402985
[2m[36m(func pid=176361)[0m top5: 0.6693097014925373
[2m[36m(func pid=176361)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=176361)[0m f1_macro: 0.17171292960862428
[2m[36m(func pid=176361)[0m f1_weighted: 0.2044501038383438
[2m[36m(func pid=176361)[0m f1_per_class: [0.133, 0.289, 0.355, 0.173, 0.029, 0.348, 0.177, 0.129, 0.028, 0.058]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.23880597014925373
[2m[36m(func pid=169477)[0m top5: 0.8376865671641791
[2m[36m(func pid=169477)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=169477)[0m f1_macro: 0.19485016681295855
[2m[36m(func pid=169477)[0m f1_weighted: 0.20818058668817782
[2m[36m(func pid=169477)[0m f1_per_class: [0.033, 0.438, 0.444, 0.155, 0.123, 0.202, 0.146, 0.271, 0.058, 0.077]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.8617 | Steps: 2 | Val loss: 2.0044 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181607)[0m top1: 0.34375
[2m[36m(func pid=181607)[0m top5: 0.8129664179104478
[2m[36m(func pid=181607)[0m f1_micro: 0.34375
[2m[36m(func pid=181607)[0m f1_macro: 0.30341638177113867
[2m[36m(func pid=181607)[0m f1_weighted: 0.3745351216742046
[2m[36m(func pid=181607)[0m f1_per_class: [0.254, 0.28, 0.429, 0.451, 0.062, 0.355, 0.414, 0.333, 0.161, 0.295]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9308 | Steps: 2 | Val loss: 13.7058 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5828 | Steps: 2 | Val loss: 2.2204 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3166 | Steps: 2 | Val loss: 8.1846 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:49:16 (running for 00:39:31.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.894 |      0.174 |                   73 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.535 |      0.195 |                   71 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.583 |      0.174 |                   42 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.862 |      0.303 |                   19 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.20662313432835822
[2m[36m(func pid=176361)[0m top5: 0.6767723880597015
[2m[36m(func pid=176361)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=176361)[0m f1_macro: 0.17439539178573668
[2m[36m(func pid=176361)[0m f1_weighted: 0.21379733091271447
[2m[36m(func pid=176361)[0m f1_per_class: [0.126, 0.305, 0.344, 0.18, 0.029, 0.351, 0.191, 0.132, 0.029, 0.058]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.28777985074626866
[2m[36m(func pid=169477)[0m top5: 0.8227611940298507
[2m[36m(func pid=169477)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=169477)[0m f1_macro: 0.22872891448757127
[2m[36m(func pid=169477)[0m f1_weighted: 0.2849171896910026
[2m[36m(func pid=169477)[0m f1_per_class: [0.023, 0.438, 0.4, 0.261, 0.19, 0.256, 0.274, 0.328, 0.048, 0.069]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.23833955223880596
[2m[36m(func pid=168639)[0m top5: 0.8325559701492538
[2m[36m(func pid=168639)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=168639)[0m f1_macro: 0.166487580718007
[2m[36m(func pid=168639)[0m f1_weighted: 0.19546390915667813
[2m[36m(func pid=168639)[0m f1_per_class: [0.106, 0.225, 0.074, 0.345, 0.0, 0.233, 0.028, 0.321, 0.034, 0.3]
[2m[36m(func pid=168639)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7481 | Steps: 2 | Val loss: 1.9856 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=181607)[0m top1: 0.3381529850746269
[2m[36m(func pid=181607)[0m top5: 0.8194962686567164
[2m[36m(func pid=181607)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=181607)[0m f1_macro: 0.29683002974573097
[2m[36m(func pid=181607)[0m f1_weighted: 0.36687044229499044
[2m[36m(func pid=181607)[0m f1_per_class: [0.24, 0.247, 0.429, 0.46, 0.065, 0.357, 0.404, 0.309, 0.165, 0.293]
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5481 | Steps: 2 | Val loss: 2.2195 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6931 | Steps: 2 | Val loss: 7.9559 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=168639)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9265 | Steps: 2 | Val loss: 13.1237 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:49:22 (running for 00:39:36.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00018 | RUNNING    | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.931 |      0.166 |                   74 |
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.317 |      0.229 |                   72 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.548 |      0.175 |                   43 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.748 |      0.297 |                   20 |
| train_35a0b_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.20755597014925373
[2m[36m(func pid=176361)[0m top5: 0.675839552238806
[2m[36m(func pid=176361)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=176361)[0m f1_macro: 0.17452635829754704
[2m[36m(func pid=176361)[0m f1_weighted: 0.21570725060472284
[2m[36m(func pid=176361)[0m f1_per_class: [0.135, 0.298, 0.324, 0.18, 0.029, 0.358, 0.196, 0.14, 0.028, 0.056]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.27798507462686567
[2m[36m(func pid=169477)[0m top5: 0.8097014925373134
[2m[36m(func pid=169477)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=169477)[0m f1_macro: 0.21617587514361927
[2m[36m(func pid=169477)[0m f1_weighted: 0.27893470271125764
[2m[36m(func pid=169477)[0m f1_per_class: [0.023, 0.451, 0.408, 0.358, 0.182, 0.106, 0.219, 0.305, 0.053, 0.058]
[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=168639)[0m top1: 0.23227611940298507
[2m[36m(func pid=168639)[0m top5: 0.8460820895522388
[2m[36m(func pid=168639)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=168639)[0m f1_macro: 0.16420187602140493
[2m[36m(func pid=168639)[0m f1_weighted: 0.20573171186671427
[2m[36m(func pid=168639)[0m f1_per_class: [0.078, 0.261, 0.0, 0.272, 0.0, 0.295, 0.095, 0.289, 0.032, 0.321]
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.7577 | Steps: 2 | Val loss: 1.9693 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=181607)[0m top1: 0.34328358208955223
[2m[36m(func pid=181607)[0m top5: 0.8292910447761194
[2m[36m(func pid=181607)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=181607)[0m f1_macro: 0.2980186345940264
[2m[36m(func pid=181607)[0m f1_weighted: 0.3724013579865365
[2m[36m(func pid=181607)[0m f1_per_class: [0.238, 0.257, 0.429, 0.463, 0.066, 0.358, 0.414, 0.307, 0.169, 0.279]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4979 | Steps: 2 | Val loss: 2.2146 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.3077 | Steps: 2 | Val loss: 7.9327 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=176361)[0m top1: 0.21175373134328357
[2m[36m(func pid=176361)[0m top5: 0.683768656716418
[2m[36m(func pid=176361)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=176361)[0m f1_macro: 0.17669734586201513
[2m[36m(func pid=176361)[0m f1_weighted: 0.2213277783381579
[2m[36m(func pid=176361)[0m f1_per_class: [0.154, 0.293, 0.301, 0.194, 0.029, 0.37, 0.2, 0.138, 0.028, 0.058]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=169477)[0m top1: 0.25466417910447764
[2m[36m(func pid=169477)[0m top5: 0.8227611940298507
[2m[36m(func pid=169477)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=169477)[0m f1_macro: 0.17677324599187408
[2m[36m(func pid=169477)[0m f1_weighted: 0.2577994498596772
[2m[36m(func pid=169477)[0m f1_per_class: [0.034, 0.429, 0.091, 0.4, 0.188, 0.016, 0.16, 0.285, 0.09, 0.076]
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.7235 | Steps: 2 | Val loss: 1.9528 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=181607)[0m top1: 0.34841417910447764
[2m[36m(func pid=181607)[0m top5: 0.8353544776119403
[2m[36m(func pid=181607)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=181607)[0m f1_macro: 0.30565720799316065
[2m[36m(func pid=181607)[0m f1_weighted: 0.3783016890361403
[2m[36m(func pid=181607)[0m f1_per_class: [0.26, 0.264, 0.421, 0.463, 0.064, 0.379, 0.413, 0.345, 0.173, 0.275]
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5209 | Steps: 2 | Val loss: 2.2078 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:49:27 (running for 00:39:41.87)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.693 |      0.216 |                   73 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.498 |      0.177 |                   44 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.758 |      0.298 |                   21 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169477)[0m 
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=186463)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=186463)[0m Configuration completed!
[2m[36m(func pid=186463)[0m New optimizer parameters:
[2m[36m(func pid=186463)[0m SGD (
[2m[36m(func pid=186463)[0m Parameter Group 0
[2m[36m(func pid=186463)[0m     dampening: 0
[2m[36m(func pid=186463)[0m     differentiable: False
[2m[36m(func pid=186463)[0m     foreach: None
[2m[36m(func pid=186463)[0m     lr: 0.01
[2m[36m(func pid=186463)[0m     maximize: False
[2m[36m(func pid=186463)[0m     momentum: 0.9
[2m[36m(func pid=186463)[0m     nesterov: False
[2m[36m(func pid=186463)[0m     weight_decay: 1e-05
[2m[36m(func pid=186463)[0m )
[2m[36m(func pid=186463)[0m 
== Status ==
Current time: 2024-01-07 04:49:32 (running for 00:39:47.29)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00019 | RUNNING    | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  1.308 |      0.177 |                   74 |
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.521 |      0.176 |                   45 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.724 |      0.306 |                   22 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.21361940298507462
[2m[36m(func pid=176361)[0m top5: 0.6884328358208955
[2m[36m(func pid=176361)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=176361)[0m f1_macro: 0.17605417713416804
[2m[36m(func pid=176361)[0m f1_weighted: 0.22625795897894954
[2m[36m(func pid=176361)[0m f1_per_class: [0.152, 0.285, 0.293, 0.207, 0.026, 0.376, 0.209, 0.13, 0.026, 0.057]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.6425 | Steps: 2 | Val loss: 1.9372 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=169477)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.3726 | Steps: 2 | Val loss: 8.7733 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5059 | Steps: 2 | Val loss: 2.2039 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9444 | Steps: 2 | Val loss: 2.2558 | Batch size: 32 | lr: 0.01 | Duration: 4.66s
[2m[36m(func pid=169477)[0m top1: 0.2080223880597015
[2m[36m(func pid=169477)[0m top5: 0.7999067164179104
[2m[36m(func pid=169477)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=169477)[0m f1_macro: 0.15857187583736693
[2m[36m(func pid=169477)[0m f1_weighted: 0.21579215757545742
[2m[36m(func pid=169477)[0m f1_per_class: [0.047, 0.334, 0.105, 0.348, 0.157, 0.0, 0.127, 0.277, 0.112, 0.079]
[2m[36m(func pid=181607)[0m top1: 0.353544776119403
[2m[36m(func pid=181607)[0m top5: 0.8372201492537313
[2m[36m(func pid=181607)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=181607)[0m f1_macro: 0.31670201495753847
[2m[36m(func pid=181607)[0m f1_weighted: 0.38392886212249483
[2m[36m(func pid=181607)[0m f1_per_class: [0.308, 0.278, 0.444, 0.463, 0.061, 0.382, 0.416, 0.36, 0.172, 0.284]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.22014925373134328
[2m[36m(func pid=176361)[0m top5: 0.6940298507462687
[2m[36m(func pid=176361)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=176361)[0m f1_macro: 0.1799770293937561
[2m[36m(func pid=176361)[0m f1_weighted: 0.2365960564106393
[2m[36m(func pid=176361)[0m f1_per_class: [0.15, 0.283, 0.301, 0.23, 0.027, 0.368, 0.227, 0.125, 0.024, 0.064]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=186463)[0m top1: 0.18097014925373134
[2m[36m(func pid=186463)[0m top5: 0.6399253731343284
[2m[36m(func pid=186463)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=186463)[0m f1_macro: 0.11283973240479761
[2m[36m(func pid=186463)[0m f1_weighted: 0.17738228037523548
[2m[36m(func pid=186463)[0m f1_per_class: [0.076, 0.364, 0.0, 0.059, 0.0, 0.039, 0.293, 0.034, 0.0, 0.264]
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.5366 | Steps: 2 | Val loss: 1.9309 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5249 | Steps: 2 | Val loss: 2.1992 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=181607)[0m top1: 0.3493470149253731
[2m[36m(func pid=181607)[0m top5: 0.8386194029850746
[2m[36m(func pid=181607)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=181607)[0m f1_macro: 0.3214075959147763
[2m[36m(func pid=181607)[0m f1_weighted: 0.38052481890910045
[2m[36m(func pid=181607)[0m f1_per_class: [0.33, 0.293, 0.462, 0.452, 0.056, 0.392, 0.397, 0.376, 0.171, 0.286]
[2m[36m(func pid=176361)[0m top1: 0.22807835820895522
[2m[36m(func pid=176361)[0m top5: 0.6986940298507462
[2m[36m(func pid=176361)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=176361)[0m f1_macro: 0.18827139055844438
[2m[36m(func pid=176361)[0m f1_weighted: 0.247381166364932
[2m[36m(func pid=176361)[0m f1_per_class: [0.159, 0.277, 0.333, 0.267, 0.027, 0.366, 0.23, 0.133, 0.022, 0.067]
== Status ==
Current time: 2024-01-07 04:49:38 (running for 00:39:52.59)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.506 |      0.18  |                   46 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.642 |      0.317 |                   23 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=187246)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=187246)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=187246)[0m Configuration completed!
[2m[36m(func pid=187246)[0m New optimizer parameters:
[2m[36m(func pid=187246)[0m SGD (
[2m[36m(func pid=187246)[0m Parameter Group 0
[2m[36m(func pid=187246)[0m     dampening: 0
[2m[36m(func pid=187246)[0m     differentiable: False
[2m[36m(func pid=187246)[0m     foreach: None
[2m[36m(func pid=187246)[0m     lr: 0.1
[2m[36m(func pid=187246)[0m     maximize: False
[2m[36m(func pid=187246)[0m     momentum: 0.9
[2m[36m(func pid=187246)[0m     nesterov: False
[2m[36m(func pid=187246)[0m     weight_decay: 1e-05
[2m[36m(func pid=187246)[0m )
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:49:45 (running for 00:39:59.54)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.506 |      0.18  |                   46 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.537 |      0.321 |                   24 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5011 | Steps: 2 | Val loss: 1.9324 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.5317 | Steps: 2 | Val loss: 2.2010 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7273 | Steps: 2 | Val loss: 2.2292 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.8803 | Steps: 2 | Val loss: 2.3802 | Batch size: 32 | lr: 0.1 | Duration: 4.94s
== Status ==
Current time: 2024-01-07 04:49:50 (running for 00:40:04.58)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.525 |      0.188 |                   47 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.537 |      0.321 |                   24 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  2.944 |      0.113 |                    1 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3423507462686567
[2m[36m(func pid=181607)[0m top5: 0.8381529850746269
[2m[36m(func pid=181607)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=181607)[0m f1_macro: 0.31469075544374026
[2m[36m(func pid=181607)[0m f1_weighted: 0.37425651637757734
[2m[36m(func pid=181607)[0m f1_per_class: [0.326, 0.303, 0.429, 0.441, 0.063, 0.389, 0.384, 0.376, 0.169, 0.267]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.19869402985074627
[2m[36m(func pid=186463)[0m top5: 0.6613805970149254
[2m[36m(func pid=186463)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=186463)[0m f1_macro: 0.1696959987770569
[2m[36m(func pid=186463)[0m f1_weighted: 0.20929022676318948
[2m[36m(func pid=186463)[0m f1_per_class: [0.091, 0.291, 0.513, 0.114, 0.0, 0.023, 0.386, 0.036, 0.0, 0.244]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=176361)[0m top1: 0.22014925373134328
[2m[36m(func pid=176361)[0m top5: 0.6968283582089553
[2m[36m(func pid=176361)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=176361)[0m f1_macro: 0.18236354683037997
[2m[36m(func pid=176361)[0m f1_weighted: 0.2363393578179691
[2m[36m(func pid=176361)[0m f1_per_class: [0.155, 0.287, 0.314, 0.241, 0.027, 0.369, 0.213, 0.129, 0.023, 0.066]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=187246)[0m top1: 0.025652985074626867
[2m[36m(func pid=187246)[0m top5: 0.5032649253731343
[2m[36m(func pid=187246)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=187246)[0m f1_macro: 0.026212541626095893
[2m[36m(func pid=187246)[0m f1_weighted: 0.01681452024107203
[2m[36m(func pid=187246)[0m f1_per_class: [0.049, 0.04, 0.014, 0.0, 0.0, 0.047, 0.0, 0.047, 0.0, 0.064]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5721 | Steps: 2 | Val loss: 1.9326 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.5213 | Steps: 2 | Val loss: 2.2019 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4411 | Steps: 2 | Val loss: 2.1964 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.1434 | Steps: 2 | Val loss: 2.1883 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:49:55 (running for 00:40:10.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.532 |      0.182 |                   48 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.572 |      0.314 |                   26 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  2.727 |      0.17  |                    2 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  2.88  |      0.026 |                    1 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.33488805970149255
[2m[36m(func pid=181607)[0m top5: 0.8348880597014925
[2m[36m(func pid=181607)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=181607)[0m f1_macro: 0.31353856057036444
[2m[36m(func pid=181607)[0m f1_weighted: 0.36652592055006866
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.302, 0.436, 0.419, 0.061, 0.391, 0.375, 0.398, 0.153, 0.267]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.22294776119402984
[2m[36m(func pid=176361)[0m top5: 0.6935634328358209
[2m[36m(func pid=176361)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=176361)[0m f1_macro: 0.1820271402169411
[2m[36m(func pid=176361)[0m f1_weighted: 0.23750642242248682
[2m[36m(func pid=176361)[0m f1_per_class: [0.156, 0.293, 0.289, 0.23, 0.027, 0.376, 0.219, 0.142, 0.025, 0.064]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=186463)[0m top1: 0.23134328358208955
[2m[36m(func pid=186463)[0m top5: 0.6749067164179104
[2m[36m(func pid=186463)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=186463)[0m f1_macro: 0.15147075525912537
[2m[36m(func pid=186463)[0m f1_weighted: 0.23617406506707048
[2m[36m(func pid=186463)[0m f1_per_class: [0.094, 0.165, 0.265, 0.189, 0.02, 0.007, 0.492, 0.0, 0.032, 0.25]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m top1: 0.24207089552238806
[2m[36m(func pid=187246)[0m top5: 0.7560634328358209
[2m[36m(func pid=187246)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=187246)[0m f1_macro: 0.13297505085012498
[2m[36m(func pid=187246)[0m f1_weighted: 0.24334332077714782
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.076, 0.026, 0.238, 0.343, 0.016, 0.53, 0.0, 0.0, 0.1]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.4336 | Steps: 2 | Val loss: 1.9144 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4686 | Steps: 2 | Val loss: 2.2025 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.1363 | Steps: 2 | Val loss: 2.1555 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.5641 | Steps: 2 | Val loss: 1.8840 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=186463)[0m top1: 0.23414179104477612
[2m[36m(func pid=186463)[0m top5: 0.7229477611940298
[2m[36m(func pid=186463)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=186463)[0m f1_macro: 0.17484220963996192
[2m[36m(func pid=186463)[0m f1_weighted: 0.2500365101722089
[2m[36m(func pid=186463)[0m f1_per_class: [0.09, 0.174, 0.296, 0.264, 0.056, 0.0, 0.456, 0.015, 0.048, 0.349]
[2m[36m(func pid=186463)[0m 
== Status ==
Current time: 2024-01-07 04:50:01 (running for 00:40:15.52)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.521 |      0.182 |                   49 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.572 |      0.314 |                   26 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  2.136 |      0.175 |                    4 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  2.143 |      0.133 |                    2 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.33955223880597013
[2m[36m(func pid=181607)[0m top5: 0.8404850746268657
[2m[36m(func pid=181607)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=181607)[0m f1_macro: 0.31850653616281865
[2m[36m(func pid=181607)[0m f1_weighted: 0.36999295270113547
[2m[36m(func pid=181607)[0m f1_per_class: [0.337, 0.31, 0.462, 0.429, 0.064, 0.385, 0.374, 0.397, 0.164, 0.264]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m top1: 0.22294776119402984
[2m[36m(func pid=176361)[0m top5: 0.6930970149253731
[2m[36m(func pid=176361)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=176361)[0m f1_macro: 0.18647703285921527
[2m[36m(func pid=176361)[0m f1_weighted: 0.23401124504945514
[2m[36m(func pid=176361)[0m f1_per_class: [0.183, 0.309, 0.306, 0.21, 0.03, 0.376, 0.211, 0.151, 0.027, 0.061]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=187246)[0m top1: 0.23507462686567165
[2m[36m(func pid=187246)[0m top5: 0.8773320895522388
[2m[36m(func pid=187246)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=187246)[0m f1_macro: 0.15030543551608924
[2m[36m(func pid=187246)[0m f1_weighted: 0.20782240921485257
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.11, 0.316, 0.509, 0.047, 0.059, 0.08, 0.205, 0.0, 0.177]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.3804 | Steps: 2 | Val loss: 1.9065 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.0043 | Steps: 2 | Val loss: 2.0348 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4722 | Steps: 2 | Val loss: 2.2014 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0560 | Steps: 2 | Val loss: 8.0861 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:50:06 (running for 00:40:20.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.469 |      0.186 |                   50 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.38  |      0.332 |                   28 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  2.136 |      0.175 |                    4 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.564 |      0.15  |                    3 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3516791044776119
[2m[36m(func pid=181607)[0m top5: 0.8409514925373134
[2m[36m(func pid=181607)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=181607)[0m f1_macro: 0.33174442274727806
[2m[36m(func pid=181607)[0m f1_weighted: 0.3837460741413074
[2m[36m(func pid=181607)[0m f1_per_class: [0.32, 0.321, 0.545, 0.449, 0.064, 0.393, 0.388, 0.397, 0.197, 0.244]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.31949626865671643
[2m[36m(func pid=186463)[0m top5: 0.757929104477612
[2m[36m(func pid=186463)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=186463)[0m f1_macro: 0.22941154891103838
[2m[36m(func pid=186463)[0m f1_weighted: 0.32722443186214667
[2m[36m(func pid=186463)[0m f1_per_class: [0.125, 0.21, 0.358, 0.455, 0.113, 0.024, 0.482, 0.105, 0.058, 0.364]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=176361)[0m top1: 0.2224813432835821
[2m[36m(func pid=176361)[0m top5: 0.6972947761194029
[2m[36m(func pid=176361)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=176361)[0m f1_macro: 0.1862491843414447
[2m[36m(func pid=176361)[0m f1_weighted: 0.2327393672553043
[2m[36m(func pid=176361)[0m f1_per_class: [0.188, 0.312, 0.306, 0.194, 0.026, 0.375, 0.22, 0.153, 0.028, 0.061]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=187246)[0m top1: 0.13852611940298507
[2m[36m(func pid=187246)[0m top5: 0.40345149253731344
[2m[36m(func pid=187246)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=187246)[0m f1_macro: 0.06763329700626894
[2m[36m(func pid=187246)[0m f1_weighted: 0.14629476405718503
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.0, 0.022, 0.0, 0.0, 0.0, 0.472, 0.07, 0.0, 0.111]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3112 | Steps: 2 | Val loss: 1.8960 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4844 | Steps: 2 | Val loss: 1.9602 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4166 | Steps: 2 | Val loss: 2.2042 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:50:11 (running for 00:40:25.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.472 |      0.186 |                   51 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.311 |      0.344 |                   29 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  2.004 |      0.229 |                    5 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.056 |      0.068 |                    4 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3619402985074627
[2m[36m(func pid=181607)[0m top5: 0.8409514925373134
[2m[36m(func pid=181607)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=181607)[0m f1_macro: 0.3437883843141022
[2m[36m(func pid=181607)[0m f1_weighted: 0.39458189575600067
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.319, 0.615, 0.466, 0.073, 0.392, 0.407, 0.408, 0.184, 0.24]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.7133 | Steps: 2 | Val loss: 7.6468 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=186463)[0m top1: 0.34421641791044777
[2m[36m(func pid=186463)[0m top5: 0.7583955223880597
[2m[36m(func pid=186463)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=186463)[0m f1_macro: 0.2692957375335404
[2m[36m(func pid=186463)[0m f1_weighted: 0.33753282977750954
[2m[36m(func pid=186463)[0m f1_per_class: [0.162, 0.196, 0.6, 0.491, 0.159, 0.016, 0.459, 0.232, 0.094, 0.283]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=176361)[0m top1: 0.2196828358208955
[2m[36m(func pid=176361)[0m top5: 0.691231343283582
[2m[36m(func pid=176361)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=176361)[0m f1_macro: 0.18628586532374142
[2m[36m(func pid=176361)[0m f1_weighted: 0.2305988414772268
[2m[36m(func pid=176361)[0m f1_per_class: [0.202, 0.298, 0.293, 0.195, 0.033, 0.377, 0.219, 0.15, 0.03, 0.065]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=187246)[0m top1: 0.027985074626865673
[2m[36m(func pid=187246)[0m top5: 0.384794776119403
[2m[36m(func pid=187246)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=187246)[0m f1_macro: 0.02700721879705654
[2m[36m(func pid=187246)[0m f1_weighted: 0.024747785998655625
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.133, 0.082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035, 0.02]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.2817 | Steps: 2 | Val loss: 1.8903 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1304 | Steps: 2 | Val loss: 2.0156 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4341 | Steps: 2 | Val loss: 2.2062 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:50:16 (running for 00:40:30.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.417 |      0.186 |                   52 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.282 |      0.342 |                   30 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  1.484 |      0.269 |                    6 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.713 |      0.027 |                    5 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3572761194029851
[2m[36m(func pid=181607)[0m top5: 0.8446828358208955
[2m[36m(func pid=181607)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=181607)[0m f1_macro: 0.34195035754586584
[2m[36m(func pid=181607)[0m f1_weighted: 0.3893231467616915
[2m[36m(func pid=181607)[0m f1_per_class: [0.329, 0.321, 0.615, 0.462, 0.072, 0.4, 0.39, 0.397, 0.196, 0.238]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.1989 | Steps: 2 | Val loss: 15.2217 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=186463)[0m top1: 0.27611940298507465
[2m[36m(func pid=186463)[0m top5: 0.7667910447761194
[2m[36m(func pid=186463)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=186463)[0m f1_macro: 0.23087595688483936
[2m[36m(func pid=186463)[0m f1_weighted: 0.2335895178139016
[2m[36m(func pid=186463)[0m f1_per_class: [0.228, 0.247, 0.429, 0.438, 0.14, 0.116, 0.073, 0.337, 0.111, 0.189]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=176361)[0m top1: 0.21688432835820895
[2m[36m(func pid=176361)[0m top5: 0.6870335820895522
[2m[36m(func pid=176361)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=176361)[0m f1_macro: 0.1838021948603917
[2m[36m(func pid=176361)[0m f1_weighted: 0.22535212340984578
[2m[36m(func pid=176361)[0m f1_per_class: [0.198, 0.305, 0.289, 0.177, 0.033, 0.37, 0.216, 0.15, 0.033, 0.066]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=187246)[0m top1: 0.014925373134328358
[2m[36m(func pid=187246)[0m top5: 0.1609141791044776
[2m[36m(func pid=187246)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=187246)[0m f1_macro: 0.014717992213498506
[2m[36m(func pid=187246)[0m f1_weighted: 0.002409182387921109
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036, 0.095]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2203 | Steps: 2 | Val loss: 1.8805 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.0356 | Steps: 2 | Val loss: 1.9830 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4395 | Steps: 2 | Val loss: 2.2033 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:50:21 (running for 00:40:36.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.434 |      0.184 |                   53 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.22  |      0.343 |                   31 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  1.13  |      0.231 |                    7 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  2.199 |      0.015 |                    6 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.35447761194029853
[2m[36m(func pid=181607)[0m top5: 0.847481343283582
[2m[36m(func pid=181607)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=181607)[0m f1_macro: 0.34275028793823475
[2m[36m(func pid=181607)[0m f1_weighted: 0.3849311211767689
[2m[36m(func pid=181607)[0m f1_per_class: [0.313, 0.321, 0.649, 0.458, 0.074, 0.382, 0.385, 0.4, 0.2, 0.245]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.28591417910447764
[2m[36m(func pid=186463)[0m top5: 0.8064365671641791
[2m[36m(func pid=186463)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=186463)[0m f1_macro: 0.2529136422930889
[2m[36m(func pid=186463)[0m f1_weighted: 0.26454947135383916
[2m[36m(func pid=186463)[0m f1_per_class: [0.193, 0.229, 0.358, 0.424, 0.169, 0.304, 0.114, 0.413, 0.144, 0.18]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.6818 | Steps: 2 | Val loss: 1477.4124 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=176361)[0m top1: 0.21875
[2m[36m(func pid=176361)[0m top5: 0.6902985074626866
[2m[36m(func pid=176361)[0m f1_micro: 0.21875
[2m[36m(func pid=176361)[0m f1_macro: 0.18706241944471969
[2m[36m(func pid=176361)[0m f1_weighted: 0.2294912923227044
[2m[36m(func pid=176361)[0m f1_per_class: [0.197, 0.303, 0.289, 0.186, 0.032, 0.37, 0.222, 0.151, 0.033, 0.086]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2936 | Steps: 2 | Val loss: 1.8798 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=187246)[0m top1: 0.006063432835820896
[2m[36m(func pid=187246)[0m top5: 0.511660447761194
[2m[36m(func pid=187246)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=187246)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=187246)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7557 | Steps: 2 | Val loss: 1.8608 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3999 | Steps: 2 | Val loss: 2.1994 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=181607)[0m top1: 0.3460820895522388
[2m[36m(func pid=181607)[0m top5: 0.8502798507462687
[2m[36m(func pid=181607)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=181607)[0m f1_macro: 0.34222035073047313
[2m[36m(func pid=181607)[0m f1_weighted: 0.3761260974609241
[2m[36m(func pid=181607)[0m f1_per_class: [0.328, 0.327, 0.649, 0.432, 0.07, 0.387, 0.372, 0.411, 0.192, 0.255]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:27 (running for 00:40:41.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.44  |      0.187 |                   54 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.294 |      0.342 |                   32 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.756 |      0.28  |                    9 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  3.682 |      0.001 |                    7 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3362873134328358
[2m[36m(func pid=186463)[0m top5: 0.8311567164179104
[2m[36m(func pid=186463)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=186463)[0m f1_macro: 0.2796959658052066
[2m[36m(func pid=186463)[0m f1_weighted: 0.33776537602031786
[2m[36m(func pid=186463)[0m f1_per_class: [0.227, 0.224, 0.421, 0.479, 0.159, 0.186, 0.355, 0.404, 0.134, 0.209]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.9282 | Steps: 2 | Val loss: 2609.5918 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=176361)[0m top1: 0.2196828358208955
[2m[36m(func pid=176361)[0m top5: 0.6949626865671642
[2m[36m(func pid=176361)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=176361)[0m f1_macro: 0.1865930012361755
[2m[36m(func pid=176361)[0m f1_weighted: 0.23237223358061593
[2m[36m(func pid=176361)[0m f1_per_class: [0.2, 0.301, 0.275, 0.196, 0.033, 0.368, 0.224, 0.157, 0.03, 0.082]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.1068 | Steps: 2 | Val loss: 1.8776 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=187246)[0m top1: 0.17117537313432835
[2m[36m(func pid=187246)[0m top5: 0.5718283582089553
[2m[36m(func pid=187246)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=187246)[0m f1_macro: 0.029254683140693506
[2m[36m(func pid=187246)[0m f1_weighted: 0.05034971118897343
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6242 | Steps: 2 | Val loss: 1.8302 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3970 | Steps: 2 | Val loss: 2.1942 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=181607)[0m top1: 0.3353544776119403
[2m[36m(func pid=181607)[0m top5: 0.8456156716417911
[2m[36m(func pid=181607)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=181607)[0m f1_macro: 0.3401837981944558
[2m[36m(func pid=181607)[0m f1_weighted: 0.3632680413197036
[2m[36m(func pid=181607)[0m f1_per_class: [0.351, 0.32, 0.686, 0.411, 0.067, 0.382, 0.355, 0.421, 0.157, 0.253]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:32 (running for 00:40:47.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.4   |      0.187 |                   55 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.107 |      0.34  |                   33 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.624 |      0.32  |                   10 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  3.928 |      0.029 |                    8 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3675373134328358
[2m[36m(func pid=186463)[0m top5: 0.8344216417910447
[2m[36m(func pid=186463)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=186463)[0m f1_macro: 0.32029154371735064
[2m[36m(func pid=186463)[0m f1_weighted: 0.37926484675814387
[2m[36m(func pid=186463)[0m f1_per_class: [0.189, 0.207, 0.75, 0.459, 0.151, 0.164, 0.531, 0.381, 0.134, 0.237]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3132 | Steps: 2 | Val loss: 635.3021 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=176361)[0m top1: 0.22527985074626866
[2m[36m(func pid=176361)[0m top5: 0.7033582089552238
[2m[36m(func pid=176361)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=176361)[0m f1_macro: 0.18980282416207286
[2m[36m(func pid=176361)[0m f1_weighted: 0.23962887637575864
[2m[36m(func pid=176361)[0m f1_per_class: [0.19, 0.31, 0.293, 0.206, 0.029, 0.383, 0.231, 0.149, 0.029, 0.078]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.2461 | Steps: 2 | Val loss: 1.8627 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=187246)[0m top1: 0.09281716417910447
[2m[36m(func pid=187246)[0m top5: 0.5013992537313433
[2m[36m(func pid=187246)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=187246)[0m f1_macro: 0.036217012914093204
[2m[36m(func pid=187246)[0m f1_weighted: 0.03334876493096002
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.186, 0.0, 0.0, 0.167, 0.0, 0.0, 0.0, 0.0, 0.01]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5713 | Steps: 2 | Val loss: 1.8222 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4253 | Steps: 2 | Val loss: 2.1914 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=181607)[0m top1: 0.3353544776119403
[2m[36m(func pid=181607)[0m top5: 0.8507462686567164
[2m[36m(func pid=181607)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=181607)[0m f1_macro: 0.3393249107656766
[2m[36m(func pid=181607)[0m f1_weighted: 0.3614739301033436
[2m[36m(func pid=181607)[0m f1_per_class: [0.358, 0.323, 0.686, 0.405, 0.07, 0.372, 0.356, 0.415, 0.16, 0.248]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:38 (running for 00:40:52.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.397 |      0.19  |                   56 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.246 |      0.339 |                   34 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.571 |      0.34  |                   11 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  2.313 |      0.036 |                    9 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.35634328358208955
[2m[36m(func pid=186463)[0m top5: 0.863339552238806
[2m[36m(func pid=186463)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=186463)[0m f1_macro: 0.3398982610149585
[2m[36m(func pid=186463)[0m f1_weighted: 0.36982201710166573
[2m[36m(func pid=186463)[0m f1_per_class: [0.203, 0.229, 0.815, 0.456, 0.177, 0.297, 0.434, 0.37, 0.152, 0.267]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.5668 | Steps: 2 | Val loss: 198.7066 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176361)[0m top1: 0.23227611940298507
[2m[36m(func pid=176361)[0m top5: 0.7080223880597015
[2m[36m(func pid=176361)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=176361)[0m f1_macro: 0.19577783259756149
[2m[36m(func pid=176361)[0m f1_weighted: 0.2486329504935868
[2m[36m(func pid=176361)[0m f1_per_class: [0.194, 0.315, 0.297, 0.228, 0.029, 0.38, 0.236, 0.154, 0.029, 0.094]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0158 | Steps: 2 | Val loss: 1.8483 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=187246)[0m top1: 0.02378731343283582
[2m[36m(func pid=187246)[0m top5: 0.37826492537313433
[2m[36m(func pid=187246)[0m f1_micro: 0.02378731343283582
[2m[36m(func pid=187246)[0m f1_macro: 0.01386646223811916
[2m[36m(func pid=187246)[0m f1_weighted: 0.009787493722571804
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.047, 0.0, 0.0, 0.072, 0.0, 0.0, 0.02, 0.0, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3085 | Steps: 2 | Val loss: 1.8449 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3991 | Steps: 2 | Val loss: 2.1850 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=181607)[0m top1: 0.3512126865671642
[2m[36m(func pid=181607)[0m top5: 0.8591417910447762
[2m[36m(func pid=181607)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=181607)[0m f1_macro: 0.34913533013661546
[2m[36m(func pid=181607)[0m f1_weighted: 0.38049876433641144
[2m[36m(func pid=181607)[0m f1_per_class: [0.364, 0.325, 0.686, 0.425, 0.073, 0.372, 0.395, 0.431, 0.182, 0.24]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:43 (running for 00:40:57.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.425 |      0.196 |                   57 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  1.016 |      0.349 |                   35 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.309 |      0.341 |                   12 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  3.567 |      0.014 |                   10 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.34468283582089554
[2m[36m(func pid=186463)[0m top5: 0.871268656716418
[2m[36m(func pid=186463)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=186463)[0m f1_macro: 0.34090395113333666
[2m[36m(func pid=186463)[0m f1_weighted: 0.3512883901067276
[2m[36m(func pid=186463)[0m f1_per_class: [0.227, 0.259, 0.857, 0.455, 0.158, 0.384, 0.318, 0.386, 0.162, 0.204]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.9597 | Steps: 2 | Val loss: 88.3458 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=176361)[0m top1: 0.23694029850746268
[2m[36m(func pid=176361)[0m top5: 0.7182835820895522
[2m[36m(func pid=176361)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=176361)[0m f1_macro: 0.20107991743494208
[2m[36m(func pid=176361)[0m f1_weighted: 0.2552882188203222
[2m[36m(func pid=176361)[0m f1_per_class: [0.191, 0.312, 0.328, 0.243, 0.031, 0.384, 0.245, 0.152, 0.027, 0.098]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9591 | Steps: 2 | Val loss: 1.8398 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=187246)[0m top1: 0.10494402985074627
[2m[36m(func pid=187246)[0m top5: 0.5946828358208955
[2m[36m(func pid=187246)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=187246)[0m f1_macro: 0.04475083825814163
[2m[36m(func pid=187246)[0m f1_weighted: 0.05227129605920106
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.251, 0.0, 0.0, 0.045, 0.0, 0.0, 0.151, 0.0, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3156 | Steps: 2 | Val loss: 1.8514 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3722 | Steps: 2 | Val loss: 2.1860 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=181607)[0m top1: 0.3596082089552239
[2m[36m(func pid=181607)[0m top5: 0.8614738805970149
[2m[36m(func pid=181607)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=181607)[0m f1_macro: 0.35035742214329413
[2m[36m(func pid=181607)[0m f1_weighted: 0.3908888750157636
[2m[36m(func pid=181607)[0m f1_per_class: [0.362, 0.325, 0.667, 0.448, 0.074, 0.374, 0.41, 0.41, 0.196, 0.238]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:48 (running for 00:41:03.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.399 |      0.201 |                   58 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.959 |      0.35  |                   36 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.316 |      0.364 |                   13 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.96  |      0.045 |                   11 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.36613805970149255
[2m[36m(func pid=186463)[0m top5: 0.8777985074626866
[2m[36m(func pid=186463)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=186463)[0m f1_macro: 0.3638450211613562
[2m[36m(func pid=186463)[0m f1_weighted: 0.37903925779670294
[2m[36m(func pid=186463)[0m f1_per_class: [0.203, 0.268, 0.857, 0.44, 0.175, 0.443, 0.382, 0.44, 0.209, 0.221]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8447 | Steps: 2 | Val loss: 56.1723 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=176361)[0m top1: 0.23787313432835822
[2m[36m(func pid=176361)[0m top5: 0.7140858208955224
[2m[36m(func pid=176361)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=176361)[0m f1_macro: 0.20357443347215248
[2m[36m(func pid=176361)[0m f1_weighted: 0.25705058458373614
[2m[36m(func pid=176361)[0m f1_per_class: [0.186, 0.315, 0.328, 0.244, 0.03, 0.393, 0.242, 0.164, 0.039, 0.095]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.9934 | Steps: 2 | Val loss: 1.8381 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=187246)[0m top1: 0.13386194029850745
[2m[36m(func pid=187246)[0m top5: 0.5592350746268657
[2m[36m(func pid=187246)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=187246)[0m f1_macro: 0.06485641027536224
[2m[36m(func pid=187246)[0m f1_weighted: 0.06746921714698818
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.329, 0.0, 0.0, 0.097, 0.0, 0.0, 0.164, 0.0, 0.059]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1887 | Steps: 2 | Val loss: 1.8374 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4085 | Steps: 2 | Val loss: 2.1852 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=181607)[0m top1: 0.3568097014925373
[2m[36m(func pid=181607)[0m top5: 0.8628731343283582
[2m[36m(func pid=181607)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=181607)[0m f1_macro: 0.3488355811974392
[2m[36m(func pid=181607)[0m f1_weighted: 0.38941566072633926
[2m[36m(func pid=181607)[0m f1_per_class: [0.364, 0.314, 0.667, 0.454, 0.073, 0.382, 0.405, 0.398, 0.204, 0.229]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:54 (running for 00:41:08.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.372 |      0.204 |                   59 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.993 |      0.349 |                   37 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.189 |      0.365 |                   14 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.845 |      0.065 |                   12 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3763992537313433
[2m[36m(func pid=186463)[0m top5: 0.8875932835820896
[2m[36m(func pid=186463)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=186463)[0m f1_macro: 0.3649306774821749
[2m[36m(func pid=186463)[0m f1_weighted: 0.3946012641046336
[2m[36m(func pid=186463)[0m f1_per_class: [0.221, 0.3, 0.846, 0.427, 0.179, 0.387, 0.459, 0.382, 0.207, 0.241]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.6639 | Steps: 2 | Val loss: 75.7904 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=176361)[0m top1: 0.23227611940298507
[2m[36m(func pid=176361)[0m top5: 0.71875
[2m[36m(func pid=176361)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=176361)[0m f1_macro: 0.20104531688872757
[2m[36m(func pid=176361)[0m f1_weighted: 0.2514401161088812
[2m[36m(func pid=176361)[0m f1_per_class: [0.194, 0.305, 0.324, 0.237, 0.028, 0.391, 0.236, 0.157, 0.041, 0.097]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.9324 | Steps: 2 | Val loss: 1.8374 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=187246)[0m top1: 0.060167910447761194
[2m[36m(func pid=187246)[0m top5: 0.3763992537313433
[2m[36m(func pid=187246)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=187246)[0m f1_macro: 0.0402808001889406
[2m[36m(func pid=187246)[0m f1_weighted: 0.06319667550253559
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.356, 0.0, 0.003, 0.029, 0.0, 0.0, 0.015, 0.0, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1721 | Steps: 2 | Val loss: 1.7685 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3347 | Steps: 2 | Val loss: 2.1865 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=181607)[0m top1: 0.3530783582089552
[2m[36m(func pid=181607)[0m top5: 0.8572761194029851
[2m[36m(func pid=181607)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=181607)[0m f1_macro: 0.35308653530887024
[2m[36m(func pid=181607)[0m f1_weighted: 0.3840052348002939
[2m[36m(func pid=181607)[0m f1_per_class: [0.348, 0.32, 0.706, 0.431, 0.075, 0.387, 0.397, 0.415, 0.222, 0.23]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:50:59 (running for 00:41:13.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.409 |      0.201 |                   60 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.932 |      0.353 |                   38 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.172 |      0.379 |                   15 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.664 |      0.04  |                   13 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.40671641791044777
[2m[36m(func pid=186463)[0m top5: 0.9118470149253731
[2m[36m(func pid=186463)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=186463)[0m f1_macro: 0.37946289880049144
[2m[36m(func pid=186463)[0m f1_weighted: 0.42305382618299453
[2m[36m(func pid=186463)[0m f1_per_class: [0.314, 0.307, 0.828, 0.459, 0.147, 0.377, 0.52, 0.373, 0.213, 0.257]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.5946 | Steps: 2 | Val loss: 96.7158 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=176361)[0m top1: 0.23367537313432835
[2m[36m(func pid=176361)[0m top5: 0.71875
[2m[36m(func pid=176361)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=176361)[0m f1_macro: 0.20758374286696468
[2m[36m(func pid=176361)[0m f1_weighted: 0.2499635509853155
[2m[36m(func pid=176361)[0m f1_per_class: [0.203, 0.311, 0.338, 0.218, 0.028, 0.391, 0.241, 0.17, 0.044, 0.132]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.9176 | Steps: 2 | Val loss: 1.8318 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=187246)[0m top1: 0.09048507462686567
[2m[36m(func pid=187246)[0m top5: 0.40951492537313433
[2m[36m(func pid=187246)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=187246)[0m f1_macro: 0.04890004502943966
[2m[36m(func pid=187246)[0m f1_weighted: 0.06989801102410972
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.401, 0.019, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.065]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1371 | Steps: 2 | Val loss: 1.7688 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3662 | Steps: 2 | Val loss: 2.1877 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=181607)[0m top1: 0.3530783582089552
[2m[36m(func pid=181607)[0m top5: 0.8563432835820896
[2m[36m(func pid=181607)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=181607)[0m f1_macro: 0.35659397439369245
[2m[36m(func pid=181607)[0m f1_weighted: 0.38375602130110914
[2m[36m(func pid=181607)[0m f1_per_class: [0.384, 0.322, 0.706, 0.425, 0.067, 0.379, 0.401, 0.416, 0.226, 0.24]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:51:04 (running for 00:41:19.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.335 |      0.208 |                   61 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.918 |      0.357 |                   39 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.137 |      0.372 |                   16 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.595 |      0.049 |                   14 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.4099813432835821
[2m[36m(func pid=186463)[0m top5: 0.9095149253731343
[2m[36m(func pid=186463)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=186463)[0m f1_macro: 0.37224021542732755
[2m[36m(func pid=186463)[0m f1_weighted: 0.4277158760808553
[2m[36m(func pid=186463)[0m f1_per_class: [0.356, 0.299, 0.727, 0.488, 0.112, 0.393, 0.501, 0.41, 0.212, 0.224]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.3111 | Steps: 2 | Val loss: 73.4232 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=176361)[0m top1: 0.23227611940298507
[2m[36m(func pid=176361)[0m top5: 0.7131529850746269
[2m[36m(func pid=176361)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=176361)[0m f1_macro: 0.20762727544557452
[2m[36m(func pid=176361)[0m f1_weighted: 0.2449929686563861
[2m[36m(func pid=176361)[0m f1_per_class: [0.215, 0.311, 0.328, 0.199, 0.032, 0.4, 0.237, 0.167, 0.048, 0.139]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9586 | Steps: 2 | Val loss: 1.8330 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=187246)[0m top1: 0.10027985074626866
[2m[36m(func pid=187246)[0m top5: 0.3917910447761194
[2m[36m(func pid=187246)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=187246)[0m f1_macro: 0.05503949108328713
[2m[36m(func pid=187246)[0m f1_weighted: 0.07764647516679422
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.406, 0.019, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.102]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0852 | Steps: 2 | Val loss: 1.8452 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3578 | Steps: 2 | Val loss: 2.1852 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=181607)[0m top1: 0.34888059701492535
[2m[36m(func pid=181607)[0m top5: 0.8582089552238806
[2m[36m(func pid=181607)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=181607)[0m f1_macro: 0.35481571865886924
[2m[36m(func pid=181607)[0m f1_weighted: 0.3775118379738783
[2m[36m(func pid=181607)[0m f1_per_class: [0.391, 0.327, 0.706, 0.428, 0.066, 0.378, 0.374, 0.426, 0.215, 0.238]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:51:10 (running for 00:41:24.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.366 |      0.208 |                   62 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.959 |      0.355 |                   40 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.085 |      0.356 |                   17 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.311 |      0.055 |                   15 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.394589552238806
[2m[36m(func pid=186463)[0m top5: 0.9067164179104478
[2m[36m(func pid=186463)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=186463)[0m f1_macro: 0.35622686454623703
[2m[36m(func pid=186463)[0m f1_weighted: 0.4142889929653889
[2m[36m(func pid=186463)[0m f1_per_class: [0.321, 0.328, 0.632, 0.48, 0.083, 0.398, 0.441, 0.461, 0.204, 0.215]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.4503 | Steps: 2 | Val loss: 35.0619 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=176361)[0m top1: 0.23787313432835822
[2m[36m(func pid=176361)[0m top5: 0.7112873134328358
[2m[36m(func pid=176361)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=176361)[0m f1_macro: 0.21575595692860156
[2m[36m(func pid=176361)[0m f1_weighted: 0.25072095411933776
[2m[36m(func pid=176361)[0m f1_per_class: [0.22, 0.316, 0.355, 0.205, 0.028, 0.394, 0.247, 0.169, 0.05, 0.173]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8913 | Steps: 2 | Val loss: 1.8316 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0804 | Steps: 2 | Val loss: 1.9282 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=187246)[0m top1: 0.1142723880597015
[2m[36m(func pid=187246)[0m top5: 0.47761194029850745
[2m[36m(func pid=187246)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=187246)[0m f1_macro: 0.051764032725750656
[2m[36m(func pid=187246)[0m f1_weighted: 0.08475853717019283
[2m[36m(func pid=187246)[0m f1_per_class: [0.026, 0.331, 0.0, 0.089, 0.0, 0.0, 0.006, 0.0, 0.0, 0.066]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3800 | Steps: 2 | Val loss: 2.1879 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=181607)[0m top1: 0.3493470149253731
[2m[36m(func pid=181607)[0m top5: 0.8614738805970149
[2m[36m(func pid=181607)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=181607)[0m f1_macro: 0.3522601387044094
[2m[36m(func pid=181607)[0m f1_weighted: 0.3770883881921422
[2m[36m(func pid=181607)[0m f1_per_class: [0.391, 0.329, 0.686, 0.433, 0.067, 0.381, 0.366, 0.429, 0.201, 0.24]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:51:15 (running for 00:41:29.99)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.358 |      0.216 |                   63 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.891 |      0.352 |                   41 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.08  |      0.33  |                   18 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.45  |      0.052 |                   16 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.37453358208955223
[2m[36m(func pid=186463)[0m top5: 0.8880597014925373
[2m[36m(func pid=186463)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=186463)[0m f1_macro: 0.33038561207119416
[2m[36m(func pid=186463)[0m f1_weighted: 0.3956021176463134
[2m[36m(func pid=186463)[0m f1_per_class: [0.301, 0.332, 0.462, 0.464, 0.068, 0.399, 0.394, 0.462, 0.235, 0.188]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5194 | Steps: 2 | Val loss: 13.8576 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=176361)[0m top1: 0.23600746268656717
[2m[36m(func pid=176361)[0m top5: 0.7080223880597015
[2m[36m(func pid=176361)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=176361)[0m f1_macro: 0.2144807655547524
[2m[36m(func pid=176361)[0m f1_weighted: 0.2472686637479333
[2m[36m(func pid=176361)[0m f1_per_class: [0.219, 0.317, 0.343, 0.192, 0.028, 0.395, 0.245, 0.182, 0.051, 0.173]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8389 | Steps: 2 | Val loss: 1.8186 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=187246)[0m top1: 0.10680970149253731
[2m[36m(func pid=187246)[0m top5: 0.6189365671641791
[2m[36m(func pid=187246)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=187246)[0m f1_macro: 0.06936989051504902
[2m[36m(func pid=187246)[0m f1_weighted: 0.10297164314025647
[2m[36m(func pid=187246)[0m f1_per_class: [0.096, 0.277, 0.0, 0.149, 0.0, 0.032, 0.015, 0.024, 0.047, 0.054]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0566 | Steps: 2 | Val loss: 1.9943 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3305 | Steps: 2 | Val loss: 2.1859 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=181607)[0m top1: 0.35867537313432835
[2m[36m(func pid=181607)[0m top5: 0.863339552238806
[2m[36m(func pid=181607)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=181607)[0m f1_macro: 0.35835967873925995
[2m[36m(func pid=181607)[0m f1_weighted: 0.3864583864988145
[2m[36m(func pid=181607)[0m f1_per_class: [0.393, 0.331, 0.686, 0.457, 0.067, 0.387, 0.367, 0.444, 0.218, 0.234]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:51:20 (running for 00:41:35.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.38  |      0.214 |                   64 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.839 |      0.358 |                   42 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.057 |      0.309 |                   19 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.519 |      0.069 |                   17 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3521455223880597
[2m[36m(func pid=186463)[0m top5: 0.8684701492537313
[2m[36m(func pid=186463)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=186463)[0m f1_macro: 0.30900559190244403
[2m[36m(func pid=186463)[0m f1_weighted: 0.3786328145143894
[2m[36m(func pid=186463)[0m f1_per_class: [0.256, 0.318, 0.364, 0.43, 0.067, 0.388, 0.385, 0.464, 0.245, 0.173]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.6865 | Steps: 2 | Val loss: 17.0108 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=176361)[0m top1: 0.23740671641791045
[2m[36m(func pid=176361)[0m top5: 0.7084888059701493
[2m[36m(func pid=176361)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=176361)[0m f1_macro: 0.21622174532155838
[2m[36m(func pid=176361)[0m f1_weighted: 0.2489036698828659
[2m[36m(func pid=176361)[0m f1_per_class: [0.239, 0.316, 0.353, 0.2, 0.033, 0.395, 0.244, 0.182, 0.034, 0.167]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7987 | Steps: 2 | Val loss: 1.8159 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=187246)[0m top1: 0.1921641791044776
[2m[36m(func pid=187246)[0m top5: 0.8166977611940298
[2m[36m(func pid=187246)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=187246)[0m f1_macro: 0.10948023446710917
[2m[36m(func pid=187246)[0m f1_weighted: 0.15761135673962326
[2m[36m(func pid=187246)[0m f1_per_class: [0.024, 0.327, 0.0, 0.186, 0.095, 0.068, 0.093, 0.201, 0.0, 0.1]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0645 | Steps: 2 | Val loss: 1.9496 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.3137 | Steps: 2 | Val loss: 2.1850 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=181607)[0m top1: 0.3675373134328358
[2m[36m(func pid=181607)[0m top5: 0.863339552238806
[2m[36m(func pid=181607)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=181607)[0m f1_macro: 0.36106294410249296
[2m[36m(func pid=181607)[0m f1_weighted: 0.3969695920157236
[2m[36m(func pid=181607)[0m f1_per_class: [0.381, 0.334, 0.686, 0.482, 0.074, 0.388, 0.378, 0.439, 0.221, 0.227]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:51:26 (running for 00:41:40.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.33  |      0.216 |                   65 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.799 |      0.361 |                   43 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.064 |      0.337 |                   20 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  3.686 |      0.109 |                   18 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3871268656716418
[2m[36m(func pid=186463)[0m top5: 0.8843283582089553
[2m[36m(func pid=186463)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=186463)[0m f1_macro: 0.33670731701158907
[2m[36m(func pid=186463)[0m f1_weighted: 0.41565749273449537
[2m[36m(func pid=186463)[0m f1_per_class: [0.297, 0.334, 0.414, 0.467, 0.112, 0.383, 0.46, 0.476, 0.235, 0.189]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5212 | Steps: 2 | Val loss: 48.2142 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=176361)[0m top1: 0.2355410447761194
[2m[36m(func pid=176361)[0m top5: 0.7084888059701493
[2m[36m(func pid=176361)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=176361)[0m f1_macro: 0.2126929643226388
[2m[36m(func pid=176361)[0m f1_weighted: 0.247793338150764
[2m[36m(func pid=176361)[0m f1_per_class: [0.22, 0.322, 0.348, 0.202, 0.024, 0.393, 0.237, 0.18, 0.051, 0.151]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8265 | Steps: 2 | Val loss: 1.8096 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=187246)[0m top1: 0.11380597014925373
[2m[36m(func pid=187246)[0m top5: 0.5872201492537313
[2m[36m(func pid=187246)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=187246)[0m f1_macro: 0.041721542787148194
[2m[36m(func pid=187246)[0m f1_weighted: 0.07142576591719285
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.336, 0.0, 0.047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0380 | Steps: 2 | Val loss: 1.9321 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=181607)[0m top1: 0.37173507462686567
[2m[36m(func pid=181607)[0m top5: 0.8642723880597015
[2m[36m(func pid=181607)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=181607)[0m f1_macro: 0.3642394786158575
[2m[36m(func pid=181607)[0m f1_weighted: 0.3993543085819674
[2m[36m(func pid=181607)[0m f1_per_class: [0.379, 0.334, 0.686, 0.491, 0.072, 0.396, 0.372, 0.449, 0.239, 0.226]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3241 | Steps: 2 | Val loss: 2.1794 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 04:51:31 (running for 00:41:46.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.314 |      0.213 |                   66 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.826 |      0.364 |                   44 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.038 |      0.354 |                   21 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  2.521 |      0.042 |                   19 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.40718283582089554
[2m[36m(func pid=186463)[0m top5: 0.8959888059701493
[2m[36m(func pid=186463)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=186463)[0m f1_macro: 0.354231681702632
[2m[36m(func pid=186463)[0m f1_weighted: 0.43586973182247574
[2m[36m(func pid=186463)[0m f1_per_class: [0.353, 0.336, 0.522, 0.473, 0.1, 0.344, 0.543, 0.385, 0.274, 0.212]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.7845 | Steps: 2 | Val loss: 16.6025 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=176361)[0m top1: 0.23973880597014927
[2m[36m(func pid=176361)[0m top5: 0.7159514925373134
[2m[36m(func pid=176361)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=176361)[0m f1_macro: 0.21175084836552957
[2m[36m(func pid=176361)[0m f1_weighted: 0.2567947233438718
[2m[36m(func pid=176361)[0m f1_per_class: [0.194, 0.318, 0.324, 0.226, 0.03, 0.399, 0.247, 0.18, 0.045, 0.155]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7388 | Steps: 2 | Val loss: 1.7979 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=187246)[0m top1: 0.11520522388059702
[2m[36m(func pid=187246)[0m top5: 0.7434701492537313
[2m[36m(func pid=187246)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=187246)[0m f1_macro: 0.08696693906063353
[2m[36m(func pid=187246)[0m f1_weighted: 0.1406746015609229
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.407, 0.04, 0.138, 0.072, 0.008, 0.085, 0.077, 0.0, 0.042]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0303 | Steps: 2 | Val loss: 1.9952 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=181607)[0m top1: 0.37919776119402987
[2m[36m(func pid=181607)[0m top5: 0.8703358208955224
[2m[36m(func pid=181607)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=181607)[0m f1_macro: 0.36546722347234684
[2m[36m(func pid=181607)[0m f1_weighted: 0.40800235844047095
[2m[36m(func pid=181607)[0m f1_per_class: [0.373, 0.34, 0.686, 0.496, 0.076, 0.373, 0.401, 0.455, 0.232, 0.223]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.3126 | Steps: 2 | Val loss: 2.1756 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:51:37 (running for 00:41:51.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.324 |      0.212 |                   67 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.739 |      0.365 |                   45 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.03  |      0.345 |                   22 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  3.785 |      0.087 |                   20 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.40671641791044777
[2m[36m(func pid=186463)[0m top5: 0.8973880597014925
[2m[36m(func pid=186463)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=186463)[0m f1_macro: 0.34484572017912896
[2m[36m(func pid=186463)[0m f1_weighted: 0.4265382662451267
[2m[36m(func pid=186463)[0m f1_per_class: [0.362, 0.347, 0.571, 0.456, 0.089, 0.215, 0.582, 0.31, 0.271, 0.243]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.5283 | Steps: 2 | Val loss: 15.7532 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=176361)[0m top1: 0.240205223880597
[2m[36m(func pid=176361)[0m top5: 0.7262126865671642
[2m[36m(func pid=176361)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=176361)[0m f1_macro: 0.21426547787260755
[2m[36m(func pid=176361)[0m f1_weighted: 0.2599485946291127
[2m[36m(func pid=176361)[0m f1_per_class: [0.199, 0.312, 0.333, 0.241, 0.031, 0.39, 0.249, 0.178, 0.042, 0.167]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7039 | Steps: 2 | Val loss: 1.7973 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=187246)[0m top1: 0.16511194029850745
[2m[36m(func pid=187246)[0m top5: 0.8586753731343284
[2m[36m(func pid=187246)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=187246)[0m f1_macro: 0.09856569669913384
[2m[36m(func pid=187246)[0m f1_weighted: 0.15633512451454093
[2m[36m(func pid=187246)[0m f1_per_class: [0.017, 0.383, 0.044, 0.156, 0.014, 0.024, 0.097, 0.249, 0.0, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m top1: 0.37406716417910446
[2m[36m(func pid=181607)[0m top5: 0.8666044776119403
[2m[36m(func pid=181607)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=181607)[0m f1_macro: 0.3648339358541102
[2m[36m(func pid=181607)[0m f1_weighted: 0.4015291581279407
[2m[36m(func pid=181607)[0m f1_per_class: [0.382, 0.349, 0.706, 0.478, 0.078, 0.363, 0.397, 0.445, 0.217, 0.234]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0321 | Steps: 2 | Val loss: 2.0692 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2534 | Steps: 2 | Val loss: 2.1715 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5261 | Steps: 2 | Val loss: 17.3690 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:51:42 (running for 00:41:57.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.313 |      0.214 |                   68 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.704 |      0.365 |                   46 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.032 |      0.338 |                   23 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.528 |      0.099 |                   21 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.24486940298507462
[2m[36m(func pid=176361)[0m top5: 0.730410447761194
[2m[36m(func pid=176361)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=176361)[0m f1_macro: 0.21859712314354054
[2m[36m(func pid=176361)[0m f1_weighted: 0.2658081654071378
[2m[36m(func pid=176361)[0m f1_per_class: [0.203, 0.308, 0.355, 0.259, 0.032, 0.391, 0.254, 0.177, 0.041, 0.165]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=186463)[0m top1: 0.40205223880597013
[2m[36m(func pid=186463)[0m top5: 0.8889925373134329
[2m[36m(func pid=186463)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=186463)[0m f1_macro: 0.3384557605520511
[2m[36m(func pid=186463)[0m f1_weighted: 0.42148509667076733
[2m[36m(func pid=186463)[0m f1_per_class: [0.331, 0.354, 0.558, 0.445, 0.085, 0.179, 0.585, 0.333, 0.263, 0.252]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7265 | Steps: 2 | Val loss: 1.7973 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=187246)[0m top1: 0.18936567164179105
[2m[36m(func pid=187246)[0m top5: 0.8568097014925373
[2m[36m(func pid=187246)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=187246)[0m f1_macro: 0.10750956259794682
[2m[36m(func pid=187246)[0m f1_weighted: 0.17349116833633615
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.442, 0.043, 0.23, 0.019, 0.016, 0.053, 0.244, 0.027, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m top1: 0.37033582089552236
[2m[36m(func pid=181607)[0m top5: 0.8684701492537313
[2m[36m(func pid=181607)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=181607)[0m f1_macro: 0.364928848281047
[2m[36m(func pid=181607)[0m f1_weighted: 0.3983726697467598
[2m[36m(func pid=181607)[0m f1_per_class: [0.367, 0.35, 0.727, 0.458, 0.077, 0.364, 0.403, 0.449, 0.225, 0.229]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3151 | Steps: 2 | Val loss: 2.1734 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0466 | Steps: 2 | Val loss: 2.1046 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 04:51:48 (running for 00:42:02.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.315 |      0.217 |                   70 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.727 |      0.365 |                   47 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.032 |      0.338 |                   23 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.526 |      0.108 |                   22 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.24067164179104478
[2m[36m(func pid=176361)[0m top5: 0.7285447761194029
[2m[36m(func pid=176361)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=176361)[0m f1_macro: 0.21672091082725772
[2m[36m(func pid=176361)[0m f1_weighted: 0.26133128528421956
[2m[36m(func pid=176361)[0m f1_per_class: [0.202, 0.314, 0.364, 0.25, 0.025, 0.385, 0.247, 0.176, 0.042, 0.162]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6489 | Steps: 2 | Val loss: 1.7928 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.4021 | Steps: 2 | Val loss: 12.8786 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=186463)[0m top1: 0.39225746268656714
[2m[36m(func pid=186463)[0m top5: 0.8815298507462687
[2m[36m(func pid=186463)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=186463)[0m f1_macro: 0.3341479600831421
[2m[36m(func pid=186463)[0m f1_weighted: 0.41797543594539155
[2m[36m(func pid=186463)[0m f1_per_class: [0.314, 0.359, 0.49, 0.43, 0.084, 0.22, 0.56, 0.396, 0.251, 0.238]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m top1: 0.37779850746268656
[2m[36m(func pid=181607)[0m top5: 0.8689365671641791
[2m[36m(func pid=181607)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=181607)[0m f1_macro: 0.36647899829629654
[2m[36m(func pid=181607)[0m f1_weighted: 0.4066607414504403
[2m[36m(func pid=181607)[0m f1_per_class: [0.371, 0.352, 0.706, 0.479, 0.078, 0.355, 0.412, 0.457, 0.221, 0.234]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.21082089552238806
[2m[36m(func pid=187246)[0m top5: 0.8596082089552238
[2m[36m(func pid=187246)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=187246)[0m f1_macro: 0.11957805393547505
[2m[36m(func pid=187246)[0m f1_weighted: 0.19890639469628255
[2m[36m(func pid=187246)[0m f1_per_class: [0.0, 0.456, 0.05, 0.26, 0.017, 0.0, 0.103, 0.26, 0.049, 0.0]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3792 | Steps: 2 | Val loss: 2.1698 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0224 | Steps: 2 | Val loss: 2.1508 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 04:51:53 (running for 00:42:07.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.379 |      0.215 |                   71 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.649 |      0.366 |                   48 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.047 |      0.334 |                   24 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.402 |      0.12  |                   23 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.24067164179104478
[2m[36m(func pid=176361)[0m top5: 0.7238805970149254
[2m[36m(func pid=176361)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=176361)[0m f1_macro: 0.21488605289577442
[2m[36m(func pid=176361)[0m f1_weighted: 0.26094959637069576
[2m[36m(func pid=176361)[0m f1_per_class: [0.197, 0.31, 0.358, 0.246, 0.025, 0.377, 0.255, 0.17, 0.062, 0.149]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7138 | Steps: 2 | Val loss: 1.7842 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3441 | Steps: 2 | Val loss: 8.8141 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=186463)[0m top1: 0.3666044776119403
[2m[36m(func pid=186463)[0m top5: 0.8731343283582089
[2m[36m(func pid=186463)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=186463)[0m f1_macro: 0.32134293204336545
[2m[36m(func pid=186463)[0m f1_weighted: 0.3988409071149591
[2m[36m(func pid=186463)[0m f1_per_class: [0.259, 0.352, 0.436, 0.391, 0.079, 0.29, 0.511, 0.415, 0.251, 0.228]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m top1: 0.3810634328358209
[2m[36m(func pid=181607)[0m top5: 0.8726679104477612
[2m[36m(func pid=181607)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=181607)[0m f1_macro: 0.36476600939604026
[2m[36m(func pid=181607)[0m f1_weighted: 0.4110242975667036
[2m[36m(func pid=181607)[0m f1_per_class: [0.367, 0.341, 0.706, 0.487, 0.082, 0.355, 0.428, 0.46, 0.197, 0.225]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.228544776119403
[2m[36m(func pid=187246)[0m top5: 0.8600746268656716
[2m[36m(func pid=187246)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=187246)[0m f1_macro: 0.16350548045992033
[2m[36m(func pid=187246)[0m f1_weighted: 0.2236710337455022
[2m[36m(func pid=187246)[0m f1_per_class: [0.08, 0.461, 0.058, 0.194, 0.025, 0.0, 0.221, 0.311, 0.043, 0.242]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2419 | Steps: 2 | Val loss: 2.1708 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0369 | Steps: 2 | Val loss: 2.1980 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5788 | Steps: 2 | Val loss: 1.7911 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:51:59 (running for 00:42:13.57)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.379 |      0.215 |                   71 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.714 |      0.365 |                   49 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.037 |      0.32  |                   26 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.344 |      0.164 |                   24 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=176361)[0m top1: 0.23600746268656717
[2m[36m(func pid=176361)[0m top5: 0.7252798507462687
[2m[36m(func pid=176361)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=176361)[0m f1_macro: 0.21152271476656934
[2m[36m(func pid=176361)[0m f1_weighted: 0.2544337618396501
[2m[36m(func pid=176361)[0m f1_per_class: [0.193, 0.312, 0.358, 0.249, 0.025, 0.371, 0.231, 0.17, 0.062, 0.143]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3572761194029851
[2m[36m(func pid=186463)[0m top5: 0.8656716417910447
[2m[36m(func pid=186463)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=186463)[0m f1_macro: 0.3200731953244332
[2m[36m(func pid=186463)[0m f1_weighted: 0.39051703423706147
[2m[36m(func pid=186463)[0m f1_per_class: [0.249, 0.358, 0.436, 0.383, 0.085, 0.314, 0.477, 0.43, 0.247, 0.222]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.0128 | Steps: 2 | Val loss: 4.6288 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181607)[0m top1: 0.373134328358209
[2m[36m(func pid=181607)[0m top5: 0.8722014925373134
[2m[36m(func pid=181607)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=181607)[0m f1_macro: 0.35732970174896317
[2m[36m(func pid=181607)[0m f1_weighted: 0.40315822182777444
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.334, 0.686, 0.475, 0.075, 0.351, 0.419, 0.467, 0.209, 0.225]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.261660447761194
[2m[36m(func pid=187246)[0m top5: 0.871268656716418
[2m[36m(func pid=187246)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=187246)[0m f1_macro: 0.20215743506061115
[2m[36m(func pid=187246)[0m f1_weighted: 0.24469538093453694
[2m[36m(func pid=187246)[0m f1_per_class: [0.089, 0.482, 0.112, 0.168, 0.032, 0.008, 0.285, 0.349, 0.024, 0.474]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2443 | Steps: 2 | Val loss: 2.1687 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0252 | Steps: 2 | Val loss: 2.2409 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:52:04 (running for 00:42:19.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.242 |      0.212 |                   72 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.579 |      0.357 |                   50 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.025 |      0.319 |                   27 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.013 |      0.202 |                   25 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5837 | Steps: 2 | Val loss: 1.8010 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=186463)[0m top1: 0.3474813432835821
[2m[36m(func pid=186463)[0m top5: 0.8652052238805971
[2m[36m(func pid=186463)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=186463)[0m f1_macro: 0.3193369408206961
[2m[36m(func pid=186463)[0m f1_weighted: 0.37636784891385444
[2m[36m(func pid=186463)[0m f1_per_class: [0.238, 0.36, 0.462, 0.37, 0.09, 0.319, 0.433, 0.459, 0.245, 0.218]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=176361)[0m top1: 0.23787313432835822
[2m[36m(func pid=176361)[0m top5: 0.7252798507462687
[2m[36m(func pid=176361)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=176361)[0m f1_macro: 0.2145346034719176
[2m[36m(func pid=176361)[0m f1_weighted: 0.25745474183175576
[2m[36m(func pid=176361)[0m f1_per_class: [0.195, 0.314, 0.375, 0.254, 0.025, 0.379, 0.233, 0.165, 0.064, 0.141]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3770 | Steps: 2 | Val loss: 3.2699 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=181607)[0m top1: 0.3619402985074627
[2m[36m(func pid=181607)[0m top5: 0.8726679104477612
[2m[36m(func pid=181607)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=181607)[0m f1_macro: 0.3524514860338066
[2m[36m(func pid=181607)[0m f1_weighted: 0.39276752888673017
[2m[36m(func pid=181607)[0m f1_per_class: [0.33, 0.323, 0.686, 0.457, 0.069, 0.355, 0.408, 0.449, 0.222, 0.225]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.25093283582089554
[2m[36m(func pid=187246)[0m top5: 0.8666044776119403
[2m[36m(func pid=187246)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=187246)[0m f1_macro: 0.23424478559338469
[2m[36m(func pid=187246)[0m f1_weighted: 0.23932290651351654
[2m[36m(func pid=187246)[0m f1_per_class: [0.094, 0.443, 0.375, 0.234, 0.037, 0.0, 0.215, 0.376, 0.045, 0.524]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0347 | Steps: 2 | Val loss: 2.2561 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2467 | Steps: 2 | Val loss: 2.1660 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6020 | Steps: 2 | Val loss: 1.8111 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 04:52:10 (running for 00:42:24.55)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.244 |      0.215 |                   73 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.584 |      0.352 |                   51 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.035 |      0.32  |                   28 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.377 |      0.234 |                   26 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3498134328358209
[2m[36m(func pid=186463)[0m top5: 0.867070895522388
[2m[36m(func pid=186463)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=186463)[0m f1_macro: 0.31993719186669384
[2m[36m(func pid=186463)[0m f1_weighted: 0.37476144782704535
[2m[36m(func pid=186463)[0m f1_per_class: [0.249, 0.368, 0.462, 0.394, 0.102, 0.33, 0.4, 0.431, 0.252, 0.211]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3763 | Steps: 2 | Val loss: 3.3584 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=176361)[0m top1: 0.24207089552238806
[2m[36m(func pid=176361)[0m top5: 0.7318097014925373
[2m[36m(func pid=176361)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=176361)[0m f1_macro: 0.21856663824044484
[2m[36m(func pid=176361)[0m f1_weighted: 0.2636875539754811
[2m[36m(func pid=176361)[0m f1_per_class: [0.202, 0.312, 0.393, 0.269, 0.025, 0.374, 0.242, 0.163, 0.059, 0.144]
[2m[36m(func pid=176361)[0m 
[2m[36m(func pid=181607)[0m top1: 0.3568097014925373
[2m[36m(func pid=181607)[0m top5: 0.871268656716418
[2m[36m(func pid=181607)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=181607)[0m f1_macro: 0.3541547775680766
[2m[36m(func pid=181607)[0m f1_weighted: 0.387457427002809
[2m[36m(func pid=181607)[0m f1_per_class: [0.342, 0.322, 0.686, 0.444, 0.065, 0.37, 0.395, 0.451, 0.234, 0.233]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.22901119402985073
[2m[36m(func pid=187246)[0m top5: 0.8544776119402985
[2m[36m(func pid=187246)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=187246)[0m f1_macro: 0.24230441741245806
[2m[36m(func pid=187246)[0m f1_weighted: 0.21320486649980602
[2m[36m(func pid=187246)[0m f1_per_class: [0.098, 0.316, 0.615, 0.282, 0.02, 0.024, 0.135, 0.404, 0.069, 0.458]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0170 | Steps: 2 | Val loss: 2.2519 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=176361)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2274 | Steps: 2 | Val loss: 2.1613 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5981 | Steps: 2 | Val loss: 1.8174 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 04:52:15 (running for 00:42:29.82)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.335
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00020 | RUNNING    | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.247 |      0.219 |                   74 |
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.602 |      0.354 |                   52 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.017 |      0.327 |                   29 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.376 |      0.242 |                   27 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3568097014925373
[2m[36m(func pid=186463)[0m top5: 0.8661380597014925
[2m[36m(func pid=186463)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=186463)[0m f1_macro: 0.32734450579563745
[2m[36m(func pid=186463)[0m f1_weighted: 0.3790676792787185
[2m[36m(func pid=186463)[0m f1_per_class: [0.254, 0.368, 0.49, 0.419, 0.093, 0.343, 0.382, 0.447, 0.266, 0.212]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=176361)[0m top1: 0.24953358208955223
[2m[36m(func pid=176361)[0m top5: 0.7364738805970149
[2m[36m(func pid=176361)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=176361)[0m f1_macro: 0.22674128537517144
[2m[36m(func pid=176361)[0m f1_weighted: 0.2728765873482639
[2m[36m(func pid=176361)[0m f1_per_class: [0.196, 0.305, 0.407, 0.28, 0.027, 0.389, 0.256, 0.19, 0.056, 0.162]
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.7166 | Steps: 2 | Val loss: 2.8524 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=181607)[0m top1: 0.34841417910447764
[2m[36m(func pid=181607)[0m top5: 0.8694029850746269
[2m[36m(func pid=181607)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=181607)[0m f1_macro: 0.34895011334553927
[2m[36m(func pid=181607)[0m f1_weighted: 0.3770078555009181
[2m[36m(func pid=181607)[0m f1_per_class: [0.35, 0.32, 0.686, 0.42, 0.066, 0.367, 0.387, 0.441, 0.228, 0.225]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.3162313432835821
[2m[36m(func pid=187246)[0m top5: 0.8638059701492538
[2m[36m(func pid=187246)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=187246)[0m f1_macro: 0.33115212387708653
[2m[36m(func pid=187246)[0m f1_weighted: 0.33076022024579393
[2m[36m(func pid=187246)[0m f1_per_class: [0.113, 0.409, 0.75, 0.339, 0.045, 0.303, 0.292, 0.499, 0.071, 0.49]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0196 | Steps: 2 | Val loss: 2.2247 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5531 | Steps: 2 | Val loss: 1.8056 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=186463)[0m top1: 0.3726679104477612
[2m[36m(func pid=186463)[0m top5: 0.8745335820895522
[2m[36m(func pid=186463)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=186463)[0m f1_macro: 0.34314991358558705
[2m[36m(func pid=186463)[0m f1_weighted: 0.391942847843828
[2m[36m(func pid=186463)[0m f1_per_class: [0.3, 0.378, 0.545, 0.443, 0.1, 0.332, 0.396, 0.446, 0.274, 0.218]
== Status ==
Current time: 2024-01-07 04:52:21 (running for 00:42:35.40)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.598 |      0.349 |                   53 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.02  |      0.343 |                   30 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.717 |      0.331 |                   28 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.1079 | Steps: 2 | Val loss: 3.9113 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m top1: 0.36100746268656714
[2m[36m(func pid=181607)[0m top5: 0.8722014925373134
[2m[36m(func pid=181607)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=181607)[0m f1_macro: 0.3538699086407027
[2m[36m(func pid=181607)[0m f1_weighted: 0.3892537653662927
[2m[36m(func pid=181607)[0m f1_per_class: [0.34, 0.339, 0.686, 0.444, 0.072, 0.356, 0.398, 0.443, 0.231, 0.229]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.2537313432835821
[2m[36m(func pid=187246)[0m top5: 0.7989738805970149
[2m[36m(func pid=187246)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=187246)[0m f1_macro: 0.24629861640971967
[2m[36m(func pid=187246)[0m f1_weighted: 0.21206514560425457
[2m[36m(func pid=187246)[0m f1_per_class: [0.189, 0.474, 0.75, 0.206, 0.034, 0.09, 0.085, 0.441, 0.042, 0.152]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0124 | Steps: 2 | Val loss: 2.1994 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4794 | Steps: 2 | Val loss: 1.7936 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0638 | Steps: 2 | Val loss: 5.1596 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=186463)[0m top1: 0.3763992537313433
[2m[36m(func pid=186463)[0m top5: 0.8759328358208955
[2m[36m(func pid=186463)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=186463)[0m f1_macro: 0.34126308019584595
[2m[36m(func pid=186463)[0m f1_weighted: 0.39586951686380134
[2m[36m(func pid=186463)[0m f1_per_class: [0.312, 0.378, 0.585, 0.454, 0.099, 0.244, 0.438, 0.407, 0.269, 0.227]
== Status ==
Current time: 2024-01-07 04:52:26 (running for 00:42:40.94)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.553 |      0.354 |                   54 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.012 |      0.341 |                   31 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.108 |      0.246 |                   29 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m top1: 0.3675373134328358
[2m[36m(func pid=181607)[0m top5: 0.8754664179104478
[2m[36m(func pid=181607)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=181607)[0m f1_macro: 0.3542490183110307
[2m[36m(func pid=181607)[0m f1_weighted: 0.3961340119642294
[2m[36m(func pid=181607)[0m f1_per_class: [0.328, 0.342, 0.686, 0.457, 0.076, 0.353, 0.41, 0.449, 0.218, 0.222]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.20382462686567165
[2m[36m(func pid=187246)[0m top5: 0.7490671641791045
[2m[36m(func pid=187246)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=187246)[0m f1_macro: 0.17462376950974753
[2m[36m(func pid=187246)[0m f1_weighted: 0.14580503366175684
[2m[36m(func pid=187246)[0m f1_per_class: [0.194, 0.441, 0.667, 0.058, 0.038, 0.016, 0.115, 0.146, 0.0, 0.071]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0132 | Steps: 2 | Val loss: 2.1917 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5642 | Steps: 2 | Val loss: 1.7915 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:52:32 (running for 00:42:46.48)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.479 |      0.354 |                   55 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.013 |      0.341 |                   32 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.064 |      0.175 |                   30 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3810634328358209
[2m[36m(func pid=186463)[0m top5: 0.8782649253731343
[2m[36m(func pid=186463)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=186463)[0m f1_macro: 0.3414126690221574
[2m[36m(func pid=186463)[0m f1_weighted: 0.39971920341163364
[2m[36m(func pid=186463)[0m f1_per_class: [0.325, 0.382, 0.667, 0.449, 0.092, 0.203, 0.481, 0.343, 0.255, 0.218]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.6407 | Steps: 2 | Val loss: 3.5876 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=181607)[0m top1: 0.3763992537313433
[2m[36m(func pid=181607)[0m top5: 0.8726679104477612
[2m[36m(func pid=181607)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=181607)[0m f1_macro: 0.3521877735683449
[2m[36m(func pid=181607)[0m f1_weighted: 0.40729043190615843
[2m[36m(func pid=181607)[0m f1_per_class: [0.316, 0.338, 0.667, 0.481, 0.078, 0.346, 0.438, 0.418, 0.209, 0.232]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.21455223880597016
[2m[36m(func pid=187246)[0m top5: 0.8470149253731343
[2m[36m(func pid=187246)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=187246)[0m f1_macro: 0.2073389334540336
[2m[36m(func pid=187246)[0m f1_weighted: 0.1901030900522836
[2m[36m(func pid=187246)[0m f1_per_class: [0.167, 0.402, 0.533, 0.069, 0.052, 0.069, 0.194, 0.453, 0.039, 0.094]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0233 | Steps: 2 | Val loss: 2.2299 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5039 | Steps: 2 | Val loss: 1.7820 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:52:37 (running for 00:42:52.11)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.564 |      0.352 |                   56 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.023 |      0.333 |                   33 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.641 |      0.207 |                   31 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.373134328358209
[2m[36m(func pid=186463)[0m top5: 0.8703358208955224
[2m[36m(func pid=186463)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=186463)[0m f1_macro: 0.332646685028179
[2m[36m(func pid=186463)[0m f1_weighted: 0.3914791558441883
[2m[36m(func pid=186463)[0m f1_per_class: [0.316, 0.376, 0.649, 0.443, 0.101, 0.185, 0.474, 0.335, 0.228, 0.219]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.1660 | Steps: 2 | Val loss: 2.9924 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181607)[0m top1: 0.3833955223880597
[2m[36m(func pid=181607)[0m top5: 0.8754664179104478
[2m[36m(func pid=181607)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=181607)[0m f1_macro: 0.35379597299053595
[2m[36m(func pid=181607)[0m f1_weighted: 0.41397634199687267
[2m[36m(func pid=181607)[0m f1_per_class: [0.318, 0.341, 0.667, 0.495, 0.08, 0.346, 0.447, 0.412, 0.197, 0.235]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.2126865671641791
[2m[36m(func pid=187246)[0m top5: 0.8586753731343284
[2m[36m(func pid=187246)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=187246)[0m f1_macro: 0.19322473141661642
[2m[36m(func pid=187246)[0m f1_weighted: 0.1832827675060487
[2m[36m(func pid=187246)[0m f1_per_class: [0.155, 0.383, 0.453, 0.192, 0.059, 0.075, 0.089, 0.335, 0.032, 0.158]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0294 | Steps: 2 | Val loss: 2.2979 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4895 | Steps: 2 | Val loss: 1.7808 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 04:52:43 (running for 00:42:57.44)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.504 |      0.354 |                   57 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.029 |      0.328 |                   34 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.166 |      0.193 |                   32 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.35774253731343286
[2m[36m(func pid=186463)[0m top5: 0.8638059701492538
[2m[36m(func pid=186463)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=186463)[0m f1_macro: 0.32776568099342657
[2m[36m(func pid=186463)[0m f1_weighted: 0.37163748276241715
[2m[36m(func pid=186463)[0m f1_per_class: [0.302, 0.376, 0.615, 0.411, 0.11, 0.181, 0.427, 0.393, 0.247, 0.215]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.9829 | Steps: 2 | Val loss: 2.8825 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=181607)[0m top1: 0.3810634328358209
[2m[36m(func pid=181607)[0m top5: 0.8782649253731343
[2m[36m(func pid=181607)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=181607)[0m f1_macro: 0.35738350158509363
[2m[36m(func pid=181607)[0m f1_weighted: 0.4108031861716348
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.34, 0.686, 0.488, 0.078, 0.346, 0.439, 0.424, 0.205, 0.235]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.269589552238806
[2m[36m(func pid=187246)[0m top5: 0.8409514925373134
[2m[36m(func pid=187246)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=187246)[0m f1_macro: 0.24277741884168683
[2m[36m(func pid=187246)[0m f1_weighted: 0.25556028308239626
[2m[36m(func pid=187246)[0m f1_per_class: [0.186, 0.439, 0.421, 0.346, 0.081, 0.222, 0.079, 0.387, 0.097, 0.169]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0151 | Steps: 2 | Val loss: 2.3317 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4902 | Steps: 2 | Val loss: 1.7917 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 04:52:48 (running for 00:43:02.89)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.49  |      0.357 |                   58 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.015 |      0.333 |                   35 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.983 |      0.243 |                   33 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.34888059701492535
[2m[36m(func pid=186463)[0m top5: 0.8614738805970149
[2m[36m(func pid=186463)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=186463)[0m f1_macro: 0.33318292570553315
[2m[36m(func pid=186463)[0m f1_weighted: 0.35869657000850474
[2m[36m(func pid=186463)[0m f1_per_class: [0.306, 0.378, 0.667, 0.404, 0.116, 0.193, 0.381, 0.397, 0.258, 0.233]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m top1: 0.36847014925373134
[2m[36m(func pid=181607)[0m top5: 0.8754664179104478
[2m[36m(func pid=181607)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=181607)[0m f1_macro: 0.35615622133160185
[2m[36m(func pid=181607)[0m f1_weighted: 0.3961959485555735
[2m[36m(func pid=181607)[0m f1_per_class: [0.344, 0.341, 0.706, 0.469, 0.069, 0.359, 0.398, 0.438, 0.215, 0.222]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.0189 | Steps: 2 | Val loss: 2.8455 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=187246)[0m top1: 0.30130597014925375
[2m[36m(func pid=187246)[0m top5: 0.847481343283582
[2m[36m(func pid=187246)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=187246)[0m f1_macro: 0.2695177145053506
[2m[36m(func pid=187246)[0m f1_weighted: 0.31132700638410304
[2m[36m(func pid=187246)[0m f1_per_class: [0.169, 0.443, 0.304, 0.308, 0.056, 0.346, 0.232, 0.496, 0.094, 0.248]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6657 | Steps: 2 | Val loss: 1.8014 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0261 | Steps: 2 | Val loss: 2.2814 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:52:53 (running for 00:43:08.21)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.666 |      0.357 |                   60 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.015 |      0.333 |                   35 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  2.019 |      0.27  |                   34 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.36240671641791045
[2m[36m(func pid=181607)[0m top5: 0.8773320895522388
[2m[36m(func pid=181607)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=181607)[0m f1_macro: 0.35660940349086856
[2m[36m(func pid=181607)[0m f1_weighted: 0.39094921127463245
[2m[36m(func pid=181607)[0m f1_per_class: [0.383, 0.343, 0.706, 0.443, 0.065, 0.366, 0.4, 0.425, 0.229, 0.206]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.36427238805970147
[2m[36m(func pid=186463)[0m top5: 0.8708022388059702
[2m[36m(func pid=186463)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=186463)[0m f1_macro: 0.3535670191856134
[2m[36m(func pid=186463)[0m f1_weighted: 0.3801263739146733
[2m[36m(func pid=186463)[0m f1_per_class: [0.319, 0.377, 0.727, 0.433, 0.1, 0.305, 0.375, 0.438, 0.248, 0.213]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2813 | Steps: 2 | Val loss: 2.7376 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=187246)[0m top1: 0.3162313432835821
[2m[36m(func pid=187246)[0m top5: 0.8647388059701493
[2m[36m(func pid=187246)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=187246)[0m f1_macro: 0.30706436002134374
[2m[36m(func pid=187246)[0m f1_weighted: 0.3167017890555967
[2m[36m(func pid=187246)[0m f1_per_class: [0.146, 0.453, 0.706, 0.189, 0.037, 0.278, 0.373, 0.536, 0.02, 0.333]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4345 | Steps: 2 | Val loss: 1.8125 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0381 | Steps: 2 | Val loss: 2.2482 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:52:59 (running for 00:43:13.60)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.435 |      0.349 |                   61 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.026 |      0.354 |                   36 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.281 |      0.307 |                   35 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3521455223880597
[2m[36m(func pid=181607)[0m top5: 0.8768656716417911
[2m[36m(func pid=181607)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=181607)[0m f1_macro: 0.3487155648232182
[2m[36m(func pid=181607)[0m f1_weighted: 0.3797481671039962
[2m[36m(func pid=181607)[0m f1_per_class: [0.363, 0.331, 0.686, 0.414, 0.067, 0.365, 0.397, 0.436, 0.227, 0.201]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.37220149253731344
[2m[36m(func pid=186463)[0m top5: 0.8577425373134329
[2m[36m(func pid=186463)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=186463)[0m f1_macro: 0.36024129060228394
[2m[36m(func pid=186463)[0m f1_weighted: 0.38623774327602467
[2m[36m(func pid=186463)[0m f1_per_class: [0.358, 0.356, 0.706, 0.472, 0.083, 0.354, 0.343, 0.462, 0.29, 0.178]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9402 | Steps: 2 | Val loss: 2.9619 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4343 | Steps: 2 | Val loss: 1.7966 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=187246)[0m top1: 0.2798507462686567
[2m[36m(func pid=187246)[0m top5: 0.8675373134328358
[2m[36m(func pid=187246)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=187246)[0m f1_macro: 0.2943456330581489
[2m[36m(func pid=187246)[0m f1_weighted: 0.264635139917254
[2m[36m(func pid=187246)[0m f1_per_class: [0.127, 0.457, 0.8, 0.153, 0.036, 0.289, 0.25, 0.388, 0.022, 0.421]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0210 | Steps: 2 | Val loss: 2.3627 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:53:04 (running for 00:43:18.79)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.434 |      0.354 |                   62 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.038 |      0.36  |                   37 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.94  |      0.294 |                   36 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.36380597014925375
[2m[36m(func pid=181607)[0m top5: 0.878731343283582
[2m[36m(func pid=181607)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=181607)[0m f1_macro: 0.3540550232074229
[2m[36m(func pid=181607)[0m f1_weighted: 0.3909852874221263
[2m[36m(func pid=181607)[0m f1_per_class: [0.356, 0.341, 0.686, 0.443, 0.07, 0.357, 0.407, 0.421, 0.239, 0.221]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.34375
[2m[36m(func pid=186463)[0m top5: 0.8297574626865671
[2m[36m(func pid=186463)[0m f1_micro: 0.34375
[2m[36m(func pid=186463)[0m f1_macro: 0.3392333915188428
[2m[36m(func pid=186463)[0m f1_weighted: 0.3679153262940495
[2m[36m(func pid=186463)[0m f1_per_class: [0.372, 0.347, 0.727, 0.494, 0.098, 0.35, 0.294, 0.344, 0.268, 0.097]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8433 | Steps: 2 | Val loss: 2.9072 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4558 | Steps: 2 | Val loss: 1.7949 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=187246)[0m top1: 0.26725746268656714
[2m[36m(func pid=187246)[0m top5: 0.8661380597014925
[2m[36m(func pid=187246)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=187246)[0m f1_macro: 0.29677154376749737
[2m[36m(func pid=187246)[0m f1_weighted: 0.27302307641720625
[2m[36m(func pid=187246)[0m f1_per_class: [0.104, 0.458, 0.8, 0.169, 0.039, 0.263, 0.274, 0.389, 0.0, 0.471]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0103 | Steps: 2 | Val loss: 2.4211 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 04:53:09 (running for 00:43:24.32)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.456 |      0.352 |                   63 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.021 |      0.339 |                   38 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.843 |      0.297 |                   37 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3736007462686567
[2m[36m(func pid=181607)[0m top5: 0.8731343283582089
[2m[36m(func pid=181607)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=181607)[0m f1_macro: 0.3518691901496495
[2m[36m(func pid=181607)[0m f1_weighted: 0.4050604630993403
[2m[36m(func pid=181607)[0m f1_per_class: [0.332, 0.341, 0.706, 0.475, 0.078, 0.331, 0.445, 0.387, 0.21, 0.214]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3283582089552239
[2m[36m(func pid=186463)[0m top5: 0.8264925373134329
[2m[36m(func pid=186463)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=186463)[0m f1_macro: 0.3277734169450069
[2m[36m(func pid=186463)[0m f1_weighted: 0.3540572924510735
[2m[36m(func pid=186463)[0m f1_per_class: [0.359, 0.341, 0.828, 0.489, 0.098, 0.337, 0.291, 0.188, 0.262, 0.085]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0824 | Steps: 2 | Val loss: 2.6790 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4901 | Steps: 2 | Val loss: 1.7834 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=187246)[0m top1: 0.3255597014925373
[2m[36m(func pid=187246)[0m top5: 0.863339552238806
[2m[36m(func pid=187246)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=187246)[0m f1_macro: 0.3384429608261584
[2m[36m(func pid=187246)[0m f1_weighted: 0.3530638578890986
[2m[36m(func pid=187246)[0m f1_per_class: [0.105, 0.439, 0.828, 0.212, 0.072, 0.281, 0.489, 0.473, 0.0, 0.486]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0216 | Steps: 2 | Val loss: 2.3785 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 04:53:15 (running for 00:43:29.88)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.49  |      0.345 |                   64 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.01  |      0.328 |                   39 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.082 |      0.338 |                   38 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.37220149253731344
[2m[36m(func pid=181607)[0m top5: 0.8768656716417911
[2m[36m(func pid=181607)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=181607)[0m f1_macro: 0.34484318851621315
[2m[36m(func pid=181607)[0m f1_weighted: 0.4053210079956833
[2m[36m(func pid=181607)[0m f1_per_class: [0.323, 0.319, 0.667, 0.486, 0.075, 0.331, 0.451, 0.384, 0.206, 0.207]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.33302238805970147
[2m[36m(func pid=186463)[0m top5: 0.8306902985074627
[2m[36m(func pid=186463)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=186463)[0m f1_macro: 0.324331765300096
[2m[36m(func pid=186463)[0m f1_weighted: 0.3601542197506078
[2m[36m(func pid=186463)[0m f1_per_class: [0.349, 0.336, 0.828, 0.482, 0.101, 0.34, 0.334, 0.118, 0.26, 0.096]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7252 | Steps: 2 | Val loss: 2.7350 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4311 | Steps: 2 | Val loss: 1.7698 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=187246)[0m top1: 0.34701492537313433
[2m[36m(func pid=187246)[0m top5: 0.8512126865671642
[2m[36m(func pid=187246)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=187246)[0m f1_macro: 0.3326337538855382
[2m[36m(func pid=187246)[0m f1_weighted: 0.376181580144167
[2m[36m(func pid=187246)[0m f1_per_class: [0.126, 0.335, 0.857, 0.284, 0.062, 0.305, 0.57, 0.367, 0.0, 0.419]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0127 | Steps: 2 | Val loss: 2.3291 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:53:20 (running for 00:43:35.00)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.431 |      0.35  |                   65 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.022 |      0.324 |                   40 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.725 |      0.333 |                   39 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.375
[2m[36m(func pid=181607)[0m top5: 0.8815298507462687
[2m[36m(func pid=181607)[0m f1_micro: 0.375
[2m[36m(func pid=181607)[0m f1_macro: 0.3501956242768509
[2m[36m(func pid=181607)[0m f1_weighted: 0.4062985018648608
[2m[36m(func pid=181607)[0m f1_per_class: [0.346, 0.334, 0.667, 0.489, 0.074, 0.331, 0.434, 0.417, 0.21, 0.2]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.34701492537313433
[2m[36m(func pid=186463)[0m top5: 0.8423507462686567
[2m[36m(func pid=186463)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=186463)[0m f1_macro: 0.3347990084097973
[2m[36m(func pid=186463)[0m f1_weighted: 0.3711077901836211
[2m[36m(func pid=186463)[0m f1_per_class: [0.335, 0.354, 0.8, 0.456, 0.128, 0.344, 0.374, 0.151, 0.283, 0.123]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6802 | Steps: 2 | Val loss: 2.9447 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3974 | Steps: 2 | Val loss: 1.7737 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=187246)[0m top1: 0.3316231343283582
[2m[36m(func pid=187246)[0m top5: 0.8484141791044776
[2m[36m(func pid=187246)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=187246)[0m f1_macro: 0.34650012898388505
[2m[36m(func pid=187246)[0m f1_weighted: 0.36229486697908003
[2m[36m(func pid=187246)[0m f1_per_class: [0.129, 0.34, 0.857, 0.32, 0.072, 0.326, 0.447, 0.531, 0.0, 0.444]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0319 | Steps: 2 | Val loss: 2.2605 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=181607)[0m top1: 0.3675373134328358
[2m[36m(func pid=181607)[0m top5: 0.8852611940298507
[2m[36m(func pid=181607)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=181607)[0m f1_macro: 0.3480715926039254
[2m[36m(func pid=181607)[0m f1_weighted: 0.39866430924433305
[2m[36m(func pid=181607)[0m f1_per_class: [0.363, 0.335, 0.686, 0.477, 0.067, 0.31, 0.427, 0.422, 0.19, 0.203]
== Status ==
Current time: 2024-01-07 04:53:25 (running for 00:43:40.22)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.397 |      0.348 |                   66 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.013 |      0.335 |                   41 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.68  |      0.347 |                   40 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3670708955223881
[2m[36m(func pid=186463)[0m top5: 0.8586753731343284
[2m[36m(func pid=186463)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=186463)[0m f1_macro: 0.34687254880046087
[2m[36m(func pid=186463)[0m f1_weighted: 0.3926725443150126
[2m[36m(func pid=186463)[0m f1_per_class: [0.324, 0.358, 0.8, 0.448, 0.112, 0.333, 0.447, 0.187, 0.29, 0.169]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7902 | Steps: 2 | Val loss: 3.1744 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3601 | Steps: 2 | Val loss: 1.7794 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0136 | Steps: 2 | Val loss: 2.2883 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=187246)[0m top1: 0.31529850746268656
[2m[36m(func pid=187246)[0m top5: 0.847481343283582
[2m[36m(func pid=187246)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=187246)[0m f1_macro: 0.34716008130155374
[2m[36m(func pid=187246)[0m f1_weighted: 0.3386330206274898
[2m[36m(func pid=187246)[0m f1_per_class: [0.152, 0.327, 0.857, 0.307, 0.074, 0.389, 0.364, 0.479, 0.048, 0.475]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:53:31 (running for 00:43:45.44)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.36  |      0.35  |                   67 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.032 |      0.347 |                   42 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.79  |      0.347 |                   41 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3689365671641791
[2m[36m(func pid=181607)[0m top5: 0.8843283582089553
[2m[36m(func pid=181607)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=181607)[0m f1_macro: 0.3500166834797062
[2m[36m(func pid=181607)[0m f1_weighted: 0.39970841863901213
[2m[36m(func pid=181607)[0m f1_per_class: [0.376, 0.341, 0.686, 0.471, 0.068, 0.311, 0.43, 0.436, 0.189, 0.194]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3726679104477612
[2m[36m(func pid=186463)[0m top5: 0.8694029850746269
[2m[36m(func pid=186463)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=186463)[0m f1_macro: 0.34973639637901743
[2m[36m(func pid=186463)[0m f1_weighted: 0.3980236973300551
[2m[36m(func pid=186463)[0m f1_per_class: [0.308, 0.359, 0.8, 0.435, 0.08, 0.227, 0.501, 0.286, 0.257, 0.245]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.8404 | Steps: 2 | Val loss: 3.8010 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4109 | Steps: 2 | Val loss: 1.7684 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0145 | Steps: 2 | Val loss: 2.3430 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=187246)[0m top1: 0.28591417910447764
[2m[36m(func pid=187246)[0m top5: 0.8465485074626866
[2m[36m(func pid=187246)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=187246)[0m f1_macro: 0.24182305949931265
[2m[36m(func pid=187246)[0m f1_weighted: 0.31524307456227324
[2m[36m(func pid=187246)[0m f1_per_class: [0.12, 0.257, 0.511, 0.222, 0.064, 0.393, 0.511, 0.0, 0.073, 0.267]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:53:36 (running for 00:43:50.82)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.411 |      0.352 |                   68 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.014 |      0.35  |                   43 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  3.84  |      0.242 |                   42 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.376865671641791
[2m[36m(func pid=181607)[0m top5: 0.8861940298507462
[2m[36m(func pid=181607)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=181607)[0m f1_macro: 0.352392813694416
[2m[36m(func pid=181607)[0m f1_weighted: 0.40660385366413787
[2m[36m(func pid=181607)[0m f1_per_class: [0.376, 0.346, 0.686, 0.495, 0.071, 0.314, 0.428, 0.432, 0.178, 0.198]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3666044776119403
[2m[36m(func pid=186463)[0m top5: 0.8680037313432836
[2m[36m(func pid=186463)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=186463)[0m f1_macro: 0.3467422093644722
[2m[36m(func pid=186463)[0m f1_weighted: 0.3886153235816225
[2m[36m(func pid=186463)[0m f1_per_class: [0.306, 0.356, 0.8, 0.407, 0.076, 0.165, 0.513, 0.327, 0.255, 0.263]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.1073 | Steps: 2 | Val loss: 5.9959 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3694 | Steps: 2 | Val loss: 1.7590 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0089 | Steps: 2 | Val loss: 2.3735 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=187246)[0m top1: 0.1455223880597015
[2m[36m(func pid=187246)[0m top5: 0.7821828358208955
[2m[36m(func pid=187246)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=187246)[0m f1_macro: 0.18364584426367664
[2m[36m(func pid=187246)[0m f1_weighted: 0.15506526798363565
[2m[36m(func pid=187246)[0m f1_per_class: [0.083, 0.179, 0.727, 0.124, 0.054, 0.286, 0.153, 0.0, 0.107, 0.124]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:53:41 (running for 00:43:55.99)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.369 |      0.356 |                   69 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.014 |      0.347 |                   44 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.107 |      0.184 |                   43 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38386194029850745
[2m[36m(func pid=181607)[0m top5: 0.8843283582089553
[2m[36m(func pid=181607)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=181607)[0m f1_macro: 0.3564462888687215
[2m[36m(func pid=181607)[0m f1_weighted: 0.4131822357685504
[2m[36m(func pid=181607)[0m f1_per_class: [0.363, 0.334, 0.667, 0.509, 0.067, 0.331, 0.434, 0.43, 0.219, 0.212]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.35634328358208955
[2m[36m(func pid=186463)[0m top5: 0.8610074626865671
[2m[36m(func pid=186463)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=186463)[0m f1_macro: 0.33056649504841984
[2m[36m(func pid=186463)[0m f1_weighted: 0.378388249004393
[2m[36m(func pid=186463)[0m f1_per_class: [0.293, 0.343, 0.667, 0.401, 0.073, 0.146, 0.496, 0.368, 0.243, 0.276]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.0731 | Steps: 2 | Val loss: 8.1868 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3873 | Steps: 2 | Val loss: 1.7538 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=187246)[0m top1: 0.11147388059701492
[2m[36m(func pid=187246)[0m top5: 0.6730410447761194
[2m[36m(func pid=187246)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=187246)[0m f1_macro: 0.08128837663055867
[2m[36m(func pid=187246)[0m f1_weighted: 0.11470254478274666
[2m[36m(func pid=187246)[0m f1_per_class: [0.063, 0.159, 0.0, 0.172, 0.047, 0.22, 0.035, 0.0, 0.041, 0.076]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0195 | Steps: 2 | Val loss: 2.4363 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:53:46 (running for 00:44:01.19)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.387 |      0.353 |                   70 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.009 |      0.331 |                   45 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.073 |      0.081 |                   44 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38759328358208955
[2m[36m(func pid=181607)[0m top5: 0.8833955223880597
[2m[36m(func pid=181607)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=181607)[0m f1_macro: 0.35347645607917694
[2m[36m(func pid=181607)[0m f1_weighted: 0.4173158851470718
[2m[36m(func pid=181607)[0m f1_per_class: [0.337, 0.317, 0.649, 0.509, 0.08, 0.32, 0.465, 0.426, 0.22, 0.213]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.33861940298507465
[2m[36m(func pid=186463)[0m top5: 0.8488805970149254
[2m[36m(func pid=186463)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=186463)[0m f1_macro: 0.32060966699888327
[2m[36m(func pid=186463)[0m f1_weighted: 0.3626990489269423
[2m[36m(func pid=186463)[0m f1_per_class: [0.269, 0.315, 0.649, 0.394, 0.071, 0.15, 0.46, 0.41, 0.229, 0.259]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0255 | Steps: 2 | Val loss: 6.8385 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3560 | Steps: 2 | Val loss: 1.7549 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0097 | Steps: 2 | Val loss: 2.4678 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=187246)[0m top1: 0.23507462686567165
[2m[36m(func pid=187246)[0m top5: 0.7910447761194029
[2m[36m(func pid=187246)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=187246)[0m f1_macro: 0.18542507624746668
[2m[36m(func pid=187246)[0m f1_weighted: 0.26840454097031197
[2m[36m(func pid=187246)[0m f1_per_class: [0.06, 0.206, 0.0, 0.267, 0.053, 0.386, 0.276, 0.475, 0.068, 0.063]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:53:52 (running for 00:44:06.42)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.356 |      0.352 |                   71 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.02  |      0.321 |                   46 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.025 |      0.185 |                   45 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.386660447761194
[2m[36m(func pid=181607)[0m top5: 0.8815298507462687
[2m[36m(func pid=181607)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=181607)[0m f1_macro: 0.3518084679087583
[2m[36m(func pid=181607)[0m f1_weighted: 0.415374937469966
[2m[36m(func pid=181607)[0m f1_per_class: [0.318, 0.302, 0.667, 0.507, 0.091, 0.324, 0.471, 0.42, 0.202, 0.218]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3269589552238806
[2m[36m(func pid=186463)[0m top5: 0.8409514925373134
[2m[36m(func pid=186463)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=186463)[0m f1_macro: 0.3178838882926169
[2m[36m(func pid=186463)[0m f1_weighted: 0.35161775060371836
[2m[36m(func pid=186463)[0m f1_per_class: [0.237, 0.306, 0.667, 0.369, 0.072, 0.184, 0.437, 0.429, 0.224, 0.254]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.0306 | Steps: 2 | Val loss: 4.0152 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3925 | Steps: 2 | Val loss: 1.7569 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0160 | Steps: 2 | Val loss: 2.4901 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:53:57 (running for 00:44:11.44)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.356 |      0.352 |                   71 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.01  |      0.318 |                   47 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.031 |      0.244 |                   46 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=187246)[0m top1: 0.31763059701492535
[2m[36m(func pid=187246)[0m top5: 0.8274253731343284
[2m[36m(func pid=187246)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=187246)[0m f1_macro: 0.24376290935535763
[2m[36m(func pid=187246)[0m f1_weighted: 0.33865800122109
[2m[36m(func pid=187246)[0m f1_per_class: [0.151, 0.224, 0.278, 0.429, 0.109, 0.378, 0.382, 0.205, 0.127, 0.155]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m top1: 0.3871268656716418
[2m[36m(func pid=181607)[0m top5: 0.8857276119402985
[2m[36m(func pid=181607)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=181607)[0m f1_macro: 0.35780757843280875
[2m[36m(func pid=181607)[0m f1_weighted: 0.41493186225420936
[2m[36m(func pid=181607)[0m f1_per_class: [0.344, 0.313, 0.667, 0.511, 0.073, 0.329, 0.448, 0.449, 0.218, 0.226]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3204291044776119
[2m[36m(func pid=186463)[0m top5: 0.8376865671641791
[2m[36m(func pid=186463)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=186463)[0m f1_macro: 0.3183846656024555
[2m[36m(func pid=186463)[0m f1_weighted: 0.3448626196673022
[2m[36m(func pid=186463)[0m f1_per_class: [0.232, 0.301, 0.686, 0.354, 0.068, 0.211, 0.421, 0.429, 0.229, 0.254]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7990 | Steps: 2 | Val loss: 5.8636 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3836 | Steps: 2 | Val loss: 1.7538 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0205 | Steps: 2 | Val loss: 2.4641 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:54:02 (running for 00:44:16.73)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.384 |      0.363 |                   73 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.016 |      0.318 |                   48 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.031 |      0.244 |                   46 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38759328358208955
[2m[36m(func pid=181607)[0m top5: 0.8889925373134329
[2m[36m(func pid=181607)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=181607)[0m f1_macro: 0.363203021253756
[2m[36m(func pid=181607)[0m f1_weighted: 0.4148344662892347
[2m[36m(func pid=181607)[0m f1_per_class: [0.383, 0.324, 0.686, 0.513, 0.07, 0.33, 0.435, 0.451, 0.216, 0.223]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.1791044776119403
[2m[36m(func pid=187246)[0m top5: 0.7877798507462687
[2m[36m(func pid=187246)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=187246)[0m f1_macro: 0.14302475642180626
[2m[36m(func pid=187246)[0m f1_weighted: 0.190069462224703
[2m[36m(func pid=187246)[0m f1_per_class: [0.114, 0.255, 0.393, 0.465, 0.066, 0.036, 0.018, 0.0, 0.041, 0.042]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3302238805970149
[2m[36m(func pid=186463)[0m top5: 0.8442164179104478
[2m[36m(func pid=186463)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=186463)[0m f1_macro: 0.3283567919670861
[2m[36m(func pid=186463)[0m f1_weighted: 0.35658033403990613
[2m[36m(func pid=186463)[0m f1_per_class: [0.241, 0.306, 0.75, 0.374, 0.072, 0.215, 0.437, 0.421, 0.225, 0.242]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3304 | Steps: 2 | Val loss: 1.7510 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.3204 | Steps: 2 | Val loss: 6.1383 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0078 | Steps: 2 | Val loss: 2.4468 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:54:07 (running for 00:44:22.06)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.33  |      0.365 |                   74 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.021 |      0.328 |                   49 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.799 |      0.143 |                   47 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3871268656716418
[2m[36m(func pid=181607)[0m top5: 0.8913246268656716
[2m[36m(func pid=181607)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=181607)[0m f1_macro: 0.3651675651254962
[2m[36m(func pid=181607)[0m f1_weighted: 0.41337316643018573
[2m[36m(func pid=181607)[0m f1_per_class: [0.4, 0.343, 0.706, 0.506, 0.078, 0.312, 0.434, 0.441, 0.217, 0.215]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.11613805970149253
[2m[36m(func pid=187246)[0m top5: 0.7779850746268657
[2m[36m(func pid=187246)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=187246)[0m f1_macro: 0.15851575738748092
[2m[36m(func pid=187246)[0m f1_weighted: 0.12695779391088405
[2m[36m(func pid=187246)[0m f1_per_class: [0.149, 0.184, 0.828, 0.295, 0.061, 0.008, 0.009, 0.0, 0.013, 0.038]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m top1: 0.34095149253731344
[2m[36m(func pid=186463)[0m top5: 0.8493470149253731
[2m[36m(func pid=186463)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=186463)[0m f1_macro: 0.33628898717491296
[2m[36m(func pid=186463)[0m f1_weighted: 0.36731091964435864
[2m[36m(func pid=186463)[0m f1_per_class: [0.251, 0.317, 0.8, 0.398, 0.068, 0.204, 0.45, 0.399, 0.246, 0.231]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3582 | Steps: 2 | Val loss: 1.7723 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7951 | Steps: 2 | Val loss: 6.0065 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 04:54:12 (running for 00:44:27.21)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.358 |      0.367 |                   75 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.008 |      0.336 |                   50 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.32  |      0.159 |                   48 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3833955223880597
[2m[36m(func pid=181607)[0m top5: 0.8899253731343284
[2m[36m(func pid=181607)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=181607)[0m f1_macro: 0.3672043117031015
[2m[36m(func pid=181607)[0m f1_weighted: 0.41121368902538513
[2m[36m(func pid=181607)[0m f1_per_class: [0.4, 0.347, 0.706, 0.487, 0.069, 0.348, 0.426, 0.455, 0.22, 0.216]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0160 | Steps: 2 | Val loss: 2.4513 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=187246)[0m top1: 0.09281716417910447
[2m[36m(func pid=187246)[0m top5: 0.7691231343283582
[2m[36m(func pid=187246)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=187246)[0m f1_macro: 0.14710093922045836
[2m[36m(func pid=187246)[0m f1_weighted: 0.08669275752270429
[2m[36m(func pid=187246)[0m f1_per_class: [0.185, 0.231, 0.762, 0.097, 0.064, 0.0, 0.027, 0.0, 0.064, 0.039]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3027 | Steps: 2 | Val loss: 1.7683 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=186463)[0m top1: 0.34375
[2m[36m(func pid=186463)[0m top5: 0.8484141791044776
[2m[36m(func pid=186463)[0m f1_micro: 0.34375
[2m[36m(func pid=186463)[0m f1_macro: 0.3362688171927696
[2m[36m(func pid=186463)[0m f1_weighted: 0.3707063332562219
[2m[36m(func pid=186463)[0m f1_per_class: [0.257, 0.331, 0.8, 0.398, 0.073, 0.204, 0.458, 0.374, 0.23, 0.237]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8971 | Steps: 2 | Val loss: 6.1576 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 04:54:18 (running for 00:44:32.44)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.303 |      0.367 |                   76 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.016 |      0.336 |                   51 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.795 |      0.147 |                   49 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38759328358208955
[2m[36m(func pid=181607)[0m top5: 0.8889925373134329
[2m[36m(func pid=181607)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=181607)[0m f1_macro: 0.366865577435754
[2m[36m(func pid=181607)[0m f1_weighted: 0.41763807980092754
[2m[36m(func pid=181607)[0m f1_per_class: [0.385, 0.338, 0.706, 0.495, 0.07, 0.333, 0.45, 0.465, 0.217, 0.21]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.08022388059701492
[2m[36m(func pid=187246)[0m top5: 0.7546641791044776
[2m[36m(func pid=187246)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=187246)[0m f1_macro: 0.14106332781120073
[2m[36m(func pid=187246)[0m f1_weighted: 0.07475484711525994
[2m[36m(func pid=187246)[0m f1_per_class: [0.135, 0.194, 0.7, 0.01, 0.073, 0.0, 0.069, 0.153, 0.038, 0.038]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0122 | Steps: 2 | Val loss: 2.4510 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3188 | Steps: 2 | Val loss: 1.7654 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=186463)[0m top1: 0.3400186567164179
[2m[36m(func pid=186463)[0m top5: 0.8456156716417911
[2m[36m(func pid=186463)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=186463)[0m f1_macro: 0.3333999771544173
[2m[36m(func pid=186463)[0m f1_weighted: 0.3682822224214457
[2m[36m(func pid=186463)[0m f1_per_class: [0.246, 0.343, 0.774, 0.388, 0.069, 0.256, 0.436, 0.379, 0.206, 0.237]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.3487 | Steps: 2 | Val loss: 5.3721 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 04:54:23 (running for 00:44:37.82)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.319 |      0.361 |                   77 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.012 |      0.333 |                   52 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.897 |      0.141 |                   50 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3894589552238806
[2m[36m(func pid=181607)[0m top5: 0.8889925373134329
[2m[36m(func pid=181607)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=181607)[0m f1_macro: 0.3613168901885115
[2m[36m(func pid=181607)[0m f1_weighted: 0.41926323752615613
[2m[36m(func pid=181607)[0m f1_per_class: [0.348, 0.335, 0.686, 0.498, 0.077, 0.34, 0.458, 0.448, 0.211, 0.213]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.11100746268656717
[2m[36m(func pid=187246)[0m top5: 0.7667910447761194
[2m[36m(func pid=187246)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=187246)[0m f1_macro: 0.18088797463167344
[2m[36m(func pid=187246)[0m f1_weighted: 0.10687146599834126
[2m[36m(func pid=187246)[0m f1_per_class: [0.193, 0.247, 0.7, 0.029, 0.06, 0.007, 0.071, 0.402, 0.055, 0.044]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0124 | Steps: 2 | Val loss: 2.4621 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2420 | Steps: 2 | Val loss: 1.7731 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=186463)[0m top1: 0.33115671641791045
[2m[36m(func pid=186463)[0m top5: 0.8386194029850746
[2m[36m(func pid=186463)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=186463)[0m f1_macro: 0.32676130209606724
[2m[36m(func pid=186463)[0m f1_weighted: 0.36169450364237693
[2m[36m(func pid=186463)[0m f1_per_class: [0.211, 0.33, 0.706, 0.378, 0.069, 0.281, 0.416, 0.419, 0.224, 0.234]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.9352 | Steps: 2 | Val loss: 4.1520 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 04:54:28 (running for 00:44:43.07)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.242 |      0.354 |                   78 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.012 |      0.327 |                   53 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.349 |      0.181 |                   51 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38013059701492535
[2m[36m(func pid=181607)[0m top5: 0.8833955223880597
[2m[36m(func pid=181607)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=181607)[0m f1_macro: 0.3537049137646006
[2m[36m(func pid=181607)[0m f1_weighted: 0.4109619715218785
[2m[36m(func pid=181607)[0m f1_per_class: [0.318, 0.319, 0.686, 0.484, 0.077, 0.347, 0.456, 0.429, 0.209, 0.212]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.19542910447761194
[2m[36m(func pid=187246)[0m top5: 0.8143656716417911
[2m[36m(func pid=187246)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=187246)[0m f1_macro: 0.23652996728924985
[2m[36m(func pid=187246)[0m f1_weighted: 0.19021381327993836
[2m[36m(func pid=187246)[0m f1_per_class: [0.14, 0.399, 0.87, 0.1, 0.053, 0.043, 0.166, 0.488, 0.059, 0.048]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0075 | Steps: 2 | Val loss: 2.5022 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2934 | Steps: 2 | Val loss: 1.7844 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=186463)[0m top1: 0.32136194029850745
[2m[36m(func pid=186463)[0m top5: 0.8316231343283582
[2m[36m(func pid=186463)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=186463)[0m f1_macro: 0.317886713502196
[2m[36m(func pid=186463)[0m f1_weighted: 0.35319999616797315
[2m[36m(func pid=186463)[0m f1_per_class: [0.19, 0.325, 0.667, 0.356, 0.07, 0.304, 0.403, 0.429, 0.216, 0.219]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.9922 | Steps: 2 | Val loss: 4.2070 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:54:34 (running for 00:44:48.40)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.293 |      0.352 |                   79 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.008 |      0.318 |                   54 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.935 |      0.237 |                   52 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.375
[2m[36m(func pid=181607)[0m top5: 0.8824626865671642
[2m[36m(func pid=181607)[0m f1_micro: 0.375
[2m[36m(func pid=181607)[0m f1_macro: 0.35217605876576064
[2m[36m(func pid=181607)[0m f1_weighted: 0.4067607304241874
[2m[36m(func pid=181607)[0m f1_per_class: [0.321, 0.318, 0.686, 0.478, 0.076, 0.342, 0.449, 0.433, 0.22, 0.2]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0075 | Steps: 2 | Val loss: 2.4914 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=187246)[0m top1: 0.19776119402985073
[2m[36m(func pid=187246)[0m top5: 0.8083022388059702
[2m[36m(func pid=187246)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=187246)[0m f1_macro: 0.2365770033621431
[2m[36m(func pid=187246)[0m f1_weighted: 0.23058559568964807
[2m[36m(func pid=187246)[0m f1_per_class: [0.137, 0.396, 0.846, 0.274, 0.046, 0.034, 0.172, 0.335, 0.082, 0.044]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2699 | Steps: 2 | Val loss: 1.7938 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=186463)[0m top1: 0.32369402985074625
[2m[36m(func pid=186463)[0m top5: 0.8325559701492538
[2m[36m(func pid=186463)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=186463)[0m f1_macro: 0.32336279826860964
[2m[36m(func pid=186463)[0m f1_weighted: 0.35497580788540184
[2m[36m(func pid=186463)[0m f1_per_class: [0.185, 0.324, 0.686, 0.366, 0.082, 0.315, 0.393, 0.442, 0.224, 0.217]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8052 | Steps: 2 | Val loss: 4.2547 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:54:39 (running for 00:44:53.79)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.27  |      0.356 |                   80 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.008 |      0.323 |                   55 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.992 |      0.237 |                   53 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3712686567164179
[2m[36m(func pid=181607)[0m top5: 0.8810634328358209
[2m[36m(func pid=181607)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=181607)[0m f1_macro: 0.35572350360904165
[2m[36m(func pid=181607)[0m f1_weighted: 0.4023819338281868
[2m[36m(func pid=181607)[0m f1_per_class: [0.321, 0.323, 0.686, 0.469, 0.083, 0.354, 0.431, 0.444, 0.23, 0.216]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m top1: 0.2103544776119403
[2m[36m(func pid=187246)[0m top5: 0.7877798507462687
[2m[36m(func pid=187246)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=187246)[0m f1_macro: 0.21446464200918197
[2m[36m(func pid=187246)[0m f1_weighted: 0.2346775801869121
[2m[36m(func pid=187246)[0m f1_per_class: [0.153, 0.434, 0.846, 0.336, 0.054, 0.051, 0.162, 0.03, 0.029, 0.048]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0213 | Steps: 2 | Val loss: 2.4670 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2871 | Steps: 2 | Val loss: 1.7967 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=186463)[0m top1: 0.3358208955223881
[2m[36m(func pid=186463)[0m top5: 0.8372201492537313
[2m[36m(func pid=186463)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=186463)[0m f1_macro: 0.3313673399415525
[2m[36m(func pid=186463)[0m f1_weighted: 0.3667395831399853
[2m[36m(func pid=186463)[0m f1_per_class: [0.201, 0.325, 0.686, 0.381, 0.075, 0.334, 0.404, 0.462, 0.236, 0.209]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7218 | Steps: 2 | Val loss: 3.0980 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:54:44 (running for 00:44:58.94)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.287 |      0.353 |                   81 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.021 |      0.331 |                   56 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.805 |      0.214 |                   54 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3670708955223881
[2m[36m(func pid=181607)[0m top5: 0.8838619402985075
[2m[36m(func pid=181607)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=181607)[0m f1_macro: 0.353358338003032
[2m[36m(func pid=181607)[0m f1_weighted: 0.3980075768758575
[2m[36m(func pid=181607)[0m f1_per_class: [0.325, 0.329, 0.686, 0.459, 0.074, 0.344, 0.425, 0.442, 0.238, 0.212]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0056 | Steps: 2 | Val loss: 2.4485 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=187246)[0m top1: 0.3101679104477612
[2m[36m(func pid=187246)[0m top5: 0.863339552238806
[2m[36m(func pid=187246)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=187246)[0m f1_macro: 0.272876330081672
[2m[36m(func pid=187246)[0m f1_weighted: 0.32463889712407745
[2m[36m(func pid=187246)[0m f1_per_class: [0.114, 0.468, 0.857, 0.25, 0.048, 0.304, 0.423, 0.0, 0.12, 0.145]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2922 | Steps: 2 | Val loss: 1.7744 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=186463)[0m top1: 0.3381529850746269
[2m[36m(func pid=186463)[0m top5: 0.8446828358208955
[2m[36m(func pid=186463)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=186463)[0m f1_macro: 0.3280395832325107
[2m[36m(func pid=186463)[0m f1_weighted: 0.36902874728190804
[2m[36m(func pid=186463)[0m f1_per_class: [0.208, 0.318, 0.632, 0.404, 0.072, 0.318, 0.399, 0.468, 0.25, 0.212]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7804 | Steps: 2 | Val loss: 3.4282 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=181607)[0m top1: 0.37779850746268656
[2m[36m(func pid=181607)[0m top5: 0.886660447761194
[2m[36m(func pid=181607)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=181607)[0m f1_macro: 0.3568067411644939
[2m[36m(func pid=181607)[0m f1_weighted: 0.4078574381072876
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.318, 0.686, 0.492, 0.078, 0.347, 0.433, 0.448, 0.229, 0.205]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:54:49 (running for 00:45:04.28)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.292 |      0.357 |                   82 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.006 |      0.328 |                   57 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.722 |      0.273 |                   55 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0122 | Steps: 2 | Val loss: 2.4017 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=187246)[0m top1: 0.2756529850746269
[2m[36m(func pid=187246)[0m top5: 0.8600746268656716
[2m[36m(func pid=187246)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=187246)[0m f1_macro: 0.29048569762121706
[2m[36m(func pid=187246)[0m f1_weighted: 0.3029132049910023
[2m[36m(func pid=187246)[0m f1_per_class: [0.11, 0.424, 0.833, 0.197, 0.077, 0.333, 0.363, 0.26, 0.099, 0.208]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2614 | Steps: 2 | Val loss: 1.7576 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=186463)[0m top1: 0.3530783582089552
[2m[36m(func pid=186463)[0m top5: 0.8558768656716418
[2m[36m(func pid=186463)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=186463)[0m f1_macro: 0.3394954479105969
[2m[36m(func pid=186463)[0m f1_weighted: 0.38420459415856056
[2m[36m(func pid=186463)[0m f1_per_class: [0.241, 0.335, 0.667, 0.429, 0.069, 0.292, 0.425, 0.451, 0.258, 0.229]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.1563 | Steps: 2 | Val loss: 4.0620 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:54:55 (running for 00:45:09.68)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.261 |      0.361 |                   83 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.012 |      0.339 |                   58 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.78  |      0.29  |                   56 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38899253731343286
[2m[36m(func pid=181607)[0m top5: 0.8880597014925373
[2m[36m(func pid=181607)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=181607)[0m f1_macro: 0.3608918954244217
[2m[36m(func pid=181607)[0m f1_weighted: 0.41704146923236507
[2m[36m(func pid=181607)[0m f1_per_class: [0.354, 0.316, 0.686, 0.517, 0.073, 0.325, 0.447, 0.45, 0.225, 0.215]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0040 | Steps: 2 | Val loss: 2.3547 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=187246)[0m top1: 0.21315298507462688
[2m[36m(func pid=187246)[0m top5: 0.8414179104477612
[2m[36m(func pid=187246)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=187246)[0m f1_macro: 0.1702249744217082
[2m[36m(func pid=187246)[0m f1_weighted: 0.22524214025150296
[2m[36m(func pid=187246)[0m f1_per_class: [0.104, 0.376, 0.0, 0.154, 0.077, 0.423, 0.193, 0.058, 0.093, 0.225]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2326 | Steps: 2 | Val loss: 1.7632 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=186463)[0m top1: 0.36380597014925375
[2m[36m(func pid=186463)[0m top5: 0.8661380597014925
[2m[36m(func pid=186463)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=186463)[0m f1_macro: 0.34496023545046306
[2m[36m(func pid=186463)[0m f1_weighted: 0.393619741745758
[2m[36m(func pid=186463)[0m f1_per_class: [0.272, 0.335, 0.727, 0.442, 0.072, 0.227, 0.475, 0.401, 0.265, 0.233]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8738 | Steps: 2 | Val loss: 3.3114 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=181607)[0m top1: 0.3894589552238806
[2m[36m(func pid=181607)[0m top5: 0.8871268656716418
[2m[36m(func pid=181607)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=181607)[0m f1_macro: 0.3599158860345556
[2m[36m(func pid=181607)[0m f1_weighted: 0.4161321767238286
[2m[36m(func pid=181607)[0m f1_per_class: [0.343, 0.321, 0.686, 0.516, 0.077, 0.33, 0.441, 0.451, 0.232, 0.203]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:55:00 (running for 00:45:14.76)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.233 |      0.36  |                   84 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.004 |      0.345 |                   59 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.156 |      0.17  |                   57 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0148 | Steps: 2 | Val loss: 2.3582 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=187246)[0m top1: 0.28451492537313433
[2m[36m(func pid=187246)[0m top5: 0.8899253731343284
[2m[36m(func pid=187246)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=187246)[0m f1_macro: 0.21111120141392886
[2m[36m(func pid=187246)[0m f1_weighted: 0.30444157675569244
[2m[36m(func pid=187246)[0m f1_per_class: [0.142, 0.372, 0.0, 0.269, 0.05, 0.062, 0.419, 0.394, 0.109, 0.293]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2250 | Steps: 2 | Val loss: 1.7696 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=186463)[0m top1: 0.3675373134328358
[2m[36m(func pid=186463)[0m top5: 0.8675373134328358
[2m[36m(func pid=186463)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=186463)[0m f1_macro: 0.335142008158004
[2m[36m(func pid=186463)[0m f1_weighted: 0.39481480212223025
[2m[36m(func pid=186463)[0m f1_per_class: [0.286, 0.339, 0.706, 0.455, 0.068, 0.15, 0.502, 0.362, 0.25, 0.234]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9813 | Steps: 2 | Val loss: 3.0424 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 04:55:05 (running for 00:45:20.02)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.225 |      0.36  |                   85 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.015 |      0.335 |                   60 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.874 |      0.211 |                   58 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38899253731343286
[2m[36m(func pid=181607)[0m top5: 0.886660447761194
[2m[36m(func pid=181607)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=181607)[0m f1_macro: 0.360091852315219
[2m[36m(func pid=181607)[0m f1_weighted: 0.41420423909600124
[2m[36m(func pid=181607)[0m f1_per_class: [0.35, 0.326, 0.706, 0.514, 0.081, 0.32, 0.44, 0.438, 0.228, 0.2]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0142 | Steps: 2 | Val loss: 2.3614 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=187246)[0m top1: 0.35867537313432835
[2m[36m(func pid=187246)[0m top5: 0.9081156716417911
[2m[36m(func pid=187246)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=187246)[0m f1_macro: 0.23933367367472855
[2m[36m(func pid=187246)[0m f1_weighted: 0.3417301769384925
[2m[36m(func pid=187246)[0m f1_per_class: [0.208, 0.455, 0.0, 0.337, 0.082, 0.008, 0.446, 0.435, 0.022, 0.4]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2720 | Steps: 2 | Val loss: 1.7734 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=186463)[0m top1: 0.37080223880597013
[2m[36m(func pid=186463)[0m top5: 0.8717350746268657
[2m[36m(func pid=186463)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=186463)[0m f1_macro: 0.3364366330994368
[2m[36m(func pid=186463)[0m f1_weighted: 0.3953887245513025
[2m[36m(func pid=186463)[0m f1_per_class: [0.295, 0.347, 0.774, 0.466, 0.07, 0.112, 0.514, 0.3, 0.25, 0.235]
[2m[36m(func pid=186463)[0m 
== Status ==
Current time: 2024-01-07 04:55:10 (running for 00:45:25.19)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.272 |      0.358 |                   86 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.014 |      0.336 |                   61 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.981 |      0.239 |                   59 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3871268656716418
[2m[36m(func pid=181607)[0m top5: 0.8857276119402985
[2m[36m(func pid=181607)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=181607)[0m f1_macro: 0.3584820658546889
[2m[36m(func pid=181607)[0m f1_weighted: 0.41375305681583224
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.325, 0.706, 0.502, 0.083, 0.321, 0.45, 0.438, 0.224, 0.202]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5261 | Steps: 2 | Val loss: 2.9600 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0051 | Steps: 2 | Val loss: 2.3515 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=187246)[0m top1: 0.375
[2m[36m(func pid=187246)[0m top5: 0.9123134328358209
[2m[36m(func pid=187246)[0m f1_micro: 0.375
[2m[36m(func pid=187246)[0m f1_macro: 0.28656599275730116
[2m[36m(func pid=187246)[0m f1_weighted: 0.3575899455665446
[2m[36m(func pid=187246)[0m f1_per_class: [0.238, 0.474, 0.375, 0.386, 0.093, 0.061, 0.413, 0.435, 0.038, 0.353]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2921 | Steps: 2 | Val loss: 1.7784 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=186463)[0m top1: 0.3689365671641791
[2m[36m(func pid=186463)[0m top5: 0.8796641791044776
[2m[36m(func pid=186463)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=186463)[0m f1_macro: 0.34277579273407854
[2m[36m(func pid=186463)[0m f1_weighted: 0.3945739419115574
[2m[36m(func pid=186463)[0m f1_per_class: [0.294, 0.347, 0.8, 0.46, 0.075, 0.149, 0.502, 0.293, 0.27, 0.238]
[2m[36m(func pid=186463)[0m 
== Status ==
Current time: 2024-01-07 04:55:16 (running for 00:45:30.49)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.292 |      0.361 |                   87 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.005 |      0.343 |                   62 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.526 |      0.287 |                   60 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38759328358208955
[2m[36m(func pid=181607)[0m top5: 0.882929104477612
[2m[36m(func pid=181607)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=181607)[0m f1_macro: 0.3609157518399361
[2m[36m(func pid=181607)[0m f1_weighted: 0.41565610515901097
[2m[36m(func pid=181607)[0m f1_per_class: [0.34, 0.34, 0.706, 0.497, 0.081, 0.332, 0.451, 0.419, 0.23, 0.213]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6094 | Steps: 2 | Val loss: 2.8738 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0085 | Steps: 2 | Val loss: 2.3340 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2187 | Steps: 2 | Val loss: 1.7927 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=187246)[0m top1: 0.3931902985074627
[2m[36m(func pid=187246)[0m top5: 0.8969216417910447
[2m[36m(func pid=187246)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=187246)[0m f1_macro: 0.3465156547680367
[2m[36m(func pid=187246)[0m f1_weighted: 0.38903948371905606
[2m[36m(func pid=187246)[0m f1_per_class: [0.264, 0.455, 0.556, 0.429, 0.095, 0.403, 0.339, 0.469, 0.104, 0.353]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:55:21 (running for 00:45:35.80)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.292 |      0.361 |                   87 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.009 |      0.343 |                   63 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.609 |      0.347 |                   61 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38199626865671643
[2m[36m(func pid=181607)[0m top5: 0.8824626865671642
[2m[36m(func pid=181607)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=181607)[0m f1_macro: 0.35555246724242917
[2m[36m(func pid=181607)[0m f1_weighted: 0.4110372762428922
[2m[36m(func pid=181607)[0m f1_per_class: [0.335, 0.331, 0.667, 0.483, 0.081, 0.342, 0.449, 0.437, 0.222, 0.209]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3694029850746269
[2m[36m(func pid=186463)[0m top5: 0.8754664179104478
[2m[36m(func pid=186463)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=186463)[0m f1_macro: 0.34268621461152754
[2m[36m(func pid=186463)[0m f1_weighted: 0.3938058717174402
[2m[36m(func pid=186463)[0m f1_per_class: [0.287, 0.356, 0.706, 0.451, 0.083, 0.228, 0.465, 0.333, 0.278, 0.24]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6026 | Steps: 2 | Val loss: 2.9491 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2271 | Steps: 2 | Val loss: 1.8003 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0192 | Steps: 2 | Val loss: 2.3860 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=187246)[0m top1: 0.37779850746268656
[2m[36m(func pid=187246)[0m top5: 0.8959888059701493
[2m[36m(func pid=187246)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=187246)[0m f1_macro: 0.3421444223083087
[2m[36m(func pid=187246)[0m f1_weighted: 0.3847236538448344
[2m[36m(func pid=187246)[0m f1_per_class: [0.299, 0.382, 0.556, 0.452, 0.083, 0.426, 0.335, 0.468, 0.098, 0.323]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:55:26 (running for 00:45:41.22)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.227 |      0.358 |                   89 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.009 |      0.343 |                   63 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.603 |      0.342 |                   62 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.38013059701492535
[2m[36m(func pid=181607)[0m top5: 0.8852611940298507
[2m[36m(func pid=181607)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=181607)[0m f1_macro: 0.3576507970880055
[2m[36m(func pid=181607)[0m f1_weighted: 0.40896850759903364
[2m[36m(func pid=181607)[0m f1_per_class: [0.362, 0.333, 0.686, 0.481, 0.08, 0.341, 0.443, 0.43, 0.222, 0.2]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3591417910447761
[2m[36m(func pid=186463)[0m top5: 0.8675373134328358
[2m[36m(func pid=186463)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=186463)[0m f1_macro: 0.3416195037384566
[2m[36m(func pid=186463)[0m f1_weighted: 0.3843897777820261
[2m[36m(func pid=186463)[0m f1_per_class: [0.235, 0.356, 0.632, 0.407, 0.105, 0.334, 0.417, 0.457, 0.262, 0.21]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3164 | Steps: 2 | Val loss: 3.0907 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2136 | Steps: 2 | Val loss: 1.8108 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0069 | Steps: 2 | Val loss: 2.4964 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=187246)[0m top1: 0.38572761194029853
[2m[36m(func pid=187246)[0m top5: 0.894589552238806
[2m[36m(func pid=187246)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=187246)[0m f1_macro: 0.3496835886013966
[2m[36m(func pid=187246)[0m f1_weighted: 0.4145695745654323
[2m[36m(func pid=187246)[0m f1_per_class: [0.337, 0.357, 0.556, 0.424, 0.086, 0.407, 0.479, 0.492, 0.092, 0.267]
[2m[36m(func pid=187246)[0m 
== Status ==
Current time: 2024-01-07 04:55:32 (running for 00:45:46.75)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.214 |      0.356 |                   90 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.019 |      0.342 |                   64 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.316 |      0.35  |                   63 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.37826492537313433
[2m[36m(func pid=181607)[0m top5: 0.8819962686567164
[2m[36m(func pid=181607)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=181607)[0m f1_macro: 0.3558944192748278
[2m[36m(func pid=181607)[0m f1_weighted: 0.40717711974915005
[2m[36m(func pid=181607)[0m f1_per_class: [0.36, 0.341, 0.686, 0.485, 0.077, 0.348, 0.428, 0.417, 0.226, 0.19]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.32975746268656714
[2m[36m(func pid=186463)[0m top5: 0.8502798507462687
[2m[36m(func pid=186463)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=186463)[0m f1_macro: 0.3261374784258786
[2m[36m(func pid=186463)[0m f1_weighted: 0.35056141277274677
[2m[36m(func pid=186463)[0m f1_per_class: [0.191, 0.347, 0.571, 0.335, 0.121, 0.365, 0.362, 0.478, 0.287, 0.203]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9882 | Steps: 2 | Val loss: 3.1701 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2114 | Steps: 2 | Val loss: 1.8225 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0037 | Steps: 2 | Val loss: 2.5503 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=187246)[0m top1: 0.40158582089552236
[2m[36m(func pid=187246)[0m top5: 0.8936567164179104
[2m[36m(func pid=187246)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=187246)[0m f1_macro: 0.3636554355241542
[2m[36m(func pid=187246)[0m f1_weighted: 0.43122949678288786
[2m[36m(func pid=187246)[0m f1_per_class: [0.247, 0.357, 0.762, 0.459, 0.07, 0.335, 0.53, 0.5, 0.102, 0.276]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m top1: 0.37919776119402987
[2m[36m(func pid=181607)[0m top5: 0.8819962686567164
[2m[36m(func pid=181607)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=181607)[0m f1_macro: 0.35577909489535153
[2m[36m(func pid=181607)[0m f1_weighted: 0.4098546758252187
[2m[36m(func pid=181607)[0m f1_per_class: [0.363, 0.342, 0.686, 0.49, 0.073, 0.352, 0.432, 0.403, 0.226, 0.19]
== Status ==
Current time: 2024-01-07 04:55:37 (running for 00:45:52.23)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.211 |      0.356 |                   91 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.007 |      0.326 |                   65 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.988 |      0.364 |                   64 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3101679104477612
[2m[36m(func pid=186463)[0m top5: 0.8372201492537313
[2m[36m(func pid=186463)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=186463)[0m f1_macro: 0.31117290372303585
[2m[36m(func pid=186463)[0m f1_weighted: 0.32511645781856996
[2m[36m(func pid=186463)[0m f1_per_class: [0.178, 0.333, 0.533, 0.317, 0.146, 0.359, 0.308, 0.468, 0.276, 0.193]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3505 | Steps: 2 | Val loss: 3.3163 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.1880 | Steps: 2 | Val loss: 1.8352 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=187246)[0m top1: 0.427705223880597
[2m[36m(func pid=187246)[0m top5: 0.8941231343283582
[2m[36m(func pid=187246)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=187246)[0m f1_macro: 0.29491578719950284
[2m[36m(func pid=187246)[0m f1_weighted: 0.448205146427293
[2m[36m(func pid=187246)[0m f1_per_class: [0.179, 0.351, 0.133, 0.539, 0.055, 0.351, 0.538, 0.445, 0.107, 0.25]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0048 | Steps: 2 | Val loss: 2.5766 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 04:55:43 (running for 00:45:57.78)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.188 |      0.356 |                   92 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.004 |      0.311 |                   66 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.35  |      0.295 |                   65 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.376865671641791
[2m[36m(func pid=181607)[0m top5: 0.8796641791044776
[2m[36m(func pid=181607)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=181607)[0m f1_macro: 0.3561832733709759
[2m[36m(func pid=181607)[0m f1_weighted: 0.40749739457736966
[2m[36m(func pid=181607)[0m f1_per_class: [0.369, 0.335, 0.686, 0.488, 0.065, 0.345, 0.428, 0.423, 0.231, 0.192]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.29850746268656714
[2m[36m(func pid=186463)[0m top5: 0.8316231343283582
[2m[36m(func pid=186463)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=186463)[0m f1_macro: 0.3014501198821875
[2m[36m(func pid=186463)[0m f1_weighted: 0.3145913322498247
[2m[36m(func pid=186463)[0m f1_per_class: [0.173, 0.319, 0.545, 0.303, 0.131, 0.356, 0.301, 0.466, 0.242, 0.178]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.9910 | Steps: 2 | Val loss: 3.0454 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2442 | Steps: 2 | Val loss: 1.8452 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=187246)[0m top1: 0.4295708955223881
[2m[36m(func pid=187246)[0m top5: 0.8899253731343284
[2m[36m(func pid=187246)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=187246)[0m f1_macro: 0.3523092665375365
[2m[36m(func pid=187246)[0m f1_weighted: 0.4430187644673029
[2m[36m(func pid=187246)[0m f1_per_class: [0.233, 0.43, 0.667, 0.515, 0.078, 0.422, 0.45, 0.517, 0.064, 0.148]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0139 | Steps: 2 | Val loss: 2.5306 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 04:55:48 (running for 00:46:02.95)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.244 |      0.358 |                   93 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.005 |      0.301 |                   67 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.991 |      0.352 |                   66 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181607)[0m top1: 0.3736007462686567
[2m[36m(func pid=181607)[0m top5: 0.878731343283582
[2m[36m(func pid=181607)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=181607)[0m f1_macro: 0.35809672534036296
[2m[36m(func pid=181607)[0m f1_weighted: 0.40299816475750005
[2m[36m(func pid=181607)[0m f1_per_class: [0.356, 0.342, 0.686, 0.479, 0.069, 0.346, 0.411, 0.454, 0.245, 0.193]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m top1: 0.3087686567164179
[2m[36m(func pid=186463)[0m top5: 0.8381529850746269
[2m[36m(func pid=186463)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=186463)[0m f1_macro: 0.308608135935166
[2m[36m(func pid=186463)[0m f1_weighted: 0.3319966990140739
[2m[36m(func pid=186463)[0m f1_per_class: [0.179, 0.316, 0.558, 0.31, 0.12, 0.348, 0.354, 0.488, 0.23, 0.184]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5844 | Steps: 2 | Val loss: 3.4559 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.1760 | Steps: 2 | Val loss: 1.8367 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=187246)[0m top1: 0.36427238805970147
[2m[36m(func pid=187246)[0m top5: 0.8698694029850746
[2m[36m(func pid=187246)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=187246)[0m f1_macro: 0.32121291803430924
[2m[36m(func pid=187246)[0m f1_weighted: 0.36497160430489406
[2m[36m(func pid=187246)[0m f1_per_class: [0.12, 0.479, 0.667, 0.386, 0.108, 0.446, 0.28, 0.5, 0.08, 0.147]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0176 | Steps: 2 | Val loss: 2.4666 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=181607)[0m top1: 0.37546641791044777
[2m[36m(func pid=181607)[0m top5: 0.8847947761194029
[2m[36m(func pid=181607)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=181607)[0m f1_macro: 0.35902285598557215
[2m[36m(func pid=181607)[0m f1_weighted: 0.4048843456082256
[2m[36m(func pid=181607)[0m f1_per_class: [0.388, 0.346, 0.686, 0.48, 0.061, 0.337, 0.417, 0.454, 0.224, 0.197]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:55:53 (running for 00:46:08.24)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.176 |      0.359 |                   94 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.014 |      0.309 |                   68 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.584 |      0.321 |                   67 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7241 | Steps: 2 | Val loss: 3.5998 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=186463)[0m top1: 0.3204291044776119
[2m[36m(func pid=186463)[0m top5: 0.8446828358208955
[2m[36m(func pid=186463)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=186463)[0m f1_macro: 0.3138710761514967
[2m[36m(func pid=186463)[0m f1_weighted: 0.35204521845715725
[2m[36m(func pid=186463)[0m f1_per_class: [0.181, 0.303, 0.571, 0.335, 0.1, 0.319, 0.418, 0.476, 0.228, 0.207]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.1769 | Steps: 2 | Val loss: 1.8216 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=187246)[0m top1: 0.32509328358208955
[2m[36m(func pid=187246)[0m top5: 0.8526119402985075
[2m[36m(func pid=187246)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=187246)[0m f1_macro: 0.31536182663351314
[2m[36m(func pid=187246)[0m f1_weighted: 0.3369182492880416
[2m[36m(func pid=187246)[0m f1_per_class: [0.099, 0.452, 0.72, 0.402, 0.117, 0.428, 0.201, 0.459, 0.056, 0.22]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0078 | Steps: 2 | Val loss: 2.4134 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=181607)[0m top1: 0.3773320895522388
[2m[36m(func pid=181607)[0m top5: 0.886660447761194
[2m[36m(func pid=181607)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=181607)[0m f1_macro: 0.35856045633510425
[2m[36m(func pid=181607)[0m f1_weighted: 0.4055732727743586
[2m[36m(func pid=181607)[0m f1_per_class: [0.373, 0.355, 0.686, 0.474, 0.063, 0.337, 0.422, 0.45, 0.224, 0.202]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:55:59 (running for 00:46:14.29)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.177 |      0.359 |                   95 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.008 |      0.32  |                   70 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.724 |      0.315 |                   68 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3376865671641791
[2m[36m(func pid=186463)[0m top5: 0.8512126865671642
[2m[36m(func pid=186463)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=186463)[0m f1_macro: 0.31992359021772176
[2m[36m(func pid=186463)[0m f1_weighted: 0.37032774623149445
[2m[36m(func pid=186463)[0m f1_per_class: [0.207, 0.312, 0.533, 0.356, 0.096, 0.299, 0.46, 0.473, 0.244, 0.221]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5881 | Steps: 2 | Val loss: 3.4034 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2159 | Steps: 2 | Val loss: 1.7958 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=187246)[0m top1: 0.3069029850746269
[2m[36m(func pid=187246)[0m top5: 0.8740671641791045
[2m[36m(func pid=187246)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=187246)[0m f1_macro: 0.27440044496750005
[2m[36m(func pid=187246)[0m f1_weighted: 0.3148920490248854
[2m[36m(func pid=187246)[0m f1_per_class: [0.093, 0.358, 0.741, 0.476, 0.143, 0.312, 0.231, 0.047, 0.072, 0.271]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0204 | Steps: 2 | Val loss: 2.4161 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=181607)[0m top1: 0.3894589552238806
[2m[36m(func pid=181607)[0m top5: 0.8875932835820896
[2m[36m(func pid=181607)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=181607)[0m f1_macro: 0.3640960937871032
[2m[36m(func pid=181607)[0m f1_weighted: 0.4173067195319319
[2m[36m(func pid=181607)[0m f1_per_class: [0.371, 0.354, 0.686, 0.5, 0.069, 0.34, 0.433, 0.462, 0.23, 0.197]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:56:05 (running for 00:46:19.82)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.216 |      0.364 |                   96 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.02  |      0.312 |                   71 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.588 |      0.274 |                   69 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.3358208955223881
[2m[36m(func pid=186463)[0m top5: 0.8535447761194029
[2m[36m(func pid=186463)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=186463)[0m f1_macro: 0.3117951821955263
[2m[36m(func pid=186463)[0m f1_weighted: 0.36758136411622105
[2m[36m(func pid=186463)[0m f1_per_class: [0.208, 0.319, 0.522, 0.341, 0.094, 0.253, 0.485, 0.443, 0.238, 0.215]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.7004 | Steps: 2 | Val loss: 3.2445 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.1658 | Steps: 2 | Val loss: 1.7804 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=187246)[0m top1: 0.341884328358209
[2m[36m(func pid=187246)[0m top5: 0.8759328358208955
[2m[36m(func pid=187246)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=187246)[0m f1_macro: 0.25970369341826427
[2m[36m(func pid=187246)[0m f1_weighted: 0.34414022582974785
[2m[36m(func pid=187246)[0m f1_per_class: [0.117, 0.304, 0.471, 0.477, 0.123, 0.422, 0.324, 0.016, 0.141, 0.204]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0070 | Steps: 2 | Val loss: 2.4219 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=181607)[0m top1: 0.394589552238806
[2m[36m(func pid=181607)[0m top5: 0.8871268656716418
[2m[36m(func pid=181607)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=181607)[0m f1_macro: 0.3682626361215283
[2m[36m(func pid=181607)[0m f1_weighted: 0.4234866450169281
[2m[36m(func pid=181607)[0m f1_per_class: [0.353, 0.332, 0.706, 0.514, 0.071, 0.353, 0.448, 0.469, 0.242, 0.196]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:56:10 (running for 00:46:25.20)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.166 |      0.368 |                   97 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.007 |      0.309 |                   72 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.7   |      0.26  |                   70 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.34095149253731344
[2m[36m(func pid=186463)[0m top5: 0.855410447761194
[2m[36m(func pid=186463)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=186463)[0m f1_macro: 0.3086498610106377
[2m[36m(func pid=186463)[0m f1_weighted: 0.3722933334436277
[2m[36m(func pid=186463)[0m f1_per_class: [0.204, 0.333, 0.522, 0.333, 0.088, 0.222, 0.519, 0.405, 0.245, 0.215]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5956 | Steps: 2 | Val loss: 3.9062 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.1848 | Steps: 2 | Val loss: 1.7843 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=187246)[0m top1: 0.2989738805970149
[2m[36m(func pid=187246)[0m top5: 0.8596082089552238
[2m[36m(func pid=187246)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=187246)[0m f1_macro: 0.19642207270420764
[2m[36m(func pid=187246)[0m f1_weighted: 0.2822370079733278
[2m[36m(func pid=187246)[0m f1_per_class: [0.166, 0.265, 0.0, 0.439, 0.109, 0.405, 0.192, 0.015, 0.066, 0.308]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0046 | Steps: 2 | Val loss: 2.4158 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181607)[0m top1: 0.39598880597014924
[2m[36m(func pid=181607)[0m top5: 0.882929104477612
[2m[36m(func pid=181607)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=181607)[0m f1_macro: 0.36170628122676085
[2m[36m(func pid=181607)[0m f1_weighted: 0.42468920343906574
[2m[36m(func pid=181607)[0m f1_per_class: [0.332, 0.333, 0.686, 0.517, 0.077, 0.348, 0.457, 0.441, 0.236, 0.19]
[2m[36m(func pid=181607)[0m 
== Status ==
Current time: 2024-01-07 04:56:16 (running for 00:46:30.76)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.185 |      0.362 |                   98 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.005 |      0.305 |                   73 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.596 |      0.196 |                   71 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.34701492537313433
[2m[36m(func pid=186463)[0m top5: 0.8540111940298507
[2m[36m(func pid=186463)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=186463)[0m f1_macro: 0.3052436296159711
[2m[36m(func pid=186463)[0m f1_weighted: 0.37836433182881113
[2m[36m(func pid=186463)[0m f1_per_class: [0.212, 0.324, 0.5, 0.341, 0.087, 0.21, 0.545, 0.398, 0.23, 0.205]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.7587 | Steps: 2 | Val loss: 3.5191 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2017 | Steps: 2 | Val loss: 1.7867 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=187246)[0m top1: 0.32509328358208955
[2m[36m(func pid=187246)[0m top5: 0.8694029850746269
[2m[36m(func pid=187246)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=187246)[0m f1_macro: 0.23963662664915403
[2m[36m(func pid=187246)[0m f1_weighted: 0.31432112745592045
[2m[36m(func pid=187246)[0m f1_per_class: [0.237, 0.425, 0.0, 0.39, 0.102, 0.431, 0.165, 0.413, 0.086, 0.148]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m top1: 0.38992537313432835
[2m[36m(func pid=181607)[0m top5: 0.8819962686567164
[2m[36m(func pid=181607)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=181607)[0m f1_macro: 0.36185347146919933
[2m[36m(func pid=181607)[0m f1_weighted: 0.41863069803810254
[2m[36m(func pid=181607)[0m f1_per_class: [0.319, 0.32, 0.706, 0.509, 0.083, 0.349, 0.451, 0.445, 0.238, 0.199]
[2m[36m(func pid=181607)[0m 
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0039 | Steps: 2 | Val loss: 2.4152 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4837 | Steps: 2 | Val loss: 3.2018 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:56:22 (running for 00:46:36.51)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.335
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00021 | RUNNING    | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.202 |      0.362 |                   99 |
| train_35a0b_00022 | RUNNING    | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.004 |      0.307 |                   74 |
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  1.759 |      0.24  |                   72 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.35027985074626866
[2m[36m(func pid=186463)[0m top5: 0.8558768656716418
[2m[36m(func pid=186463)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=186463)[0m f1_macro: 0.30680437187628284
[2m[36m(func pid=186463)[0m f1_weighted: 0.3816145055121553
[2m[36m(func pid=186463)[0m f1_per_class: [0.215, 0.326, 0.5, 0.354, 0.08, 0.195, 0.546, 0.393, 0.254, 0.205]
[2m[36m(func pid=186463)[0m 
[2m[36m(func pid=181607)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.1475 | Steps: 2 | Val loss: 1.7884 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=187246)[0m top1: 0.3344216417910448
[2m[36m(func pid=187246)[0m top5: 0.8745335820895522
[2m[36m(func pid=187246)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=187246)[0m f1_macro: 0.24287774918928678
[2m[36m(func pid=187246)[0m f1_weighted: 0.3402850255072944
[2m[36m(func pid=187246)[0m f1_per_class: [0.195, 0.471, 0.0, 0.266, 0.079, 0.449, 0.34, 0.406, 0.074, 0.148]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=181607)[0m top1: 0.386660447761194
[2m[36m(func pid=181607)[0m top5: 0.886660447761194
[2m[36m(func pid=181607)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=181607)[0m f1_macro: 0.36106160610506344
[2m[36m(func pid=181607)[0m f1_weighted: 0.4137792587003128
[2m[36m(func pid=181607)[0m f1_per_class: [0.333, 0.326, 0.706, 0.509, 0.078, 0.346, 0.431, 0.452, 0.218, 0.211]
[2m[36m(func pid=186463)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0116 | Steps: 2 | Val loss: 2.4069 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6856 | Steps: 2 | Val loss: 4.0426 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:56:27 (running for 00:46:41.94)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.335
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.484 |      0.243 |                   73 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
| train_35a0b_00017 | TERMINATED | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.021 |      0.309 |                   75 |
| train_35a0b_00018 | TERMINATED | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.927 |      0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=186463)[0m top1: 0.35261194029850745
[2m[36m(func pid=186463)[0m top5: 0.8544776119402985
[2m[36m(func pid=186463)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=186463)[0m f1_macro: 0.31417514576829186
[2m[36m(func pid=186463)[0m f1_weighted: 0.3851329638800828
[2m[36m(func pid=186463)[0m f1_per_class: [0.225, 0.326, 0.533, 0.369, 0.079, 0.204, 0.538, 0.397, 0.259, 0.211]
[2m[36m(func pid=187246)[0m top1: 0.2980410447761194
[2m[36m(func pid=187246)[0m top5: 0.8549440298507462
[2m[36m(func pid=187246)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=187246)[0m f1_macro: 0.21800836912234206
[2m[36m(func pid=187246)[0m f1_weighted: 0.3176481471201958
[2m[36m(func pid=187246)[0m f1_per_class: [0.141, 0.438, 0.0, 0.093, 0.055, 0.356, 0.488, 0.4, 0.061, 0.148]
[2m[36m(func pid=187246)[0m 
[2m[36m(func pid=187246)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7148 | Steps: 2 | Val loss: 6.3850 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 04:56:34 (running for 00:46:49.32)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.335
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00023 | RUNNING    | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.686 |      0.218 |                   74 |
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
| train_35a0b_00017 | TERMINATED | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.021 |      0.309 |                   75 |
| train_35a0b_00018 | TERMINATED | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.927 |      0.164 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 04:56:35 (running for 00:46:50.14)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.335
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.13 GiB heap, 0.0/55.47 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_35a0b_00000 | TERMINATED | 192.168.7.53:87918  | 0.0001 |       0.99 |         0      |  0.544 |      0.35  |                  100 |
| train_35a0b_00001 | TERMINATED | 192.168.7.53:88300  | 0.001  |       0.99 |         0      |  0.006 |      0.332 |                  100 |
| train_35a0b_00002 | TERMINATED | 192.168.7.53:88716  | 0.01   |       0.99 |         0      |  0.835 |      0.187 |                   75 |
| train_35a0b_00003 | TERMINATED | 192.168.7.53:89134  | 0.1    |       0.99 |         0      |  1.237 |      0.191 |                   75 |
| train_35a0b_00004 | TERMINATED | 192.168.7.53:105422 | 0.0001 |       0.9  |         0      |  2.33  |      0.233 |                   75 |
| train_35a0b_00005 | TERMINATED | 192.168.7.53:106010 | 0.001  |       0.9  |         0      |  0.159 |      0.366 |                  100 |
| train_35a0b_00006 | TERMINATED | 192.168.7.53:111263 | 0.01   |       0.9  |         0      |  0.094 |      0.331 |                   75 |
| train_35a0b_00007 | TERMINATED | 192.168.7.53:111842 | 0.1    |       0.9  |         0      |  0.071 |      0.137 |                   75 |
| train_35a0b_00008 | TERMINATED | 192.168.7.53:122997 | 0.0001 |       0.99 |         0.0001 |  1.077 |      0.33  |                   75 |
| train_35a0b_00009 | TERMINATED | 192.168.7.53:128361 | 0.001  |       0.99 |         0.0001 |  0.002 |      0.316 |                   75 |
| train_35a0b_00010 | TERMINATED | 192.168.7.53:128502 | 0.01   |       0.99 |         0.0001 |  0.481 |      0.11  |                   75 |
| train_35a0b_00011 | TERMINATED | 192.168.7.53:129297 | 0.1    |       0.99 |         0.0001 |  1.548 |      0.15  |                   75 |
| train_35a0b_00012 | TERMINATED | 192.168.7.53:140594 | 0.0001 |       0.9  |         0.0001 |  2.246 |      0.214 |                   75 |
| train_35a0b_00013 | TERMINATED | 192.168.7.53:145749 | 0.001  |       0.9  |         0.0001 |  0.187 |      0.312 |                  100 |
| train_35a0b_00014 | TERMINATED | 192.168.7.53:146482 | 0.01   |       0.9  |         0.0001 |  0.004 |      0.345 |                  100 |
| train_35a0b_00015 | TERMINATED | 192.168.7.53:147051 | 0.1    |       0.9  |         0.0001 |  0.144 |      0.251 |                   75 |
| train_35a0b_00016 | TERMINATED | 192.168.7.53:158211 | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.335 |                   75 |
| train_35a0b_00017 | TERMINATED | 192.168.7.53:163905 | 0.001  |       0.99 |         1e-05  |  0.021 |      0.309 |                   75 |
| train_35a0b_00018 | TERMINATED | 192.168.7.53:168639 | 0.01   |       0.99 |         1e-05  |  0.927 |      0.164 |                   75 |
| train_35a0b_00019 | TERMINATED | 192.168.7.53:169477 | 0.1    |       0.99 |         1e-05  |  2.373 |      0.159 |                   75 |
| train_35a0b_00020 | TERMINATED | 192.168.7.53:176361 | 0.0001 |       0.9  |         1e-05  |  2.227 |      0.227 |                   75 |
| train_35a0b_00021 | TERMINATED | 192.168.7.53:181607 | 0.001  |       0.9  |         1e-05  |  0.148 |      0.361 |                  100 |
| train_35a0b_00022 | TERMINATED | 192.168.7.53:186463 | 0.01   |       0.9  |         1e-05  |  0.012 |      0.314 |                   75 |
| train_35a0b_00023 | TERMINATED | 192.168.7.53:187246 | 0.1    |       0.9  |         1e-05  |  0.715 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 04:56:35,811	INFO tune.py:798 -- Total run time: 2811.22 seconds (2810.13 seconds for the tuning loop).
[2m[36m(func pid=187246)[0m top1: 0.261660447761194
[2m[36m(func pid=187246)[0m top5: 0.800839552238806
[2m[36m(func pid=187246)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=187246)[0m f1_macro: 0.2022649102934826
[2m[36m(func pid=187246)[0m f1_weighted: 0.2907937370220433
[2m[36m(func pid=187246)[0m f1_per_class: [0.112, 0.347, 0.0, 0.065, 0.077, 0.334, 0.486, 0.421, 0.038, 0.143]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341338.1 ON aap04 CANCELLED AT 2024-01-07T04:56:43 ***
srun: error: aap04: task 0: Exited with exit code 1

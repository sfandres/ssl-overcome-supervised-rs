IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 14:32:46,477	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 14:32:46,477	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 14:32:49,020	SUCC scripts.py:747 -- --------------------
2024-01-07 14:32:49,020	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 14:32:49,020	SUCC scripts.py:749 -- --------------------
2024-01-07 14:32:49,020	INFO scripts.py:751 -- Next steps
2024-01-07 14:32:49,020	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 14:32:49,021	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 14:32:49,021	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 14:32:49,021	INFO scripts.py:773 -- import ray
2024-01-07 14:32:49,021	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 14:32:49,021	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 14:32:49,021	INFO scripts.py:791 --   ray status
2024-01-07 14:32:49,021	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 14:32:49,021	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 14:32:49,021	INFO scripts.py:810 --   ray stop
2024-01-07 14:32:49,022	INFO scripts.py:891 -- --block
2024-01-07 14:32:49,022	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 14:32:49,022	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              16451453921565224747
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7fcb51b5b0d0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [10 10 10 10 10 10 10 10 10 10]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.99
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [10 10 10 10 10 10 10 10 10 10]
Done!

Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 14:33:33,269	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 14:33:33,282	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 14:33:56,120	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 14:33:57 (running for 00:00:22.61)
Memory usage on this node: 13.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |
| train_5ae7f_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_5ae7f_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_5ae7f_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13050)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=13050)[0m Configuration completed!
[2m[36m(func pid=13050)[0m New optimizer parameters:
[2m[36m(func pid=13050)[0m SGD (
[2m[36m(func pid=13050)[0m Parameter Group 0
[2m[36m(func pid=13050)[0m     dampening: 0
[2m[36m(func pid=13050)[0m     differentiable: False
[2m[36m(func pid=13050)[0m     foreach: None
[2m[36m(func pid=13050)[0m     lr: 0.0001
[2m[36m(func pid=13050)[0m     maximize: False
[2m[36m(func pid=13050)[0m     momentum: 0.99
[2m[36m(func pid=13050)[0m     nesterov: False
[2m[36m(func pid=13050)[0m     weight_decay: 0
[2m[36m(func pid=13050)[0m )
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9506 | Steps: 4 | Val loss: 2.3370 | Batch size: 32 | lr: 0.0001 | Duration: 5.10s
[2m[36m(func pid=13050)[0m top1: 0.16837686567164178
[2m[36m(func pid=13050)[0m top5: 0.5149253731343284
[2m[36m(func pid=13050)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=13050)[0m f1_macro: 0.10076685590898841
[2m[36m(func pid=13050)[0m f1_weighted: 0.12163502856408658
[2m[36m(func pid=13050)[0m f1_per_class: [0.189, 0.296, 0.0, 0.106, 0.01, 0.273, 0.012, 0.02, 0.0, 0.102]
== Status ==
Current time: 2024-01-07 14:34:07 (running for 00:00:32.58)
Memory usage on this node: 15.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |
| train_5ae7f_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_5ae7f_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13435)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=13435)[0m Configuration completed!
[2m[36m(func pid=13435)[0m New optimizer parameters:
[2m[36m(func pid=13435)[0m SGD (
[2m[36m(func pid=13435)[0m Parameter Group 0
[2m[36m(func pid=13435)[0m     dampening: 0
[2m[36m(func pid=13435)[0m     differentiable: False
[2m[36m(func pid=13435)[0m     foreach: None
[2m[36m(func pid=13435)[0m     lr: 0.001
[2m[36m(func pid=13435)[0m     maximize: False
[2m[36m(func pid=13435)[0m     momentum: 0.99
[2m[36m(func pid=13435)[0m     nesterov: False
[2m[36m(func pid=13435)[0m     weight_decay: 0
[2m[36m(func pid=13435)[0m )
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9369 | Steps: 4 | Val loss: 2.3976 | Batch size: 32 | lr: 0.001 | Duration: 4.97s
[2m[36m(func pid=13435)[0m top1: 0.125
[2m[36m(func pid=13435)[0m top5: 0.4533582089552239
[2m[36m(func pid=13435)[0m f1_micro: 0.125
[2m[36m(func pid=13435)[0m f1_macro: 0.07345172179449677
[2m[36m(func pid=13435)[0m f1_weighted: 0.09200848271385598
[2m[36m(func pid=13435)[0m f1_per_class: [0.12, 0.19, 0.0, 0.089, 0.0, 0.252, 0.003, 0.023, 0.013, 0.044]
== Status ==
Current time: 2024-01-07 14:34:16 (running for 00:00:41.63)
Memory usage on this node: 18.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |
| train_5ae7f_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13861)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=13861)[0m Configuration completed!
[2m[36m(func pid=13861)[0m New optimizer parameters:
[2m[36m(func pid=13861)[0m SGD (
[2m[36m(func pid=13861)[0m Parameter Group 0
[2m[36m(func pid=13861)[0m     dampening: 0
[2m[36m(func pid=13861)[0m     differentiable: False
[2m[36m(func pid=13861)[0m     foreach: None
[2m[36m(func pid=13861)[0m     lr: 0.01
[2m[36m(func pid=13861)[0m     maximize: False
[2m[36m(func pid=13861)[0m     momentum: 0.99
[2m[36m(func pid=13861)[0m     nesterov: False
[2m[36m(func pid=13861)[0m     weight_decay: 0
[2m[36m(func pid=13861)[0m )
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9041 | Steps: 4 | Val loss: 2.4276 | Batch size: 32 | lr: 0.01 | Duration: 4.65s
[2m[36m(func pid=13861)[0m top1: 0.11380597014925373
[2m[36m(func pid=13861)[0m top5: 0.3726679104477612
[2m[36m(func pid=13861)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=13861)[0m f1_macro: 0.08757307638466313
[2m[36m(func pid=13861)[0m f1_weighted: 0.06738975939531489
[2m[36m(func pid=13861)[0m f1_per_class: [0.134, 0.053, 0.179, 0.022, 0.0, 0.334, 0.018, 0.063, 0.0, 0.071]
== Status ==
Current time: 2024-01-07 14:34:24 (running for 00:00:50.09)
Memory usage on this node: 20.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 14:34:32 (running for 00:00:58.39)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.951 |      0.101 |                    1 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |        |            |                      |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |        |            |                      |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |        |            |                      |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=14288)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=14288)[0m Configuration completed!
[2m[36m(func pid=14288)[0m New optimizer parameters:
[2m[36m(func pid=14288)[0m SGD (
[2m[36m(func pid=14288)[0m Parameter Group 0
[2m[36m(func pid=14288)[0m     dampening: 0
[2m[36m(func pid=14288)[0m     differentiable: False
[2m[36m(func pid=14288)[0m     foreach: None
[2m[36m(func pid=14288)[0m     lr: 0.1
[2m[36m(func pid=14288)[0m     maximize: False
[2m[36m(func pid=14288)[0m     momentum: 0.99
[2m[36m(func pid=14288)[0m     nesterov: False
[2m[36m(func pid=14288)[0m     weight_decay: 0
[2m[36m(func pid=14288)[0m )
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0107 | Steps: 4 | Val loss: 2.3689 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9017 | Steps: 4 | Val loss: 2.3493 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6950 | Steps: 4 | Val loss: 2.3705 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0161 | Steps: 4 | Val loss: 3.5882 | Batch size: 32 | lr: 0.1 | Duration: 4.91s
== Status ==
Current time: 2024-01-07 14:34:37 (running for 00:01:03.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.951 |      0.101 |                    1 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.937 |      0.073 |                    1 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.904 |      0.088 |                    1 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |        |            |                      |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.14319029850746268
[2m[36m(func pid=13050)[0m top5: 0.4944029850746269
[2m[36m(func pid=13050)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=13050)[0m f1_macro: 0.08247242072323642
[2m[36m(func pid=13050)[0m f1_weighted: 0.10597280075433123
[2m[36m(func pid=13050)[0m f1_per_class: [0.114, 0.218, 0.0, 0.104, 0.02, 0.274, 0.012, 0.023, 0.0, 0.06]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m top1: 0.12546641791044777
[2m[36m(func pid=13435)[0m top5: 0.5475746268656716
[2m[36m(func pid=13435)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=13435)[0m f1_macro: 0.07799515568638074
[2m[36m(func pid=13435)[0m f1_weighted: 0.10105716826456834
[2m[36m(func pid=13435)[0m f1_per_class: [0.109, 0.19, 0.024, 0.092, 0.026, 0.254, 0.027, 0.047, 0.01, 0.0]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.1226679104477612
[2m[36m(func pid=13861)[0m top5: 0.5013992537313433
[2m[36m(func pid=13861)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=13861)[0m f1_macro: 0.09429498157409917
[2m[36m(func pid=13861)[0m f1_weighted: 0.06609538670397512
[2m[36m(func pid=13861)[0m f1_per_class: [0.353, 0.0, 0.048, 0.066, 0.0, 0.288, 0.0, 0.11, 0.0, 0.079]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.014458955223880597
[2m[36m(func pid=14288)[0m top5: 0.3125
[2m[36m(func pid=14288)[0m f1_micro: 0.014458955223880597
[2m[36m(func pid=14288)[0m f1_macro: 0.015604998415661767
[2m[36m(func pid=14288)[0m f1_weighted: 0.0020228787405742518
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.022, 0.0, 0.075, 0.0, 0.0, 0.0, 0.03, 0.029]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9544 | Steps: 4 | Val loss: 2.3691 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8017 | Steps: 4 | Val loss: 2.2304 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.1941 | Steps: 4 | Val loss: 2.0987 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.3537 | Steps: 4 | Val loss: 7.6928 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=13050)[0m top1: 0.13152985074626866
[2m[36m(func pid=13050)[0m top5: 0.503731343283582
[2m[36m(func pid=13050)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=13050)[0m f1_macro: 0.07137181325433463
[2m[36m(func pid=13050)[0m f1_weighted: 0.10850723328585019
[2m[36m(func pid=13050)[0m f1_per_class: [0.051, 0.183, 0.0, 0.114, 0.009, 0.257, 0.038, 0.051, 0.01, 0.0]
[2m[36m(func pid=13050)[0m 
== Status ==
Current time: 2024-01-07 14:34:43 (running for 00:01:09.23)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.954 |      0.071 |                    3 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.902 |      0.078 |                    2 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.695 |      0.094 |                    2 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.016 |      0.016 |                    1 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.18563432835820895
[2m[36m(func pid=13435)[0m top5: 0.7042910447761194
[2m[36m(func pid=13435)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=13435)[0m f1_macro: 0.1338635314119958
[2m[36m(func pid=13435)[0m f1_weighted: 0.20258169022570247
[2m[36m(func pid=13435)[0m f1_per_class: [0.102, 0.22, 0.245, 0.182, 0.0, 0.171, 0.278, 0.142, 0.0, 0.0]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.2490671641791045
[2m[36m(func pid=13861)[0m top5: 0.7028917910447762
[2m[36m(func pid=13861)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=13861)[0m f1_macro: 0.21368935382044948
[2m[36m(func pid=13861)[0m f1_weighted: 0.20993085908497144
[2m[36m(func pid=13861)[0m f1_per_class: [0.466, 0.0, 0.282, 0.482, 0.076, 0.326, 0.012, 0.358, 0.02, 0.115]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.02332089552238806
[2m[36m(func pid=14288)[0m top5: 0.5606343283582089
[2m[36m(func pid=14288)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=14288)[0m f1_macro: 0.016286418991829813
[2m[36m(func pid=14288)[0m f1_weighted: 0.026545484769769152
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.154, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9357 | Steps: 4 | Val loss: 2.3393 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6656 | Steps: 4 | Val loss: 2.1285 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.9909 | Steps: 4 | Val loss: 1.9808 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.6337 | Steps: 4 | Val loss: 95.2969 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:34:49 (running for 00:01:14.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.954 |      0.071 |                    3 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.666 |      0.173 |                    4 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.194 |      0.214 |                    3 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.354 |      0.016 |                    2 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.1455223880597015
[2m[36m(func pid=13050)[0m top5: 0.5303171641791045
[2m[36m(func pid=13050)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=13050)[0m f1_macro: 0.08104484142487986
[2m[36m(func pid=13050)[0m f1_weighted: 0.13778836758356336
[2m[36m(func pid=13050)[0m f1_per_class: [0.021, 0.197, 0.0, 0.132, 0.026, 0.252, 0.113, 0.069, 0.0, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2630597014925373
[2m[36m(func pid=13435)[0m top5: 0.7975746268656716
[2m[36m(func pid=13435)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=13435)[0m f1_macro: 0.17254828752080226
[2m[36m(func pid=13435)[0m f1_weighted: 0.27123509593072553
[2m[36m(func pid=13435)[0m f1_per_class: [0.117, 0.275, 0.383, 0.267, 0.079, 0.091, 0.433, 0.081, 0.0, 0.0]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.21875
[2m[36m(func pid=13861)[0m top5: 0.800839552238806
[2m[36m(func pid=13861)[0m f1_micro: 0.21875
[2m[36m(func pid=13861)[0m f1_macro: 0.20686256341870454
[2m[36m(func pid=13861)[0m f1_weighted: 0.19860058826919555
[2m[36m(func pid=13861)[0m f1_per_class: [0.136, 0.0, 0.324, 0.384, 0.109, 0.339, 0.071, 0.311, 0.156, 0.238]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.2756529850746269
[2m[36m(func pid=14288)[0m top5: 0.7276119402985075
[2m[36m(func pid=14288)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=14288)[0m f1_macro: 0.04498267512041289
[2m[36m(func pid=14288)[0m f1_weighted: 0.12310422822781134
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.435, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.5760 | Steps: 4 | Val loss: 2.0408 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9080 | Steps: 4 | Val loss: 2.3230 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.6187 | Steps: 4 | Val loss: 1.6084 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.6705 | Steps: 4 | Val loss: 386.3053 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=13435)[0m top1: 0.31156716417910446
[2m[36m(func pid=13435)[0m top5: 0.8260261194029851
[2m[36m(func pid=13435)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=13435)[0m f1_macro: 0.21515073543736007
[2m[36m(func pid=13435)[0m f1_weighted: 0.3120100930209392
[2m[36m(func pid=13435)[0m f1_per_class: [0.122, 0.303, 0.615, 0.351, 0.071, 0.031, 0.489, 0.083, 0.0, 0.085]
[2m[36m(func pid=13435)[0m 
== Status ==
Current time: 2024-01-07 14:34:54 (running for 00:01:20.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.936 |      0.081 |                    4 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.576 |      0.215 |                    5 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.991 |      0.207 |                    4 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.634 |      0.045 |                    3 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.15811567164179105
[2m[36m(func pid=13050)[0m top5: 0.5517723880597015
[2m[36m(func pid=13050)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=13050)[0m f1_macro: 0.09649497161192389
[2m[36m(func pid=13050)[0m f1_weighted: 0.15547104243103366
[2m[36m(func pid=13050)[0m f1_per_class: [0.059, 0.172, 0.03, 0.184, 0.023, 0.27, 0.121, 0.106, 0.0, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.38013059701492535
[2m[36m(func pid=13861)[0m top5: 0.9365671641791045
[2m[36m(func pid=13861)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=13861)[0m f1_macro: 0.33480542892090853
[2m[36m(func pid=13861)[0m f1_weighted: 0.3998526546407956
[2m[36m(func pid=13861)[0m f1_per_class: [0.324, 0.403, 0.571, 0.465, 0.142, 0.422, 0.393, 0.26, 0.23, 0.137]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.03311567164179104
[2m[36m(func pid=14288)[0m top5: 0.6338619402985075
[2m[36m(func pid=14288)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=14288)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=14288)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3941 | Steps: 4 | Val loss: 1.9621 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8917 | Steps: 4 | Val loss: 2.2846 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4405 | Steps: 4 | Val loss: 2.3332 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.3226 | Steps: 4 | Val loss: 249.7733 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:34:59 (running for 00:01:25.45)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.908 |      0.096 |                    5 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.394 |      0.238 |                    6 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.619 |      0.335 |                    5 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.671 |      0.006 |                    4 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.34095149253731344
[2m[36m(func pid=13435)[0m top5: 0.8348880597014925
[2m[36m(func pid=13435)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=13435)[0m f1_macro: 0.2379055349680085
[2m[36m(func pid=13435)[0m f1_weighted: 0.33865961977030035
[2m[36m(func pid=13435)[0m f1_per_class: [0.124, 0.25, 0.6, 0.449, 0.103, 0.039, 0.512, 0.062, 0.0, 0.239]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.16837686567164178
[2m[36m(func pid=13050)[0m top5: 0.6063432835820896
[2m[36m(func pid=13050)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=13050)[0m f1_macro: 0.10915444476522511
[2m[36m(func pid=13050)[0m f1_weighted: 0.17521158590901967
[2m[36m(func pid=13050)[0m f1_per_class: [0.095, 0.179, 0.025, 0.205, 0.027, 0.274, 0.154, 0.133, 0.0, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.18936567164179105
[2m[36m(func pid=13861)[0m top5: 0.8731343283582089
[2m[36m(func pid=13861)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=13861)[0m f1_macro: 0.16234123955152038
[2m[36m(func pid=13861)[0m f1_weighted: 0.13060389540824913
[2m[36m(func pid=13861)[0m f1_per_class: [0.102, 0.436, 0.106, 0.01, 0.196, 0.099, 0.044, 0.221, 0.31, 0.1]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.03311567164179104
[2m[36m(func pid=14288)[0m top5: 0.582089552238806
[2m[36m(func pid=14288)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=14288)[0m f1_macro: 0.006431159420289855
[2m[36m(func pid=14288)[0m f1_weighted: 0.002129721636383301
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8722 | Steps: 4 | Val loss: 2.2666 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3638 | Steps: 4 | Val loss: 1.9249 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.8513 | Steps: 4 | Val loss: 2.5978 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 6.0579 | Steps: 4 | Val loss: 229.3467 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:35:05 (running for 00:01:31.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.892 |      0.109 |                    6 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.364 |      0.241 |                    7 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.44  |      0.162 |                    6 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.323 |      0.006 |                    5 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.332089552238806
[2m[36m(func pid=13435)[0m top5: 0.84375
[2m[36m(func pid=13435)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=13435)[0m f1_macro: 0.24112730922517916
[2m[36m(func pid=13435)[0m f1_weighted: 0.33938600433898064
[2m[36m(func pid=13435)[0m f1_per_class: [0.119, 0.177, 0.524, 0.438, 0.163, 0.148, 0.525, 0.05, 0.026, 0.241]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.19496268656716417
[2m[36m(func pid=13861)[0m top5: 0.8432835820895522
[2m[36m(func pid=13861)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=13861)[0m f1_macro: 0.19486859521423655
[2m[36m(func pid=13861)[0m f1_weighted: 0.2112599992096468
[2m[36m(func pid=13861)[0m f1_per_class: [0.14, 0.237, 0.098, 0.288, 0.333, 0.182, 0.13, 0.31, 0.184, 0.047]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.17723880597014927
[2m[36m(func pid=13050)[0m top5: 0.6375932835820896
[2m[36m(func pid=13050)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=13050)[0m f1_macro: 0.1228452482888264
[2m[36m(func pid=13050)[0m f1_weighted: 0.1838266720224269
[2m[36m(func pid=13050)[0m f1_per_class: [0.102, 0.182, 0.119, 0.26, 0.019, 0.285, 0.122, 0.138, 0.0, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m top1: 0.03684701492537314
[2m[36m(func pid=14288)[0m top5: 0.5946828358208955
[2m[36m(func pid=14288)[0m f1_micro: 0.03684701492537314
[2m[36m(func pid=14288)[0m f1_macro: 0.016335668974667275
[2m[36m(func pid=14288)[0m f1_weighted: 0.0033512317346886364
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.067, 0.096]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.4061 | Steps: 4 | Val loss: 4.4765 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8316 | Steps: 4 | Val loss: 2.2594 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2337 | Steps: 4 | Val loss: 1.9016 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.6747 | Steps: 4 | Val loss: 256.5944 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:35:10 (running for 00:01:36.33)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.872 |      0.123 |                    7 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.364 |      0.241 |                    7 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.406 |      0.09  |                    8 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  6.058 |      0.016 |                    6 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.34375
[2m[36m(func pid=13435)[0m top5: 0.8213619402985075
[2m[36m(func pid=13435)[0m f1_micro: 0.34375
[2m[36m(func pid=13435)[0m f1_macro: 0.2666737382416724
[2m[36m(func pid=13435)[0m f1_weighted: 0.36559869377602366
[2m[36m(func pid=13435)[0m f1_per_class: [0.138, 0.18, 0.471, 0.481, 0.14, 0.287, 0.481, 0.24, 0.045, 0.205]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.10261194029850747
[2m[36m(func pid=13861)[0m top5: 0.792910447761194
[2m[36m(func pid=13861)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=13861)[0m f1_macro: 0.08960403061215849
[2m[36m(func pid=13861)[0m f1_weighted: 0.1187314465832316
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.0, 0.0, 0.097, 0.364, 0.04, 0.271, 0.0, 0.095, 0.029]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.18470149253731344
[2m[36m(func pid=13050)[0m top5: 0.6352611940298507
[2m[36m(func pid=13050)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=13050)[0m f1_macro: 0.13639372666385974
[2m[36m(func pid=13050)[0m f1_weighted: 0.19452476436043517
[2m[36m(func pid=13050)[0m f1_per_class: [0.124, 0.177, 0.176, 0.277, 0.022, 0.313, 0.132, 0.127, 0.015, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m top1: 0.16884328358208955
[2m[36m(func pid=14288)[0m top5: 0.5694962686567164
[2m[36m(func pid=14288)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=14288)[0m f1_macro: 0.031152958152958148
[2m[36m(func pid=14288)[0m f1_weighted: 0.05237745929443691
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.29, 0.0, 0.0, 0.0, 0.021, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.0383 | Steps: 4 | Val loss: 1.8626 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.1325 | Steps: 4 | Val loss: 6.5951 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7817 | Steps: 4 | Val loss: 2.2408 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.0070 | Steps: 4 | Val loss: 160.3306 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=13435)[0m top1: 0.3572761194029851
[2m[36m(func pid=13435)[0m top5: 0.8213619402985075
[2m[36m(func pid=13435)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=13435)[0m f1_macro: 0.2913619092730206
[2m[36m(func pid=13435)[0m f1_weighted: 0.37740936484305754
[2m[36m(func pid=13435)[0m f1_per_class: [0.201, 0.234, 0.421, 0.515, 0.099, 0.336, 0.411, 0.31, 0.13, 0.256]
[2m[36m(func pid=13435)[0m 
== Status ==
Current time: 2024-01-07 14:35:15 (running for 00:01:41.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.832 |      0.136 |                    8 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  2.038 |      0.291 |                    9 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.406 |      0.09  |                    8 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.675 |      0.031 |                    7 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.19076492537313433
[2m[36m(func pid=13050)[0m top5: 0.6595149253731343
[2m[36m(func pid=13050)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=13050)[0m f1_macro: 0.1511483098548155
[2m[36m(func pid=13050)[0m f1_weighted: 0.20524004988080194
[2m[36m(func pid=13050)[0m f1_per_class: [0.123, 0.212, 0.262, 0.301, 0.02, 0.307, 0.123, 0.145, 0.019, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.045242537313432835
[2m[36m(func pid=13861)[0m top5: 0.5778917910447762
[2m[36m(func pid=13861)[0m f1_micro: 0.045242537313432835
[2m[36m(func pid=13861)[0m f1_macro: 0.038753707776227664
[2m[36m(func pid=13861)[0m f1_weighted: 0.020215778812184827
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.0, 0.0, 0.049, 0.182, 0.0, 0.006, 0.0, 0.075, 0.075]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.24813432835820895
[2m[36m(func pid=14288)[0m top5: 0.6156716417910447
[2m[36m(func pid=14288)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=14288)[0m f1_macro: 0.06544756806488802
[2m[36m(func pid=14288)[0m f1_weighted: 0.15372885667533728
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.239, 0.0, 0.396, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9030 | Steps: 4 | Val loss: 1.8264 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9051 | Steps: 4 | Val loss: 6.4743 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7338 | Steps: 4 | Val loss: 2.2330 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:35:21 (running for 00:01:46.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.782 |      0.151 |                    9 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.903 |      0.282 |                   10 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.132 |      0.039 |                    9 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  4.007 |      0.065 |                    8 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.3591417910447761
[2m[36m(func pid=13435)[0m top5: 0.8339552238805971
[2m[36m(func pid=13435)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=13435)[0m f1_macro: 0.2817936522733005
[2m[36m(func pid=13435)[0m f1_weighted: 0.3734250779035024
[2m[36m(func pid=13435)[0m f1_per_class: [0.248, 0.227, 0.286, 0.534, 0.093, 0.351, 0.367, 0.377, 0.131, 0.205]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.2013 | Steps: 4 | Val loss: 294.1872 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=13861)[0m top1: 0.16044776119402984
[2m[36m(func pid=13861)[0m top5: 0.6497201492537313
[2m[36m(func pid=13861)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=13861)[0m f1_macro: 0.09301619865679814
[2m[36m(func pid=13861)[0m f1_weighted: 0.16683481458172833
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.358, 0.0, 0.367, 0.111, 0.0, 0.0, 0.0, 0.042, 0.052]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.19636194029850745
[2m[36m(func pid=13050)[0m top5: 0.6613805970149254
[2m[36m(func pid=13050)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=13050)[0m f1_macro: 0.1567914466554655
[2m[36m(func pid=13050)[0m f1_weighted: 0.20752347491931267
[2m[36m(func pid=13050)[0m f1_per_class: [0.127, 0.174, 0.303, 0.349, 0.025, 0.307, 0.102, 0.163, 0.018, 0.0]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m top1: 0.013059701492537313
[2m[36m(func pid=14288)[0m top5: 0.7019589552238806
[2m[36m(func pid=14288)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=14288)[0m f1_macro: 0.0035020559823042184
[2m[36m(func pid=14288)[0m f1_weighted: 0.003685958807696805
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0, 0.024]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7438 | Steps: 4 | Val loss: 6.2483 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.8139 | Steps: 4 | Val loss: 1.8207 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7209 | Steps: 4 | Val loss: 2.2148 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.0662 | Steps: 4 | Val loss: 383.4522 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:35:26 (running for 00:01:52.41)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.734 |      0.157 |                   10 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.903 |      0.282 |                   10 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.744 |      0.113 |                   11 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.201 |      0.004 |                    9 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.33722014925373134
[2m[36m(func pid=13435)[0m top5: 0.8274253731343284
[2m[36m(func pid=13435)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=13435)[0m f1_macro: 0.275218995826085
[2m[36m(func pid=13435)[0m f1_weighted: 0.3395257445768176
[2m[36m(func pid=13435)[0m f1_per_class: [0.342, 0.225, 0.24, 0.543, 0.085, 0.281, 0.257, 0.394, 0.203, 0.183]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.15671641791044777
[2m[36m(func pid=13861)[0m top5: 0.8451492537313433
[2m[36m(func pid=13861)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=13861)[0m f1_macro: 0.11311781775857954
[2m[36m(func pid=13861)[0m f1_weighted: 0.12824854607969594
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.305, 0.0, 0.195, 0.167, 0.016, 0.0, 0.24, 0.09, 0.118]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.20848880597014927
[2m[36m(func pid=13050)[0m top5: 0.6800373134328358
[2m[36m(func pid=13050)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=13050)[0m f1_macro: 0.17365831164158968
[2m[36m(func pid=13050)[0m f1_weighted: 0.21659502335727684
[2m[36m(func pid=13050)[0m f1_per_class: [0.149, 0.192, 0.344, 0.375, 0.032, 0.33, 0.078, 0.202, 0.018, 0.016]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m top1: 0.014458955223880597
[2m[36m(func pid=14288)[0m top5: 0.6539179104477612
[2m[36m(func pid=14288)[0m f1_micro: 0.014458955223880597
[2m[36m(func pid=14288)[0m f1_macro: 0.00577656493774663
[2m[36m(func pid=14288)[0m f1_weighted: 0.0066891967112660666
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.014, 0.024]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.6167 | Steps: 4 | Val loss: 1.8128 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.4637 | Steps: 4 | Val loss: 17.2650 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6074 | Steps: 4 | Val loss: 2.2033 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:35:32 (running for 00:01:57.74)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.721 |      0.174 |                   11 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.617 |      0.263 |                   12 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.744 |      0.113 |                   11 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.066 |      0.006 |                   10 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.31716417910447764
[2m[36m(func pid=13435)[0m top5: 0.8386194029850746
[2m[36m(func pid=13435)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=13435)[0m f1_macro: 0.2631625481803325
[2m[36m(func pid=13435)[0m f1_weighted: 0.31394948370744047
[2m[36m(func pid=13435)[0m f1_per_class: [0.413, 0.249, 0.216, 0.561, 0.103, 0.217, 0.172, 0.326, 0.222, 0.152]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.2196828358208955
[2m[36m(func pid=13050)[0m top5: 0.6828358208955224
[2m[36m(func pid=13050)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=13050)[0m f1_macro: 0.17996891464064504
[2m[36m(func pid=13050)[0m f1_weighted: 0.21717485635245978
[2m[36m(func pid=13050)[0m f1_per_class: [0.152, 0.187, 0.379, 0.403, 0.035, 0.335, 0.054, 0.206, 0.0, 0.048]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.05083955223880597
[2m[36m(func pid=13861)[0m top5: 0.417910447761194
[2m[36m(func pid=13861)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=13861)[0m f1_macro: 0.029094029584416176
[2m[36m(func pid=13861)[0m f1_weighted: 0.02183099615366172
[2m[36m(func pid=13861)[0m f1_per_class: [0.068, 0.104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054, 0.065]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.0344 | Steps: 4 | Val loss: 201.2609 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=14288)[0m top1: 0.03917910447761194
[2m[36m(func pid=14288)[0m top5: 0.5522388059701493
[2m[36m(func pid=14288)[0m f1_micro: 0.03917910447761194
[2m[36m(func pid=14288)[0m f1_macro: 0.015015219662895827
[2m[36m(func pid=14288)[0m f1_weighted: 0.03795988284690763
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.0, 0.024]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.4273 | Steps: 4 | Val loss: 1.8213 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.3742 | Steps: 4 | Val loss: 9.6813 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6267 | Steps: 4 | Val loss: 2.1879 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:35:37 (running for 00:02:03.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.607 |      0.18  |                   12 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.617 |      0.263 |                   12 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.374 |      0.073 |                   13 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.034 |      0.015 |                   11 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.10960820895522388
[2m[36m(func pid=13861)[0m top5: 0.5620335820895522
[2m[36m(func pid=13861)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=13861)[0m f1_macro: 0.07305687981628262
[2m[36m(func pid=13861)[0m f1_weighted: 0.11420409969490224
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.185, 0.0, 0.354, 0.073, 0.041, 0.072]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.22761194029850745
[2m[36m(func pid=13050)[0m top5: 0.7014925373134329
[2m[36m(func pid=13050)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=13050)[0m f1_macro: 0.18411937317010632
[2m[36m(func pid=13050)[0m f1_weighted: 0.2218200283252042
[2m[36m(func pid=13050)[0m f1_per_class: [0.163, 0.138, 0.379, 0.428, 0.037, 0.339, 0.065, 0.237, 0.0, 0.053]
[2m[36m(func pid=13435)[0m top1: 0.31576492537313433
[2m[36m(func pid=13435)[0m top5: 0.8656716417910447
[2m[36m(func pid=13435)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=13435)[0m f1_macro: 0.28493019415505716
[2m[36m(func pid=13435)[0m f1_weighted: 0.3211769257965916
[2m[36m(func pid=13435)[0m f1_per_class: [0.541, 0.26, 0.27, 0.552, 0.086, 0.209, 0.187, 0.329, 0.266, 0.15]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.0701 | Steps: 4 | Val loss: 107.6717 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m top1: 0.29524253731343286
[2m[36m(func pid=14288)[0m top5: 0.5419776119402985
[2m[36m(func pid=14288)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=14288)[0m f1_macro: 0.047629796839729115
[2m[36m(func pid=14288)[0m f1_weighted: 0.14195634412587174
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.476, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.0938 | Steps: 4 | Val loss: 15.4373 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6456 | Steps: 4 | Val loss: 2.1613 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.5141 | Steps: 4 | Val loss: 1.9637 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:35:43 (running for 00:02:08.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.627 |      0.184 |                   13 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.427 |      0.285 |                   13 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.094 |      0.038 |                   14 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.07  |      0.048 |                   12 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.027985074626865673
[2m[36m(func pid=13861)[0m top5: 0.5083955223880597
[2m[36m(func pid=13861)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=13861)[0m f1_macro: 0.037867054065368215
[2m[36m(func pid=13861)[0m f1_weighted: 0.020537042046451456
[2m[36m(func pid=13861)[0m f1_per_class: [0.024, 0.096, 0.0, 0.0, 0.176, 0.0, 0.0, 0.0, 0.057, 0.025]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8078 | Steps: 4 | Val loss: 37.2481 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=13050)[0m top1: 0.2574626865671642
[2m[36m(func pid=13050)[0m top5: 0.7094216417910447
[2m[36m(func pid=13050)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=13050)[0m f1_macro: 0.19495839214626337
[2m[36m(func pid=13050)[0m f1_weighted: 0.24198065637860752
[2m[36m(func pid=13050)[0m f1_per_class: [0.179, 0.12, 0.379, 0.481, 0.048, 0.333, 0.095, 0.236, 0.0, 0.078]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2593283582089552
[2m[36m(func pid=13435)[0m top5: 0.8652052238805971
[2m[36m(func pid=13435)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=13435)[0m f1_macro: 0.25434307088193503
[2m[36m(func pid=13435)[0m f1_weighted: 0.27543133754301785
[2m[36m(func pid=13435)[0m f1_per_class: [0.451, 0.357, 0.264, 0.356, 0.076, 0.171, 0.188, 0.321, 0.232, 0.128]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m top1: 0.07136194029850747
[2m[36m(func pid=14288)[0m top5: 0.5816231343283582
[2m[36m(func pid=14288)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=14288)[0m f1_macro: 0.036957922963634385
[2m[36m(func pid=14288)[0m f1_weighted: 0.027762299215805687
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.042, 0.0, 0.0, 0.071, 0.093, 0.003, 0.126, 0.034, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6331 | Steps: 4 | Val loss: 29.6293 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5560 | Steps: 4 | Val loss: 2.1511 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.2630 | Steps: 4 | Val loss: 2.0838 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:35:48 (running for 00:02:14.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.646 |      0.195 |                   14 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.514 |      0.254 |                   14 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.633 |      0.038 |                   15 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.808 |      0.037 |                   13 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.025186567164179104
[2m[36m(func pid=13861)[0m top5: 0.3493470149253731
[2m[36m(func pid=13861)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=13861)[0m f1_macro: 0.03770678801309307
[2m[36m(func pid=13861)[0m f1_weighted: 0.01808073926280749
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.026, 0.0, 0.0, 0.242, 0.0, 0.033, 0.0, 0.054, 0.022]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.0583 | Steps: 4 | Val loss: 35.7317 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=13050)[0m top1: 0.2644589552238806
[2m[36m(func pid=13050)[0m top5: 0.7164179104477612
[2m[36m(func pid=13050)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=13050)[0m f1_macro: 0.19155435850128905
[2m[36m(func pid=13050)[0m f1_weighted: 0.235587083168694
[2m[36m(func pid=13050)[0m f1_per_class: [0.204, 0.098, 0.338, 0.504, 0.054, 0.306, 0.069, 0.257, 0.0, 0.085]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m top1: 0.22574626865671643
[2m[36m(func pid=13435)[0m top5: 0.8638059701492538
[2m[36m(func pid=13435)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=13435)[0m f1_macro: 0.2478125017601373
[2m[36m(func pid=13435)[0m f1_weighted: 0.22576035383536977
[2m[36m(func pid=13435)[0m f1_per_class: [0.413, 0.36, 0.444, 0.199, 0.07, 0.194, 0.167, 0.29, 0.189, 0.153]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m top1: 0.09375
[2m[36m(func pid=14288)[0m top5: 0.6026119402985075
[2m[36m(func pid=14288)[0m f1_micro: 0.09375
[2m[36m(func pid=14288)[0m f1_macro: 0.06409757956346876
[2m[36m(func pid=14288)[0m f1_weighted: 0.054427277738721344
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.183, 0.0, 0.0, 0.0, 0.089, 0.0, 0.161, 0.052, 0.157]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8622 | Steps: 4 | Val loss: 32.6289 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5094 | Steps: 4 | Val loss: 2.0015 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5936 | Steps: 4 | Val loss: 2.1237 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:35:53 (running for 00:02:19.46)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.556 |      0.192 |                   15 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.263 |      0.248 |                   15 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.862 |      0.026 |                   16 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.058 |      0.064 |                   14 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.04057835820895522
[2m[36m(func pid=13861)[0m top5: 0.42677238805970147
[2m[36m(func pid=13861)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=13861)[0m f1_macro: 0.02646932540722772
[2m[36m(func pid=13861)[0m f1_weighted: 0.03501458832459252
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.165, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.05, 0.034]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2555970149253731
[2m[36m(func pid=13435)[0m top5: 0.8763992537313433
[2m[36m(func pid=13435)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=13435)[0m f1_macro: 0.27624620476052353
[2m[36m(func pid=13435)[0m f1_weighted: 0.25539362532107734
[2m[36m(func pid=13435)[0m f1_per_class: [0.381, 0.391, 0.49, 0.28, 0.063, 0.274, 0.132, 0.302, 0.255, 0.195]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.28218283582089554
[2m[36m(func pid=13050)[0m top5: 0.7313432835820896
[2m[36m(func pid=13050)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=13050)[0m f1_macro: 0.21878473547139038
[2m[36m(func pid=13050)[0m f1_weighted: 0.2423465680851607
[2m[36m(func pid=13050)[0m f1_per_class: [0.231, 0.114, 0.462, 0.526, 0.059, 0.277, 0.063, 0.264, 0.0, 0.192]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.0161 | Steps: 4 | Val loss: 23.9162 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=14288)[0m top1: 0.05503731343283582
[2m[36m(func pid=14288)[0m top5: 0.6030783582089553
[2m[36m(func pid=14288)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=14288)[0m f1_macro: 0.035846014271978754
[2m[36m(func pid=14288)[0m f1_weighted: 0.0326311468945472
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.117, 0.0, 0.0, 0.0, 0.03, 0.0, 0.118, 0.054, 0.039]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2831 | Steps: 4 | Val loss: 61.1454 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.9780 | Steps: 4 | Val loss: 1.7934 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4730 | Steps: 4 | Val loss: 2.1227 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:35:59 (running for 00:02:24.86)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.594 |      0.219 |                   16 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.509 |      0.276 |                   16 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.283 |      0.012 |                   17 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.016 |      0.036 |                   15 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.02332089552238806
[2m[36m(func pid=13861)[0m top5: 0.5055970149253731
[2m[36m(func pid=13861)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=13861)[0m f1_macro: 0.011917694063073137
[2m[36m(func pid=13861)[0m f1_weighted: 0.016839007427465344
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.025]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3376865671641791
[2m[36m(func pid=13435)[0m top5: 0.8917910447761194
[2m[36m(func pid=13435)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=13435)[0m f1_macro: 0.3498437178171804
[2m[36m(func pid=13435)[0m f1_weighted: 0.3537260783163131
[2m[36m(func pid=13435)[0m f1_per_class: [0.521, 0.449, 0.571, 0.431, 0.072, 0.377, 0.225, 0.347, 0.264, 0.243]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8460 | Steps: 4 | Val loss: 18.5121 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=13050)[0m top1: 0.28078358208955223
[2m[36m(func pid=13050)[0m top5: 0.7299440298507462
[2m[36m(func pid=13050)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=13050)[0m f1_macro: 0.21405708834195952
[2m[36m(func pid=13050)[0m f1_weighted: 0.2317158346652119
[2m[36m(func pid=13050)[0m f1_per_class: [0.234, 0.057, 0.436, 0.532, 0.066, 0.259, 0.056, 0.28, 0.026, 0.195]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m top1: 0.03544776119402985
[2m[36m(func pid=14288)[0m top5: 0.6338619402985075
[2m[36m(func pid=14288)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=14288)[0m f1_macro: 0.025002090416509304
[2m[36m(func pid=14288)[0m f1_weighted: 0.01864661631182271
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.031, 0.0, 0.0, 0.0, 0.016, 0.012, 0.102, 0.046, 0.043]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2845 | Steps: 4 | Val loss: 48.9502 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.0093 | Steps: 4 | Val loss: 1.7363 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.3813 | Steps: 4 | Val loss: 2.1047 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:36:04 (running for 00:02:30.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.473 |      0.214 |                   17 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.978 |      0.35  |                   17 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.284 |      0.013 |                   18 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.846 |      0.025 |                   16 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.40951492537313433
[2m[36m(func pid=13435)[0m top5: 0.875
[2m[36m(func pid=13435)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=13435)[0m f1_macro: 0.3446385380517221
[2m[36m(func pid=13435)[0m f1_weighted: 0.44483339407261074
[2m[36m(func pid=13435)[0m f1_per_class: [0.506, 0.486, 0.375, 0.468, 0.084, 0.362, 0.522, 0.16, 0.258, 0.225]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.020522388059701493
[2m[36m(func pid=13861)[0m top5: 0.59375
[2m[36m(func pid=13861)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=13861)[0m f1_macro: 0.013313660942874861
[2m[36m(func pid=13861)[0m f1_weighted: 0.010021859140978872
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.048, 0.027]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.27845149253731344
[2m[36m(func pid=13050)[0m top5: 0.7509328358208955
[2m[36m(func pid=13050)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=13050)[0m f1_macro: 0.21554122151525518
[2m[36m(func pid=13050)[0m f1_weighted: 0.22506312123722527
[2m[36m(func pid=13050)[0m f1_per_class: [0.264, 0.067, 0.453, 0.529, 0.073, 0.231, 0.036, 0.297, 0.027, 0.178]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.1061 | Steps: 4 | Val loss: 11.1489 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=14288)[0m top1: 0.11940298507462686
[2m[36m(func pid=14288)[0m top5: 0.6394589552238806
[2m[36m(func pid=14288)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=14288)[0m f1_macro: 0.0710032361625277
[2m[36m(func pid=14288)[0m f1_weighted: 0.11017743959905482
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.005, 0.002, 0.013, 0.0, 0.0, 0.292, 0.277, 0.059, 0.062]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7655 | Steps: 4 | Val loss: 1.7219 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5027 | Steps: 4 | Val loss: 40.5219 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4190 | Steps: 4 | Val loss: 2.0935 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:36:10 (running for 00:02:35.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.381 |      0.216 |                   18 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.009 |      0.345 |                   18 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.503 |      0.076 |                   19 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.106 |      0.071 |                   17 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.427705223880597
[2m[36m(func pid=13435)[0m top5: 0.8773320895522388
[2m[36m(func pid=13435)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=13435)[0m f1_macro: 0.322491108767593
[2m[36m(func pid=13435)[0m f1_weighted: 0.4491636544999574
[2m[36m(func pid=13435)[0m f1_per_class: [0.5, 0.519, 0.276, 0.482, 0.129, 0.21, 0.578, 0.102, 0.242, 0.187]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.08908582089552239
[2m[36m(func pid=13861)[0m top5: 0.6380597014925373
[2m[36m(func pid=13861)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=13861)[0m f1_macro: 0.07647378735185621
[2m[36m(func pid=13861)[0m f1_weighted: 0.038681512355742784
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.104, 0.0, 0.013, 0.142, 0.008, 0.0, 0.177, 0.056, 0.265]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.27611940298507465
[2m[36m(func pid=13050)[0m top5: 0.7541977611940298
[2m[36m(func pid=13050)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=13050)[0m f1_macro: 0.20650160905403228
[2m[36m(func pid=13050)[0m f1_weighted: 0.21598020060307244
[2m[36m(func pid=13050)[0m f1_per_class: [0.283, 0.062, 0.414, 0.534, 0.068, 0.178, 0.024, 0.295, 0.027, 0.18]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.1471 | Steps: 4 | Val loss: 7.3351 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=14288)[0m top1: 0.18330223880597016
[2m[36m(func pid=14288)[0m top5: 0.6553171641791045
[2m[36m(func pid=14288)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=14288)[0m f1_macro: 0.07586690050945302
[2m[36m(func pid=14288)[0m f1_weighted: 0.1881726352630752
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.011, 0.005, 0.094, 0.0, 0.0, 0.528, 0.0, 0.055, 0.066]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8653 | Steps: 4 | Val loss: 1.6254 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.3651 | Steps: 4 | Val loss: 27.0599 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3437 | Steps: 4 | Val loss: 2.0747 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 14:36:15 (running for 00:02:41.08)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.419 |      0.207 |                   19 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.865 |      0.334 |                   20 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.503 |      0.076 |                   19 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.147 |      0.076 |                   18 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.4505597014925373
[2m[36m(func pid=13435)[0m top5: 0.9001865671641791
[2m[36m(func pid=13435)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=13435)[0m f1_macro: 0.33435680311750665
[2m[36m(func pid=13435)[0m f1_weighted: 0.46012589424534683
[2m[36m(func pid=13435)[0m f1_per_class: [0.5, 0.531, 0.282, 0.48, 0.131, 0.191, 0.609, 0.123, 0.264, 0.233]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.1310634328358209
[2m[36m(func pid=13861)[0m top5: 0.6576492537313433
[2m[36m(func pid=13861)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=13861)[0m f1_macro: 0.08926656338249857
[2m[36m(func pid=13861)[0m f1_weighted: 0.0782208604758483
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.321, 0.0, 0.0, 0.0, 0.016, 0.012, 0.217, 0.058, 0.269]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8561 | Steps: 4 | Val loss: 3.6653 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=13050)[0m top1: 0.28171641791044777
[2m[36m(func pid=13050)[0m top5: 0.7607276119402985
[2m[36m(func pid=13050)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=13050)[0m f1_macro: 0.20684735984373354
[2m[36m(func pid=13050)[0m f1_weighted: 0.21436193515057353
[2m[36m(func pid=13050)[0m f1_per_class: [0.29, 0.048, 0.4, 0.54, 0.068, 0.184, 0.015, 0.313, 0.027, 0.184]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.0719 | Steps: 4 | Val loss: 1.7852 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=14288)[0m top1: 0.17957089552238806
[2m[36m(func pid=14288)[0m top5: 0.7845149253731343
[2m[36m(func pid=14288)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=14288)[0m f1_macro: 0.11995695543563707
[2m[36m(func pid=14288)[0m f1_weighted: 0.1756816245311376
[2m[36m(func pid=14288)[0m f1_per_class: [0.043, 0.005, 0.096, 0.267, 0.108, 0.0, 0.267, 0.268, 0.059, 0.086]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.5814 | Steps: 4 | Val loss: 23.2270 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3659 | Steps: 4 | Val loss: 2.0427 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=13435)[0m top1: 0.4281716417910448
[2m[36m(func pid=13435)[0m top5: 0.9011194029850746
[2m[36m(func pid=13435)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=13435)[0m f1_macro: 0.31878462943216185
[2m[36m(func pid=13435)[0m f1_weighted: 0.4375373156726861
[2m[36m(func pid=13435)[0m f1_per_class: [0.488, 0.538, 0.381, 0.443, 0.149, 0.165, 0.598, 0.014, 0.244, 0.169]
[2m[36m(func pid=13435)[0m 
== Status ==
Current time: 2024-01-07 14:36:20 (running for 00:02:46.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.344 |      0.207 |                   20 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.072 |      0.319 |                   21 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  3.365 |      0.089 |                   20 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.856 |      0.12  |                   19 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.15298507462686567
[2m[36m(func pid=13861)[0m top5: 0.605410447761194
[2m[36m(func pid=13861)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=13861)[0m f1_macro: 0.09207155555312954
[2m[36m(func pid=13861)[0m f1_weighted: 0.09892679906573619
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.285, 0.0, 0.0, 0.0, 0.073, 0.071, 0.286, 0.064, 0.141]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.2873134328358209
[2m[36m(func pid=13050)[0m top5: 0.7784514925373134
[2m[36m(func pid=13050)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=13050)[0m f1_macro: 0.2104445820421686
[2m[36m(func pid=13050)[0m f1_weighted: 0.21540404321081316
[2m[36m(func pid=13050)[0m f1_per_class: [0.333, 0.04, 0.429, 0.556, 0.072, 0.161, 0.018, 0.287, 0.027, 0.18]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.3030 | Steps: 4 | Val loss: 4.7585 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=14288)[0m top1: 0.15438432835820895
[2m[36m(func pid=14288)[0m top5: 0.8152985074626866
[2m[36m(func pid=14288)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=14288)[0m f1_macro: 0.09187315281578713
[2m[36m(func pid=14288)[0m f1_weighted: 0.1357559931507102
[2m[36m(func pid=14288)[0m f1_per_class: [0.093, 0.223, 0.0, 0.298, 0.0, 0.0, 0.0, 0.185, 0.019, 0.101]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8955 | Steps: 4 | Val loss: 1.8977 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6358 | Steps: 4 | Val loss: 19.8077 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3719 | Steps: 4 | Val loss: 2.0283 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=13861)[0m top1: 0.11380597014925373
[2m[36m(func pid=13861)[0m top5: 0.5750932835820896
[2m[36m(func pid=13861)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=13861)[0m f1_macro: 0.08651928925221335
[2m[36m(func pid=13861)[0m f1_weighted: 0.10325838591258732
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.207, 0.075, 0.0, 0.0, 0.189, 0.107, 0.187, 0.071, 0.028]
== Status ==
Current time: 2024-01-07 14:36:26 (running for 00:02:51.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.366 |      0.21  |                   21 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.072 |      0.319 |                   21 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.636 |      0.087 |                   22 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.303 |      0.092 |                   20 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.396455223880597
[2m[36m(func pid=13435)[0m top5: 0.8903917910447762
[2m[36m(func pid=13435)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=13435)[0m f1_macro: 0.2941396579594594
[2m[36m(func pid=13435)[0m f1_weighted: 0.41569962496654045
[2m[36m(func pid=13435)[0m f1_per_class: [0.337, 0.547, 0.261, 0.368, 0.105, 0.297, 0.548, 0.042, 0.223, 0.214]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6726 | Steps: 4 | Val loss: 5.0901 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=13050)[0m top1: 0.30223880597014924
[2m[36m(func pid=13050)[0m top5: 0.769589552238806
[2m[36m(func pid=13050)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=13050)[0m f1_macro: 0.2218978828818828
[2m[36m(func pid=13050)[0m f1_weighted: 0.2219142608637578
[2m[36m(func pid=13050)[0m f1_per_class: [0.324, 0.05, 0.471, 0.561, 0.081, 0.184, 0.015, 0.312, 0.027, 0.194]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5062 | Steps: 4 | Val loss: 1.9704 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8335 | Steps: 4 | Val loss: 34.1270 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=14288)[0m top1: 0.18703358208955223
[2m[36m(func pid=14288)[0m top5: 0.75
[2m[36m(func pid=14288)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=14288)[0m f1_macro: 0.10953079608842531
[2m[36m(func pid=14288)[0m f1_weighted: 0.16253960111350543
[2m[36m(func pid=14288)[0m f1_per_class: [0.076, 0.318, 0.0, 0.335, 0.1, 0.0, 0.0, 0.19, 0.0, 0.075]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2688 | Steps: 4 | Val loss: 2.0123 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:36:31 (running for 00:02:57.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.372 |      0.222 |                   22 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.506 |      0.321 |                   23 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.636 |      0.087 |                   22 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.673 |      0.11  |                   21 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.38199626865671643
[2m[36m(func pid=13435)[0m top5: 0.8894589552238806
[2m[36m(func pid=13435)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=13435)[0m f1_macro: 0.32071657299575895
[2m[36m(func pid=13435)[0m f1_weighted: 0.4040758663951433
[2m[36m(func pid=13435)[0m f1_per_class: [0.283, 0.508, 0.168, 0.287, 0.099, 0.406, 0.513, 0.284, 0.263, 0.396]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.14365671641791045
[2m[36m(func pid=13861)[0m top5: 0.5788246268656716
[2m[36m(func pid=13861)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=13861)[0m f1_macro: 0.08019965810660883
[2m[36m(func pid=13861)[0m f1_weighted: 0.16377773001919418
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.135, 0.0, 0.01, 0.032, 0.032, 0.422, 0.132, 0.0, 0.039]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.30363805970149255
[2m[36m(func pid=13050)[0m top5: 0.7700559701492538
[2m[36m(func pid=13050)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=13050)[0m f1_macro: 0.22315307497759357
[2m[36m(func pid=13050)[0m f1_weighted: 0.22050033521597515
[2m[36m(func pid=13050)[0m f1_per_class: [0.341, 0.068, 0.49, 0.564, 0.084, 0.161, 0.006, 0.306, 0.027, 0.185]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.9066 | Steps: 4 | Val loss: 5.7307 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6622 | Steps: 4 | Val loss: 1.9043 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.2733 | Steps: 4 | Val loss: 48.7465 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=14288)[0m top1: 0.1767723880597015
[2m[36m(func pid=14288)[0m top5: 0.7047574626865671
[2m[36m(func pid=14288)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=14288)[0m f1_macro: 0.09570882637218273
[2m[36m(func pid=14288)[0m f1_weighted: 0.1520209588805312
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.224, 0.0, 0.363, 0.101, 0.0, 0.0, 0.18, 0.0, 0.089]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.2715 | Steps: 4 | Val loss: 1.9940 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:36:36 (running for 00:03:02.20)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.269 |      0.223 |                   23 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.662 |      0.341 |                   24 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.833 |      0.08  |                   23 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.907 |      0.096 |                   22 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.3987873134328358
[2m[36m(func pid=13435)[0m top5: 0.9067164179104478
[2m[36m(func pid=13435)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=13435)[0m f1_macro: 0.3408532098925529
[2m[36m(func pid=13435)[0m f1_weighted: 0.4211564142140046
[2m[36m(func pid=13435)[0m f1_per_class: [0.299, 0.523, 0.147, 0.356, 0.097, 0.391, 0.468, 0.431, 0.317, 0.38]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.21315298507462688
[2m[36m(func pid=13861)[0m top5: 0.6198694029850746
[2m[36m(func pid=13861)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=13861)[0m f1_macro: 0.08954741972063367
[2m[36m(func pid=13861)[0m f1_weighted: 0.19982546163609072
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.142, 0.0, 0.01, 0.026, 0.0, 0.573, 0.0, 0.0, 0.145]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5878 | Steps: 4 | Val loss: 4.9777 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=13050)[0m top1: 0.3045708955223881
[2m[36m(func pid=13050)[0m top5: 0.7826492537313433
[2m[36m(func pid=13050)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=13050)[0m f1_macro: 0.22969684008320193
[2m[36m(func pid=13050)[0m f1_weighted: 0.2243197403567967
[2m[36m(func pid=13050)[0m f1_per_class: [0.4, 0.06, 0.5, 0.566, 0.089, 0.148, 0.022, 0.31, 0.027, 0.175]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3705 | Steps: 4 | Val loss: 1.7390 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8774 | Steps: 4 | Val loss: 56.6349 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=14288)[0m top1: 0.09235074626865672
[2m[36m(func pid=14288)[0m top5: 0.7518656716417911
[2m[36m(func pid=14288)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=14288)[0m f1_macro: 0.06141139400492614
[2m[36m(func pid=14288)[0m f1_weighted: 0.06297377177196792
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.125, 0.0, 0.095, 0.039, 0.0, 0.0, 0.193, 0.073, 0.088]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1811 | Steps: 4 | Val loss: 1.9712 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 14:36:41 (running for 00:03:07.55)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.271 |      0.23  |                   24 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.371 |      0.377 |                   25 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.273 |      0.09  |                   24 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.588 |      0.061 |                   23 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.45149253731343286
[2m[36m(func pid=13435)[0m top5: 0.9146455223880597
[2m[36m(func pid=13435)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=13435)[0m f1_macro: 0.37699778217566876
[2m[36m(func pid=13435)[0m f1_weighted: 0.46986610133561524
[2m[36m(func pid=13435)[0m f1_per_class: [0.377, 0.533, 0.197, 0.414, 0.123, 0.398, 0.554, 0.448, 0.365, 0.36]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.3306902985074627
[2m[36m(func pid=13861)[0m top5: 0.5918843283582089
[2m[36m(func pid=13861)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=13861)[0m f1_macro: 0.10976951148654857
[2m[36m(func pid=13861)[0m f1_weighted: 0.23439177120203605
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.371, 0.0, 0.01, 0.0, 0.0, 0.557, 0.0, 0.0, 0.16]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3064365671641791
[2m[36m(func pid=13050)[0m top5: 0.7915111940298507
[2m[36m(func pid=13050)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=13050)[0m f1_macro: 0.23375380281102545
[2m[36m(func pid=13050)[0m f1_weighted: 0.23203296745649146
[2m[36m(func pid=13050)[0m f1_per_class: [0.42, 0.097, 0.471, 0.567, 0.085, 0.153, 0.022, 0.315, 0.027, 0.181]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.9615 | Steps: 4 | Val loss: 4.2681 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.2262 | Steps: 4 | Val loss: 1.6386 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.8154 | Steps: 4 | Val loss: 63.4338 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14288)[0m top1: 0.0708955223880597
[2m[36m(func pid=14288)[0m top5: 0.7905783582089553
[2m[36m(func pid=14288)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=14288)[0m f1_macro: 0.045522324769659166
[2m[36m(func pid=14288)[0m f1_weighted: 0.028372324509465825
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.076, 0.009, 0.0, 0.0, 0.0, 0.0, 0.215, 0.045, 0.109]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1436 | Steps: 4 | Val loss: 1.9582 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:36:47 (running for 00:03:12.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.181 |      0.234 |                   25 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.226 |      0.43  |                   26 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.877 |      0.11  |                   25 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.962 |      0.046 |                   24 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.48134328358208955
[2m[36m(func pid=13435)[0m top5: 0.929570895522388
[2m[36m(func pid=13435)[0m f1_micro: 0.48134328358208955
[2m[36m(func pid=13435)[0m f1_macro: 0.4299278131587295
[2m[36m(func pid=13435)[0m f1_weighted: 0.49172603537765586
[2m[36m(func pid=13435)[0m f1_per_class: [0.63, 0.503, 0.462, 0.436, 0.118, 0.368, 0.616, 0.423, 0.373, 0.37]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.3185634328358209
[2m[36m(func pid=13861)[0m top5: 0.6996268656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=13861)[0m f1_macro: 0.10073727258555618
[2m[36m(func pid=13861)[0m f1_weighted: 0.22470694875815925
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.37, 0.0, 0.013, 0.0, 0.0, 0.524, 0.0, 0.0, 0.1]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3041044776119403
[2m[36m(func pid=13050)[0m top5: 0.8036380597014925
[2m[36m(func pid=13050)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=13050)[0m f1_macro: 0.24468769499249254
[2m[36m(func pid=13050)[0m f1_weighted: 0.23484209558038419
[2m[36m(func pid=13050)[0m f1_per_class: [0.449, 0.1, 0.533, 0.564, 0.098, 0.155, 0.028, 0.302, 0.051, 0.167]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6073 | Steps: 4 | Val loss: 3.8189 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5094 | Steps: 4 | Val loss: 1.7175 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.9017 | Steps: 4 | Val loss: 30.4946 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14288)[0m top1: 0.06576492537313433
[2m[36m(func pid=14288)[0m top5: 0.7751865671641791
[2m[36m(func pid=14288)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=14288)[0m f1_macro: 0.04116635001556512
[2m[36m(func pid=14288)[0m f1_weighted: 0.02268461421083314
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.053, 0.027, 0.0, 0.0, 0.0, 0.0, 0.19, 0.037, 0.105]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0268 | Steps: 4 | Val loss: 1.9436 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:36:52 (running for 00:03:17.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.144 |      0.245 |                   26 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.509 |      0.403 |                   27 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  3.815 |      0.101 |                   26 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.607 |      0.041 |                   25 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.4566231343283582
[2m[36m(func pid=13435)[0m top5: 0.9286380597014925
[2m[36m(func pid=13435)[0m f1_micro: 0.4566231343283582
[2m[36m(func pid=13435)[0m f1_macro: 0.4034753411765181
[2m[36m(func pid=13435)[0m f1_weighted: 0.47410359318415474
[2m[36m(func pid=13435)[0m f1_per_class: [0.602, 0.484, 0.343, 0.443, 0.136, 0.267, 0.614, 0.369, 0.372, 0.405]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.30923507462686567
[2m[36m(func pid=13861)[0m top5: 0.726679104477612
[2m[36m(func pid=13861)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=13861)[0m f1_macro: 0.08458682182424015
[2m[36m(func pid=13861)[0m f1_weighted: 0.21049346838644403
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.287, 0.0, 0.023, 0.0, 0.008, 0.513, 0.015, 0.0, 0.0]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3045708955223881
[2m[36m(func pid=13050)[0m top5: 0.8120335820895522
[2m[36m(func pid=13050)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=13050)[0m f1_macro: 0.25086096414381676
[2m[36m(func pid=13050)[0m f1_weighted: 0.24578285615552914
[2m[36m(func pid=13050)[0m f1_per_class: [0.509, 0.125, 0.49, 0.569, 0.086, 0.169, 0.037, 0.306, 0.05, 0.168]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4046 | Steps: 4 | Val loss: 3.2523 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6255 | Steps: 4 | Val loss: 2.0685 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2169 | Steps: 4 | Val loss: 21.2528 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=14288)[0m top1: 0.0625
[2m[36m(func pid=14288)[0m top5: 0.7882462686567164
[2m[36m(func pid=14288)[0m f1_micro: 0.0625
[2m[36m(func pid=14288)[0m f1_macro: 0.04580663089041365
[2m[36m(func pid=14288)[0m f1_weighted: 0.027967098511880346
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.058, 0.014, 0.0, 0.0, 0.039, 0.0, 0.18, 0.052, 0.115]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.1021 | Steps: 4 | Val loss: 1.9007 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:36:57 (running for 00:03:23.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.027 |      0.251 |                   27 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.625 |      0.324 |                   28 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.902 |      0.085 |                   27 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.405 |      0.046 |                   26 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.3712686567164179
[2m[36m(func pid=13435)[0m top5: 0.9006529850746269
[2m[36m(func pid=13435)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=13435)[0m f1_macro: 0.3243410618910189
[2m[36m(func pid=13435)[0m f1_weighted: 0.4103709760123131
[2m[36m(func pid=13435)[0m f1_per_class: [0.431, 0.436, 0.13, 0.482, 0.089, 0.353, 0.409, 0.272, 0.236, 0.404]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.1357276119402985
[2m[36m(func pid=13861)[0m top5: 0.7938432835820896
[2m[36m(func pid=13861)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=13861)[0m f1_macro: 0.12963379561595598
[2m[36m(func pid=13861)[0m f1_weighted: 0.17108100135992602
[2m[36m(func pid=13861)[0m f1_per_class: [0.122, 0.305, 0.312, 0.283, 0.041, 0.008, 0.099, 0.047, 0.036, 0.042]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.31576492537313433
[2m[36m(func pid=13050)[0m top5: 0.8218283582089553
[2m[36m(func pid=13050)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=13050)[0m f1_macro: 0.25224083589391627
[2m[36m(func pid=13050)[0m f1_weighted: 0.2590672025680364
[2m[36m(func pid=13050)[0m f1_per_class: [0.449, 0.173, 0.48, 0.569, 0.103, 0.176, 0.055, 0.311, 0.051, 0.157]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8176 | Steps: 4 | Val loss: 3.0148 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4572 | Steps: 4 | Val loss: 3.4711 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.9670 | Steps: 4 | Val loss: 39.1198 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=14288)[0m top1: 0.13013059701492538
[2m[36m(func pid=14288)[0m top5: 0.7635261194029851
[2m[36m(func pid=14288)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=14288)[0m f1_macro: 0.08094901444145594
[2m[36m(func pid=14288)[0m f1_weighted: 0.11048197590668722
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.037, 0.017, 0.0, 0.0, 0.102, 0.252, 0.25, 0.049, 0.103]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.1036 | Steps: 4 | Val loss: 1.8946 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13435)[0m top1: 0.2178171641791045
[2m[36m(func pid=13435)[0m top5: 0.7336753731343284
[2m[36m(func pid=13435)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=13435)[0m f1_macro: 0.19355614566303278
[2m[36m(func pid=13435)[0m f1_weighted: 0.22984773149072513
[2m[36m(func pid=13435)[0m f1_per_class: [0.406, 0.332, 0.079, 0.377, 0.074, 0.353, 0.036, 0.0, 0.18, 0.098]
[2m[36m(func pid=13435)[0m 
== Status ==
Current time: 2024-01-07 14:37:02 (running for 00:03:28.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.102 |      0.252 |                   28 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.457 |      0.194 |                   29 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.217 |      0.13  |                   28 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.818 |      0.081 |                   27 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.08442164179104478
[2m[36m(func pid=13861)[0m top5: 0.6506529850746269
[2m[36m(func pid=13861)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=13861)[0m f1_macro: 0.08221326053791603
[2m[36m(func pid=13861)[0m f1_weighted: 0.08494988616146452
[2m[36m(func pid=13861)[0m f1_per_class: [0.126, 0.312, 0.158, 0.069, 0.041, 0.054, 0.0, 0.0, 0.063, 0.0]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.32136194029850745
[2m[36m(func pid=13050)[0m top5: 0.8236940298507462
[2m[36m(func pid=13050)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=13050)[0m f1_macro: 0.25349745849394417
[2m[36m(func pid=13050)[0m f1_weighted: 0.2585421440236393
[2m[36m(func pid=13050)[0m f1_per_class: [0.455, 0.161, 0.436, 0.563, 0.095, 0.218, 0.043, 0.342, 0.049, 0.174]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4738 | Steps: 4 | Val loss: 2.4419 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6265 | Steps: 4 | Val loss: 3.4589 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.0219 | Steps: 4 | Val loss: 39.5965 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=14288)[0m top1: 0.2630597014925373
[2m[36m(func pid=14288)[0m top5: 0.8222947761194029
[2m[36m(func pid=14288)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=14288)[0m f1_macro: 0.11703477093114534
[2m[36m(func pid=14288)[0m f1_weighted: 0.2346743055497148
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.1, 0.03, 0.0, 0.0, 0.24, 0.625, 0.0, 0.076, 0.1]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9501 | Steps: 4 | Val loss: 1.8663 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:37:08 (running for 00:03:33.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2.104 |      0.253 |                   29 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.627 |      0.176 |                   30 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.967 |      0.082 |                   29 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.474 |      0.117 |                   28 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.18796641791044777
[2m[36m(func pid=13435)[0m top5: 0.7751865671641791
[2m[36m(func pid=13435)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=13435)[0m f1_macro: 0.17608208443066134
[2m[36m(func pid=13435)[0m f1_weighted: 0.21076647685198624
[2m[36m(func pid=13435)[0m f1_per_class: [0.355, 0.274, 0.06, 0.317, 0.063, 0.338, 0.072, 0.0, 0.171, 0.111]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.1455223880597015
[2m[36m(func pid=13861)[0m top5: 0.5960820895522388
[2m[36m(func pid=13861)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=13861)[0m f1_macro: 0.09698401153798654
[2m[36m(func pid=13861)[0m f1_weighted: 0.09164926162475678
[2m[36m(func pid=13861)[0m f1_per_class: [0.119, 0.429, 0.238, 0.0, 0.0, 0.102, 0.0, 0.0, 0.062, 0.021]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.33488805970149255
[2m[36m(func pid=13050)[0m top5: 0.832089552238806
[2m[36m(func pid=13050)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=13050)[0m f1_macro: 0.26429195904311154
[2m[36m(func pid=13050)[0m f1_weighted: 0.27724416624738296
[2m[36m(func pid=13050)[0m f1_per_class: [0.475, 0.201, 0.407, 0.576, 0.124, 0.253, 0.057, 0.334, 0.047, 0.168]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6596 | Steps: 4 | Val loss: 2.6090 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5862 | Steps: 4 | Val loss: 3.0005 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8921 | Steps: 4 | Val loss: 21.7246 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=14288)[0m top1: 0.2667910447761194
[2m[36m(func pid=14288)[0m top5: 0.8036380597014925
[2m[36m(func pid=14288)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=14288)[0m f1_macro: 0.10757124197987655
[2m[36m(func pid=14288)[0m f1_weighted: 0.21423177256493908
[2m[36m(func pid=14288)[0m f1_per_class: [0.041, 0.021, 0.035, 0.0, 0.033, 0.154, 0.631, 0.0, 0.081, 0.08]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.8809 | Steps: 4 | Val loss: 1.8494 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:37:13 (running for 00:03:38.96)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.95  |      0.264 |                   30 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.586 |      0.285 |                   31 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  3.022 |      0.097 |                   30 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.66  |      0.108 |                   29 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.2681902985074627
[2m[36m(func pid=13435)[0m top5: 0.8586753731343284
[2m[36m(func pid=13435)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=13435)[0m f1_macro: 0.2847566275452999
[2m[36m(func pid=13435)[0m f1_weighted: 0.32453777020385854
[2m[36m(func pid=13435)[0m f1_per_class: [0.439, 0.32, 0.086, 0.233, 0.042, 0.194, 0.459, 0.373, 0.28, 0.422]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.10587686567164178
[2m[36m(func pid=13861)[0m top5: 0.8138992537313433
[2m[36m(func pid=13861)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=13861)[0m f1_macro: 0.09553106056583766
[2m[36m(func pid=13861)[0m f1_weighted: 0.14199259735521538
[2m[36m(func pid=13861)[0m f1_per_class: [0.101, 0.25, 0.0, 0.062, 0.0, 0.186, 0.164, 0.132, 0.043, 0.017]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6135 | Steps: 4 | Val loss: 3.2607 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=13050)[0m top1: 0.33488805970149255
[2m[36m(func pid=13050)[0m top5: 0.8404850746268657
[2m[36m(func pid=13050)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=13050)[0m f1_macro: 0.2674536826936632
[2m[36m(func pid=13050)[0m f1_weighted: 0.28097518669109356
[2m[36m(func pid=13050)[0m f1_per_class: [0.5, 0.208, 0.4, 0.581, 0.125, 0.264, 0.057, 0.326, 0.047, 0.166]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2919 | Steps: 4 | Val loss: 2.8743 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.8699 | Steps: 4 | Val loss: 18.6946 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=14288)[0m top1: 0.2947761194029851
[2m[36m(func pid=14288)[0m top5: 0.7597947761194029
[2m[36m(func pid=14288)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=14288)[0m f1_macro: 0.1362153139745106
[2m[36m(func pid=14288)[0m f1_weighted: 0.21612540415761625
[2m[36m(func pid=14288)[0m f1_per_class: [0.079, 0.0, 0.0, 0.016, 0.0, 0.055, 0.577, 0.502, 0.047, 0.086]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0004 | Steps: 4 | Val loss: 1.8567 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:37:18 (running for 00:03:44.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.881 |      0.267 |                   31 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.292 |      0.291 |                   32 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.892 |      0.096 |                   31 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.614 |      0.136 |                   30 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.29990671641791045
[2m[36m(func pid=13435)[0m top5: 0.8703358208955224
[2m[36m(func pid=13435)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=13435)[0m f1_macro: 0.2906168711227103
[2m[36m(func pid=13435)[0m f1_weighted: 0.34485984213226645
[2m[36m(func pid=13435)[0m f1_per_class: [0.51, 0.347, 0.086, 0.229, 0.049, 0.141, 0.528, 0.396, 0.294, 0.327]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.12080223880597014
[2m[36m(func pid=13861)[0m top5: 0.7765858208955224
[2m[36m(func pid=13861)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=13861)[0m f1_macro: 0.0846374540872637
[2m[36m(func pid=13861)[0m f1_weighted: 0.11247007364117785
[2m[36m(func pid=13861)[0m f1_per_class: [0.039, 0.066, 0.0, 0.02, 0.0, 0.008, 0.232, 0.375, 0.08, 0.026]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7157 | Steps: 4 | Val loss: 5.5872 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=13050)[0m top1: 0.33255597014925375
[2m[36m(func pid=13050)[0m top5: 0.8372201492537313
[2m[36m(func pid=13050)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=13050)[0m f1_macro: 0.2633574264225259
[2m[36m(func pid=13050)[0m f1_weighted: 0.2861897607352687
[2m[36m(func pid=13050)[0m f1_per_class: [0.475, 0.166, 0.312, 0.577, 0.11, 0.29, 0.085, 0.354, 0.109, 0.158]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8710 | Steps: 4 | Val loss: 2.6552 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.9311 | Steps: 4 | Val loss: 19.6099 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=14288)[0m top1: 0.11707089552238806
[2m[36m(func pid=14288)[0m top5: 0.7084888059701493
[2m[36m(func pid=14288)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=14288)[0m f1_macro: 0.06003200778119834
[2m[36m(func pid=14288)[0m f1_weighted: 0.06707167980959082
[2m[36m(func pid=14288)[0m f1_per_class: [0.097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.167, 0.231, 0.037, 0.069]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9322 | Steps: 4 | Val loss: 1.8384 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:37:24 (running for 00:03:49.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  2     |      0.263 |                   32 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.871 |      0.286 |                   33 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.87  |      0.085 |                   32 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.716 |      0.06  |                   31 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.33255597014925375
[2m[36m(func pid=13435)[0m top5: 0.8675373134328358
[2m[36m(func pid=13435)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=13435)[0m f1_macro: 0.28649188761288996
[2m[36m(func pid=13435)[0m f1_weighted: 0.36059635989009625
[2m[36m(func pid=13435)[0m f1_per_class: [0.41, 0.378, 0.103, 0.226, 0.073, 0.164, 0.554, 0.481, 0.27, 0.207]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.11567164179104478
[2m[36m(func pid=13861)[0m top5: 0.6791044776119403
[2m[36m(func pid=13861)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=13861)[0m f1_macro: 0.07850492041877634
[2m[36m(func pid=13861)[0m f1_weighted: 0.10354591735685673
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.076, 0.0, 0.0, 0.0, 0.0, 0.22, 0.379, 0.083, 0.028]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3302238805970149
[2m[36m(func pid=13050)[0m top5: 0.8470149253731343
[2m[36m(func pid=13050)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=13050)[0m f1_macro: 0.2732202359607196
[2m[36m(func pid=13050)[0m f1_weighted: 0.29333934131866174
[2m[36m(func pid=13050)[0m f1_per_class: [0.524, 0.194, 0.353, 0.573, 0.107, 0.274, 0.101, 0.342, 0.105, 0.159]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.4656 | Steps: 4 | Val loss: 7.2005 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2991 | Steps: 4 | Val loss: 3.5118 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5580 | Steps: 4 | Val loss: 13.6225 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=14288)[0m top1: 0.08255597014925373
[2m[36m(func pid=14288)[0m top5: 0.7644589552238806
[2m[36m(func pid=14288)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=14288)[0m f1_macro: 0.0517176837345024
[2m[36m(func pid=14288)[0m f1_weighted: 0.025894110855618047
[2m[36m(func pid=14288)[0m f1_per_class: [0.086, 0.0, 0.0, 0.0, 0.053, 0.0, 0.027, 0.214, 0.085, 0.053]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.7487 | Steps: 4 | Val loss: 1.8261 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:37:29 (running for 00:03:55.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.932 |      0.273 |                   33 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.299 |      0.248 |                   34 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.931 |      0.079 |                   33 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.466 |      0.052 |                   32 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.2621268656716418
[2m[36m(func pid=13435)[0m top5: 0.784981343283582
[2m[36m(func pid=13435)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=13435)[0m f1_macro: 0.24802308118341826
[2m[36m(func pid=13435)[0m f1_weighted: 0.2934576917361808
[2m[36m(func pid=13435)[0m f1_per_class: [0.345, 0.322, 0.316, 0.14, 0.081, 0.221, 0.469, 0.264, 0.261, 0.063]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.11707089552238806
[2m[36m(func pid=13861)[0m top5: 0.6571828358208955
[2m[36m(func pid=13861)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=13861)[0m f1_macro: 0.0779780444741058
[2m[36m(func pid=13861)[0m f1_weighted: 0.0969474714179112
[2m[36m(func pid=13861)[0m f1_per_class: [0.038, 0.109, 0.0, 0.0, 0.0, 0.0, 0.184, 0.345, 0.062, 0.041]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3358208955223881
[2m[36m(func pid=13050)[0m top5: 0.8512126865671642
[2m[36m(func pid=13050)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=13050)[0m f1_macro: 0.28948877100194453
[2m[36m(func pid=13050)[0m f1_weighted: 0.30591503958326405
[2m[36m(func pid=13050)[0m f1_per_class: [0.528, 0.182, 0.436, 0.573, 0.095, 0.295, 0.133, 0.364, 0.132, 0.157]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8982 | Steps: 4 | Val loss: 6.9253 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6077 | Steps: 4 | Val loss: 9.0806 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2996 | Steps: 4 | Val loss: 3.5157 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=14288)[0m top1: 0.10121268656716417
[2m[36m(func pid=14288)[0m top5: 0.8013059701492538
[2m[36m(func pid=14288)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=14288)[0m f1_macro: 0.06459761871976534
[2m[36m(func pid=14288)[0m f1_weighted: 0.056932655467490614
[2m[36m(func pid=14288)[0m f1_per_class: [0.081, 0.0, 0.0, 0.137, 0.054, 0.0, 0.0, 0.227, 0.085, 0.062]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7644 | Steps: 4 | Val loss: 1.8051 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=13435)[0m top1: 0.2933768656716418
[2m[36m(func pid=13435)[0m top5: 0.8264925373134329
[2m[36m(func pid=13435)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=13435)[0m f1_macro: 0.26833247208193994
[2m[36m(func pid=13435)[0m f1_weighted: 0.30676331662716255
[2m[36m(func pid=13435)[0m f1_per_class: [0.333, 0.257, 0.343, 0.124, 0.09, 0.244, 0.521, 0.447, 0.268, 0.057]
== Status ==
Current time: 2024-01-07 14:37:35 (running for 00:04:00.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.749 |      0.289 |                   34 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.3   |      0.268 |                   35 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.558 |      0.078 |                   34 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.898 |      0.065 |                   33 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.13712686567164178
[2m[36m(func pid=13861)[0m top5: 0.6711753731343284
[2m[36m(func pid=13861)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=13861)[0m f1_macro: 0.09342331997648358
[2m[36m(func pid=13861)[0m f1_weighted: 0.1178371345095027
[2m[36m(func pid=13861)[0m f1_per_class: [0.078, 0.185, 0.0, 0.0, 0.0, 0.0, 0.209, 0.329, 0.069, 0.065]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.34421641791044777
[2m[36m(func pid=13050)[0m top5: 0.8591417910447762
[2m[36m(func pid=13050)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=13050)[0m f1_macro: 0.2857002191744233
[2m[36m(func pid=13050)[0m f1_weighted: 0.32321728179245346
[2m[36m(func pid=13050)[0m f1_per_class: [0.5, 0.212, 0.364, 0.578, 0.097, 0.297, 0.174, 0.357, 0.124, 0.155]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.8235 | Steps: 4 | Val loss: 5.9188 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.2363 | Steps: 4 | Val loss: 2.8533 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.0710 | Steps: 4 | Val loss: 7.2945 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=14288)[0m top1: 0.24207089552238806
[2m[36m(func pid=14288)[0m top5: 0.8194962686567164
[2m[36m(func pid=14288)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=14288)[0m f1_macro: 0.12320099209403164
[2m[36m(func pid=14288)[0m f1_weighted: 0.18743770819269864
[2m[36m(func pid=14288)[0m f1_per_class: [0.08, 0.0, 0.0, 0.51, 0.056, 0.232, 0.0, 0.233, 0.077, 0.043]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7063 | Steps: 4 | Val loss: 1.7996 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:37:40 (running for 00:04:05.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.764 |      0.286 |                   35 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.3   |      0.268 |                   35 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.071 |      0.1   |                   36 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.824 |      0.123 |                   34 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.16324626865671643
[2m[36m(func pid=13861)[0m top5: 0.6856343283582089
[2m[36m(func pid=13861)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=13861)[0m f1_macro: 0.09959454161126256
[2m[36m(func pid=13861)[0m f1_weighted: 0.16862859454538512
[2m[36m(func pid=13861)[0m f1_per_class: [0.066, 0.212, 0.0, 0.023, 0.0, 0.0, 0.368, 0.203, 0.064, 0.059]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.31716417910447764
[2m[36m(func pid=13435)[0m top5: 0.8801305970149254
[2m[36m(func pid=13435)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=13435)[0m f1_macro: 0.24251957564176035
[2m[36m(func pid=13435)[0m f1_weighted: 0.3228819657305252
[2m[36m(func pid=13435)[0m f1_per_class: [0.264, 0.326, 0.219, 0.201, 0.115, 0.215, 0.515, 0.339, 0.115, 0.115]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3516791044776119
[2m[36m(func pid=13050)[0m top5: 0.8596082089552238
[2m[36m(func pid=13050)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=13050)[0m f1_macro: 0.30379945443672846
[2m[36m(func pid=13050)[0m f1_weighted: 0.33803690078360027
[2m[36m(func pid=13050)[0m f1_per_class: [0.517, 0.232, 0.429, 0.582, 0.092, 0.308, 0.195, 0.375, 0.155, 0.153]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7447 | Steps: 4 | Val loss: 3.8080 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.1699 | Steps: 4 | Val loss: 6.2310 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3344 | Steps: 4 | Val loss: 3.0549 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=14288)[0m top1: 0.28544776119402987
[2m[36m(func pid=14288)[0m top5: 0.8246268656716418
[2m[36m(func pid=14288)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=14288)[0m f1_macro: 0.12241269113136122
[2m[36m(func pid=14288)[0m f1_weighted: 0.20953996113409268
[2m[36m(func pid=14288)[0m f1_per_class: [0.078, 0.0, 0.0, 0.559, 0.034, 0.21, 0.036, 0.274, 0.033, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6319 | Steps: 4 | Val loss: 1.7898 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:37:45 (running for 00:04:11.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.706 |      0.304 |                   36 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.236 |      0.243 |                   36 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.17  |      0.114 |                   37 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.745 |      0.122 |                   35 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.20615671641791045
[2m[36m(func pid=13861)[0m top5: 0.6963619402985075
[2m[36m(func pid=13861)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=13861)[0m f1_macro: 0.1136198170028396
[2m[36m(func pid=13861)[0m f1_weighted: 0.22411536416715694
[2m[36m(func pid=13861)[0m f1_per_class: [0.06, 0.109, 0.065, 0.118, 0.0, 0.008, 0.538, 0.111, 0.076, 0.052]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.29617537313432835
[2m[36m(func pid=13435)[0m top5: 0.9071828358208955
[2m[36m(func pid=13435)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=13435)[0m f1_macro: 0.22986043485618568
[2m[36m(func pid=13435)[0m f1_weighted: 0.3119861789810751
[2m[36m(func pid=13435)[0m f1_per_class: [0.295, 0.283, 0.126, 0.357, 0.12, 0.062, 0.421, 0.271, 0.155, 0.208]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3596082089552239
[2m[36m(func pid=13050)[0m top5: 0.8591417910447762
[2m[36m(func pid=13050)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=13050)[0m f1_macro: 0.30156263052993604
[2m[36m(func pid=13050)[0m f1_weighted: 0.35045991324305636
[2m[36m(func pid=13050)[0m f1_per_class: [0.472, 0.288, 0.369, 0.578, 0.098, 0.341, 0.199, 0.389, 0.132, 0.149]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.8396 | Steps: 4 | Val loss: 3.3212 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7115 | Steps: 4 | Val loss: 3.4089 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7021 | Steps: 4 | Val loss: 6.7171 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=14288)[0m top1: 0.30363805970149255
[2m[36m(func pid=14288)[0m top5: 0.851679104477612
[2m[36m(func pid=14288)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=14288)[0m f1_macro: 0.13060797221498474
[2m[36m(func pid=14288)[0m f1_weighted: 0.23499350457758114
[2m[36m(func pid=14288)[0m f1_per_class: [0.03, 0.0, 0.0, 0.562, 0.039, 0.212, 0.112, 0.328, 0.023, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6024 | Steps: 4 | Val loss: 1.7844 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:37:50 (running for 00:04:16.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.632 |      0.302 |                   37 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.334 |      0.23  |                   37 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.702 |      0.12  |                   38 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.84  |      0.131 |                   36 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.2756529850746269
[2m[36m(func pid=13435)[0m top5: 0.8908582089552238
[2m[36m(func pid=13435)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=13435)[0m f1_macro: 0.23721047782188717
[2m[36m(func pid=13435)[0m f1_weighted: 0.3023919262870152
[2m[36m(func pid=13435)[0m f1_per_class: [0.347, 0.29, 0.059, 0.321, 0.115, 0.047, 0.408, 0.293, 0.218, 0.273]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.2392723880597015
[2m[36m(func pid=13861)[0m top5: 0.6772388059701493
[2m[36m(func pid=13861)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=13861)[0m f1_macro: 0.1200750642003638
[2m[36m(func pid=13861)[0m f1_weighted: 0.25002795797675575
[2m[36m(func pid=13861)[0m f1_per_class: [0.073, 0.058, 0.061, 0.193, 0.026, 0.024, 0.584, 0.111, 0.019, 0.053]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.37546641791044777
[2m[36m(func pid=13050)[0m top5: 0.8614738805970149
[2m[36m(func pid=13050)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=13050)[0m f1_macro: 0.3178553035919963
[2m[36m(func pid=13050)[0m f1_weighted: 0.3722272850275659
[2m[36m(func pid=13050)[0m f1_per_class: [0.422, 0.331, 0.358, 0.576, 0.101, 0.346, 0.234, 0.416, 0.23, 0.164]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8117 | Steps: 4 | Val loss: 3.1972 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6216 | Steps: 4 | Val loss: 3.6345 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5974 | Steps: 4 | Val loss: 6.8232 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14288)[0m top1: 0.32369402985074625
[2m[36m(func pid=14288)[0m top5: 0.8689365671641791
[2m[36m(func pid=14288)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=14288)[0m f1_macro: 0.1466743339904921
[2m[36m(func pid=14288)[0m f1_weighted: 0.27067709709189236
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.041, 0.0, 0.569, 0.067, 0.188, 0.208, 0.343, 0.024, 0.027]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6368 | Steps: 4 | Val loss: 1.7789 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:37:56 (running for 00:04:21.80)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.602 |      0.318 |                   38 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.712 |      0.237 |                   38 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.597 |      0.123 |                   39 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.812 |      0.147 |                   37 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.1982276119402985
[2m[36m(func pid=13861)[0m top5: 0.7723880597014925
[2m[36m(func pid=13861)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=13861)[0m f1_macro: 0.12330624307110494
[2m[36m(func pid=13861)[0m f1_weighted: 0.21460523912964174
[2m[36m(func pid=13861)[0m f1_per_class: [0.055, 0.059, 0.035, 0.204, 0.026, 0.164, 0.383, 0.212, 0.0, 0.096]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2392723880597015
[2m[36m(func pid=13435)[0m top5: 0.9015858208955224
[2m[36m(func pid=13435)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=13435)[0m f1_macro: 0.22882358954161708
[2m[36m(func pid=13435)[0m f1_weighted: 0.26383058130528514
[2m[36m(func pid=13435)[0m f1_per_class: [0.333, 0.351, 0.049, 0.25, 0.099, 0.032, 0.316, 0.285, 0.232, 0.341]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3596082089552239
[2m[36m(func pid=13050)[0m top5: 0.8600746268656716
[2m[36m(func pid=13050)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=13050)[0m f1_macro: 0.30981271308755
[2m[36m(func pid=13050)[0m f1_weighted: 0.37884006598052544
[2m[36m(func pid=13050)[0m f1_per_class: [0.403, 0.357, 0.273, 0.551, 0.068, 0.36, 0.262, 0.427, 0.214, 0.183]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7386 | Steps: 4 | Val loss: 3.4447 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.0614 | Steps: 4 | Val loss: 7.8373 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3353 | Steps: 4 | Val loss: 3.4701 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=14288)[0m top1: 0.33255597014925375
[2m[36m(func pid=14288)[0m top5: 0.8582089552238806
[2m[36m(func pid=14288)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=14288)[0m f1_macro: 0.18067716426414293
[2m[36m(func pid=14288)[0m f1_weighted: 0.29448659204927347
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.086, 0.216, 0.561, 0.062, 0.123, 0.284, 0.348, 0.052, 0.074]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5589 | Steps: 4 | Val loss: 1.7579 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:38:01 (running for 00:04:26.93)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.637 |      0.31  |                   39 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.622 |      0.229 |                   39 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.061 |      0.123 |                   40 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.739 |      0.181 |                   38 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.17490671641791045
[2m[36m(func pid=13861)[0m top5: 0.7915111940298507
[2m[36m(func pid=13861)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=13861)[0m f1_macro: 0.12342259934535682
[2m[36m(func pid=13861)[0m f1_weighted: 0.19085370915802033
[2m[36m(func pid=13861)[0m f1_per_class: [0.03, 0.067, 0.023, 0.201, 0.051, 0.28, 0.255, 0.213, 0.022, 0.092]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.27005597014925375
[2m[36m(func pid=13435)[0m top5: 0.8987873134328358
[2m[36m(func pid=13435)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=13435)[0m f1_macro: 0.2433457471534756
[2m[36m(func pid=13435)[0m f1_weighted: 0.29444774729258405
[2m[36m(func pid=13435)[0m f1_per_class: [0.19, 0.402, 0.055, 0.283, 0.096, 0.039, 0.351, 0.305, 0.293, 0.419]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.373134328358209
[2m[36m(func pid=13050)[0m top5: 0.8666044776119403
[2m[36m(func pid=13050)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=13050)[0m f1_macro: 0.3240276104725708
[2m[36m(func pid=13050)[0m f1_weighted: 0.3951614342625169
[2m[36m(func pid=13050)[0m f1_per_class: [0.441, 0.359, 0.27, 0.562, 0.065, 0.396, 0.286, 0.438, 0.215, 0.209]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.1788 | Steps: 4 | Val loss: 3.2299 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.9458 | Steps: 4 | Val loss: 8.0305 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3540 | Steps: 4 | Val loss: 3.3591 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=14288)[0m top1: 0.15391791044776118
[2m[36m(func pid=14288)[0m top5: 0.8376865671641791
[2m[36m(func pid=14288)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=14288)[0m f1_macro: 0.11493170440685149
[2m[36m(func pid=14288)[0m f1_weighted: 0.16579782495178438
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.12, 0.016, 0.043, 0.034, 0.164, 0.302, 0.378, 0.047, 0.046]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5974 | Steps: 4 | Val loss: 1.7329 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:38:06 (running for 00:04:32.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.559 |      0.324 |                   40 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.335 |      0.243 |                   40 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.946 |      0.158 |                   41 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.179 |      0.115 |                   39 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.22108208955223882
[2m[36m(func pid=13861)[0m top5: 0.738339552238806
[2m[36m(func pid=13861)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=13861)[0m f1_macro: 0.15808264160309426
[2m[36m(func pid=13861)[0m f1_weighted: 0.20561548392133314
[2m[36m(func pid=13861)[0m f1_per_class: [0.033, 0.313, 0.096, 0.125, 0.068, 0.367, 0.194, 0.225, 0.023, 0.137]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.29197761194029853
[2m[36m(func pid=13435)[0m top5: 0.8871268656716418
[2m[36m(func pid=13435)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=13435)[0m f1_macro: 0.24735752583195247
[2m[36m(func pid=13435)[0m f1_weighted: 0.32866676131450134
[2m[36m(func pid=13435)[0m f1_per_class: [0.182, 0.367, 0.056, 0.304, 0.072, 0.054, 0.469, 0.278, 0.282, 0.409]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.38992537313432835
[2m[36m(func pid=13050)[0m top5: 0.8689365671641791
[2m[36m(func pid=13050)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=13050)[0m f1_macro: 0.33477774286891826
[2m[36m(func pid=13050)[0m f1_weighted: 0.4142069328128616
[2m[36m(func pid=13050)[0m f1_per_class: [0.446, 0.407, 0.276, 0.563, 0.073, 0.399, 0.319, 0.441, 0.219, 0.205]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4509 | Steps: 4 | Val loss: 3.7448 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.7674 | Steps: 4 | Val loss: 7.0185 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9773 | Steps: 4 | Val loss: 3.0512 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=14288)[0m top1: 0.1417910447761194
[2m[36m(func pid=14288)[0m top5: 0.816231343283582
[2m[36m(func pid=14288)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=14288)[0m f1_macro: 0.1037378962330461
[2m[36m(func pid=14288)[0m f1_weighted: 0.13717542709345737
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.084, 0.013, 0.0, 0.0, 0.158, 0.27, 0.35, 0.092, 0.071]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.4683 | Steps: 4 | Val loss: 1.7195 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:38:11 (running for 00:04:37.18)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.597 |      0.335 |                   41 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.354 |      0.247 |                   41 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  3.767 |      0.165 |                   42 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.451 |      0.104 |                   40 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.25093283582089554
[2m[36m(func pid=13861)[0m top5: 0.7294776119402985
[2m[36m(func pid=13861)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=13861)[0m f1_macro: 0.16502959836704992
[2m[36m(func pid=13861)[0m f1_weighted: 0.20731697099458743
[2m[36m(func pid=13861)[0m f1_per_class: [0.0, 0.408, 0.136, 0.092, 0.051, 0.363, 0.17, 0.276, 0.022, 0.133]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3376865671641791
[2m[36m(func pid=13435)[0m top5: 0.8885261194029851
[2m[36m(func pid=13435)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=13435)[0m f1_macro: 0.26198473226562474
[2m[36m(func pid=13435)[0m f1_weighted: 0.35165616841704567
[2m[36m(func pid=13435)[0m f1_per_class: [0.238, 0.454, 0.115, 0.304, 0.09, 0.037, 0.514, 0.177, 0.316, 0.375]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.7875 | Steps: 4 | Val loss: 5.9989 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=13050)[0m top1: 0.39552238805970147
[2m[36m(func pid=13050)[0m top5: 0.8773320895522388
[2m[36m(func pid=13050)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=13050)[0m f1_macro: 0.34321908631842835
[2m[36m(func pid=13050)[0m f1_weighted: 0.4215399181733786
[2m[36m(func pid=13050)[0m f1_per_class: [0.462, 0.413, 0.312, 0.563, 0.075, 0.402, 0.336, 0.457, 0.201, 0.213]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 3.2797 | Steps: 4 | Val loss: 5.4691 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6883 | Steps: 4 | Val loss: 3.0943 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=14288)[0m top1: 0.10914179104477612
[2m[36m(func pid=14288)[0m top5: 0.7910447761194029
[2m[36m(func pid=14288)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=14288)[0m f1_macro: 0.07431946357123415
[2m[36m(func pid=14288)[0m f1_weighted: 0.08756698908431307
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.021, 0.003, 0.0, 0.0, 0.184, 0.154, 0.25, 0.048, 0.083]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.3913 | Steps: 4 | Val loss: 1.7249 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=13861)[0m top1: 0.19682835820895522
[2m[36m(func pid=13861)[0m top5: 0.7481343283582089
[2m[36m(func pid=13861)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=13861)[0m f1_macro: 0.1594473688220001
[2m[36m(func pid=13861)[0m f1_weighted: 0.18055551401459957
[2m[36m(func pid=13861)[0m f1_per_class: [0.067, 0.196, 0.164, 0.111, 0.107, 0.382, 0.168, 0.287, 0.044, 0.067]
[2m[36m(func pid=13861)[0m 
== Status ==
Current time: 2024-01-07 14:38:16 (running for 00:04:42.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.468 |      0.343 |                   42 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.977 |      0.262 |                   42 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  3.28  |      0.159 |                   43 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.788 |      0.074 |                   41 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.3283582089552239
[2m[36m(func pid=13435)[0m top5: 0.8819962686567164
[2m[36m(func pid=13435)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=13435)[0m f1_macro: 0.2641802137914577
[2m[36m(func pid=13435)[0m f1_weighted: 0.3369547348343496
[2m[36m(func pid=13435)[0m f1_per_class: [0.205, 0.431, 0.301, 0.311, 0.102, 0.044, 0.483, 0.14, 0.232, 0.392]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.39738805970149255
[2m[36m(func pid=13050)[0m top5: 0.871268656716418
[2m[36m(func pid=13050)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=13050)[0m f1_macro: 0.3556991233783596
[2m[36m(func pid=13050)[0m f1_weighted: 0.4263256771710585
[2m[36m(func pid=13050)[0m f1_per_class: [0.453, 0.433, 0.414, 0.56, 0.071, 0.381, 0.35, 0.431, 0.238, 0.226]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7598 | Steps: 4 | Val loss: 7.3115 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7280 | Steps: 4 | Val loss: 6.3162 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4043 | Steps: 4 | Val loss: 3.3384 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=14288)[0m top1: 0.09514925373134328
[2m[36m(func pid=14288)[0m top5: 0.7154850746268657
[2m[36m(func pid=14288)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=14288)[0m f1_macro: 0.05705245036084257
[2m[36m(func pid=14288)[0m f1_weighted: 0.062257437824145556
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.088, 0.123, 0.214, 0.069, 0.077]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:38:22 (running for 00:04:47.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.391 |      0.356 |                   43 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.688 |      0.264 |                   43 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.728 |      0.138 |                   44 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.76  |      0.057 |                   42 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.17117537313432835
[2m[36m(func pid=13861)[0m top5: 0.7308768656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=13861)[0m f1_macro: 0.1376750659288712
[2m[36m(func pid=13861)[0m f1_weighted: 0.1677520489215233
[2m[36m(func pid=13861)[0m f1_per_class: [0.059, 0.103, 0.131, 0.081, 0.042, 0.359, 0.221, 0.274, 0.052, 0.056]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.5424 | Steps: 4 | Val loss: 1.6998 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=13435)[0m top1: 0.29197761194029853
[2m[36m(func pid=13435)[0m top5: 0.8171641791044776
[2m[36m(func pid=13435)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=13435)[0m f1_macro: 0.22611017086672058
[2m[36m(func pid=13435)[0m f1_weighted: 0.31498320073264513
[2m[36m(func pid=13435)[0m f1_per_class: [0.188, 0.314, 0.209, 0.318, 0.12, 0.043, 0.48, 0.142, 0.223, 0.224]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7761 | Steps: 4 | Val loss: 5.1720 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=13050)[0m top1: 0.41324626865671643
[2m[36m(func pid=13050)[0m top5: 0.8740671641791045
[2m[36m(func pid=13050)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=13050)[0m f1_macro: 0.3631132547377373
[2m[36m(func pid=13050)[0m f1_weighted: 0.4414780470008198
[2m[36m(func pid=13050)[0m f1_per_class: [0.43, 0.43, 0.471, 0.568, 0.085, 0.399, 0.394, 0.423, 0.197, 0.235]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2554 | Steps: 4 | Val loss: 5.1469 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.2280 | Steps: 4 | Val loss: 3.5069 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=14288)[0m top1: 0.2621268656716418
[2m[36m(func pid=14288)[0m top5: 0.6520522388059702
[2m[36m(func pid=14288)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=14288)[0m f1_macro: 0.08057340587735265
[2m[36m(func pid=14288)[0m f1_weighted: 0.17244412796206415
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.068, 0.022, 0.556, 0.0, 0.082, 0.077]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:38:27 (running for 00:04:53.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.542 |      0.363 |                   44 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.404 |      0.226 |                   44 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.255 |      0.133 |                   45 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.776 |      0.081 |                   43 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5031 | Steps: 4 | Val loss: 1.6699 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13861)[0m top1: 0.16791044776119404
[2m[36m(func pid=13861)[0m top5: 0.7835820895522388
[2m[36m(func pid=13861)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=13861)[0m f1_macro: 0.13310831171805065
[2m[36m(func pid=13861)[0m f1_weighted: 0.17693827478494903
[2m[36m(func pid=13861)[0m f1_per_class: [0.076, 0.062, 0.07, 0.067, 0.038, 0.333, 0.304, 0.227, 0.093, 0.063]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.28218283582089554
[2m[36m(func pid=13435)[0m top5: 0.7873134328358209
[2m[36m(func pid=13435)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=13435)[0m f1_macro: 0.22107887952057412
[2m[36m(func pid=13435)[0m f1_weighted: 0.2959312759851743
[2m[36m(func pid=13435)[0m f1_per_class: [0.22, 0.222, 0.153, 0.269, 0.147, 0.07, 0.488, 0.226, 0.222, 0.194]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.42024253731343286
[2m[36m(func pid=13050)[0m top5: 0.8773320895522388
[2m[36m(func pid=13050)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=13050)[0m f1_macro: 0.37672104252549954
[2m[36m(func pid=13050)[0m f1_weighted: 0.4482485699721281
[2m[36m(func pid=13050)[0m f1_per_class: [0.471, 0.447, 0.49, 0.55, 0.087, 0.416, 0.407, 0.446, 0.217, 0.237]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7357 | Steps: 4 | Val loss: 3.9419 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.9448 | Steps: 4 | Val loss: 4.3620 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3765 | Steps: 4 | Val loss: 3.1859 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=14288)[0m top1: 0.13759328358208955
[2m[36m(func pid=14288)[0m top5: 0.6907649253731343
[2m[36m(func pid=14288)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=14288)[0m f1_macro: 0.07047073805431037
[2m[36m(func pid=14288)[0m f1_weighted: 0.07802331379651954
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.243, 0.018, 0.0, 0.028, 0.293, 0.0, 0.0, 0.046, 0.077]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:38:32 (running for 00:04:58.38)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.503 |      0.377 |                   45 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.228 |      0.221 |                   45 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.945 |      0.145 |                   46 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.736 |      0.07  |                   44 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.14132462686567165
[2m[36m(func pid=13861)[0m top5: 0.7887126865671642
[2m[36m(func pid=13861)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=13861)[0m f1_macro: 0.14539096158459355
[2m[36m(func pid=13861)[0m f1_weighted: 0.16625388846687686
[2m[36m(func pid=13861)[0m f1_per_class: [0.122, 0.222, 0.3, 0.075, 0.024, 0.245, 0.231, 0.035, 0.068, 0.133]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5555 | Steps: 4 | Val loss: 1.7081 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=13435)[0m top1: 0.3003731343283582
[2m[36m(func pid=13435)[0m top5: 0.8027052238805971
[2m[36m(func pid=13435)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=13435)[0m f1_macro: 0.22162676295341485
[2m[36m(func pid=13435)[0m f1_weighted: 0.31079679224115575
[2m[36m(func pid=13435)[0m f1_per_class: [0.246, 0.225, 0.034, 0.337, 0.161, 0.064, 0.462, 0.3, 0.217, 0.169]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.3931902985074627
[2m[36m(func pid=13050)[0m top5: 0.8703358208955224
[2m[36m(func pid=13050)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=13050)[0m f1_macro: 0.3285302648295425
[2m[36m(func pid=13050)[0m f1_weighted: 0.43664047523993105
[2m[36m(func pid=13050)[0m f1_per_class: [0.346, 0.457, 0.242, 0.509, 0.075, 0.386, 0.437, 0.406, 0.176, 0.25]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.6058 | Steps: 4 | Val loss: 4.0980 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.9095 | Steps: 4 | Val loss: 3.3836 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6057 | Steps: 4 | Val loss: 3.3021 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 14:38:37 (running for 00:05:03.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.555 |      0.329 |                   46 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.376 |      0.222 |                   46 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.606 |      0.173 |                   47 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.736 |      0.07  |                   44 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.21828358208955223
[2m[36m(func pid=13861)[0m top5: 0.7980410447761194
[2m[36m(func pid=13861)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=13861)[0m f1_macro: 0.1727614388053929
[2m[36m(func pid=13861)[0m f1_weighted: 0.21372836533774436
[2m[36m(func pid=13861)[0m f1_per_class: [0.103, 0.465, 0.353, 0.083, 0.0, 0.194, 0.26, 0.045, 0.069, 0.157]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.1044776119402985
[2m[36m(func pid=14288)[0m top5: 0.7112873134328358
[2m[36m(func pid=14288)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=14288)[0m f1_macro: 0.06715375221915655
[2m[36m(func pid=14288)[0m f1_weighted: 0.042776239416231904
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.021, 0.066, 0.0, 0.019, 0.299, 0.0, 0.0, 0.06, 0.207]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.3494 | Steps: 4 | Val loss: 1.7044 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=13435)[0m top1: 0.2971082089552239
[2m[36m(func pid=13435)[0m top5: 0.8166977611940298
[2m[36m(func pid=13435)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=13435)[0m f1_macro: 0.22311568345251204
[2m[36m(func pid=13435)[0m f1_weighted: 0.3043015492392794
[2m[36m(func pid=13435)[0m f1_per_class: [0.255, 0.234, 0.052, 0.345, 0.19, 0.05, 0.435, 0.309, 0.159, 0.203]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.0489 | Steps: 4 | Val loss: 3.8520 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=13050)[0m top1: 0.40158582089552236
[2m[36m(func pid=13050)[0m top5: 0.8722014925373134
[2m[36m(func pid=13050)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=13050)[0m f1_macro: 0.3345185759956893
[2m[36m(func pid=13050)[0m f1_weighted: 0.4457277110959145
[2m[36m(func pid=13050)[0m f1_per_class: [0.329, 0.481, 0.231, 0.491, 0.075, 0.396, 0.463, 0.412, 0.217, 0.25]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7208 | Steps: 4 | Val loss: 3.9633 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6992 | Steps: 4 | Val loss: 3.7630 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:38:43 (running for 00:05:08.88)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.349 |      0.335 |                   47 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.606 |      0.223 |                   47 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.049 |      0.196 |                   48 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.909 |      0.067 |                   45 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.2439365671641791
[2m[36m(func pid=13861)[0m top5: 0.816231343283582
[2m[36m(func pid=13861)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=13861)[0m f1_macro: 0.1960555318802712
[2m[36m(func pid=13861)[0m f1_weighted: 0.23422005375371682
[2m[36m(func pid=13861)[0m f1_per_class: [0.103, 0.461, 0.375, 0.074, 0.056, 0.207, 0.33, 0.025, 0.095, 0.235]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.07276119402985075
[2m[36m(func pid=14288)[0m top5: 0.6800373134328358
[2m[36m(func pid=14288)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=14288)[0m f1_macro: 0.06844657717981407
[2m[36m(func pid=14288)[0m f1_weighted: 0.022906035135305525
[2m[36m(func pid=14288)[0m f1_per_class: [0.122, 0.021, 0.206, 0.0, 0.018, 0.0, 0.0, 0.23, 0.049, 0.039]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7683 | Steps: 4 | Val loss: 1.6804 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=13435)[0m top1: 0.2574626865671642
[2m[36m(func pid=13435)[0m top5: 0.784981343283582
[2m[36m(func pid=13435)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=13435)[0m f1_macro: 0.20396938886780197
[2m[36m(func pid=13435)[0m f1_weighted: 0.2684873505097476
[2m[36m(func pid=13435)[0m f1_per_class: [0.167, 0.275, 0.0, 0.204, 0.19, 0.101, 0.406, 0.339, 0.136, 0.221]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.416044776119403
[2m[36m(func pid=13050)[0m top5: 0.8694029850746269
[2m[36m(func pid=13050)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=13050)[0m f1_macro: 0.3425946632474954
[2m[36m(func pid=13050)[0m f1_weighted: 0.4526378852116893
[2m[36m(func pid=13050)[0m f1_per_class: [0.324, 0.515, 0.25, 0.487, 0.079, 0.381, 0.476, 0.409, 0.209, 0.296]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.0669 | Steps: 4 | Val loss: 3.0110 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.9641 | Steps: 4 | Val loss: 5.8250 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9952 | Steps: 4 | Val loss: 3.7668 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 14:38:48 (running for 00:05:14.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.768 |      0.343 |                   48 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.699 |      0.204 |                   48 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.067 |      0.251 |                   49 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.721 |      0.068 |                   46 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.23507462686567165
[2m[36m(func pid=13861)[0m top5: 0.8143656716417911
[2m[36m(func pid=13861)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=13861)[0m f1_macro: 0.25098707115276764
[2m[36m(func pid=13861)[0m f1_weighted: 0.2415130339241863
[2m[36m(func pid=13861)[0m f1_per_class: [0.094, 0.445, 0.75, 0.156, 0.092, 0.221, 0.248, 0.129, 0.142, 0.231]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.08069029850746269
[2m[36m(func pid=14288)[0m top5: 0.6875
[2m[36m(func pid=14288)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=14288)[0m f1_macro: 0.0676320267231245
[2m[36m(func pid=14288)[0m f1_weighted: 0.017443059117570936
[2m[36m(func pid=14288)[0m f1_per_class: [0.09, 0.0, 0.24, 0.0, 0.011, 0.0, 0.0, 0.198, 0.047, 0.091]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.3981 | Steps: 4 | Val loss: 1.7052 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=13435)[0m top1: 0.2224813432835821
[2m[36m(func pid=13435)[0m top5: 0.7989738805970149
[2m[36m(func pid=13435)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=13435)[0m f1_macro: 0.21519676869777826
[2m[36m(func pid=13435)[0m f1_weighted: 0.22829271954572827
[2m[36m(func pid=13435)[0m f1_per_class: [0.127, 0.233, 0.333, 0.124, 0.204, 0.136, 0.355, 0.333, 0.156, 0.151]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.5559 | Steps: 4 | Val loss: 2.8304 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=13050)[0m top1: 0.416044776119403
[2m[36m(func pid=13050)[0m top5: 0.8610074626865671
[2m[36m(func pid=13050)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=13050)[0m f1_macro: 0.3366428250517138
[2m[36m(func pid=13050)[0m f1_weighted: 0.44779139214366653
[2m[36m(func pid=13050)[0m f1_per_class: [0.354, 0.519, 0.255, 0.495, 0.083, 0.331, 0.483, 0.316, 0.225, 0.305]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7719 | Steps: 4 | Val loss: 8.6375 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7300 | Steps: 4 | Val loss: 4.3198 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=13861)[0m top1: 0.21082089552238806
[2m[36m(func pid=13861)[0m top5: 0.804570895522388
[2m[36m(func pid=13861)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=13861)[0m f1_macro: 0.24054827421193242
[2m[36m(func pid=13861)[0m f1_weighted: 0.20566225315888656
[2m[36m(func pid=13861)[0m f1_per_class: [0.108, 0.349, 0.72, 0.296, 0.052, 0.276, 0.018, 0.205, 0.132, 0.25]
== Status ==
Current time: 2024-01-07 14:38:53 (running for 00:05:19.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.398 |      0.337 |                   49 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.995 |      0.215 |                   49 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.556 |      0.241 |                   50 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.964 |      0.068 |                   47 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.07649253731343283
[2m[36m(func pid=14288)[0m top5: 0.6828358208955224
[2m[36m(func pid=14288)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=14288)[0m f1_macro: 0.07514484416612852
[2m[36m(func pid=14288)[0m f1_weighted: 0.016541356017348362
[2m[36m(func pid=14288)[0m f1_per_class: [0.071, 0.0, 0.246, 0.0, 0.062, 0.0, 0.0, 0.166, 0.052, 0.154]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2534 | Steps: 4 | Val loss: 1.7011 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13435)[0m top1: 0.21082089552238806
[2m[36m(func pid=13435)[0m top5: 0.7644589552238806
[2m[36m(func pid=13435)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=13435)[0m f1_macro: 0.220247218384716
[2m[36m(func pid=13435)[0m f1_weighted: 0.22227281397538395
[2m[36m(func pid=13435)[0m f1_per_class: [0.136, 0.153, 0.4, 0.036, 0.308, 0.166, 0.451, 0.348, 0.121, 0.084]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.9722 | Steps: 4 | Val loss: 3.4771 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=13050)[0m top1: 0.42490671641791045
[2m[36m(func pid=13050)[0m top5: 0.8582089552238806
[2m[36m(func pid=13050)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=13050)[0m f1_macro: 0.3326235827274454
[2m[36m(func pid=13050)[0m f1_weighted: 0.4491372791423878
[2m[36m(func pid=13050)[0m f1_per_class: [0.363, 0.526, 0.27, 0.499, 0.1, 0.326, 0.501, 0.208, 0.225, 0.31]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.4791 | Steps: 4 | Val loss: 6.2838 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7436 | Steps: 4 | Val loss: 4.5912 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:38:59 (running for 00:05:24.72)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.253 |      0.333 |                   50 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.73  |      0.22  |                   50 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.972 |      0.203 |                   51 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.772 |      0.075 |                   48 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.18516791044776118
[2m[36m(func pid=13861)[0m top5: 0.8115671641791045
[2m[36m(func pid=13861)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=13861)[0m f1_macro: 0.20270528233896715
[2m[36m(func pid=13861)[0m f1_weighted: 0.16656705172023709
[2m[36m(func pid=13861)[0m f1_per_class: [0.12, 0.148, 0.426, 0.268, 0.053, 0.293, 0.0, 0.352, 0.133, 0.234]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.07929104477611941
[2m[36m(func pid=14288)[0m top5: 0.7080223880597015
[2m[36m(func pid=14288)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=14288)[0m f1_macro: 0.058942369770185556
[2m[36m(func pid=14288)[0m f1_weighted: 0.02012547559698691
[2m[36m(func pid=14288)[0m f1_per_class: [0.072, 0.0, 0.21, 0.017, 0.0, 0.0, 0.0, 0.175, 0.059, 0.057]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0888 | Steps: 4 | Val loss: 1.6787 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=13435)[0m top1: 0.19636194029850745
[2m[36m(func pid=13435)[0m top5: 0.7402052238805971
[2m[36m(func pid=13435)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=13435)[0m f1_macro: 0.22303089128372977
[2m[36m(func pid=13435)[0m f1_weighted: 0.20948693789729117
[2m[36m(func pid=13435)[0m f1_per_class: [0.214, 0.112, 0.621, 0.02, 0.274, 0.142, 0.478, 0.197, 0.112, 0.061]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6853 | Steps: 4 | Val loss: 3.7170 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=13050)[0m top1: 0.4314365671641791
[2m[36m(func pid=13050)[0m top5: 0.8703358208955224
[2m[36m(func pid=13050)[0m f1_micro: 0.4314365671641791
[2m[36m(func pid=13050)[0m f1_macro: 0.34073487098836236
[2m[36m(func pid=13050)[0m f1_weighted: 0.45176089072989534
[2m[36m(func pid=13050)[0m f1_per_class: [0.436, 0.536, 0.387, 0.51, 0.094, 0.316, 0.51, 0.118, 0.213, 0.288]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.1206 | Steps: 4 | Val loss: 2.9886 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5115 | Steps: 4 | Val loss: 4.1124 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 14:39:04 (running for 00:05:30.01)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.089 |      0.341 |                   51 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.744 |      0.223 |                   51 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.685 |      0.165 |                   52 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.479 |      0.059 |                   49 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.17817164179104478
[2m[36m(func pid=13861)[0m top5: 0.808768656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=13861)[0m f1_macro: 0.16464483272664726
[2m[36m(func pid=13861)[0m f1_weighted: 0.1550751519089162
[2m[36m(func pid=13861)[0m f1_per_class: [0.098, 0.04, 0.247, 0.321, 0.053, 0.249, 0.006, 0.321, 0.099, 0.21]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.06809701492537314
[2m[36m(func pid=14288)[0m top5: 0.7360074626865671
[2m[36m(func pid=14288)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=14288)[0m f1_macro: 0.050793185381746764
[2m[36m(func pid=14288)[0m f1_weighted: 0.058057115854447755
[2m[36m(func pid=14288)[0m f1_per_class: [0.067, 0.0, 0.169, 0.185, 0.0, 0.0, 0.0, 0.068, 0.0, 0.019]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1675 | Steps: 4 | Val loss: 1.6929 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=13435)[0m top1: 0.2513992537313433
[2m[36m(func pid=13435)[0m top5: 0.7434701492537313
[2m[36m(func pid=13435)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=13435)[0m f1_macro: 0.25200022190378546
[2m[36m(func pid=13435)[0m f1_weighted: 0.25598940392232417
[2m[36m(func pid=13435)[0m f1_per_class: [0.238, 0.257, 0.452, 0.039, 0.145, 0.289, 0.434, 0.401, 0.168, 0.098]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5712 | Steps: 4 | Val loss: 3.7980 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=13050)[0m top1: 0.42723880597014924
[2m[36m(func pid=13050)[0m top5: 0.867070895522388
[2m[36m(func pid=13050)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=13050)[0m f1_macro: 0.32793956226107907
[2m[36m(func pid=13050)[0m f1_weighted: 0.44923873905080036
[2m[36m(func pid=13050)[0m f1_per_class: [0.382, 0.527, 0.329, 0.502, 0.098, 0.329, 0.519, 0.09, 0.216, 0.288]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.1813 | Steps: 4 | Val loss: 2.0957 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.9332 | Steps: 4 | Val loss: 3.8722 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 14:39:09 (running for 00:05:35.21)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.167 |      0.328 |                   52 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.512 |      0.252 |                   52 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.571 |      0.161 |                   53 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.121 |      0.051 |                   50 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.20102611940298507
[2m[36m(func pid=13861)[0m top5: 0.8246268656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=13861)[0m f1_macro: 0.1612537479028878
[2m[36m(func pid=13861)[0m f1_weighted: 0.20357173797799624
[2m[36m(func pid=13861)[0m f1_per_class: [0.086, 0.062, 0.216, 0.332, 0.053, 0.094, 0.21, 0.345, 0.047, 0.167]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m top1: 0.39505597014925375
[2m[36m(func pid=14288)[0m top5: 0.7481343283582089
[2m[36m(func pid=14288)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=14288)[0m f1_macro: 0.15256204219206362
[2m[36m(func pid=14288)[0m f1_weighted: 0.3276110667126987
[2m[36m(func pid=14288)[0m f1_per_class: [0.028, 0.13, 0.292, 0.521, 0.0, 0.0, 0.528, 0.0, 0.0, 0.027]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.2939 | Steps: 4 | Val loss: 1.6868 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=13435)[0m top1: 0.259794776119403
[2m[36m(func pid=13435)[0m top5: 0.7742537313432836
[2m[36m(func pid=13435)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=13435)[0m f1_macro: 0.26187755283722847
[2m[36m(func pid=13435)[0m f1_weighted: 0.2687316314562007
[2m[36m(func pid=13435)[0m f1_per_class: [0.264, 0.326, 0.431, 0.042, 0.153, 0.321, 0.418, 0.398, 0.203, 0.063]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.0540 | Steps: 4 | Val loss: 3.7938 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=13050)[0m top1: 0.427705223880597
[2m[36m(func pid=13050)[0m top5: 0.871268656716418
[2m[36m(func pid=13050)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=13050)[0m f1_macro: 0.3283861364505454
[2m[36m(func pid=13050)[0m f1_weighted: 0.45625141310116807
[2m[36m(func pid=13050)[0m f1_per_class: [0.365, 0.521, 0.289, 0.506, 0.116, 0.347, 0.535, 0.102, 0.214, 0.29]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.9791 | Steps: 4 | Val loss: 1.9692 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.3254 | Steps: 4 | Val loss: 4.3828 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:39:14 (running for 00:05:40.32)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.294 |      0.328 |                   53 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.933 |      0.262 |                   53 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  3.054 |      0.197 |                   54 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.181 |      0.153 |                   51 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.25326492537313433
[2m[36m(func pid=13861)[0m top5: 0.8334888059701493
[2m[36m(func pid=13861)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=13861)[0m f1_macro: 0.19723230872303416
[2m[36m(func pid=13861)[0m f1_weighted: 0.2603704964309208
[2m[36m(func pid=13861)[0m f1_per_class: [0.06, 0.337, 0.296, 0.324, 0.071, 0.061, 0.255, 0.383, 0.052, 0.134]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.24440298507462688
[2m[36m(func pid=13435)[0m top5: 0.7527985074626866
[2m[36m(func pid=13435)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=13435)[0m f1_macro: 0.20883016751984723
[2m[36m(func pid=13435)[0m f1_weighted: 0.25727663146789576
[2m[36m(func pid=13435)[0m f1_per_class: [0.038, 0.365, 0.273, 0.023, 0.068, 0.301, 0.416, 0.315, 0.233, 0.056]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m top1: 0.4701492537313433
[2m[36m(func pid=14288)[0m top5: 0.8185634328358209
[2m[36m(func pid=14288)[0m f1_micro: 0.47014925373134325
[2m[36m(func pid=14288)[0m f1_macro: 0.18914707013581775
[2m[36m(func pid=14288)[0m f1_weighted: 0.38697065484162924
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.263, 0.32, 0.563, 0.0, 0.0, 0.606, 0.0, 0.022, 0.118]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.2860 | Steps: 4 | Val loss: 1.6815 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3171 | Steps: 4 | Val loss: 3.6203 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=13050)[0m top1: 0.41697761194029853
[2m[36m(func pid=13050)[0m top5: 0.8815298507462687
[2m[36m(func pid=13050)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=13050)[0m f1_macro: 0.3283651337207833
[2m[36m(func pid=13050)[0m f1_weighted: 0.4535331927097445
[2m[36m(func pid=13050)[0m f1_per_class: [0.328, 0.51, 0.22, 0.463, 0.089, 0.363, 0.549, 0.192, 0.236, 0.333]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9571 | Steps: 4 | Val loss: 4.1257 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.9626 | Steps: 4 | Val loss: 2.2023 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=13861)[0m top1: 0.35261194029850745
[2m[36m(func pid=13861)[0m top5: 0.84375
[2m[36m(func pid=13861)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=13861)[0m f1_macro: 0.23525091933359854
[2m[36m(func pid=13861)[0m f1_weighted: 0.3598342339209501
[2m[36m(func pid=13861)[0m f1_per_class: [0.055, 0.445, 0.324, 0.389, 0.055, 0.038, 0.466, 0.451, 0.02, 0.109]
[2m[36m(func pid=13861)[0m 
== Status ==
Current time: 2024-01-07 14:39:20 (running for 00:05:45.60)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.286 |      0.328 |                   54 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.325 |      0.209 |                   54 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.317 |      0.235 |                   55 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.979 |      0.189 |                   52 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.28544776119402987
[2m[36m(func pid=13435)[0m top5: 0.7887126865671642
[2m[36m(func pid=13435)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=13435)[0m f1_macro: 0.18436390542426948
[2m[36m(func pid=13435)[0m f1_weighted: 0.27728918427514465
[2m[36m(func pid=13435)[0m f1_per_class: [0.043, 0.344, 0.173, 0.05, 0.043, 0.273, 0.524, 0.127, 0.191, 0.077]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m top1: 0.4351679104477612
[2m[36m(func pid=14288)[0m top5: 0.8199626865671642
[2m[36m(func pid=14288)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=14288)[0m f1_macro: 0.15313759160377094
[2m[36m(func pid=14288)[0m f1_weighted: 0.37840404943775335
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.351, 0.0, 0.497, 0.0, 0.0, 0.596, 0.0, 0.037, 0.05]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3960 | Steps: 4 | Val loss: 1.6299 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5465 | Steps: 4 | Val loss: 3.1650 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=13050)[0m top1: 0.43330223880597013
[2m[36m(func pid=13050)[0m top5: 0.8908582089552238
[2m[36m(func pid=13050)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=13050)[0m f1_macro: 0.35765452626549277
[2m[36m(func pid=13050)[0m f1_weighted: 0.46777338156467996
[2m[36m(func pid=13050)[0m f1_per_class: [0.453, 0.519, 0.255, 0.493, 0.097, 0.386, 0.529, 0.273, 0.243, 0.328]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9651 | Steps: 4 | Val loss: 3.4922 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.9744 | Steps: 4 | Val loss: 2.5483 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:39:25 (running for 00:05:51.01)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.396 |      0.358 |                   55 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.957 |      0.184 |                   55 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.547 |      0.272 |                   56 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.963 |      0.153 |                   53 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3885261194029851
[2m[36m(func pid=13861)[0m top5: 0.855410447761194
[2m[36m(func pid=13861)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=13861)[0m f1_macro: 0.2715913756428402
[2m[36m(func pid=13861)[0m f1_weighted: 0.39561025792706683
[2m[36m(func pid=13861)[0m f1_per_class: [0.107, 0.433, 0.564, 0.423, 0.024, 0.068, 0.552, 0.382, 0.047, 0.116]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.34794776119402987
[2m[36m(func pid=13435)[0m top5: 0.8502798507462687
[2m[36m(func pid=13435)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=13435)[0m f1_macro: 0.18929882356386568
[2m[36m(func pid=13435)[0m f1_weighted: 0.306607833138091
[2m[36m(func pid=13435)[0m f1_per_class: [0.07, 0.37, 0.176, 0.087, 0.053, 0.274, 0.586, 0.09, 0.11, 0.076]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9608 | Steps: 4 | Val loss: 1.6095 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=14288)[0m top1: 0.29757462686567165
[2m[36m(func pid=14288)[0m top5: 0.7915111940298507
[2m[36m(func pid=14288)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=14288)[0m f1_macro: 0.1007364357758953
[2m[36m(func pid=14288)[0m f1_weighted: 0.23256789953462836
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.317, 0.0, 0.0, 0.0, 0.0, 0.588, 0.0, 0.072, 0.03]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.5673 | Steps: 4 | Val loss: 2.5798 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=13050)[0m top1: 0.43703358208955223
[2m[36m(func pid=13050)[0m top5: 0.8973880597014925
[2m[36m(func pid=13050)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=13050)[0m f1_macro: 0.365331490534747
[2m[36m(func pid=13050)[0m f1_weighted: 0.4717850595934312
[2m[36m(func pid=13050)[0m f1_per_class: [0.419, 0.509, 0.293, 0.502, 0.094, 0.412, 0.52, 0.327, 0.243, 0.333]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3412 | Steps: 4 | Val loss: 3.1666 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.4301 | Steps: 4 | Val loss: 2.7536 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:39:30 (running for 00:05:56.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.961 |      0.365 |                   56 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.965 |      0.189 |                   56 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.567 |      0.282 |                   57 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.974 |      0.101 |                   54 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.43236940298507465
[2m[36m(func pid=13861)[0m top5: 0.8638059701492538
[2m[36m(func pid=13861)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=13861)[0m f1_macro: 0.2821496023713363
[2m[36m(func pid=13861)[0m f1_weighted: 0.4242595420178568
[2m[36m(func pid=13861)[0m f1_per_class: [0.085, 0.42, 0.4, 0.473, 0.127, 0.102, 0.588, 0.446, 0.0, 0.18]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.365205223880597
[2m[36m(func pid=13435)[0m top5: 0.8661380597014925
[2m[36m(func pid=13435)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=13435)[0m f1_macro: 0.19825417057825492
[2m[36m(func pid=13435)[0m f1_weighted: 0.32088317858492954
[2m[36m(func pid=13435)[0m f1_per_class: [0.203, 0.376, 0.168, 0.221, 0.061, 0.072, 0.57, 0.117, 0.073, 0.121]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m top1: 0.09654850746268656
[2m[36m(func pid=14288)[0m top5: 0.6422574626865671
[2m[36m(func pid=14288)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=14288)[0m f1_macro: 0.05527310050936063
[2m[36m(func pid=14288)[0m f1_weighted: 0.05982342054136318
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.026, 0.2, 0.058, 0.048]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0595 | Steps: 4 | Val loss: 1.6116 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.8052 | Steps: 4 | Val loss: 2.1946 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.1435 | Steps: 4 | Val loss: 3.0818 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=13050)[0m top1: 0.43236940298507465
[2m[36m(func pid=13050)[0m top5: 0.8969216417910447
[2m[36m(func pid=13050)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=13050)[0m f1_macro: 0.3616097462093485
[2m[36m(func pid=13050)[0m f1_weighted: 0.46726818343391396
[2m[36m(func pid=13050)[0m f1_per_class: [0.371, 0.513, 0.279, 0.488, 0.097, 0.409, 0.517, 0.341, 0.249, 0.351]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.3944 | Steps: 4 | Val loss: 2.8317 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:39:36 (running for 00:06:01.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.059 |      0.362 |                   57 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.341 |      0.198 |                   57 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.805 |      0.282 |                   58 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.43  |      0.055 |                   55 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.4216417910447761
[2m[36m(func pid=13861)[0m top5: 0.8642723880597015
[2m[36m(func pid=13861)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=13861)[0m f1_macro: 0.28179101276160534
[2m[36m(func pid=13861)[0m f1_weighted: 0.41104603322477773
[2m[36m(func pid=13861)[0m f1_per_class: [0.084, 0.253, 0.4, 0.501, 0.137, 0.168, 0.587, 0.439, 0.0, 0.248]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3614738805970149
[2m[36m(func pid=13435)[0m top5: 0.871268656716418
[2m[36m(func pid=13435)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=13435)[0m f1_macro: 0.2261603115421825
[2m[36m(func pid=13435)[0m f1_weighted: 0.3396735154186892
[2m[36m(func pid=13435)[0m f1_per_class: [0.215, 0.368, 0.145, 0.3, 0.077, 0.038, 0.519, 0.425, 0.059, 0.116]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=14288)[0m top1: 0.14738805970149255
[2m[36m(func pid=14288)[0m top5: 0.6814365671641791
[2m[36m(func pid=14288)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=14288)[0m f1_macro: 0.085218061965953
[2m[36m(func pid=14288)[0m f1_weighted: 0.0901723925242093
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.293, 0.0, 0.0, 0.0, 0.212, 0.0, 0.225, 0.051, 0.072]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.1716 | Steps: 4 | Val loss: 1.6519 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0872 | Steps: 4 | Val loss: 2.3412 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=13050)[0m top1: 0.4207089552238806
[2m[36m(func pid=13050)[0m top5: 0.8913246268656716
[2m[36m(func pid=13050)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=13050)[0m f1_macro: 0.35286246557044526
[2m[36m(func pid=13050)[0m f1_weighted: 0.4598390053314822
[2m[36m(func pid=13050)[0m f1_per_class: [0.311, 0.521, 0.209, 0.464, 0.091, 0.407, 0.505, 0.382, 0.278, 0.362]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7737 | Steps: 4 | Val loss: 3.2614 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7078 | Steps: 4 | Val loss: 2.5362 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=13861)[0m top1: 0.4025186567164179
[2m[36m(func pid=13861)[0m top5: 0.8605410447761194
[2m[36m(func pid=13861)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=13861)[0m f1_macro: 0.28032471944459625
[2m[36m(func pid=13861)[0m f1_weighted: 0.3817602901164724
[2m[36m(func pid=13861)[0m f1_per_class: [0.122, 0.116, 0.316, 0.5, 0.135, 0.195, 0.546, 0.46, 0.027, 0.386]
[2m[36m(func pid=13861)[0m 
== Status ==
Current time: 2024-01-07 14:39:41 (running for 00:06:06.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.172 |      0.353 |                   58 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.144 |      0.226 |                   58 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.087 |      0.28  |                   59 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.394 |      0.085 |                   56 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.2826492537313433
[2m[36m(func pid=13435)[0m top5: 0.8768656716417911
[2m[36m(func pid=13435)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=13435)[0m f1_macro: 0.20781627835312827
[2m[36m(func pid=13435)[0m f1_weighted: 0.28348224137667827
[2m[36m(func pid=13435)[0m f1_per_class: [0.25, 0.395, 0.152, 0.365, 0.138, 0.052, 0.273, 0.288, 0.044, 0.122]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0997 | Steps: 4 | Val loss: 1.6015 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=14288)[0m top1: 0.14505597014925373
[2m[36m(func pid=14288)[0m top5: 0.6847014925373134
[2m[36m(func pid=14288)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=14288)[0m f1_macro: 0.07349741054157378
[2m[36m(func pid=14288)[0m f1_weighted: 0.09194931177443282
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.293, 0.03, 0.0, 0.0, 0.357, 0.0, 0.0, 0.0, 0.055]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2937 | Steps: 4 | Val loss: 2.4134 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=13050)[0m top1: 0.43330223880597013
[2m[36m(func pid=13050)[0m top5: 0.9043843283582089
[2m[36m(func pid=13050)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=13050)[0m f1_macro: 0.38101393600064515
[2m[36m(func pid=13050)[0m f1_weighted: 0.4674694355073006
[2m[36m(func pid=13050)[0m f1_per_class: [0.443, 0.516, 0.329, 0.487, 0.08, 0.425, 0.487, 0.421, 0.274, 0.348]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6212 | Steps: 4 | Val loss: 4.1731 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.9152 | Steps: 4 | Val loss: 2.9119 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:39:46 (running for 00:06:12.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.1   |      0.381 |                   59 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.774 |      0.208 |                   59 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.294 |      0.271 |                   60 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.708 |      0.073 |                   57 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3843283582089552
[2m[36m(func pid=13861)[0m top5: 0.8507462686567164
[2m[36m(func pid=13861)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=13861)[0m f1_macro: 0.270589543595517
[2m[36m(func pid=13861)[0m f1_weighted: 0.36654126344084303
[2m[36m(func pid=13861)[0m f1_per_class: [0.109, 0.026, 0.24, 0.488, 0.14, 0.333, 0.505, 0.477, 0.026, 0.361]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.24953358208955223
[2m[36m(func pid=13435)[0m top5: 0.8586753731343284
[2m[36m(func pid=13435)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=13435)[0m f1_macro: 0.1936174745232157
[2m[36m(func pid=13435)[0m f1_weighted: 0.2563539977085983
[2m[36m(func pid=13435)[0m f1_per_class: [0.209, 0.43, 0.069, 0.365, 0.133, 0.067, 0.162, 0.251, 0.068, 0.182]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.1721 | Steps: 4 | Val loss: 1.6036 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=14288)[0m top1: 0.052705223880597014
[2m[36m(func pid=14288)[0m top5: 0.6198694029850746
[2m[36m(func pid=14288)[0m f1_micro: 0.05270522388059702
[2m[36m(func pid=14288)[0m f1_macro: 0.04086320073928645
[2m[36m(func pid=14288)[0m f1_weighted: 0.041272892965721154
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.057, 0.028, 0.03, 0.043, 0.191, 0.0, 0.0, 0.0, 0.06]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8494 | Steps: 4 | Val loss: 2.5744 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=13050)[0m top1: 0.425839552238806
[2m[36m(func pid=13050)[0m top5: 0.9034514925373134
[2m[36m(func pid=13050)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=13050)[0m f1_macro: 0.3810631156382903
[2m[36m(func pid=13050)[0m f1_weighted: 0.45673803221255554
[2m[36m(func pid=13050)[0m f1_per_class: [0.437, 0.518, 0.338, 0.478, 0.085, 0.395, 0.464, 0.444, 0.27, 0.382]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8626 | Steps: 4 | Val loss: 4.2011 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.8418 | Steps: 4 | Val loss: 2.8876 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=13861)[0m top1: 0.31949626865671643
[2m[36m(func pid=13861)[0m top5: 0.8190298507462687
[2m[36m(func pid=13861)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=13861)[0m f1_macro: 0.2639494374778303
[2m[36m(func pid=13861)[0m f1_weighted: 0.298093706548019
[2m[36m(func pid=13861)[0m f1_per_class: [0.11, 0.026, 0.368, 0.478, 0.095, 0.383, 0.268, 0.43, 0.047, 0.434]
[2m[36m(func pid=13861)[0m 
== Status ==
Current time: 2024-01-07 14:39:51 (running for 00:06:17.50)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.172 |      0.381 |                   60 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.621 |      0.194 |                   60 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.849 |      0.264 |                   61 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.915 |      0.041 |                   58 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.2579291044776119
[2m[36m(func pid=13435)[0m top5: 0.8675373134328358
[2m[36m(func pid=13435)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=13435)[0m f1_macro: 0.21630484620729334
[2m[36m(func pid=13435)[0m f1_weighted: 0.26176728522711173
[2m[36m(func pid=13435)[0m f1_per_class: [0.249, 0.45, 0.013, 0.391, 0.333, 0.054, 0.135, 0.268, 0.116, 0.154]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.0991 | Steps: 4 | Val loss: 1.5544 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=14288)[0m top1: 0.051305970149253734
[2m[36m(func pid=14288)[0m top5: 0.5788246268656716
[2m[36m(func pid=14288)[0m f1_micro: 0.051305970149253734
[2m[36m(func pid=14288)[0m f1_macro: 0.03747957621082114
[2m[36m(func pid=14288)[0m f1_weighted: 0.04235979850288741
[2m[36m(func pid=14288)[0m f1_per_class: [0.081, 0.139, 0.0, 0.014, 0.031, 0.11, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.8564 | Steps: 4 | Val loss: 2.5084 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1964 | Steps: 4 | Val loss: 3.9209 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=13050)[0m top1: 0.43796641791044777
[2m[36m(func pid=13050)[0m top5: 0.9155783582089553
[2m[36m(func pid=13050)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=13050)[0m f1_macro: 0.4003604988242688
[2m[36m(func pid=13050)[0m f1_weighted: 0.4654818444686969
[2m[36m(func pid=13050)[0m f1_per_class: [0.508, 0.526, 0.414, 0.486, 0.079, 0.416, 0.465, 0.459, 0.27, 0.381]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.0551 | Steps: 4 | Val loss: 2.9835 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:39:57 (running for 00:06:22.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.099 |      0.4   |                   61 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.863 |      0.216 |                   61 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.856 |      0.246 |                   62 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.842 |      0.037 |                   59 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.27005597014925375
[2m[36m(func pid=13861)[0m top5: 0.8246268656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=13861)[0m f1_macro: 0.24627124109257775
[2m[36m(func pid=13861)[0m f1_weighted: 0.24206883695945308
[2m[36m(func pid=13861)[0m f1_per_class: [0.1, 0.055, 0.444, 0.431, 0.081, 0.358, 0.117, 0.423, 0.065, 0.387]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.28451492537313433
[2m[36m(func pid=13435)[0m top5: 0.8684701492537313
[2m[36m(func pid=13435)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=13435)[0m f1_macro: 0.2147463598291714
[2m[36m(func pid=13435)[0m f1_weighted: 0.2688471256325891
[2m[36m(func pid=13435)[0m f1_per_class: [0.246, 0.433, 0.014, 0.509, 0.267, 0.045, 0.06, 0.269, 0.143, 0.161]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1376 | Steps: 4 | Val loss: 1.5336 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=14288)[0m top1: 0.051305970149253734
[2m[36m(func pid=14288)[0m top5: 0.5965485074626866
[2m[36m(func pid=14288)[0m f1_micro: 0.051305970149253734
[2m[36m(func pid=14288)[0m f1_macro: 0.03998240599408083
[2m[36m(func pid=14288)[0m f1_weighted: 0.03956696538648665
[2m[36m(func pid=14288)[0m f1_per_class: [0.082, 0.166, 0.0, 0.0, 0.029, 0.075, 0.0, 0.0, 0.0, 0.048]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2527 | Steps: 4 | Val loss: 2.7273 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3337 | Steps: 4 | Val loss: 4.2708 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=13050)[0m top1: 0.44029850746268656
[2m[36m(func pid=13050)[0m top5: 0.9160447761194029
[2m[36m(func pid=13050)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=13050)[0m f1_macro: 0.40392243548812834
[2m[36m(func pid=13050)[0m f1_weighted: 0.4627536666623687
[2m[36m(func pid=13050)[0m f1_per_class: [0.517, 0.532, 0.421, 0.483, 0.086, 0.426, 0.448, 0.462, 0.281, 0.384]
[2m[36m(func pid=13050)[0m 
== Status ==
Current time: 2024-01-07 14:40:02 (running for 00:06:28.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.138 |      0.404 |                   62 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.196 |      0.215 |                   62 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.253 |      0.197 |                   63 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.055 |      0.04  |                   60 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.23180970149253732
[2m[36m(func pid=13861)[0m top5: 0.8125
[2m[36m(func pid=13861)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=13861)[0m f1_macro: 0.19687365363568793
[2m[36m(func pid=13861)[0m f1_weighted: 0.20590691102937247
[2m[36m(func pid=13861)[0m f1_per_class: [0.087, 0.09, 0.188, 0.38, 0.082, 0.328, 0.054, 0.375, 0.058, 0.327]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.9500 | Steps: 4 | Val loss: 2.8717 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=13435)[0m top1: 0.2574626865671642
[2m[36m(func pid=13435)[0m top5: 0.8250932835820896
[2m[36m(func pid=13435)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=13435)[0m f1_macro: 0.19109216857547168
[2m[36m(func pid=13435)[0m f1_weighted: 0.24181128821513367
[2m[36m(func pid=13435)[0m f1_per_class: [0.197, 0.427, 0.008, 0.482, 0.149, 0.014, 0.021, 0.239, 0.12, 0.254]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0702 | Steps: 4 | Val loss: 1.5375 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=14288)[0m top1: 0.09281716417910447
[2m[36m(func pid=14288)[0m top5: 0.6277985074626866
[2m[36m(func pid=14288)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=14288)[0m f1_macro: 0.06918581988256307
[2m[36m(func pid=14288)[0m f1_weighted: 0.11137679973986307
[2m[36m(func pid=14288)[0m f1_per_class: [0.09, 0.171, 0.0, 0.01, 0.032, 0.082, 0.224, 0.0, 0.0, 0.083]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1361 | Steps: 4 | Val loss: 2.7126 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=13050)[0m top1: 0.44636194029850745
[2m[36m(func pid=13050)[0m top5: 0.914179104477612
[2m[36m(func pid=13050)[0m f1_micro: 0.44636194029850745
[2m[36m(func pid=13050)[0m f1_macro: 0.4080415175494728
[2m[36m(func pid=13050)[0m f1_weighted: 0.4698688946736237
[2m[36m(func pid=13050)[0m f1_per_class: [0.526, 0.536, 0.4, 0.488, 0.087, 0.428, 0.464, 0.448, 0.295, 0.408]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6652 | Steps: 4 | Val loss: 4.3172 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:40:07 (running for 00:06:33.40)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.07  |      0.408 |                   63 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.334 |      0.191 |                   63 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.136 |      0.197 |                   64 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.95  |      0.069 |                   61 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.22388059701492538
[2m[36m(func pid=13861)[0m top5: 0.8157649253731343
[2m[36m(func pid=13861)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=13861)[0m f1_macro: 0.1974971778120464
[2m[36m(func pid=13861)[0m f1_weighted: 0.20881911910121861
[2m[36m(func pid=13861)[0m f1_per_class: [0.073, 0.158, 0.132, 0.354, 0.085, 0.346, 0.039, 0.357, 0.139, 0.29]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.8345 | Steps: 4 | Val loss: 2.4504 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=13435)[0m top1: 0.22574626865671643
[2m[36m(func pid=13435)[0m top5: 0.8180970149253731
[2m[36m(func pid=13435)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=13435)[0m f1_macro: 0.17376225452873367
[2m[36m(func pid=13435)[0m f1_weighted: 0.21965256293697144
[2m[36m(func pid=13435)[0m f1_per_class: [0.184, 0.349, 0.029, 0.418, 0.131, 0.026, 0.053, 0.239, 0.09, 0.219]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8557 | Steps: 4 | Val loss: 1.5549 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=14288)[0m top1: 0.22761194029850745
[2m[36m(func pid=14288)[0m top5: 0.7481343283582089
[2m[36m(func pid=14288)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=14288)[0m f1_macro: 0.10531135084197193
[2m[36m(func pid=14288)[0m f1_weighted: 0.2779825077416767
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.122, 0.006, 0.431, 0.036, 0.0, 0.458, 0.0, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.9500 | Steps: 4 | Val loss: 2.6370 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6012 | Steps: 4 | Val loss: 3.6847 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=13050)[0m top1: 0.4458955223880597
[2m[36m(func pid=13050)[0m top5: 0.9039179104477612
[2m[36m(func pid=13050)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=13050)[0m f1_macro: 0.39794712424148276
[2m[36m(func pid=13050)[0m f1_weighted: 0.47082805171243525
[2m[36m(func pid=13050)[0m f1_per_class: [0.449, 0.548, 0.364, 0.494, 0.088, 0.427, 0.462, 0.45, 0.294, 0.404]
[2m[36m(func pid=13050)[0m 
== Status ==
Current time: 2024-01-07 14:40:13 (running for 00:06:38.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.856 |      0.398 |                   64 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.665 |      0.174 |                   64 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.95  |      0.202 |                   65 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.835 |      0.105 |                   62 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.20988805970149255
[2m[36m(func pid=13861)[0m top5: 0.8176305970149254
[2m[36m(func pid=13861)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=13861)[0m f1_macro: 0.20206666386390185
[2m[36m(func pid=13861)[0m f1_weighted: 0.20229190306361336
[2m[36m(func pid=13861)[0m f1_per_class: [0.093, 0.207, 0.202, 0.302, 0.071, 0.3, 0.056, 0.318, 0.177, 0.295]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6495 | Steps: 4 | Val loss: 2.3516 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=13435)[0m top1: 0.24860074626865672
[2m[36m(func pid=13435)[0m top5: 0.8847947761194029
[2m[36m(func pid=13435)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=13435)[0m f1_macro: 0.20953086767013543
[2m[36m(func pid=13435)[0m f1_weighted: 0.2462706363742452
[2m[36m(func pid=13435)[0m f1_per_class: [0.221, 0.334, 0.109, 0.367, 0.182, 0.133, 0.143, 0.271, 0.11, 0.226]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.1337 | Steps: 4 | Val loss: 1.5666 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.5796 | Steps: 4 | Val loss: 2.4706 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=14288)[0m top1: 0.269589552238806
[2m[36m(func pid=14288)[0m top5: 0.757929104477612
[2m[36m(func pid=14288)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=14288)[0m f1_macro: 0.11928167230535473
[2m[36m(func pid=14288)[0m f1_weighted: 0.2988908443372024
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.117, 0.006, 0.434, 0.0, 0.0, 0.503, 0.133, 0.0, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6836 | Steps: 4 | Val loss: 3.4585 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=13050)[0m top1: 0.4412313432835821
[2m[36m(func pid=13050)[0m top5: 0.9011194029850746
[2m[36m(func pid=13050)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=13050)[0m f1_macro: 0.3918369927476363
[2m[36m(func pid=13050)[0m f1_weighted: 0.46371339750840623
[2m[36m(func pid=13050)[0m f1_per_class: [0.434, 0.54, 0.348, 0.492, 0.097, 0.426, 0.447, 0.439, 0.295, 0.4]
[2m[36m(func pid=13050)[0m 
== Status ==
Current time: 2024-01-07 14:40:18 (running for 00:06:43.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.134 |      0.392 |                   65 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.601 |      0.21  |                   65 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.58  |      0.189 |                   66 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.65  |      0.119 |                   63 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.20242537313432835
[2m[36m(func pid=13861)[0m top5: 0.8148320895522388
[2m[36m(func pid=13861)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=13861)[0m f1_macro: 0.18927779459682523
[2m[36m(func pid=13861)[0m f1_weighted: 0.1984545737882448
[2m[36m(func pid=13861)[0m f1_per_class: [0.111, 0.227, 0.183, 0.303, 0.05, 0.164, 0.079, 0.354, 0.16, 0.263]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.7413 | Steps: 4 | Val loss: 2.5096 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=13435)[0m top1: 0.2905783582089552
[2m[36m(func pid=13435)[0m top5: 0.8847947761194029
[2m[36m(func pid=13435)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=13435)[0m f1_macro: 0.24226989389401368
[2m[36m(func pid=13435)[0m f1_weighted: 0.29814402253536754
[2m[36m(func pid=13435)[0m f1_per_class: [0.231, 0.381, 0.159, 0.292, 0.222, 0.171, 0.338, 0.289, 0.129, 0.211]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.9992 | Steps: 4 | Val loss: 1.5836 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.5133 | Steps: 4 | Val loss: 2.2678 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14288)[0m top1: 0.06529850746268656
[2m[36m(func pid=14288)[0m top5: 0.7388059701492538
[2m[36m(func pid=14288)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=14288)[0m f1_macro: 0.041252669610570204
[2m[36m(func pid=14288)[0m f1_weighted: 0.031391207880047796
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.113, 0.027, 0.0, 0.0, 0.0, 0.0, 0.16, 0.055, 0.057]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6009 | Steps: 4 | Val loss: 3.7853 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=13050)[0m top1: 0.4398320895522388
[2m[36m(func pid=13050)[0m top5: 0.9006529850746269
[2m[36m(func pid=13050)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=13050)[0m f1_macro: 0.38242860690155633
[2m[36m(func pid=13050)[0m f1_weighted: 0.4655901601040531
[2m[36m(func pid=13050)[0m f1_per_class: [0.416, 0.535, 0.279, 0.492, 0.096, 0.424, 0.461, 0.434, 0.294, 0.393]
[2m[36m(func pid=13050)[0m 
== Status ==
Current time: 2024-01-07 14:40:23 (running for 00:06:49.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.999 |      0.382 |                   66 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.684 |      0.242 |                   66 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.513 |      0.18  |                   67 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.741 |      0.041 |                   64 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.20755597014925373
[2m[36m(func pid=13861)[0m top5: 0.8334888059701493
[2m[36m(func pid=13861)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=13861)[0m f1_macro: 0.18007612905568315
[2m[36m(func pid=13861)[0m f1_weighted: 0.20292376512114396
[2m[36m(func pid=13861)[0m f1_per_class: [0.094, 0.157, 0.189, 0.348, 0.048, 0.0, 0.156, 0.358, 0.141, 0.31]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.7354 | Steps: 4 | Val loss: 2.9345 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=13435)[0m top1: 0.2873134328358209
[2m[36m(func pid=13435)[0m top5: 0.8675373134328358
[2m[36m(func pid=13435)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=13435)[0m f1_macro: 0.2526779063283137
[2m[36m(func pid=13435)[0m f1_weighted: 0.28100786661047567
[2m[36m(func pid=13435)[0m f1_per_class: [0.192, 0.415, 0.265, 0.192, 0.179, 0.159, 0.349, 0.354, 0.086, 0.337]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.0116 | Steps: 4 | Val loss: 1.6215 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.5433 | Steps: 4 | Val loss: 2.1779 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=14288)[0m top1: 0.07555970149253731
[2m[36m(func pid=14288)[0m top5: 0.6665111940298507
[2m[36m(func pid=14288)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=14288)[0m f1_macro: 0.038332445606951884
[2m[36m(func pid=14288)[0m f1_weighted: 0.03096727668203662
[2m[36m(func pid=14288)[0m f1_per_class: [0.086, 0.114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.143, 0.041, 0.0]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4281716417910448
[2m[36m(func pid=13050)[0m top5: 0.8973880597014925
[2m[36m(func pid=13050)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=13050)[0m f1_macro: 0.381727128748212
[2m[36m(func pid=13050)[0m f1_weighted: 0.45310053132693434
[2m[36m(func pid=13050)[0m f1_per_class: [0.451, 0.52, 0.293, 0.477, 0.087, 0.408, 0.441, 0.451, 0.315, 0.375]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7998 | Steps: 4 | Val loss: 4.2127 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:40:28 (running for 00:06:54.30)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.012 |      0.382 |                   67 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.601 |      0.253 |                   67 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.543 |      0.196 |                   68 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.735 |      0.038 |                   65 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.22761194029850745
[2m[36m(func pid=13861)[0m top5: 0.8390858208955224
[2m[36m(func pid=13861)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=13861)[0m f1_macro: 0.19599773451208463
[2m[36m(func pid=13861)[0m f1_weighted: 0.24286828437596097
[2m[36m(func pid=13861)[0m f1_per_class: [0.095, 0.15, 0.204, 0.348, 0.05, 0.0, 0.292, 0.363, 0.146, 0.313]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.5308 | Steps: 4 | Val loss: 2.8296 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13435)[0m top1: 0.24953358208955223
[2m[36m(func pid=13435)[0m top5: 0.8465485074626866
[2m[36m(func pid=13435)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=13435)[0m f1_macro: 0.2386172487761557
[2m[36m(func pid=13435)[0m f1_weighted: 0.23795990190415245
[2m[36m(func pid=13435)[0m f1_per_class: [0.163, 0.425, 0.353, 0.163, 0.149, 0.089, 0.248, 0.33, 0.203, 0.264]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8911 | Steps: 4 | Val loss: 1.6509 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.6119 | Steps: 4 | Val loss: 2.2076 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=14288)[0m top1: 0.16651119402985073
[2m[36m(func pid=14288)[0m top5: 0.7397388059701493
[2m[36m(func pid=14288)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=14288)[0m f1_macro: 0.0832383751583413
[2m[36m(func pid=14288)[0m f1_weighted: 0.08988742348300313
[2m[36m(func pid=14288)[0m f1_per_class: [0.033, 0.374, 0.036, 0.0, 0.0, 0.0, 0.033, 0.213, 0.044, 0.1]
[2m[36m(func pid=14288)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4230410447761194
[2m[36m(func pid=13050)[0m top5: 0.8903917910447762
[2m[36m(func pid=13050)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=13050)[0m f1_macro: 0.37426614840542205
[2m[36m(func pid=13050)[0m f1_weighted: 0.4528008919080749
[2m[36m(func pid=13050)[0m f1_per_class: [0.403, 0.52, 0.242, 0.485, 0.094, 0.415, 0.434, 0.459, 0.291, 0.4]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5135 | Steps: 4 | Val loss: 4.7384 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:40:33 (running for 00:06:59.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.891 |      0.374 |                   68 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.8   |      0.239 |                   68 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.612 |      0.217 |                   69 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.531 |      0.083 |                   66 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.26259328358208955
[2m[36m(func pid=13861)[0m top5: 0.8409514925373134
[2m[36m(func pid=13861)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=13861)[0m f1_macro: 0.21671798942476336
[2m[36m(func pid=13861)[0m f1_weighted: 0.29364456827786634
[2m[36m(func pid=13861)[0m f1_per_class: [0.105, 0.215, 0.276, 0.254, 0.046, 0.0, 0.519, 0.333, 0.129, 0.291]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3659 | Steps: 4 | Val loss: 2.7448 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=13435)[0m top1: 0.21548507462686567
[2m[36m(func pid=13435)[0m top5: 0.8418843283582089
[2m[36m(func pid=13435)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=13435)[0m f1_macro: 0.21019123014153135
[2m[36m(func pid=13435)[0m f1_weighted: 0.19983719036939562
[2m[36m(func pid=13435)[0m f1_per_class: [0.15, 0.395, 0.328, 0.185, 0.12, 0.052, 0.136, 0.301, 0.248, 0.188]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7322 | Steps: 4 | Val loss: 1.6270 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.8009 | Steps: 4 | Val loss: 2.2494 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=14288)[0m top1: 0.17024253731343283
[2m[36m(func pid=14288)[0m top5: 0.7765858208955224
[2m[36m(func pid=14288)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=14288)[0m f1_macro: 0.10129921603393192
[2m[36m(func pid=14288)[0m f1_weighted: 0.1647157672383939
[2m[36m(func pid=14288)[0m f1_per_class: [0.009, 0.119, 0.077, 0.374, 0.058, 0.0, 0.08, 0.225, 0.053, 0.018]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:40:39 (running for 00:07:04.66)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.732 |      0.387 |                   69 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.514 |      0.21  |                   69 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.612 |      0.217 |                   69 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.366 |      0.101 |                   67 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.4295708955223881
[2m[36m(func pid=13050)[0m top5: 0.8950559701492538
[2m[36m(func pid=13050)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=13050)[0m f1_macro: 0.3873080231496457
[2m[36m(func pid=13050)[0m f1_weighted: 0.451958582237676
[2m[36m(func pid=13050)[0m f1_per_class: [0.44, 0.521, 0.364, 0.489, 0.094, 0.415, 0.426, 0.438, 0.298, 0.389]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3690 | Steps: 4 | Val loss: 5.1712 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=13861)[0m top1: 0.2971082089552239
[2m[36m(func pid=13861)[0m top5: 0.8498134328358209
[2m[36m(func pid=13861)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=13861)[0m f1_macro: 0.2576990018239945
[2m[36m(func pid=13861)[0m f1_weighted: 0.31924766468127136
[2m[36m(func pid=13861)[0m f1_per_class: [0.111, 0.337, 0.632, 0.199, 0.044, 0.0, 0.586, 0.307, 0.125, 0.236]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5596 | Steps: 4 | Val loss: 2.8732 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=13435)[0m top1: 0.22761194029850745
[2m[36m(func pid=13435)[0m top5: 0.8353544776119403
[2m[36m(func pid=13435)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=13435)[0m f1_macro: 0.2113039960389524
[2m[36m(func pid=13435)[0m f1_weighted: 0.21495105540032053
[2m[36m(func pid=13435)[0m f1_per_class: [0.148, 0.385, 0.302, 0.283, 0.104, 0.05, 0.099, 0.307, 0.278, 0.156]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.0461 | Steps: 4 | Val loss: 1.5982 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7458 | Steps: 4 | Val loss: 2.3469 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=14288)[0m top1: 0.12033582089552239
[2m[36m(func pid=14288)[0m top5: 0.6888992537313433
[2m[36m(func pid=14288)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=14288)[0m f1_macro: 0.08110202605670751
[2m[36m(func pid=14288)[0m f1_weighted: 0.12214221173498285
[2m[36m(func pid=14288)[0m f1_per_class: [0.009, 0.089, 0.0, 0.243, 0.039, 0.0, 0.075, 0.234, 0.062, 0.061]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:40:44 (running for 00:07:10.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.046 |      0.393 |                   70 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.369 |      0.211 |                   70 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.801 |      0.258 |                   70 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.56  |      0.081 |                   68 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.44169776119402987
[2m[36m(func pid=13050)[0m top5: 0.9011194029850746
[2m[36m(func pid=13050)[0m f1_micro: 0.4416977611940298
[2m[36m(func pid=13050)[0m f1_macro: 0.39338338227785896
[2m[36m(func pid=13050)[0m f1_weighted: 0.46431626803808773
[2m[36m(func pid=13050)[0m f1_per_class: [0.481, 0.527, 0.324, 0.508, 0.089, 0.41, 0.442, 0.44, 0.328, 0.385]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.2896455223880597
[2m[36m(func pid=13861)[0m top5: 0.8512126865671642
[2m[36m(func pid=13861)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=13861)[0m f1_macro: 0.2630091599513875
[2m[36m(func pid=13861)[0m f1_weighted: 0.302629334087213
[2m[36m(func pid=13861)[0m f1_per_class: [0.122, 0.275, 0.733, 0.146, 0.058, 0.0, 0.602, 0.368, 0.126, 0.2]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6389 | Steps: 4 | Val loss: 5.3481 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6968 | Steps: 4 | Val loss: 3.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=13435)[0m top1: 0.2355410447761194
[2m[36m(func pid=13435)[0m top5: 0.840018656716418
[2m[36m(func pid=13435)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=13435)[0m f1_macro: 0.19037038451907926
[2m[36m(func pid=13435)[0m f1_weighted: 0.22812395856243461
[2m[36m(func pid=13435)[0m f1_per_class: [0.159, 0.343, 0.053, 0.35, 0.098, 0.053, 0.109, 0.31, 0.27, 0.158]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8126 | Steps: 4 | Val loss: 1.5490 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.7781 | Steps: 4 | Val loss: 2.6427 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=14288)[0m top1: 0.10541044776119403
[2m[36m(func pid=14288)[0m top5: 0.5475746268656716
[2m[36m(func pid=14288)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=14288)[0m f1_macro: 0.0668760619847483
[2m[36m(func pid=14288)[0m f1_weighted: 0.07853570150595962
[2m[36m(func pid=14288)[0m f1_per_class: [0.011, 0.077, 0.0, 0.0, 0.072, 0.0, 0.16, 0.24, 0.074, 0.035]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:40:49 (running for 00:07:15.53)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.813 |      0.402 |                   71 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.639 |      0.19  |                   71 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.746 |      0.263 |                   71 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.697 |      0.067 |                   69 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.45382462686567165
[2m[36m(func pid=13050)[0m top5: 0.9132462686567164
[2m[36m(func pid=13050)[0m f1_micro: 0.45382462686567165
[2m[36m(func pid=13050)[0m f1_macro: 0.40172048092394447
[2m[36m(func pid=13050)[0m f1_weighted: 0.4742574584118746
[2m[36m(func pid=13050)[0m f1_per_class: [0.5, 0.525, 0.343, 0.5, 0.107, 0.405, 0.481, 0.45, 0.347, 0.359]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.25093283582089554
[2m[36m(func pid=13861)[0m top5: 0.8446828358208955
[2m[36m(func pid=13861)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=13861)[0m f1_macro: 0.21117839110925946
[2m[36m(func pid=13861)[0m f1_weighted: 0.27359239895363247
[2m[36m(func pid=13861)[0m f1_per_class: [0.103, 0.292, 0.467, 0.126, 0.047, 0.0, 0.541, 0.286, 0.097, 0.152]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5782 | Steps: 4 | Val loss: 4.9699 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.0326 | Steps: 4 | Val loss: 3.6993 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=13435)[0m top1: 0.25
[2m[36m(func pid=13435)[0m top5: 0.8404850746268657
[2m[36m(func pid=13435)[0m f1_micro: 0.25
[2m[36m(func pid=13435)[0m f1_macro: 0.2133160696089766
[2m[36m(func pid=13435)[0m f1_weighted: 0.253273141540714
[2m[36m(func pid=13435)[0m f1_per_class: [0.182, 0.379, 0.161, 0.374, 0.074, 0.172, 0.106, 0.304, 0.237, 0.143]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.0746 | Steps: 4 | Val loss: 1.5400 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.8901 | Steps: 4 | Val loss: 2.8704 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=14288)[0m top1: 0.21641791044776118
[2m[36m(func pid=14288)[0m top5: 0.6520522388059702
[2m[36m(func pid=14288)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=14288)[0m f1_macro: 0.13876840312379166
[2m[36m(func pid=14288)[0m f1_weighted: 0.20582849815231022
[2m[36m(func pid=14288)[0m f1_per_class: [0.068, 0.087, 0.183, 0.0, 0.073, 0.0, 0.555, 0.378, 0.0, 0.044]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:40:55 (running for 00:07:21.05)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.075 |      0.406 |                   72 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.578 |      0.213 |                   72 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.778 |      0.211 |                   72 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.033 |      0.139 |                   70 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.44776119402985076
[2m[36m(func pid=13050)[0m top5: 0.9239738805970149
[2m[36m(func pid=13050)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=13050)[0m f1_macro: 0.4061068660138951
[2m[36m(func pid=13050)[0m f1_weighted: 0.4677790381748386
[2m[36m(func pid=13050)[0m f1_per_class: [0.495, 0.515, 0.316, 0.495, 0.1, 0.427, 0.452, 0.466, 0.391, 0.404]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4563 | Steps: 4 | Val loss: 4.3717 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=13861)[0m top1: 0.2248134328358209
[2m[36m(func pid=13861)[0m top5: 0.84375
[2m[36m(func pid=13861)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=13861)[0m f1_macro: 0.18132190092971595
[2m[36m(func pid=13861)[0m f1_weighted: 0.25616019837219356
[2m[36m(func pid=13861)[0m f1_per_class: [0.098, 0.288, 0.2, 0.197, 0.048, 0.0, 0.417, 0.329, 0.095, 0.142]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.4999 | Steps: 4 | Val loss: 3.2480 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=13435)[0m top1: 0.2826492537313433
[2m[36m(func pid=13435)[0m top5: 0.851679104477612
[2m[36m(func pid=13435)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=13435)[0m f1_macro: 0.23734677325539133
[2m[36m(func pid=13435)[0m f1_weighted: 0.28929733194795065
[2m[36m(func pid=13435)[0m f1_per_class: [0.219, 0.353, 0.139, 0.447, 0.079, 0.271, 0.128, 0.33, 0.234, 0.173]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8139 | Steps: 4 | Val loss: 1.5357 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.7925 | Steps: 4 | Val loss: 2.7915 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=14288)[0m top1: 0.22061567164179105
[2m[36m(func pid=14288)[0m top5: 0.7252798507462687
[2m[36m(func pid=14288)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=14288)[0m f1_macro: 0.14880432911100197
[2m[36m(func pid=14288)[0m f1_weighted: 0.25336902989341864
[2m[36m(func pid=14288)[0m f1_per_class: [0.05, 0.127, 0.069, 0.25, 0.068, 0.024, 0.456, 0.324, 0.063, 0.057]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:41:00 (running for 00:07:26.50)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.814 |      0.398 |                   73 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.456 |      0.237 |                   73 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.89  |      0.181 |                   73 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.5   |      0.149 |                   71 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.447294776119403
[2m[36m(func pid=13050)[0m top5: 0.9211753731343284
[2m[36m(func pid=13050)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=13050)[0m f1_macro: 0.3975025869209347
[2m[36m(func pid=13050)[0m f1_weighted: 0.46810895171687883
[2m[36m(func pid=13050)[0m f1_per_class: [0.495, 0.509, 0.3, 0.491, 0.119, 0.421, 0.468, 0.467, 0.369, 0.336]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.19682835820895522
[2m[36m(func pid=13861)[0m top5: 0.8544776119402985
[2m[36m(func pid=13861)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=13861)[0m f1_macro: 0.16159092177300194
[2m[36m(func pid=13861)[0m f1_weighted: 0.20276917251151164
[2m[36m(func pid=13861)[0m f1_per_class: [0.079, 0.285, 0.136, 0.239, 0.062, 0.0, 0.192, 0.381, 0.093, 0.149]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5871 | Steps: 4 | Val loss: 3.7376 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.8617 | Steps: 4 | Val loss: 3.5495 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=13435)[0m top1: 0.3302238805970149
[2m[36m(func pid=13435)[0m top5: 0.871268656716418
[2m[36m(func pid=13435)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=13435)[0m f1_macro: 0.2719818191640782
[2m[36m(func pid=13435)[0m f1_weighted: 0.34329558333214616
[2m[36m(func pid=13435)[0m f1_per_class: [0.206, 0.386, 0.207, 0.509, 0.068, 0.271, 0.22, 0.373, 0.236, 0.243]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7216 | Steps: 4 | Val loss: 1.5616 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6082 | Steps: 4 | Val loss: 2.5320 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=14288)[0m top1: 0.18983208955223882
[2m[36m(func pid=14288)[0m top5: 0.7509328358208955
[2m[36m(func pid=14288)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=14288)[0m f1_macro: 0.08876503051897525
[2m[36m(func pid=14288)[0m f1_weighted: 0.15861920359176637
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.347, 0.0, 0.069, 0.0, 0.172, 0.184, 0.078, 0.0, 0.037]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:41:06 (running for 00:07:31.97)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.722 |      0.388 |                   74 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.587 |      0.272 |                   74 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.793 |      0.162 |                   74 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.862 |      0.089 |                   72 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.4351679104477612
[2m[36m(func pid=13050)[0m top5: 0.9183768656716418
[2m[36m(func pid=13050)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=13050)[0m f1_macro: 0.3881015057220016
[2m[36m(func pid=13050)[0m f1_weighted: 0.4573069254027996
[2m[36m(func pid=13050)[0m f1_per_class: [0.495, 0.502, 0.25, 0.485, 0.11, 0.418, 0.438, 0.489, 0.386, 0.307]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.2019589552238806
[2m[36m(func pid=13861)[0m top5: 0.871268656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=13861)[0m f1_macro: 0.1699825421465617
[2m[36m(func pid=13861)[0m f1_weighted: 0.20201978953601232
[2m[36m(func pid=13861)[0m f1_per_class: [0.089, 0.256, 0.226, 0.312, 0.057, 0.0, 0.139, 0.356, 0.101, 0.162]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8775 | Steps: 4 | Val loss: 3.4904 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.9543 | Steps: 4 | Val loss: 3.3626 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6893 | Steps: 4 | Val loss: 1.5939 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13435)[0m top1: 0.3833955223880597
[2m[36m(func pid=13435)[0m top5: 0.8694029850746269
[2m[36m(func pid=13435)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=13435)[0m f1_macro: 0.30300454984791414
[2m[36m(func pid=13435)[0m f1_weighted: 0.41082285524328754
[2m[36m(func pid=13435)[0m f1_per_class: [0.174, 0.368, 0.228, 0.56, 0.075, 0.321, 0.379, 0.428, 0.266, 0.232]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.4802 | Steps: 4 | Val loss: 2.3411 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=14288)[0m top1: 0.24067164179104478
[2m[36m(func pid=14288)[0m top5: 0.7672574626865671
[2m[36m(func pid=14288)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=14288)[0m f1_macro: 0.12062912690861752
[2m[36m(func pid=14288)[0m f1_weighted: 0.21151586482060405
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.204, 0.066, 0.431, 0.0, 0.375, 0.03, 0.058, 0.0, 0.042]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:41:11 (running for 00:07:37.21)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.33999999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.689 |      0.377 |                   75 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.877 |      0.303 |                   75 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.608 |      0.17  |                   75 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.954 |      0.121 |                   73 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.4230410447761194
[2m[36m(func pid=13050)[0m top5: 0.9127798507462687
[2m[36m(func pid=13050)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=13050)[0m f1_macro: 0.3766521500156947
[2m[36m(func pid=13050)[0m f1_weighted: 0.4456346481170247
[2m[36m(func pid=13050)[0m f1_per_class: [0.466, 0.504, 0.224, 0.477, 0.102, 0.423, 0.411, 0.467, 0.379, 0.312]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.20755597014925373
[2m[36m(func pid=13861)[0m top5: 0.8843283582089553
[2m[36m(func pid=13861)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=13861)[0m f1_macro: 0.1708676358602119
[2m[36m(func pid=13861)[0m f1_weighted: 0.20072481052214683
[2m[36m(func pid=13861)[0m f1_per_class: [0.073, 0.257, 0.136, 0.328, 0.066, 0.0, 0.117, 0.337, 0.16, 0.236]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.0693 | Steps: 4 | Val loss: 3.4975 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 3.2196 | Steps: 4 | Val loss: 2.8155 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=13435)[0m top1: 0.39598880597014924
[2m[36m(func pid=13435)[0m top5: 0.8614738805970149
[2m[36m(func pid=13435)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=13435)[0m f1_macro: 0.28951224840354
[2m[36m(func pid=13435)[0m f1_weighted: 0.41819380616233565
[2m[36m(func pid=13435)[0m f1_per_class: [0.153, 0.38, 0.241, 0.577, 0.066, 0.329, 0.429, 0.113, 0.35, 0.257]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.7612 | Steps: 4 | Val loss: 1.5716 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.9090 | Steps: 4 | Val loss: 2.2158 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=14288)[0m top1: 0.2332089552238806
[2m[36m(func pid=14288)[0m top5: 0.7448694029850746
[2m[36m(func pid=14288)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=14288)[0m f1_macro: 0.1424493973460035
[2m[36m(func pid=14288)[0m f1_weighted: 0.19428688676063147
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.267, 0.053, 0.298, 0.0, 0.343, 0.009, 0.384, 0.0, 0.07]
[2m[36m(func pid=14288)[0m 
== Status ==
Current time: 2024-01-07 14:41:17 (running for 00:07:42.66)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.33999999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING  | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.761 |      0.376 |                   76 |
| train_5ae7f_00001 | RUNNING  | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.069 |      0.29  |                   76 |
| train_5ae7f_00002 | RUNNING  | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.48  |      0.171 |                   76 |
| train_5ae7f_00003 | RUNNING  | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  3.22  |      0.142 |                   74 |
| train_5ae7f_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m top1: 0.4197761194029851
[2m[36m(func pid=13050)[0m top5: 0.9202425373134329
[2m[36m(func pid=13050)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=13050)[0m f1_macro: 0.3762449558329872
[2m[36m(func pid=13050)[0m f1_weighted: 0.4411623397446214
[2m[36m(func pid=13050)[0m f1_per_class: [0.495, 0.49, 0.242, 0.456, 0.107, 0.429, 0.422, 0.463, 0.37, 0.288]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.22388059701492538
[2m[36m(func pid=13861)[0m top5: 0.8978544776119403
[2m[36m(func pid=13861)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=13861)[0m f1_macro: 0.17929455443972847
[2m[36m(func pid=13861)[0m f1_weighted: 0.20376778348654223
[2m[36m(func pid=13861)[0m f1_per_class: [0.056, 0.213, 0.145, 0.395, 0.092, 0.096, 0.054, 0.331, 0.151, 0.259]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.7238 | Steps: 4 | Val loss: 3.8376 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=14288)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.9306 | Steps: 4 | Val loss: 3.0155 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=13435)[0m top1: 0.33115671641791045
[2m[36m(func pid=13435)[0m top5: 0.8395522388059702
[2m[36m(func pid=13435)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=13435)[0m f1_macro: 0.24488206938257182
[2m[36m(func pid=13435)[0m f1_weighted: 0.3599515006575059
[2m[36m(func pid=13435)[0m f1_per_class: [0.155, 0.268, 0.259, 0.498, 0.039, 0.344, 0.404, 0.0, 0.214, 0.268]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.6733 | Steps: 4 | Val loss: 1.5764 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.9768 | Steps: 4 | Val loss: 2.2024 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14288)[0m top1: 0.1478544776119403
[2m[36m(func pid=14288)[0m top5: 0.6394589552238806
[2m[36m(func pid=14288)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=14288)[0m f1_macro: 0.09509921825029366
[2m[36m(func pid=14288)[0m f1_weighted: 0.07393555470883909
[2m[36m(func pid=14288)[0m f1_per_class: [0.0, 0.071, 0.216, 0.0, 0.0, 0.279, 0.03, 0.332, 0.0, 0.023]
[2m[36m(func pid=13050)[0m top1: 0.41884328358208955
[2m[36m(func pid=13050)[0m top5: 0.9197761194029851
[2m[36m(func pid=13050)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=13050)[0m f1_macro: 0.3826584056232753
[2m[36m(func pid=13050)[0m f1_weighted: 0.44120418283390633
[2m[36m(func pid=13050)[0m f1_per_class: [0.519, 0.482, 0.253, 0.451, 0.097, 0.414, 0.429, 0.481, 0.388, 0.313]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m top1: 0.2453358208955224
[2m[36m(func pid=13861)[0m top5: 0.8973880597014925
[2m[36m(func pid=13861)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=13861)[0m f1_macro: 0.1903156906331276
[2m[36m(func pid=13861)[0m f1_weighted: 0.21704508501272407
[2m[36m(func pid=13861)[0m f1_per_class: [0.068, 0.162, 0.179, 0.439, 0.134, 0.233, 0.039, 0.329, 0.096, 0.224]
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.5712 | Steps: 4 | Val loss: 4.1843 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=13435)[0m top1: 0.27798507462686567
[2m[36m(func pid=13435)[0m top5: 0.8222947761194029
[2m[36m(func pid=13435)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=13435)[0m f1_macro: 0.22148736490405568
[2m[36m(func pid=13435)[0m f1_weighted: 0.30657555307380857
[2m[36m(func pid=13435)[0m f1_per_class: [0.131, 0.229, 0.015, 0.455, 0.04, 0.359, 0.231, 0.287, 0.235, 0.231]
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.5654 | Steps: 4 | Val loss: 1.5514 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=13050)[0m top1: 0.4216417910447761
[2m[36m(func pid=13050)[0m top5: 0.9267723880597015
[2m[36m(func pid=13050)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=13050)[0m f1_macro: 0.38253949794410924
[2m[36m(func pid=13050)[0m f1_weighted: 0.4395608456245143
[2m[36m(func pid=13050)[0m f1_per_class: [0.523, 0.491, 0.255, 0.45, 0.11, 0.426, 0.42, 0.455, 0.374, 0.321]
== Status ==
Current time: 2024-01-07 14:41:22 (running for 00:07:48.01)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.673 |      0.383 |                   77 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.724 |      0.245 |                   77 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.909 |      0.179 |                   77 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 14:41:29 (running for 00:07:54.68)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.673 |      0.383 |                   77 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.571 |      0.221 |                   78 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.909 |      0.179 |                   77 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=32295)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=32295)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=32295)[0m Configuration completed!
[2m[36m(func pid=32295)[0m New optimizer parameters:
[2m[36m(func pid=32295)[0m SGD (
[2m[36m(func pid=32295)[0m Parameter Group 0
[2m[36m(func pid=32295)[0m     dampening: 0
[2m[36m(func pid=32295)[0m     differentiable: False
[2m[36m(func pid=32295)[0m     foreach: None
[2m[36m(func pid=32295)[0m     lr: 0.0001
[2m[36m(func pid=32295)[0m     maximize: False
[2m[36m(func pid=32295)[0m     momentum: 0.9
[2m[36m(func pid=32295)[0m     nesterov: False
[2m[36m(func pid=32295)[0m     weight_decay: 0
[2m[36m(func pid=32295)[0m )
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.7717 | Steps: 4 | Val loss: 1.5866 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2749 | Steps: 4 | Val loss: 4.2867 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 2.2116 | Steps: 4 | Val loss: 2.1872 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:41:34 (running for 00:07:59.70)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.565 |      0.383 |                   78 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.571 |      0.221 |                   78 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.977 |      0.19  |                   78 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9987 | Steps: 4 | Val loss: 2.3380 | Batch size: 32 | lr: 0.0001 | Duration: 5.06s
[2m[36m(func pid=13435)[0m top1: 0.2873134328358209
[2m[36m(func pid=13435)[0m top5: 0.8274253731343284
[2m[36m(func pid=13435)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=13435)[0m f1_macro: 0.24338249269023282
[2m[36m(func pid=13435)[0m f1_weighted: 0.30670555798456944
[2m[36m(func pid=13435)[0m f1_per_class: [0.095, 0.294, 0.01, 0.452, 0.043, 0.369, 0.15, 0.484, 0.313, 0.224]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13861)[0m top1: 0.27005597014925375
[2m[36m(func pid=13861)[0m top5: 0.8880597014925373
[2m[36m(func pid=13861)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=13861)[0m f1_macro: 0.2141449818909628
[2m[36m(func pid=13861)[0m f1_weighted: 0.24683539399044435
[2m[36m(func pid=13861)[0m f1_per_class: [0.054, 0.191, 0.148, 0.435, 0.153, 0.335, 0.073, 0.385, 0.133, 0.234]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m top1: 0.41044776119402987
[2m[36m(func pid=13050)[0m top5: 0.9225746268656716
[2m[36m(func pid=13050)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=13050)[0m f1_macro: 0.36798505115762586
[2m[36m(func pid=13050)[0m f1_weighted: 0.4327578914358204
[2m[36m(func pid=13050)[0m f1_per_class: [0.467, 0.481, 0.247, 0.447, 0.104, 0.412, 0.419, 0.441, 0.385, 0.276]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m top1: 0.16557835820895522
[2m[36m(func pid=32295)[0m top5: 0.5107276119402985
[2m[36m(func pid=32295)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=32295)[0m f1_macro: 0.09706091972374155
[2m[36m(func pid=32295)[0m f1_weighted: 0.11988213668451889
[2m[36m(func pid=32295)[0m f1_per_class: [0.156, 0.309, 0.0, 0.098, 0.01, 0.256, 0.012, 0.039, 0.0, 0.091]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4437 | Steps: 4 | Val loss: 3.7185 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 2.0222 | Steps: 4 | Val loss: 2.0621 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.7543 | Steps: 4 | Val loss: 1.5770 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=13435)[0m top1: 0.3204291044776119
[2m[36m(func pid=13435)[0m top5: 0.8652052238805971
[2m[36m(func pid=13435)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=13435)[0m f1_macro: 0.2550746812614247
[2m[36m(func pid=13435)[0m f1_weighted: 0.32541506170104034
[2m[36m(func pid=13435)[0m f1_per_class: [0.069, 0.343, 0.137, 0.485, 0.067, 0.342, 0.175, 0.449, 0.264, 0.22]
[2m[36m(func pid=13435)[0m 
== Status ==
Current time: 2024-01-07 14:41:39 (running for 00:08:05.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.772 |      0.368 |                   79 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.444 |      0.255 |                   80 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.212 |      0.214 |                   79 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.999 |      0.097 |                    1 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.2980410447761194
[2m[36m(func pid=13861)[0m top5: 0.8931902985074627
[2m[36m(func pid=13861)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=13861)[0m f1_macro: 0.23254860306884834
[2m[36m(func pid=13861)[0m f1_weighted: 0.27639893703722823
[2m[36m(func pid=13861)[0m f1_per_class: [0.063, 0.239, 0.169, 0.457, 0.143, 0.389, 0.103, 0.401, 0.084, 0.277]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9722 | Steps: 4 | Val loss: 2.3785 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=13050)[0m top1: 0.41884328358208955
[2m[36m(func pid=13050)[0m top5: 0.9244402985074627
[2m[36m(func pid=13050)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=13050)[0m f1_macro: 0.3743835044247887
[2m[36m(func pid=13050)[0m f1_weighted: 0.4390642446176242
[2m[36m(func pid=13050)[0m f1_per_class: [0.5, 0.494, 0.25, 0.437, 0.107, 0.424, 0.436, 0.433, 0.392, 0.271]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m top1: 0.1417910447761194
[2m[36m(func pid=32295)[0m top5: 0.48507462686567165
[2m[36m(func pid=32295)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=32295)[0m f1_macro: 0.08399404352541605
[2m[36m(func pid=32295)[0m f1_weighted: 0.10671976731351711
[2m[36m(func pid=32295)[0m f1_per_class: [0.144, 0.248, 0.0, 0.095, 0.017, 0.267, 0.006, 0.028, 0.0, 0.034]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3483 | Steps: 4 | Val loss: 3.9681 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.8925 | Steps: 4 | Val loss: 1.9739 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.7656 | Steps: 4 | Val loss: 1.5629 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:41:45 (running for 00:08:10.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.754 |      0.374 |                   80 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.444 |      0.255 |                   80 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.892 |      0.247 |                   81 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.972 |      0.084 |                    2 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3162313432835821
[2m[36m(func pid=13861)[0m top5: 0.8885261194029851
[2m[36m(func pid=13861)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=13861)[0m f1_macro: 0.2474655896634855
[2m[36m(func pid=13861)[0m f1_weighted: 0.30028009399303285
[2m[36m(func pid=13861)[0m f1_per_class: [0.072, 0.226, 0.169, 0.474, 0.136, 0.403, 0.167, 0.401, 0.081, 0.346]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.271455223880597
[2m[36m(func pid=13435)[0m top5: 0.8805970149253731
[2m[36m(func pid=13435)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=13435)[0m f1_macro: 0.24201156874480448
[2m[36m(func pid=13435)[0m f1_weighted: 0.2629569826659527
[2m[36m(func pid=13435)[0m f1_per_class: [0.148, 0.371, 0.27, 0.294, 0.061, 0.268, 0.155, 0.443, 0.225, 0.185]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9418 | Steps: 4 | Val loss: 2.3891 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13050)[0m top1: 0.4216417910447761
[2m[36m(func pid=13050)[0m top5: 0.9267723880597015
[2m[36m(func pid=13050)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=13050)[0m f1_macro: 0.3719162613740639
[2m[36m(func pid=13050)[0m f1_weighted: 0.4397143096242027
[2m[36m(func pid=13050)[0m f1_per_class: [0.484, 0.496, 0.247, 0.4, 0.126, 0.417, 0.475, 0.438, 0.395, 0.24]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m top1: 0.1333955223880597
[2m[36m(func pid=32295)[0m top5: 0.49113805970149255
[2m[36m(func pid=32295)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=32295)[0m f1_macro: 0.07175116990475941
[2m[36m(func pid=32295)[0m f1_weighted: 0.10795607985237306
[2m[36m(func pid=32295)[0m f1_per_class: [0.078, 0.203, 0.0, 0.12, 0.0, 0.268, 0.018, 0.03, 0.0, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.7243 | Steps: 4 | Val loss: 4.3264 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.6353 | Steps: 4 | Val loss: 1.9318 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.1229 | Steps: 4 | Val loss: 1.5598 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:41:50 (running for 00:08:16.03)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.766 |      0.372 |                   81 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.348 |      0.242 |                   81 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.635 |      0.256 |                   82 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.942 |      0.072 |                    3 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3302238805970149
[2m[36m(func pid=13861)[0m top5: 0.8894589552238806
[2m[36m(func pid=13861)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=13861)[0m f1_macro: 0.2564498952647062
[2m[36m(func pid=13861)[0m f1_weighted: 0.3276389374738922
[2m[36m(func pid=13861)[0m f1_per_class: [0.073, 0.27, 0.175, 0.469, 0.113, 0.41, 0.235, 0.423, 0.043, 0.353]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2658582089552239
[2m[36m(func pid=13435)[0m top5: 0.878731343283582
[2m[36m(func pid=13435)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=13435)[0m f1_macro: 0.2420622896323957
[2m[36m(func pid=13435)[0m f1_weighted: 0.24837521371778104
[2m[36m(func pid=13435)[0m f1_per_class: [0.185, 0.401, 0.293, 0.188, 0.06, 0.228, 0.2, 0.444, 0.214, 0.207]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9739 | Steps: 4 | Val loss: 2.3640 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=13050)[0m top1: 0.435634328358209
[2m[36m(func pid=13050)[0m top5: 0.9309701492537313
[2m[36m(func pid=13050)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=13050)[0m f1_macro: 0.39316467699182434
[2m[36m(func pid=13050)[0m f1_weighted: 0.4538593537024923
[2m[36m(func pid=13050)[0m f1_per_class: [0.542, 0.49, 0.353, 0.404, 0.131, 0.413, 0.517, 0.435, 0.413, 0.234]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.5149 | Steps: 4 | Val loss: 2.0256 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=32295)[0m top1: 0.13246268656716417
[2m[36m(func pid=32295)[0m top5: 0.503731343283582
[2m[36m(func pid=32295)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=32295)[0m f1_macro: 0.073174912216578
[2m[36m(func pid=32295)[0m f1_weighted: 0.11430000009695976
[2m[36m(func pid=32295)[0m f1_per_class: [0.05, 0.174, 0.0, 0.128, 0.0, 0.26, 0.047, 0.061, 0.011, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2578 | Steps: 4 | Val loss: 4.5450 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.7056 | Steps: 4 | Val loss: 1.5716 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:41:55 (running for 00:08:21.26)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.123 |      0.393 |                   82 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.724 |      0.242 |                   82 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.515 |      0.268 |                   83 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.974 |      0.073 |                    4 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3208955223880597
[2m[36m(func pid=13861)[0m top5: 0.8726679104477612
[2m[36m(func pid=13861)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=13861)[0m f1_macro: 0.2677791237735485
[2m[36m(func pid=13861)[0m f1_weighted: 0.3398772667683322
[2m[36m(func pid=13861)[0m f1_per_class: [0.092, 0.368, 0.146, 0.414, 0.089, 0.379, 0.271, 0.462, 0.076, 0.381]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.27052238805970147
[2m[36m(func pid=13435)[0m top5: 0.8563432835820896
[2m[36m(func pid=13435)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=13435)[0m f1_macro: 0.24302309657472948
[2m[36m(func pid=13435)[0m f1_weighted: 0.26383412527701144
[2m[36m(func pid=13435)[0m f1_per_class: [0.237, 0.432, 0.3, 0.189, 0.028, 0.141, 0.274, 0.4, 0.178, 0.25]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4361007462686567
[2m[36m(func pid=13050)[0m top5: 0.925839552238806
[2m[36m(func pid=13050)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=13050)[0m f1_macro: 0.39031047903036464
[2m[36m(func pid=13050)[0m f1_weighted: 0.4564185299310325
[2m[36m(func pid=13050)[0m f1_per_class: [0.564, 0.479, 0.387, 0.399, 0.128, 0.411, 0.547, 0.387, 0.391, 0.211]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9081 | Steps: 4 | Val loss: 2.3328 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.9241 | Steps: 4 | Val loss: 2.0766 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=32295)[0m top1: 0.14878731343283583
[2m[36m(func pid=32295)[0m top5: 0.5457089552238806
[2m[36m(func pid=32295)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=32295)[0m f1_macro: 0.08427448931138995
[2m[36m(func pid=32295)[0m f1_weighted: 0.13930431269808452
[2m[36m(func pid=32295)[0m f1_per_class: [0.038, 0.194, 0.0, 0.137, 0.009, 0.261, 0.107, 0.087, 0.012, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.6049 | Steps: 4 | Val loss: 4.6515 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.6366 | Steps: 4 | Val loss: 1.6244 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=13861)[0m top1: 0.3362873134328358
[2m[36m(func pid=13861)[0m top5: 0.8763992537313433
[2m[36m(func pid=13861)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=13861)[0m f1_macro: 0.29168889037246004
[2m[36m(func pid=13861)[0m f1_weighted: 0.378186730254846
[2m[36m(func pid=13861)[0m f1_per_class: [0.104, 0.42, 0.154, 0.407, 0.068, 0.342, 0.378, 0.507, 0.058, 0.478]
[2m[36m(func pid=13861)[0m 
== Status ==
Current time: 2024-01-07 14:42:01 (running for 00:08:26.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.706 |      0.39  |                   83 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.258 |      0.243 |                   83 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.924 |      0.292 |                   84 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.908 |      0.084 |                    5 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13435)[0m top1: 0.27705223880597013
[2m[36m(func pid=13435)[0m top5: 0.8386194029850746
[2m[36m(func pid=13435)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=13435)[0m f1_macro: 0.2612229973214391
[2m[36m(func pid=13435)[0m f1_weighted: 0.2716860904942803
[2m[36m(func pid=13435)[0m f1_per_class: [0.312, 0.477, 0.4, 0.226, 0.028, 0.122, 0.254, 0.318, 0.182, 0.292]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4239738805970149
[2m[36m(func pid=13050)[0m top5: 0.9235074626865671
[2m[36m(func pid=13050)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=13050)[0m f1_macro: 0.3754556532749239
[2m[36m(func pid=13050)[0m f1_weighted: 0.4510435452151277
[2m[36m(func pid=13050)[0m f1_per_class: [0.516, 0.464, 0.32, 0.404, 0.103, 0.398, 0.539, 0.409, 0.386, 0.214]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9385 | Steps: 4 | Val loss: 2.3159 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.6966 | Steps: 4 | Val loss: 2.1049 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4044 | Steps: 4 | Val loss: 4.4514 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=32295)[0m top1: 0.15205223880597016
[2m[36m(func pid=32295)[0m top5: 0.5643656716417911
[2m[36m(func pid=32295)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=32295)[0m f1_macro: 0.08677809465301327
[2m[36m(func pid=32295)[0m f1_weighted: 0.14672623311086097
[2m[36m(func pid=32295)[0m f1_per_class: [0.027, 0.211, 0.0, 0.159, 0.009, 0.255, 0.104, 0.082, 0.022, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.5928 | Steps: 4 | Val loss: 1.6405 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:42:06 (running for 00:08:31.84)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.637 |      0.375 |                   84 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.605 |      0.261 |                   84 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.697 |      0.272 |                   85 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.938 |      0.087 |                    6 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3414179104477612
[2m[36m(func pid=13861)[0m top5: 0.8777985074626866
[2m[36m(func pid=13861)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=13861)[0m f1_macro: 0.2718326766202827
[2m[36m(func pid=13861)[0m f1_weighted: 0.3848724202885551
[2m[36m(func pid=13861)[0m f1_per_class: [0.122, 0.449, 0.194, 0.378, 0.061, 0.312, 0.431, 0.511, 0.06, 0.2]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.27658582089552236
[2m[36m(func pid=13435)[0m top5: 0.8227611940298507
[2m[36m(func pid=13435)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=13435)[0m f1_macro: 0.2815621575342743
[2m[36m(func pid=13435)[0m f1_weighted: 0.275814597575155
[2m[36m(func pid=13435)[0m f1_per_class: [0.447, 0.506, 0.394, 0.307, 0.025, 0.126, 0.165, 0.315, 0.159, 0.372]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.42257462686567165
[2m[36m(func pid=13050)[0m top5: 0.9249067164179104
[2m[36m(func pid=13050)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=13050)[0m f1_macro: 0.3725498676596532
[2m[36m(func pid=13050)[0m f1_weighted: 0.45261246885405415
[2m[36m(func pid=13050)[0m f1_per_class: [0.525, 0.443, 0.267, 0.411, 0.103, 0.402, 0.547, 0.412, 0.388, 0.226]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8692 | Steps: 4 | Val loss: 2.2982 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.4879 | Steps: 4 | Val loss: 2.1759 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.7539 | Steps: 4 | Val loss: 4.3501 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=32295)[0m top1: 0.16977611940298507
[2m[36m(func pid=32295)[0m top5: 0.5844216417910447
[2m[36m(func pid=32295)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=32295)[0m f1_macro: 0.10225974730159122
[2m[36m(func pid=32295)[0m f1_weighted: 0.17187453492362495
[2m[36m(func pid=32295)[0m f1_per_class: [0.039, 0.228, 0.036, 0.196, 0.016, 0.264, 0.137, 0.095, 0.011, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.6613 | Steps: 4 | Val loss: 1.6075 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 14:42:11 (running for 00:08:37.17)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.593 |      0.373 |                   85 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.404 |      0.282 |                   85 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.488 |      0.281 |                   86 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.869 |      0.102 |                    7 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3414179104477612
[2m[36m(func pid=13861)[0m top5: 0.871268656716418
[2m[36m(func pid=13861)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=13861)[0m f1_macro: 0.2808523170074466
[2m[36m(func pid=13861)[0m f1_weighted: 0.3881138607885134
[2m[36m(func pid=13861)[0m f1_per_class: [0.145, 0.48, 0.247, 0.348, 0.057, 0.287, 0.453, 0.508, 0.14, 0.143]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.29197761194029853
[2m[36m(func pid=13435)[0m top5: 0.8353544776119403
[2m[36m(func pid=13435)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=13435)[0m f1_macro: 0.3052211095721405
[2m[36m(func pid=13435)[0m f1_weighted: 0.2903091694632762
[2m[36m(func pid=13435)[0m f1_per_class: [0.426, 0.505, 0.553, 0.323, 0.027, 0.164, 0.178, 0.33, 0.178, 0.368]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4319029850746269
[2m[36m(func pid=13050)[0m top5: 0.9211753731343284
[2m[36m(func pid=13050)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=13050)[0m f1_macro: 0.3788189627383577
[2m[36m(func pid=13050)[0m f1_weighted: 0.46380342961770393
[2m[36m(func pid=13050)[0m f1_per_class: [0.496, 0.456, 0.22, 0.468, 0.097, 0.411, 0.516, 0.443, 0.376, 0.304]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9310 | Steps: 4 | Val loss: 2.2571 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.6732 | Steps: 4 | Val loss: 2.1454 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.5243 | Steps: 4 | Val loss: 4.2935 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=32295)[0m top1: 0.17350746268656717
[2m[36m(func pid=32295)[0m top5: 0.6567164179104478
[2m[36m(func pid=32295)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=32295)[0m f1_macro: 0.10343500328895713
[2m[36m(func pid=32295)[0m f1_weighted: 0.17792877661641837
[2m[36m(func pid=32295)[0m f1_per_class: [0.028, 0.242, 0.03, 0.21, 0.022, 0.267, 0.139, 0.072, 0.025, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.6055 | Steps: 4 | Val loss: 1.5905 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:42:16 (running for 00:08:42.51)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.661 |      0.379 |                   86 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.754 |      0.305 |                   86 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.673 |      0.298 |                   87 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.931 |      0.103 |                    8 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.35774253731343286
[2m[36m(func pid=13861)[0m top5: 0.8782649253731343
[2m[36m(func pid=13861)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=13861)[0m f1_macro: 0.2982746972189317
[2m[36m(func pid=13861)[0m f1_weighted: 0.40079702338535544
[2m[36m(func pid=13861)[0m f1_per_class: [0.154, 0.463, 0.324, 0.344, 0.063, 0.34, 0.482, 0.52, 0.163, 0.129]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2989738805970149
[2m[36m(func pid=13435)[0m top5: 0.835820895522388
[2m[36m(func pid=13435)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=13435)[0m f1_macro: 0.3117225515432722
[2m[36m(func pid=13435)[0m f1_weighted: 0.29558891385697594
[2m[36m(func pid=13435)[0m f1_per_class: [0.347, 0.493, 0.632, 0.319, 0.031, 0.203, 0.193, 0.326, 0.203, 0.37]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.42490671641791045
[2m[36m(func pid=13050)[0m top5: 0.9314365671641791
[2m[36m(func pid=13050)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=13050)[0m f1_macro: 0.403886264114414
[2m[36m(func pid=13050)[0m f1_weighted: 0.45116331752743605
[2m[36m(func pid=13050)[0m f1_per_class: [0.614, 0.474, 0.343, 0.445, 0.081, 0.398, 0.471, 0.454, 0.44, 0.321]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8425 | Steps: 4 | Val loss: 2.2551 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.3998 | Steps: 4 | Val loss: 2.2160 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2155 | Steps: 4 | Val loss: 4.3437 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=32295)[0m top1: 0.17723880597014927
[2m[36m(func pid=32295)[0m top5: 0.6403917910447762
[2m[36m(func pid=32295)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=32295)[0m f1_macro: 0.11177100699996614
[2m[36m(func pid=32295)[0m f1_weighted: 0.1794013026591465
[2m[36m(func pid=32295)[0m f1_per_class: [0.028, 0.267, 0.09, 0.203, 0.022, 0.269, 0.132, 0.082, 0.026, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.0026 | Steps: 4 | Val loss: 1.5821 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:42:22 (running for 00:08:47.82)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.606 |      0.404 |                   87 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.524 |      0.312 |                   87 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.4   |      0.293 |                   88 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.843 |      0.112 |                    9 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3521455223880597
[2m[36m(func pid=13861)[0m top5: 0.8759328358208955
[2m[36m(func pid=13861)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=13861)[0m f1_macro: 0.29311713294484737
[2m[36m(func pid=13861)[0m f1_weighted: 0.40305093293161953
[2m[36m(func pid=13861)[0m f1_per_class: [0.131, 0.453, 0.282, 0.344, 0.057, 0.334, 0.505, 0.496, 0.147, 0.182]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2891791044776119
[2m[36m(func pid=13435)[0m top5: 0.835820895522388
[2m[36m(func pid=13435)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=13435)[0m f1_macro: 0.3165096656521219
[2m[36m(func pid=13435)[0m f1_weighted: 0.281489516719779
[2m[36m(func pid=13435)[0m f1_per_class: [0.247, 0.481, 0.815, 0.264, 0.044, 0.209, 0.207, 0.304, 0.204, 0.39]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.42723880597014924
[2m[36m(func pid=13050)[0m top5: 0.9309701492537313
[2m[36m(func pid=13050)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=13050)[0m f1_macro: 0.3920966995754006
[2m[36m(func pid=13050)[0m f1_weighted: 0.4507450625968687
[2m[36m(func pid=13050)[0m f1_per_class: [0.564, 0.469, 0.304, 0.468, 0.109, 0.407, 0.46, 0.426, 0.413, 0.302]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8706 | Steps: 4 | Val loss: 2.2501 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.4508 | Steps: 4 | Val loss: 2.3115 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.6567 | Steps: 4 | Val loss: 4.1801 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.6481 | Steps: 4 | Val loss: 1.5802 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=32295)[0m top1: 0.18516791044776118
[2m[36m(func pid=32295)[0m top5: 0.6492537313432836
[2m[36m(func pid=32295)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=32295)[0m f1_macro: 0.12234271206978442
[2m[36m(func pid=32295)[0m f1_weighted: 0.1877415440374446
[2m[36m(func pid=32295)[0m f1_per_class: [0.067, 0.26, 0.095, 0.232, 0.03, 0.281, 0.124, 0.107, 0.027, 0.0]
[2m[36m(func pid=32295)[0m 
== Status ==
Current time: 2024-01-07 14:42:27 (running for 00:08:53.05)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  1.003 |      0.392 |                   88 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.215 |      0.317 |                   88 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.451 |      0.295 |                   89 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.871 |      0.122 |                   10 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3400186567164179
[2m[36m(func pid=13861)[0m top5: 0.8843283582089553
[2m[36m(func pid=13861)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=13861)[0m f1_macro: 0.29535233019456236
[2m[36m(func pid=13861)[0m f1_weighted: 0.39148536568588305
[2m[36m(func pid=13861)[0m f1_per_class: [0.132, 0.38, 0.264, 0.342, 0.05, 0.341, 0.506, 0.479, 0.15, 0.311]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2989738805970149
[2m[36m(func pid=13435)[0m top5: 0.8456156716417911
[2m[36m(func pid=13435)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=13435)[0m f1_macro: 0.3153273323646063
[2m[36m(func pid=13435)[0m f1_weighted: 0.29733248927172534
[2m[36m(func pid=13435)[0m f1_per_class: [0.234, 0.468, 0.786, 0.282, 0.069, 0.202, 0.275, 0.201, 0.196, 0.441]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.41884328358208955
[2m[36m(func pid=13050)[0m top5: 0.933768656716418
[2m[36m(func pid=13050)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=13050)[0m f1_macro: 0.38706687329749023
[2m[36m(func pid=13050)[0m f1_weighted: 0.4411456774595432
[2m[36m(func pid=13050)[0m f1_per_class: [0.549, 0.468, 0.358, 0.488, 0.111, 0.405, 0.419, 0.41, 0.36, 0.303]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8546 | Steps: 4 | Val loss: 2.2423 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.8614 | Steps: 4 | Val loss: 2.4428 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.4601 | Steps: 4 | Val loss: 4.1370 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.4779 | Steps: 4 | Val loss: 1.5431 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=32295)[0m top1: 0.18983208955223882
[2m[36m(func pid=32295)[0m top5: 0.6492537313432836
[2m[36m(func pid=32295)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=32295)[0m f1_macro: 0.12425045134684065
[2m[36m(func pid=32295)[0m f1_weighted: 0.19346025655974436
[2m[36m(func pid=32295)[0m f1_per_class: [0.042, 0.263, 0.148, 0.238, 0.015, 0.277, 0.14, 0.106, 0.013, 0.0]
[2m[36m(func pid=32295)[0m 
== Status ==
Current time: 2024-01-07 14:42:32 (running for 00:08:58.32)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.648 |      0.387 |                   89 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.657 |      0.315 |                   89 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.861 |      0.295 |                   90 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.855 |      0.124 |                   11 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.32649253731343286
[2m[36m(func pid=13861)[0m top5: 0.8777985074626866
[2m[36m(func pid=13861)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=13861)[0m f1_macro: 0.29523047067929237
[2m[36m(func pid=13861)[0m f1_weighted: 0.37738029429886927
[2m[36m(func pid=13861)[0m f1_per_class: [0.13, 0.384, 0.264, 0.34, 0.054, 0.309, 0.467, 0.475, 0.168, 0.362]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.30783582089552236
[2m[36m(func pid=13435)[0m top5: 0.8423507462686567
[2m[36m(func pid=13435)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=13435)[0m f1_macro: 0.317550338613629
[2m[36m(func pid=13435)[0m f1_weighted: 0.3158536623106201
[2m[36m(func pid=13435)[0m f1_per_class: [0.249, 0.448, 0.667, 0.262, 0.08, 0.206, 0.346, 0.307, 0.201, 0.411]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4398320895522388
[2m[36m(func pid=13050)[0m top5: 0.9356343283582089
[2m[36m(func pid=13050)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=13050)[0m f1_macro: 0.40326081129844604
[2m[36m(func pid=13050)[0m f1_weighted: 0.46271336751065323
[2m[36m(func pid=13050)[0m f1_per_class: [0.545, 0.488, 0.338, 0.5, 0.112, 0.412, 0.458, 0.424, 0.392, 0.364]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8449 | Steps: 4 | Val loss: 2.2415 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.2481 | Steps: 4 | Val loss: 2.5296 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.8395 | Steps: 4 | Val loss: 4.1514 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.5749 | Steps: 4 | Val loss: 1.5960 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=32295)[0m top1: 0.19402985074626866
[2m[36m(func pid=32295)[0m top5: 0.6501865671641791
[2m[36m(func pid=32295)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=32295)[0m f1_macro: 0.13854188827443603
[2m[36m(func pid=32295)[0m f1_weighted: 0.19729300002258257
[2m[36m(func pid=32295)[0m f1_per_class: [0.049, 0.258, 0.269, 0.253, 0.021, 0.299, 0.132, 0.104, 0.0, 0.0]
[2m[36m(func pid=32295)[0m 
== Status ==
Current time: 2024-01-07 14:42:38 (running for 00:09:03.58)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.478 |      0.403 |                   90 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.46  |      0.318 |                   90 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.248 |      0.264 |                   91 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.845 |      0.139 |                   12 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.2980410447761194
[2m[36m(func pid=13861)[0m top5: 0.867070895522388
[2m[36m(func pid=13861)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=13861)[0m f1_macro: 0.26396566511464026
[2m[36m(func pid=13861)[0m f1_weighted: 0.3478736000999342
[2m[36m(func pid=13861)[0m f1_per_class: [0.117, 0.288, 0.24, 0.316, 0.055, 0.3, 0.46, 0.467, 0.133, 0.262]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3125
[2m[36m(func pid=13435)[0m top5: 0.8376865671641791
[2m[36m(func pid=13435)[0m f1_micro: 0.3125
[2m[36m(func pid=13435)[0m f1_macro: 0.29422411475616
[2m[36m(func pid=13435)[0m f1_weighted: 0.32725053926870196
[2m[36m(func pid=13435)[0m f1_per_class: [0.251, 0.418, 0.605, 0.298, 0.087, 0.199, 0.405, 0.162, 0.184, 0.333]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.42630597014925375
[2m[36m(func pid=13050)[0m top5: 0.9221082089552238
[2m[36m(func pid=13050)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=13050)[0m f1_macro: 0.38374646600902856
[2m[36m(func pid=13050)[0m f1_weighted: 0.4481262668467012
[2m[36m(func pid=13050)[0m f1_per_class: [0.477, 0.497, 0.258, 0.496, 0.106, 0.382, 0.421, 0.458, 0.366, 0.377]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8354 | Steps: 4 | Val loss: 2.2399 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.4378 | Steps: 4 | Val loss: 2.4439 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.5471 | Steps: 4 | Val loss: 3.9243 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.6094 | Steps: 4 | Val loss: 1.5819 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=32295)[0m top1: 0.19076492537313433
[2m[36m(func pid=32295)[0m top5: 0.6576492537313433
[2m[36m(func pid=32295)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=32295)[0m f1_macro: 0.1438595633444081
[2m[36m(func pid=32295)[0m f1_weighted: 0.19834577568569586
[2m[36m(func pid=32295)[0m f1_per_class: [0.054, 0.243, 0.327, 0.275, 0.007, 0.29, 0.125, 0.097, 0.021, 0.0]
[2m[36m(func pid=32295)[0m 
== Status ==
Current time: 2024-01-07 14:42:43 (running for 00:09:08.77)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.575 |      0.384 |                   91 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.839 |      0.294 |                   91 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.438 |      0.273 |                   92 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.835 |      0.144 |                   13 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.310634328358209
[2m[36m(func pid=13861)[0m top5: 0.867070895522388
[2m[36m(func pid=13861)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=13861)[0m f1_macro: 0.2734177289613199
[2m[36m(func pid=13861)[0m f1_weighted: 0.3561194359804549
[2m[36m(func pid=13861)[0m f1_per_class: [0.117, 0.231, 0.289, 0.332, 0.06, 0.322, 0.489, 0.504, 0.137, 0.253]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.32369402985074625
[2m[36m(func pid=13435)[0m top5: 0.8507462686567164
[2m[36m(func pid=13435)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=13435)[0m f1_macro: 0.2751041099083621
[2m[36m(func pid=13435)[0m f1_weighted: 0.34399766845963026
[2m[36m(func pid=13435)[0m f1_per_class: [0.256, 0.369, 0.4, 0.326, 0.108, 0.205, 0.47, 0.141, 0.181, 0.296]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.44449626865671643
[2m[36m(func pid=13050)[0m top5: 0.9165111940298507
[2m[36m(func pid=13050)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=13050)[0m f1_macro: 0.3957681653679559
[2m[36m(func pid=13050)[0m f1_weighted: 0.4679135140215371
[2m[36m(func pid=13050)[0m f1_per_class: [0.477, 0.518, 0.255, 0.52, 0.115, 0.398, 0.443, 0.464, 0.371, 0.396]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7678 | Steps: 4 | Val loss: 2.2242 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 2.2956 | Steps: 4 | Val loss: 2.4795 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3227 | Steps: 4 | Val loss: 3.5564 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.5343 | Steps: 4 | Val loss: 1.5712 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=32295)[0m top1: 0.20009328358208955
[2m[36m(func pid=32295)[0m top5: 0.6772388059701493
[2m[36m(func pid=32295)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=32295)[0m f1_macro: 0.14841689574367284
[2m[36m(func pid=32295)[0m f1_weighted: 0.2078605138147647
[2m[36m(func pid=32295)[0m f1_per_class: [0.061, 0.254, 0.314, 0.291, 0.014, 0.299, 0.13, 0.109, 0.012, 0.0]
[2m[36m(func pid=32295)[0m 
== Status ==
Current time: 2024-01-07 14:42:48 (running for 00:09:14.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.609 |      0.396 |                   92 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.547 |      0.275 |                   92 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.296 |      0.264 |                   93 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.768 |      0.148 |                   14 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.28777985074626866
[2m[36m(func pid=13861)[0m top5: 0.8656716417910447
[2m[36m(func pid=13861)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=13861)[0m f1_macro: 0.2639135267113112
[2m[36m(func pid=13861)[0m f1_weighted: 0.3158009822271998
[2m[36m(func pid=13861)[0m f1_per_class: [0.125, 0.183, 0.377, 0.303, 0.068, 0.34, 0.411, 0.449, 0.119, 0.262]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.36473880597014924
[2m[36m(func pid=13435)[0m top5: 0.8591417910447762
[2m[36m(func pid=13435)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=13435)[0m f1_macro: 0.2923007098154262
[2m[36m(func pid=13435)[0m f1_weighted: 0.379621728912006
[2m[36m(func pid=13435)[0m f1_per_class: [0.321, 0.344, 0.361, 0.364, 0.127, 0.191, 0.565, 0.143, 0.221, 0.286]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4528917910447761
[2m[36m(func pid=13050)[0m top5: 0.9155783582089553
[2m[36m(func pid=13050)[0m f1_micro: 0.4528917910447761
[2m[36m(func pid=13050)[0m f1_macro: 0.38923101505089686
[2m[36m(func pid=13050)[0m f1_weighted: 0.47719772846111347
[2m[36m(func pid=13050)[0m f1_per_class: [0.435, 0.53, 0.247, 0.518, 0.125, 0.39, 0.481, 0.471, 0.326, 0.37]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7506 | Steps: 4 | Val loss: 2.2229 | Batch size: 32 | lr: 0.0001 | Duration: 3.30s
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.6493 | Steps: 4 | Val loss: 2.5978 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.0061 | Steps: 4 | Val loss: 3.4985 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.6035 | Steps: 4 | Val loss: 1.5417 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:42:53 (running for 00:09:19.24)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.534 |      0.389 |                   93 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.323 |      0.292 |                   93 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  2.296 |      0.264 |                   93 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.751 |      0.15  |                   15 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.1958955223880597
[2m[36m(func pid=32295)[0m top5: 0.6772388059701493
[2m[36m(func pid=32295)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=32295)[0m f1_macro: 0.15033277296991954
[2m[36m(func pid=32295)[0m f1_weighted: 0.2047206712883804
[2m[36m(func pid=32295)[0m f1_per_class: [0.061, 0.257, 0.348, 0.281, 0.019, 0.297, 0.129, 0.099, 0.013, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13861)[0m top1: 0.26399253731343286
[2m[36m(func pid=13861)[0m top5: 0.8642723880597015
[2m[36m(func pid=13861)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=13861)[0m f1_macro: 0.25163486861273765
[2m[36m(func pid=13861)[0m f1_weighted: 0.28278132234383035
[2m[36m(func pid=13861)[0m f1_per_class: [0.127, 0.185, 0.431, 0.306, 0.084, 0.331, 0.313, 0.38, 0.113, 0.247]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3521455223880597
[2m[36m(func pid=13435)[0m top5: 0.8577425373134329
[2m[36m(func pid=13435)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=13435)[0m f1_macro: 0.28549588377724505
[2m[36m(func pid=13435)[0m f1_weighted: 0.37049921827533777
[2m[36m(func pid=13435)[0m f1_per_class: [0.385, 0.29, 0.185, 0.354, 0.121, 0.244, 0.547, 0.169, 0.211, 0.348]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4603544776119403
[2m[36m(func pid=13050)[0m top5: 0.9211753731343284
[2m[36m(func pid=13050)[0m f1_micro: 0.4603544776119403
[2m[36m(func pid=13050)[0m f1_macro: 0.391538339240706
[2m[36m(func pid=13050)[0m f1_weighted: 0.48471092206042066
[2m[36m(func pid=13050)[0m f1_per_class: [0.424, 0.538, 0.224, 0.518, 0.138, 0.397, 0.501, 0.456, 0.33, 0.389]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.8682 | Steps: 4 | Val loss: 2.5124 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8070 | Steps: 4 | Val loss: 2.2175 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2438 | Steps: 4 | Val loss: 3.7433 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.6640 | Steps: 4 | Val loss: 1.5776 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:42:59 (running for 00:09:24.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.603 |      0.392 |                   94 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  1.006 |      0.285 |                   94 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.868 |      0.252 |                   95 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.751 |      0.15  |                   15 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.27611940298507465
[2m[36m(func pid=13861)[0m top5: 0.8652052238805971
[2m[36m(func pid=13861)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=13861)[0m f1_macro: 0.2519768290126314
[2m[36m(func pid=13861)[0m f1_weighted: 0.29415147009186227
[2m[36m(func pid=13861)[0m f1_per_class: [0.135, 0.221, 0.367, 0.361, 0.082, 0.305, 0.29, 0.375, 0.125, 0.259]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=32295)[0m top1: 0.20475746268656717
[2m[36m(func pid=32295)[0m top5: 0.6842350746268657
[2m[36m(func pid=32295)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=32295)[0m f1_macro: 0.1560258836444975
[2m[36m(func pid=32295)[0m f1_weighted: 0.21213117592307762
[2m[36m(func pid=32295)[0m f1_per_class: [0.069, 0.274, 0.333, 0.277, 0.021, 0.305, 0.139, 0.129, 0.013, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m top1: 0.33115671641791045
[2m[36m(func pid=13435)[0m top5: 0.8638059701492538
[2m[36m(func pid=13435)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=13435)[0m f1_macro: 0.278377452563884
[2m[36m(func pid=13435)[0m f1_weighted: 0.3475089129438835
[2m[36m(func pid=13435)[0m f1_per_class: [0.392, 0.295, 0.164, 0.31, 0.108, 0.159, 0.529, 0.23, 0.197, 0.4]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.46175373134328357
[2m[36m(func pid=13050)[0m top5: 0.914179104477612
[2m[36m(func pid=13050)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=13050)[0m f1_macro: 0.38875412974843854
[2m[36m(func pid=13050)[0m f1_weighted: 0.48516901765251463
[2m[36m(func pid=13050)[0m f1_per_class: [0.406, 0.543, 0.26, 0.517, 0.133, 0.38, 0.509, 0.457, 0.329, 0.353]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.9267 | Steps: 4 | Val loss: 2.3867 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7569 | Steps: 4 | Val loss: 2.2130 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4151 | Steps: 4 | Val loss: 3.8584 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.5330 | Steps: 4 | Val loss: 1.5743 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=13861)[0m top1: 0.2971082089552239
[2m[36m(func pid=13861)[0m top5: 0.8619402985074627
[2m[36m(func pid=13861)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=13861)[0m f1_macro: 0.2604651907087079
[2m[36m(func pid=13861)[0m f1_weighted: 0.31385162271916284
[2m[36m(func pid=13861)[0m f1_per_class: [0.145, 0.3, 0.369, 0.41, 0.077, 0.222, 0.289, 0.398, 0.147, 0.248]
[2m[36m(func pid=13861)[0m 
== Status ==
Current time: 2024-01-07 14:43:04 (running for 00:09:29.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.664 |      0.389 |                   95 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.244 |      0.278 |                   95 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.927 |      0.26  |                   96 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.807 |      0.156 |                   16 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2103544776119403
[2m[36m(func pid=32295)[0m top5: 0.6888992537313433
[2m[36m(func pid=32295)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=32295)[0m f1_macro: 0.16425279977446391
[2m[36m(func pid=32295)[0m f1_weighted: 0.2204103675497289
[2m[36m(func pid=32295)[0m f1_per_class: [0.089, 0.27, 0.348, 0.271, 0.027, 0.314, 0.166, 0.144, 0.013, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3148320895522388
[2m[36m(func pid=13435)[0m top5: 0.8763992537313433
[2m[36m(func pid=13435)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=13435)[0m f1_macro: 0.2785773221463611
[2m[36m(func pid=13435)[0m f1_weighted: 0.3321052702348047
[2m[36m(func pid=13435)[0m f1_per_class: [0.379, 0.303, 0.182, 0.35, 0.101, 0.15, 0.426, 0.303, 0.185, 0.406]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.457089552238806
[2m[36m(func pid=13050)[0m top5: 0.9081156716417911
[2m[36m(func pid=13050)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=13050)[0m f1_macro: 0.37625068227863717
[2m[36m(func pid=13050)[0m f1_weighted: 0.4814991053521865
[2m[36m(func pid=13050)[0m f1_per_class: [0.355, 0.547, 0.214, 0.526, 0.123, 0.357, 0.502, 0.439, 0.33, 0.368]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.4842 | Steps: 4 | Val loss: 2.3939 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7692 | Steps: 4 | Val loss: 2.1949 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.7145 | Steps: 4 | Val loss: 4.0120 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.5388 | Steps: 4 | Val loss: 1.5229 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:43:09 (running for 00:09:35.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.533 |      0.376 |                   96 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.415 |      0.279 |                   96 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.484 |      0.274 |                   97 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.757 |      0.164 |                   17 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3246268656716418
[2m[36m(func pid=13861)[0m top5: 0.8568097014925373
[2m[36m(func pid=13861)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=13861)[0m f1_macro: 0.273790281552207
[2m[36m(func pid=13861)[0m f1_weighted: 0.33976813016772056
[2m[36m(func pid=13861)[0m f1_per_class: [0.103, 0.381, 0.316, 0.394, 0.076, 0.262, 0.323, 0.424, 0.166, 0.294]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=32295)[0m top1: 0.23180970149253732
[2m[36m(func pid=32295)[0m top5: 0.7033582089552238
[2m[36m(func pid=32295)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=32295)[0m f1_macro: 0.17445807669293484
[2m[36m(func pid=32295)[0m f1_weighted: 0.2357365698955643
[2m[36m(func pid=32295)[0m f1_per_class: [0.131, 0.291, 0.327, 0.289, 0.016, 0.33, 0.18, 0.139, 0.016, 0.026]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m top1: 0.3003731343283582
[2m[36m(func pid=13435)[0m top5: 0.8759328358208955
[2m[36m(func pid=13435)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=13435)[0m f1_macro: 0.25378987769658035
[2m[36m(func pid=13435)[0m f1_weighted: 0.31494270058243
[2m[36m(func pid=13435)[0m f1_per_class: [0.194, 0.286, 0.149, 0.339, 0.093, 0.153, 0.4, 0.295, 0.212, 0.417]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.4822761194029851
[2m[36m(func pid=13050)[0m top5: 0.9183768656716418
[2m[36m(func pid=13050)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=13050)[0m f1_macro: 0.39444134167990563
[2m[36m(func pid=13050)[0m f1_weighted: 0.5014699855081198
[2m[36m(func pid=13050)[0m f1_per_class: [0.409, 0.55, 0.245, 0.538, 0.137, 0.328, 0.56, 0.434, 0.362, 0.382]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.5446 | Steps: 4 | Val loss: 2.1638 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7536 | Steps: 4 | Val loss: 2.2000 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3613 | Steps: 4 | Val loss: 3.9877 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:43:14 (running for 00:09:40.32)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.539 |      0.394 |                   97 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.254 |                   97 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.545 |      0.267 |                   98 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.769 |      0.174 |                   18 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.3362873134328358
[2m[36m(func pid=13861)[0m top5: 0.8666044776119403
[2m[36m(func pid=13861)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=13861)[0m f1_macro: 0.26707132271600836
[2m[36m(func pid=13861)[0m f1_weighted: 0.3553566864815685
[2m[36m(func pid=13861)[0m f1_per_class: [0.078, 0.362, 0.216, 0.436, 0.085, 0.213, 0.367, 0.437, 0.163, 0.314]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.5157 | Steps: 4 | Val loss: 1.5974 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=32295)[0m top1: 0.2178171641791045
[2m[36m(func pid=32295)[0m top5: 0.7019589552238806
[2m[36m(func pid=32295)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=32295)[0m f1_macro: 0.17496653405572937
[2m[36m(func pid=32295)[0m f1_weighted: 0.22201361369352182
[2m[36m(func pid=32295)[0m f1_per_class: [0.129, 0.28, 0.375, 0.275, 0.029, 0.311, 0.157, 0.153, 0.015, 0.026]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m top1: 0.31296641791044777
[2m[36m(func pid=13435)[0m top5: 0.8647388059701493
[2m[36m(func pid=13435)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=13435)[0m f1_macro: 0.24098762636133403
[2m[36m(func pid=13435)[0m f1_weighted: 0.33088905087100245
[2m[36m(func pid=13435)[0m f1_per_class: [0.094, 0.355, 0.103, 0.315, 0.111, 0.171, 0.432, 0.314, 0.273, 0.241]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.46875
[2m[36m(func pid=13050)[0m top5: 0.9039179104477612
[2m[36m(func pid=13050)[0m f1_micro: 0.46875
[2m[36m(func pid=13050)[0m f1_macro: 0.36704573445079447
[2m[36m(func pid=13050)[0m f1_weighted: 0.4912688664789585
[2m[36m(func pid=13050)[0m f1_per_class: [0.375, 0.538, 0.186, 0.552, 0.131, 0.252, 0.564, 0.416, 0.304, 0.353]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.6055 | Steps: 4 | Val loss: 2.1406 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6912 | Steps: 4 | Val loss: 2.1895 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.5584 | Steps: 4 | Val loss: 4.0462 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 14:43:19 (running for 00:09:45.55)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.516 |      0.367 |                   98 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.361 |      0.241 |                   98 |
| train_5ae7f_00002 | RUNNING    | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.605 |      0.277 |                   99 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.754 |      0.175 |                   19 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13861)[0m top1: 0.35401119402985076
[2m[36m(func pid=13861)[0m top5: 0.863339552238806
[2m[36m(func pid=13861)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=13861)[0m f1_macro: 0.27735061157976343
[2m[36m(func pid=13861)[0m f1_weighted: 0.3800379770647477
[2m[36m(func pid=13861)[0m f1_per_class: [0.051, 0.363, 0.189, 0.398, 0.089, 0.276, 0.452, 0.491, 0.18, 0.286]
[2m[36m(func pid=13861)[0m 
[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.5064 | Steps: 4 | Val loss: 1.6066 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=32295)[0m top1: 0.23227611940298507
[2m[36m(func pid=32295)[0m top5: 0.7145522388059702
[2m[36m(func pid=32295)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=32295)[0m f1_macro: 0.18172273531186348
[2m[36m(func pid=32295)[0m f1_weighted: 0.23854962184670417
[2m[36m(func pid=32295)[0m f1_per_class: [0.141, 0.284, 0.383, 0.306, 0.022, 0.331, 0.174, 0.148, 0.03, 0.0]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m top1: 0.30830223880597013
[2m[36m(func pid=13435)[0m top5: 0.8493470149253731
[2m[36m(func pid=13435)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=13435)[0m f1_macro: 0.24442287908892216
[2m[36m(func pid=13435)[0m f1_weighted: 0.33184458940188527
[2m[36m(func pid=13435)[0m f1_per_class: [0.061, 0.384, 0.09, 0.291, 0.111, 0.238, 0.411, 0.333, 0.318, 0.207]
[2m[36m(func pid=13435)[0m 
[2m[36m(func pid=13050)[0m top1: 0.46968283582089554
[2m[36m(func pid=13050)[0m top5: 0.902518656716418
[2m[36m(func pid=13050)[0m f1_micro: 0.46968283582089554
[2m[36m(func pid=13050)[0m f1_macro: 0.366193130545122
[2m[36m(func pid=13050)[0m f1_weighted: 0.4900131384477066
[2m[36m(func pid=13050)[0m f1_per_class: [0.444, 0.537, 0.185, 0.556, 0.132, 0.207, 0.571, 0.409, 0.3, 0.321]
[2m[36m(func pid=13050)[0m 
[2m[36m(func pid=13861)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.3269 | Steps: 4 | Val loss: 2.1539 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7533 | Steps: 4 | Val loss: 2.1918 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=13435)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.7145 | Steps: 4 | Val loss: 4.6049 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=13861)[0m top1: 0.35401119402985076
[2m[36m(func pid=13861)[0m top5: 0.8638059701492538
[2m[36m(func pid=13861)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=13861)[0m f1_macro: 0.27717392168944766
[2m[36m(func pid=13861)[0m f1_weighted: 0.38655413113833914
[2m[36m(func pid=13861)[0m f1_per_class: [0.059, 0.364, 0.121, 0.349, 0.093, 0.297, 0.503, 0.531, 0.186, 0.269]
== Status ==
Current time: 2024-01-07 14:43:25 (running for 00:09:50.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3215
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | RUNNING    | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.506 |      0.366 |                   99 |
| train_5ae7f_00001 | RUNNING    | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.558 |      0.244 |                   99 |
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.691 |      0.182 |                   20 |
| train_5ae7f_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13050)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.4328 | Steps: 4 | Val loss: 1.6397 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=32295)[0m top1: 0.22901119402985073
[2m[36m(func pid=32295)[0m top5: 0.7117537313432836
[2m[36m(func pid=32295)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=32295)[0m f1_macro: 0.1844955963710966
[2m[36m(func pid=32295)[0m f1_weighted: 0.23556008281683807
[2m[36m(func pid=32295)[0m f1_per_class: [0.155, 0.295, 0.353, 0.305, 0.029, 0.307, 0.159, 0.172, 0.04, 0.029]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=13435)[0m top1: 0.2887126865671642
[2m[36m(func pid=13435)[0m top5: 0.8185634328358209
[2m[36m(func pid=13435)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=13435)[0m f1_macro: 0.23473832008880416
[2m[36m(func pid=13435)[0m f1_weighted: 0.32052420017306016
[2m[36m(func pid=13435)[0m f1_per_class: [0.056, 0.383, 0.074, 0.282, 0.118, 0.244, 0.383, 0.308, 0.364, 0.136]
[2m[36m(func pid=13050)[0m top1: 0.46175373134328357
[2m[36m(func pid=13050)[0m top5: 0.8955223880597015
[2m[36m(func pid=13050)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=13050)[0m f1_macro: 0.3622998290281932
[2m[36m(func pid=13050)[0m f1_weighted: 0.4855451941093622
[2m[36m(func pid=13050)[0m f1_per_class: [0.423, 0.528, 0.179, 0.549, 0.133, 0.2, 0.566, 0.445, 0.299, 0.3]
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6942 | Steps: 4 | Val loss: 2.1845 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=32295)[0m top1: 0.2392723880597015
[2m[36m(func pid=32295)[0m top5: 0.7140858208955224
[2m[36m(func pid=32295)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=32295)[0m f1_macro: 0.19283263226432276
[2m[36m(func pid=32295)[0m f1_weighted: 0.24930457621332677
[2m[36m(func pid=32295)[0m f1_per_class: [0.162, 0.305, 0.375, 0.341, 0.03, 0.299, 0.169, 0.172, 0.025, 0.051]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37733)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=37733)[0m Configuration completed!
[2m[36m(func pid=37733)[0m New optimizer parameters:
[2m[36m(func pid=37733)[0m SGD (
[2m[36m(func pid=37733)[0m Parameter Group 0
[2m[36m(func pid=37733)[0m     dampening: 0
[2m[36m(func pid=37733)[0m     differentiable: False
[2m[36m(func pid=37733)[0m     foreach: None
[2m[36m(func pid=37733)[0m     lr: 0.001
[2m[36m(func pid=37733)[0m     maximize: False
[2m[36m(func pid=37733)[0m     momentum: 0.9
[2m[36m(func pid=37733)[0m     nesterov: False
[2m[36m(func pid=37733)[0m     weight_decay: 0
[2m[36m(func pid=37733)[0m )
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7215 | Steps: 4 | Val loss: 2.1888 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=37804)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37804)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=37804)[0m Configuration completed!
[2m[36m(func pid=37804)[0m New optimizer parameters:
[2m[36m(func pid=37804)[0m SGD (
[2m[36m(func pid=37804)[0m Parameter Group 0
[2m[36m(func pid=37804)[0m     dampening: 0
[2m[36m(func pid=37804)[0m     differentiable: False
[2m[36m(func pid=37804)[0m     foreach: None
[2m[36m(func pid=37804)[0m     lr: 0.01
[2m[36m(func pid=37804)[0m     maximize: False
[2m[36m(func pid=37804)[0m     momentum: 0.9
[2m[36m(func pid=37804)[0m     nesterov: False
[2m[36m(func pid=37804)[0m     weight_decay: 0
[2m[36m(func pid=37804)[0m )
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m top1: 0.22061567164179105
[2m[36m(func pid=32295)[0m top5: 0.7154850746268657
[2m[36m(func pid=32295)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=32295)[0m f1_macro: 0.18018031771729898
[2m[36m(func pid=32295)[0m f1_weighted: 0.2308842415622515
[2m[36m(func pid=32295)[0m f1_per_class: [0.135, 0.282, 0.321, 0.283, 0.031, 0.307, 0.168, 0.21, 0.015, 0.049]
== Status ==
Current time: 2024-01-07 14:43:31 (running for 00:09:57.48)
Memory usage on this node: 17.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.694 |      0.193 |                   22 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37854)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=37854)[0m Configuration completed!
[2m[36m(func pid=37854)[0m New optimizer parameters:
[2m[36m(func pid=37854)[0m SGD (
[2m[36m(func pid=37854)[0m Parameter Group 0
[2m[36m(func pid=37854)[0m     dampening: 0
[2m[36m(func pid=37854)[0m     differentiable: False
[2m[36m(func pid=37854)[0m     foreach: None
[2m[36m(func pid=37854)[0m     lr: 0.1
[2m[36m(func pid=37854)[0m     maximize: False
[2m[36m(func pid=37854)[0m     momentum: 0.9
[2m[36m(func pid=37854)[0m     nesterov: False
[2m[36m(func pid=37854)[0m     weight_decay: 0
[2m[36m(func pid=37854)[0m )
[2m[36m(func pid=37854)[0m 
== Status ==
Current time: 2024-01-07 14:43:37 (running for 00:10:02.95)
Memory usage on this node: 23.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.722 |      0.18  |                   23 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |        |            |                      |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |        |            |                      |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9244 | Steps: 4 | Val loss: 2.3378 | Batch size: 32 | lr: 0.001 | Duration: 5.05s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9634 | Steps: 4 | Val loss: 2.4164 | Batch size: 32 | lr: 0.01 | Duration: 4.61s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7298 | Steps: 4 | Val loss: 2.1829 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37733)[0m top1: 0.17024253731343283
[2m[36m(func pid=37733)[0m top5: 0.5237873134328358
[2m[36m(func pid=37733)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=37733)[0m f1_macro: 0.11427200851928668
[2m[36m(func pid=37733)[0m f1_weighted: 0.1214479921529444
[2m[36m(func pid=37733)[0m f1_per_class: [0.275, 0.318, 0.0, 0.091, 0.01, 0.257, 0.0, 0.086, 0.0, 0.105]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0145 | Steps: 4 | Val loss: 2.4691 | Batch size: 32 | lr: 0.1 | Duration: 4.66s
== Status ==
Current time: 2024-01-07 14:43:42 (running for 00:10:08.18)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.722 |      0.18  |                   23 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.924 |      0.114 |                    1 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  2.963 |      0.082 |                    1 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |        |            |                      |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m top1: 0.08582089552238806
[2m[36m(func pid=37804)[0m top5: 0.4146455223880597
[2m[36m(func pid=37804)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=37804)[0m f1_macro: 0.08229385372779649
[2m[36m(func pid=37804)[0m f1_weighted: 0.052900406936256215
[2m[36m(func pid=37804)[0m f1_per_class: [0.082, 0.086, 0.169, 0.021, 0.076, 0.11, 0.009, 0.227, 0.0, 0.043]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m top1: 0.22527985074626866
[2m[36m(func pid=32295)[0m top5: 0.722481343283582
[2m[36m(func pid=32295)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=32295)[0m f1_macro: 0.18609256394830204
[2m[36m(func pid=32295)[0m f1_weighted: 0.2373365069010155
[2m[36m(func pid=32295)[0m f1_per_class: [0.152, 0.288, 0.339, 0.289, 0.038, 0.292, 0.184, 0.214, 0.016, 0.049]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9609 | Steps: 4 | Val loss: 2.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=37854)[0m top1: 0.0480410447761194
[2m[36m(func pid=37854)[0m top5: 0.691231343283582
[2m[36m(func pid=37854)[0m f1_micro: 0.0480410447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.050593627181087675
[2m[36m(func pid=37854)[0m f1_weighted: 0.04271810698907213
[2m[36m(func pid=37854)[0m f1_per_class: [0.265, 0.211, 0.015, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.5847 | Steps: 4 | Val loss: 2.4012 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6515 | Steps: 4 | Val loss: 2.1742 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=37733)[0m top1: 0.19309701492537312
[2m[36m(func pid=37733)[0m top5: 0.5531716417910447
[2m[36m(func pid=37733)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=37733)[0m f1_macro: 0.13860879984001886
[2m[36m(func pid=37733)[0m f1_weighted: 0.1428269436770314
[2m[36m(func pid=37733)[0m f1_per_class: [0.236, 0.329, 0.062, 0.08, 0.023, 0.31, 0.033, 0.196, 0.024, 0.093]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.2932 | Steps: 4 | Val loss: 3.2477 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37804)[0m top1: 0.08069029850746269
[2m[36m(func pid=37804)[0m top5: 0.5457089552238806
[2m[36m(func pid=37804)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=37804)[0m f1_macro: 0.09195717887530633
[2m[36m(func pid=37804)[0m f1_weighted: 0.058650524402011454
[2m[36m(func pid=37804)[0m f1_per_class: [0.103, 0.016, 0.055, 0.087, 0.042, 0.032, 0.0, 0.396, 0.0, 0.189]
== Status ==
Current time: 2024-01-07 14:43:48 (running for 00:10:13.69)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.73  |      0.186 |                   24 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.961 |      0.139 |                    2 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  2.585 |      0.092 |                    2 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.014 |      0.051 |                    1 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m top1: 0.23367537313432835
[2m[36m(func pid=32295)[0m top5: 0.726679104477612
[2m[36m(func pid=32295)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=32295)[0m f1_macro: 0.19035066822647567
[2m[36m(func pid=32295)[0m f1_weighted: 0.2456449680105876
[2m[36m(func pid=32295)[0m f1_per_class: [0.139, 0.297, 0.392, 0.3, 0.034, 0.308, 0.195, 0.198, 0.015, 0.026]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8631 | Steps: 4 | Val loss: 2.2452 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=37854)[0m top1: 0.11753731343283583
[2m[36m(func pid=37854)[0m top5: 0.6105410447761194
[2m[36m(func pid=37854)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=37854)[0m f1_macro: 0.06340296711537037
[2m[36m(func pid=37854)[0m f1_weighted: 0.1336757940389235
[2m[36m(func pid=37854)[0m f1_per_class: [0.046, 0.0, 0.098, 0.032, 0.0, 0.0, 0.41, 0.0, 0.014, 0.034]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.1230 | Steps: 4 | Val loss: 2.1205 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6320 | Steps: 4 | Val loss: 2.1740 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=37733)[0m top1: 0.2140858208955224
[2m[36m(func pid=37733)[0m top5: 0.652518656716418
[2m[36m(func pid=37733)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=37733)[0m f1_macro: 0.18279400185746147
[2m[36m(func pid=37733)[0m f1_weighted: 0.18577371831249548
[2m[36m(func pid=37733)[0m f1_per_class: [0.249, 0.302, 0.242, 0.132, 0.02, 0.379, 0.107, 0.226, 0.0, 0.17]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8521 | Steps: 4 | Val loss: 46.3364 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:43:53 (running for 00:10:19.11)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.652 |      0.19  |                   25 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.863 |      0.183 |                    3 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  2.123 |      0.157 |                    3 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.293 |      0.063 |                    2 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m top1: 0.1707089552238806
[2m[36m(func pid=37804)[0m top5: 0.7509328358208955
[2m[36m(func pid=37804)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=37804)[0m f1_macro: 0.15720424246011683
[2m[36m(func pid=37804)[0m f1_weighted: 0.1569448207495936
[2m[36m(func pid=37804)[0m f1_per_class: [0.276, 0.178, 0.053, 0.31, 0.074, 0.117, 0.0, 0.278, 0.028, 0.258]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m top1: 0.25
[2m[36m(func pid=32295)[0m top5: 0.7290111940298507
[2m[36m(func pid=32295)[0m f1_micro: 0.25
[2m[36m(func pid=32295)[0m f1_macro: 0.20359263498146393
[2m[36m(func pid=32295)[0m f1_weighted: 0.2616131196829514
[2m[36m(func pid=32295)[0m f1_per_class: [0.164, 0.309, 0.435, 0.361, 0.036, 0.301, 0.182, 0.206, 0.015, 0.027]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m top1: 0.010727611940298507
[2m[36m(func pid=37854)[0m top5: 0.3871268656716418
[2m[36m(func pid=37854)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=37854)[0m f1_macro: 0.01098476890633539
[2m[36m(func pid=37854)[0m f1_weighted: 0.008636386327094438
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.046, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7799 | Steps: 4 | Val loss: 2.2238 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0705 | Steps: 4 | Val loss: 1.7935 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6944 | Steps: 4 | Val loss: 2.1642 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37733)[0m top1: 0.20009328358208955
[2m[36m(func pid=37733)[0m top5: 0.65625
[2m[36m(func pid=37733)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=37733)[0m f1_macro: 0.15262063619910718
[2m[36m(func pid=37733)[0m f1_weighted: 0.18716747322208221
[2m[36m(func pid=37733)[0m f1_per_class: [0.18, 0.294, 0.235, 0.17, 0.026, 0.174, 0.186, 0.118, 0.0, 0.143]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 4.1224 | Steps: 4 | Val loss: 9534.8477 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=37804)[0m top1: 0.3041044776119403
[2m[36m(func pid=37804)[0m top5: 0.8596082089552238
[2m[36m(func pid=37804)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=37804)[0m f1_macro: 0.25003339122857676
[2m[36m(func pid=37804)[0m f1_weighted: 0.27527256511024906
[2m[36m(func pid=37804)[0m f1_per_class: [0.473, 0.245, 0.094, 0.541, 0.123, 0.299, 0.039, 0.321, 0.11, 0.255]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:43:59 (running for 00:10:24.73)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.632 |      0.204 |                   26 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.78  |      0.153 |                    4 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  2.07  |      0.25  |                    4 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.852 |      0.011 |                    3 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.240205223880597
[2m[36m(func pid=32295)[0m top5: 0.7439365671641791
[2m[36m(func pid=32295)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=32295)[0m f1_macro: 0.19947775920662103
[2m[36m(func pid=32295)[0m f1_weighted: 0.25701585480673095
[2m[36m(func pid=32295)[0m f1_per_class: [0.131, 0.323, 0.44, 0.316, 0.046, 0.291, 0.21, 0.184, 0.015, 0.038]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m top1: 0.006063432835820896
[2m[36m(func pid=37854)[0m top5: 0.5093283582089553
[2m[36m(func pid=37854)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=37854)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=37854)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7074 | Steps: 4 | Val loss: 2.1531 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.5887 | Steps: 4 | Val loss: 1.5957 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6692 | Steps: 4 | Val loss: 2.1662 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37733)[0m top1: 0.2635261194029851
[2m[36m(func pid=37733)[0m top5: 0.7551305970149254
[2m[36m(func pid=37733)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=37733)[0m f1_macro: 0.20258746268355096
[2m[36m(func pid=37733)[0m f1_weighted: 0.27171321512437124
[2m[36m(func pid=37733)[0m f1_per_class: [0.325, 0.299, 0.253, 0.249, 0.019, 0.263, 0.355, 0.077, 0.0, 0.186]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.7217 | Steps: 4 | Val loss: 925.2689 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:44:04 (running for 00:10:30.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.694 |      0.199 |                   27 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.707 |      0.203 |                    5 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.589 |      0.399 |                    5 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  4.122 |      0.001 |                    4 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m top1: 0.4193097014925373
[2m[36m(func pid=37804)[0m top5: 0.9053171641791045
[2m[36m(func pid=37804)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=37804)[0m f1_macro: 0.3990310749091913
[2m[36m(func pid=37804)[0m f1_weighted: 0.4168261379095778
[2m[36m(func pid=37804)[0m f1_per_class: [0.5, 0.176, 0.774, 0.509, 0.19, 0.453, 0.479, 0.404, 0.202, 0.302]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m top1: 0.23507462686567165
[2m[36m(func pid=32295)[0m top5: 0.7364738805970149
[2m[36m(func pid=32295)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=32295)[0m f1_macro: 0.19688600792311692
[2m[36m(func pid=32295)[0m f1_weighted: 0.24794469031480754
[2m[36m(func pid=32295)[0m f1_per_class: [0.129, 0.331, 0.431, 0.307, 0.045, 0.272, 0.186, 0.214, 0.015, 0.038]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m top1: 0.22667910447761194
[2m[36m(func pid=37854)[0m top5: 0.7262126865671642
[2m[36m(func pid=37854)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.05965454018079295
[2m[36m(func pid=37854)[0m f1_weighted: 0.16277642983441448
[2m[36m(func pid=37854)[0m f1_per_class: [0.014, 0.0, 0.0, 0.583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6625 | Steps: 4 | Val loss: 2.0900 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6515 | Steps: 4 | Val loss: 2.1656 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3164 | Steps: 4 | Val loss: 1.4679 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.5946 | Steps: 4 | Val loss: 311.3416 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=37733)[0m top1: 0.3125
[2m[36m(func pid=37733)[0m top5: 0.8120335820895522
[2m[36m(func pid=37733)[0m f1_micro: 0.3125
[2m[36m(func pid=37733)[0m f1_macro: 0.23740273994914776
[2m[36m(func pid=37733)[0m f1_weighted: 0.33055439622085275
[2m[36m(func pid=37733)[0m f1_per_class: [0.4, 0.294, 0.183, 0.357, 0.0, 0.343, 0.402, 0.152, 0.024, 0.219]
[2m[36m(func pid=37733)[0m 
== Status ==
Current time: 2024-01-07 14:44:10 (running for 00:10:35.63)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.651 |      0.197 |                   29 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.663 |      0.237 |                    6 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.589 |      0.399 |                    5 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.722 |      0.06  |                    5 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.23694029850746268
[2m[36m(func pid=32295)[0m top5: 0.7392723880597015
[2m[36m(func pid=32295)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=32295)[0m f1_macro: 0.19722325368859553
[2m[36m(func pid=32295)[0m f1_weighted: 0.2494484791888415
[2m[36m(func pid=32295)[0m f1_per_class: [0.133, 0.331, 0.4, 0.306, 0.037, 0.281, 0.187, 0.22, 0.016, 0.062]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.4449626865671642
[2m[36m(func pid=37804)[0m top5: 0.9272388059701493
[2m[36m(func pid=37804)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=37804)[0m f1_macro: 0.39194180372004533
[2m[36m(func pid=37804)[0m f1_weighted: 0.4181635433674087
[2m[36m(func pid=37804)[0m f1_per_class: [0.476, 0.096, 0.706, 0.566, 0.228, 0.464, 0.465, 0.468, 0.19, 0.261]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.03544776119402985
[2m[36m(func pid=37854)[0m top5: 0.534981343283582
[2m[36m(func pid=37854)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=37854)[0m f1_macro: 0.00944726120729607
[2m[36m(func pid=37854)[0m f1_weighted: 0.0073087506866687515
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4861 | Steps: 4 | Val loss: 2.0272 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6450 | Steps: 4 | Val loss: 2.1645 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.0792 | Steps: 4 | Val loss: 1.5046 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=37733)[0m top1: 0.3656716417910448
[2m[36m(func pid=37733)[0m top5: 0.8353544776119403
[2m[36m(func pid=37733)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=37733)[0m f1_macro: 0.2571722317133121
[2m[36m(func pid=37733)[0m f1_weighted: 0.35547851170541156
[2m[36m(func pid=37733)[0m f1_per_class: [0.426, 0.169, 0.229, 0.514, 0.034, 0.33, 0.408, 0.172, 0.0, 0.289]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.4067 | Steps: 4 | Val loss: 54.8387 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:44:15 (running for 00:10:41.08)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.645 |      0.196 |                   30 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.486 |      0.257 |                    7 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.316 |      0.392 |                    6 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.595 |      0.009 |                    6 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.23647388059701493
[2m[36m(func pid=32295)[0m top5: 0.7378731343283582
[2m[36m(func pid=32295)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=32295)[0m f1_macro: 0.19636367283713713
[2m[36m(func pid=32295)[0m f1_weighted: 0.24799127906378546
[2m[36m(func pid=32295)[0m f1_per_class: [0.148, 0.318, 0.373, 0.337, 0.047, 0.282, 0.159, 0.218, 0.017, 0.065]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.40718283582089554
[2m[36m(func pid=37804)[0m top5: 0.9407649253731343
[2m[36m(func pid=37804)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=37804)[0m f1_macro: 0.3527312616351749
[2m[36m(func pid=37804)[0m f1_weighted: 0.4196537425892051
[2m[36m(func pid=37804)[0m f1_per_class: [0.565, 0.384, 0.48, 0.508, 0.143, 0.301, 0.447, 0.368, 0.159, 0.171]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.1646455223880597
[2m[36m(func pid=37854)[0m top5: 0.5620335820895522
[2m[36m(func pid=37854)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=37854)[0m f1_macro: 0.032523997741389046
[2m[36m(func pid=37854)[0m f1_weighted: 0.053690469167432184
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.286, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5240 | Steps: 4 | Val loss: 1.9821 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5635 | Steps: 4 | Val loss: 2.1641 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9821 | Steps: 4 | Val loss: 1.8179 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7505 | Steps: 4 | Val loss: 16.2306 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37733)[0m top1: 0.3931902985074627
[2m[36m(func pid=37733)[0m top5: 0.8381529850746269
[2m[36m(func pid=37733)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=37733)[0m f1_macro: 0.268496066958023
[2m[36m(func pid=37733)[0m f1_weighted: 0.3558760016478357
[2m[36m(func pid=37733)[0m f1_per_class: [0.495, 0.081, 0.216, 0.556, 0.075, 0.356, 0.398, 0.195, 0.019, 0.293]
[2m[36m(func pid=37733)[0m 
== Status ==
Current time: 2024-01-07 14:44:20 (running for 00:10:46.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.564 |      0.203 |                   31 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.524 |      0.268 |                    8 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.079 |      0.353 |                    7 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  4.407 |      0.033 |                    7 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2392723880597015
[2m[36m(func pid=32295)[0m top5: 0.7378731343283582
[2m[36m(func pid=32295)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=32295)[0m f1_macro: 0.20260763044808572
[2m[36m(func pid=32295)[0m f1_weighted: 0.2507088278303129
[2m[36m(func pid=32295)[0m f1_per_class: [0.158, 0.318, 0.386, 0.322, 0.045, 0.3, 0.172, 0.22, 0.032, 0.072]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.35261194029850745
[2m[36m(func pid=37804)[0m top5: 0.8936567164179104
[2m[36m(func pid=37804)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=37804)[0m f1_macro: 0.30693505985610525
[2m[36m(func pid=37804)[0m f1_weighted: 0.3469200084829072
[2m[36m(func pid=37804)[0m f1_per_class: [0.369, 0.446, 0.229, 0.414, 0.158, 0.247, 0.271, 0.388, 0.316, 0.233]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.032182835820895525
[2m[36m(func pid=37854)[0m top5: 0.5736940298507462
[2m[36m(func pid=37854)[0m f1_micro: 0.032182835820895525
[2m[36m(func pid=37854)[0m f1_macro: 0.018685573150780666
[2m[36m(func pid=37854)[0m f1_weighted: 0.027205725406347126
[2m[36m(func pid=37854)[0m f1_per_class: [0.005, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4664 | Steps: 4 | Val loss: 1.9488 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5943 | Steps: 4 | Val loss: 2.1622 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7544 | Steps: 4 | Val loss: 2.1180 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5612 | Steps: 4 | Val loss: 4.3727 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37733)[0m top1: 0.3987873134328358
[2m[36m(func pid=37733)[0m top5: 0.8334888059701493
[2m[36m(func pid=37733)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=37733)[0m f1_macro: 0.2874009465635891
[2m[36m(func pid=37733)[0m f1_weighted: 0.3631453944948169
[2m[36m(func pid=37733)[0m f1_per_class: [0.549, 0.088, 0.238, 0.583, 0.107, 0.271, 0.4, 0.297, 0.02, 0.32]
[2m[36m(func pid=37733)[0m 
== Status ==
Current time: 2024-01-07 14:44:26 (running for 00:10:51.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.594 |      0.21  |                   32 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.466 |      0.287 |                    9 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.982 |      0.307 |                    8 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.751 |      0.019 |                    8 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.24486940298507462
[2m[36m(func pid=32295)[0m top5: 0.7322761194029851
[2m[36m(func pid=32295)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=32295)[0m f1_macro: 0.2099821091530653
[2m[36m(func pid=32295)[0m f1_weighted: 0.25496862043859003
[2m[36m(func pid=32295)[0m f1_per_class: [0.208, 0.319, 0.367, 0.35, 0.052, 0.281, 0.16, 0.231, 0.031, 0.1]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.279384328358209
[2m[36m(func pid=37804)[0m top5: 0.875
[2m[36m(func pid=37804)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=37804)[0m f1_macro: 0.21345975436782375
[2m[36m(func pid=37804)[0m f1_weighted: 0.31639527000024026
[2m[36m(func pid=37804)[0m f1_per_class: [0.239, 0.28, 0.0, 0.388, 0.068, 0.255, 0.359, 0.163, 0.205, 0.177]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.08395522388059702
[2m[36m(func pid=37854)[0m top5: 0.6198694029850746
[2m[36m(func pid=37854)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=37854)[0m f1_macro: 0.03919479691368533
[2m[36m(func pid=37854)[0m f1_weighted: 0.051262636024621396
[2m[36m(func pid=37854)[0m f1_per_class: [0.023, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.066, 0.0, 0.032]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2457 | Steps: 4 | Val loss: 1.9517 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.5907 | Steps: 4 | Val loss: 2.1599 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.0463 | Steps: 4 | Val loss: 1.6736 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6127 | Steps: 4 | Val loss: 2.9139 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=37733)[0m top1: 0.3670708955223881
[2m[36m(func pid=37733)[0m top5: 0.8278917910447762
[2m[36m(func pid=37733)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=37733)[0m f1_macro: 0.28637751490382485
[2m[36m(func pid=37733)[0m f1_weighted: 0.3431918503415931
[2m[36m(func pid=37733)[0m f1_per_class: [0.605, 0.08, 0.264, 0.568, 0.087, 0.292, 0.336, 0.329, 0.02, 0.283]
[2m[36m(func pid=37733)[0m 
== Status ==
Current time: 2024-01-07 14:44:31 (running for 00:10:57.38)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.591 |      0.209 |                   33 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.246 |      0.286 |                   10 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.754 |      0.213 |                    9 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.561 |      0.039 |                    9 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2462686567164179
[2m[36m(func pid=32295)[0m top5: 0.7406716417910447
[2m[36m(func pid=32295)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=32295)[0m f1_macro: 0.20937300373277007
[2m[36m(func pid=32295)[0m f1_weighted: 0.2604864413768904
[2m[36m(func pid=32295)[0m f1_per_class: [0.179, 0.324, 0.333, 0.326, 0.041, 0.313, 0.189, 0.229, 0.036, 0.125]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.4239738805970149
[2m[36m(func pid=37804)[0m top5: 0.9211753731343284
[2m[36m(func pid=37804)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=37804)[0m f1_macro: 0.30500799406412726
[2m[36m(func pid=37804)[0m f1_weighted: 0.43283803928859127
[2m[36m(func pid=37804)[0m f1_per_class: [0.403, 0.315, 0.205, 0.51, 0.106, 0.256, 0.594, 0.125, 0.288, 0.249]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.09048507462686567
[2m[36m(func pid=37854)[0m top5: 0.6879664179104478
[2m[36m(func pid=37854)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=37854)[0m f1_macro: 0.14969375755228825
[2m[36m(func pid=37854)[0m f1_weighted: 0.06081054456043875
[2m[36m(func pid=37854)[0m f1_per_class: [0.056, 0.199, 0.727, 0.0, 0.157, 0.0, 0.0, 0.34, 0.0, 0.018]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2442 | Steps: 4 | Val loss: 1.9629 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5695 | Steps: 4 | Val loss: 2.1504 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7725 | Steps: 4 | Val loss: 2.3612 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.9736 | Steps: 4 | Val loss: 2.4490 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=37733)[0m top1: 0.34794776119402987
[2m[36m(func pid=37733)[0m top5: 0.8274253731343284
[2m[36m(func pid=37733)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=37733)[0m f1_macro: 0.28026432258176953
[2m[36m(func pid=37733)[0m f1_weighted: 0.3339178909547518
[2m[36m(func pid=37733)[0m f1_per_class: [0.579, 0.092, 0.233, 0.584, 0.07, 0.232, 0.3, 0.339, 0.078, 0.296]
[2m[36m(func pid=37733)[0m 
== Status ==
Current time: 2024-01-07 14:44:37 (running for 00:11:02.73)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.57  |      0.222 |                   34 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.244 |      0.28  |                   11 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.046 |      0.305 |                   10 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.613 |      0.15  |                   10 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2644589552238806
[2m[36m(func pid=32295)[0m top5: 0.7439365671641791
[2m[36m(func pid=32295)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=32295)[0m f1_macro: 0.22215307121630196
[2m[36m(func pid=32295)[0m f1_weighted: 0.27481273959097846
[2m[36m(func pid=32295)[0m f1_per_class: [0.199, 0.342, 0.379, 0.362, 0.047, 0.33, 0.186, 0.228, 0.018, 0.13]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.28125
[2m[36m(func pid=37804)[0m top5: 0.847481343283582
[2m[36m(func pid=37804)[0m f1_micro: 0.28125
[2m[36m(func pid=37804)[0m f1_macro: 0.22459589930156704
[2m[36m(func pid=37804)[0m f1_weighted: 0.28315992082752395
[2m[36m(func pid=37804)[0m f1_per_class: [0.163, 0.412, 0.414, 0.255, 0.126, 0.0, 0.418, 0.045, 0.108, 0.305]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.11054104477611941
[2m[36m(func pid=37854)[0m top5: 0.7495335820895522
[2m[36m(func pid=37854)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=37854)[0m f1_macro: 0.10496816699345264
[2m[36m(func pid=37854)[0m f1_weighted: 0.12669042084776436
[2m[36m(func pid=37854)[0m f1_per_class: [0.055, 0.133, 0.0, 0.251, 0.0, 0.0, 0.0, 0.522, 0.063, 0.026]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2110 | Steps: 4 | Val loss: 1.9670 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6644 | Steps: 4 | Val loss: 2.1493 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7516 | Steps: 4 | Val loss: 3.7593 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6332 | Steps: 4 | Val loss: 2.4227 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:44:42 (running for 00:11:07.91)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.57  |      0.222 |                   34 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.211 |      0.257 |                   12 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.772 |      0.225 |                   11 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.974 |      0.105 |                   11 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m top1: 0.32369402985074625
[2m[36m(func pid=37733)[0m top5: 0.8101679104477612
[2m[36m(func pid=37733)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=37733)[0m f1_macro: 0.2567324311242834
[2m[36m(func pid=37733)[0m f1_weighted: 0.2921822140882524
[2m[36m(func pid=37733)[0m f1_per_class: [0.541, 0.078, 0.218, 0.582, 0.071, 0.215, 0.182, 0.326, 0.078, 0.276]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m top1: 0.2579291044776119
[2m[36m(func pid=32295)[0m top5: 0.7504664179104478
[2m[36m(func pid=32295)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=32295)[0m f1_macro: 0.2152285560047673
[2m[36m(func pid=32295)[0m f1_weighted: 0.2718739040563994
[2m[36m(func pid=32295)[0m f1_per_class: [0.179, 0.323, 0.367, 0.367, 0.038, 0.322, 0.185, 0.237, 0.033, 0.102]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.13059701492537312
[2m[36m(func pid=37804)[0m top5: 0.6375932835820896
[2m[36m(func pid=37804)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=37804)[0m f1_macro: 0.12937012471401046
[2m[36m(func pid=37804)[0m f1_weighted: 0.1317404337341168
[2m[36m(func pid=37804)[0m f1_per_class: [0.322, 0.41, 0.0, 0.084, 0.163, 0.0, 0.085, 0.0, 0.081, 0.148]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.14972014925373134
[2m[36m(func pid=37854)[0m top5: 0.7611940298507462
[2m[36m(func pid=37854)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=37854)[0m f1_macro: 0.10496919399337698
[2m[36m(func pid=37854)[0m f1_weighted: 0.1565326342240352
[2m[36m(func pid=37854)[0m f1_per_class: [0.096, 0.088, 0.0, 0.175, 0.0, 0.0, 0.221, 0.398, 0.043, 0.029]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0919 | Steps: 4 | Val loss: 1.9461 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6264 | Steps: 4 | Val loss: 2.1482 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.0358 | Steps: 4 | Val loss: 3.8388 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4914 | Steps: 4 | Val loss: 2.2200 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:44:47 (running for 00:11:13.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.626 |      0.219 |                   36 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.211 |      0.257 |                   12 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.752 |      0.129 |                   12 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.633 |      0.105 |                   12 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.259794776119403
[2m[36m(func pid=32295)[0m top5: 0.7472014925373134
[2m[36m(func pid=32295)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=32295)[0m f1_macro: 0.2188958344965609
[2m[36m(func pid=32295)[0m f1_weighted: 0.27345039622473444
[2m[36m(func pid=32295)[0m f1_per_class: [0.194, 0.315, 0.355, 0.381, 0.056, 0.315, 0.18, 0.238, 0.044, 0.11]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.31763059701492535
[2m[36m(func pid=37733)[0m top5: 0.8222947761194029
[2m[36m(func pid=37733)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=37733)[0m f1_macro: 0.2584117412861371
[2m[36m(func pid=37733)[0m f1_weighted: 0.2906680746615138
[2m[36m(func pid=37733)[0m f1_per_class: [0.522, 0.12, 0.222, 0.579, 0.074, 0.193, 0.161, 0.331, 0.096, 0.286]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m top1: 0.11986940298507463
[2m[36m(func pid=37804)[0m top5: 0.636660447761194
[2m[36m(func pid=37804)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=37804)[0m f1_macro: 0.12752517892266263
[2m[36m(func pid=37804)[0m f1_weighted: 0.09844631842122595
[2m[36m(func pid=37804)[0m f1_per_class: [0.192, 0.432, 0.245, 0.045, 0.127, 0.0, 0.003, 0.0, 0.077, 0.155]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.18097014925373134
[2m[36m(func pid=37854)[0m top5: 0.8003731343283582
[2m[36m(func pid=37854)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=37854)[0m f1_macro: 0.13118937415394574
[2m[36m(func pid=37854)[0m f1_weighted: 0.1818198616450798
[2m[36m(func pid=37854)[0m f1_per_class: [0.103, 0.176, 0.0, 0.086, 0.09, 0.063, 0.315, 0.368, 0.055, 0.057]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6059 | Steps: 4 | Val loss: 2.1372 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.0263 | Steps: 4 | Val loss: 1.9939 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7230 | Steps: 4 | Val loss: 2.9708 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5740 | Steps: 4 | Val loss: 2.1391 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:44:53 (running for 00:11:18.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.606 |      0.218 |                   37 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.092 |      0.258 |                   13 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.036 |      0.128 |                   13 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.491 |      0.131 |                   13 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.26492537313432835
[2m[36m(func pid=32295)[0m top5: 0.7583955223880597
[2m[36m(func pid=32295)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=32295)[0m f1_macro: 0.21826916644399552
[2m[36m(func pid=32295)[0m f1_weighted: 0.2786135272088702
[2m[36m(func pid=32295)[0m f1_per_class: [0.185, 0.333, 0.328, 0.378, 0.047, 0.341, 0.185, 0.225, 0.031, 0.129]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.2719216417910448
[2m[36m(func pid=37733)[0m top5: 0.7989738805970149
[2m[36m(func pid=37733)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=37733)[0m f1_macro: 0.23676492354381842
[2m[36m(func pid=37733)[0m f1_weighted: 0.24319136473275368
[2m[36m(func pid=37733)[0m f1_per_class: [0.245, 0.07, 0.407, 0.497, 0.072, 0.198, 0.118, 0.336, 0.112, 0.312]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m top1: 0.1828358208955224
[2m[36m(func pid=37804)[0m top5: 0.7434701492537313
[2m[36m(func pid=37804)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=37804)[0m f1_macro: 0.1502961255792491
[2m[36m(func pid=37804)[0m f1_weighted: 0.1946889696458658
[2m[36m(func pid=37804)[0m f1_per_class: [0.129, 0.26, 0.078, 0.085, 0.099, 0.246, 0.291, 0.0, 0.187, 0.129]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.2355410447761194
[2m[36m(func pid=37854)[0m top5: 0.7938432835820896
[2m[36m(func pid=37854)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.1947361401017607
[2m[36m(func pid=37854)[0m f1_weighted: 0.21285952163427826
[2m[36m(func pid=37854)[0m f1_per_class: [0.101, 0.174, 0.579, 0.01, 0.0, 0.224, 0.419, 0.389, 0.053, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6681 | Steps: 4 | Val loss: 2.1392 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0734 | Steps: 4 | Val loss: 1.9801 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5813 | Steps: 4 | Val loss: 3.7518 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.1842 | Steps: 4 | Val loss: 2.2016 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:44:58 (running for 00:11:24.17)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.606 |      0.218 |                   37 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  2.073 |      0.235 |                   15 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.723 |      0.15  |                   14 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.574 |      0.195 |                   14 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m top1: 0.23833955223880596
[2m[36m(func pid=37733)[0m top5: 0.8180970149253731
[2m[36m(func pid=37733)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=37733)[0m f1_macro: 0.23462715135640844
[2m[36m(func pid=37733)[0m f1_weighted: 0.23074367312643287
[2m[36m(func pid=37733)[0m f1_per_class: [0.185, 0.105, 0.381, 0.408, 0.066, 0.295, 0.101, 0.337, 0.159, 0.31]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m top1: 0.2635261194029851
[2m[36m(func pid=32295)[0m top5: 0.7546641791044776
[2m[36m(func pid=32295)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=32295)[0m f1_macro: 0.21173554288941995
[2m[36m(func pid=32295)[0m f1_weighted: 0.27701370997946445
[2m[36m(func pid=32295)[0m f1_per_class: [0.176, 0.32, 0.282, 0.368, 0.053, 0.366, 0.185, 0.245, 0.033, 0.088]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.134794776119403
[2m[36m(func pid=37804)[0m top5: 0.7989738805970149
[2m[36m(func pid=37804)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=37804)[0m f1_macro: 0.14113944756609356
[2m[36m(func pid=37804)[0m f1_weighted: 0.12592446467134122
[2m[36m(func pid=37804)[0m f1_per_class: [0.167, 0.124, 0.037, 0.157, 0.075, 0.016, 0.091, 0.298, 0.237, 0.209]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.17397388059701493
[2m[36m(func pid=37854)[0m top5: 0.777518656716418
[2m[36m(func pid=37854)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=37854)[0m f1_macro: 0.15385169791771253
[2m[36m(func pid=37854)[0m f1_weighted: 0.1289203235319321
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.308, 0.783, 0.035, 0.033, 0.013, 0.16, 0.206, 0.0, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9829 | Steps: 4 | Val loss: 1.9929 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5568 | Steps: 4 | Val loss: 2.1374 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5889 | Steps: 4 | Val loss: 3.7552 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3054 | Steps: 4 | Val loss: 1.9169 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:45:04 (running for 00:11:29.68)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.668 |      0.212 |                   38 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.983 |      0.232 |                   16 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.581 |      0.141 |                   15 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.184 |      0.154 |                   15 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m top1: 0.22014925373134328
[2m[36m(func pid=37733)[0m top5: 0.8041044776119403
[2m[36m(func pid=37733)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=37733)[0m f1_macro: 0.23220846736409265
[2m[36m(func pid=37733)[0m f1_weighted: 0.2244625138453137
[2m[36m(func pid=37733)[0m f1_per_class: [0.164, 0.223, 0.338, 0.261, 0.064, 0.326, 0.135, 0.365, 0.16, 0.286]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m top1: 0.2560634328358209
[2m[36m(func pid=32295)[0m top5: 0.7541977611940298
[2m[36m(func pid=32295)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=32295)[0m f1_macro: 0.22445962534426694
[2m[36m(func pid=32295)[0m f1_weighted: 0.27032448486434507
[2m[36m(func pid=32295)[0m f1_per_class: [0.199, 0.344, 0.393, 0.3, 0.063, 0.34, 0.222, 0.211, 0.053, 0.119]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m top1: 0.16930970149253732
[2m[36m(func pid=37804)[0m top5: 0.7332089552238806
[2m[36m(func pid=37804)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=37804)[0m f1_macro: 0.13355958003835647
[2m[36m(func pid=37804)[0m f1_weighted: 0.20350821162600816
[2m[36m(func pid=37804)[0m f1_per_class: [0.071, 0.261, 0.052, 0.197, 0.051, 0.0, 0.307, 0.028, 0.192, 0.176]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.37593283582089554
[2m[36m(func pid=37854)[0m top5: 0.7952425373134329
[2m[36m(func pid=37854)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=37854)[0m f1_macro: 0.20432323030145846
[2m[36m(func pid=37854)[0m f1_weighted: 0.33589040883836585
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.267, 0.143, 0.246, 0.048, 0.201, 0.551, 0.544, 0.043, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5250 | Steps: 4 | Val loss: 2.1340 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.8226 | Steps: 4 | Val loss: 1.9716 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3609 | Steps: 4 | Val loss: 3.4647 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.5573 | Steps: 4 | Val loss: 1.9807 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:45:09 (running for 00:11:35.03)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.525 |      0.228 |                   40 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.983 |      0.232 |                   16 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.589 |      0.134 |                   16 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.305 |      0.204 |                   16 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2644589552238806
[2m[36m(func pid=32295)[0m top5: 0.7569962686567164
[2m[36m(func pid=32295)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=32295)[0m f1_macro: 0.2283253120774727
[2m[36m(func pid=32295)[0m f1_weighted: 0.2786380754600017
[2m[36m(func pid=32295)[0m f1_per_class: [0.194, 0.336, 0.386, 0.313, 0.06, 0.379, 0.225, 0.235, 0.037, 0.118]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.24300373134328357
[2m[36m(func pid=37733)[0m top5: 0.832089552238806
[2m[36m(func pid=37733)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=37733)[0m f1_macro: 0.25407843928188745
[2m[36m(func pid=37733)[0m f1_weighted: 0.27045511919127824
[2m[36m(func pid=37733)[0m f1_per_class: [0.169, 0.171, 0.258, 0.295, 0.052, 0.348, 0.262, 0.406, 0.235, 0.343]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m top1: 0.2042910447761194
[2m[36m(func pid=37804)[0m top5: 0.8218283582089553
[2m[36m(func pid=37804)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=37804)[0m f1_macro: 0.19373910430407448
[2m[36m(func pid=37804)[0m f1_weighted: 0.1876379242744788
[2m[36m(func pid=37804)[0m f1_per_class: [0.261, 0.374, 0.066, 0.084, 0.06, 0.141, 0.161, 0.29, 0.322, 0.179]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.20055970149253732
[2m[36m(func pid=37854)[0m top5: 0.7994402985074627
[2m[36m(func pid=37854)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=37854)[0m f1_macro: 0.10047465463839593
[2m[36m(func pid=37854)[0m f1_weighted: 0.1833096886754009
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.294, 0.0, 0.384, 0.064, 0.201, 0.0, 0.0, 0.061, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5828 | Steps: 4 | Val loss: 2.1307 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7443 | Steps: 4 | Val loss: 1.9284 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3397 | Steps: 4 | Val loss: 2.8303 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.0718 | Steps: 4 | Val loss: 2.0716 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=32295)[0m top1: 0.2644589552238806
[2m[36m(func pid=32295)[0m top5: 0.7691231343283582
[2m[36m(func pid=32295)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=32295)[0m f1_macro: 0.2265235799384572
[2m[36m(func pid=32295)[0m f1_weighted: 0.283580750853936
[2m[36m(func pid=32295)[0m f1_per_class: [0.168, 0.341, 0.367, 0.323, 0.072, 0.352, 0.243, 0.214, 0.063, 0.122]
== Status ==
Current time: 2024-01-07 14:45:14 (running for 00:11:40.50)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.583 |      0.227 |                   41 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.823 |      0.254 |                   17 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.361 |      0.194 |                   17 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.557 |      0.1   |                   17 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.2835820895522388
[2m[36m(func pid=37733)[0m top5: 0.8409514925373134
[2m[36m(func pid=37733)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=37733)[0m f1_macro: 0.27811423053077444
[2m[36m(func pid=37733)[0m f1_weighted: 0.3238660041194222
[2m[36m(func pid=37733)[0m f1_per_class: [0.214, 0.203, 0.238, 0.365, 0.053, 0.312, 0.359, 0.451, 0.244, 0.342]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m top1: 0.30130597014925375
[2m[36m(func pid=37804)[0m top5: 0.8819962686567164
[2m[36m(func pid=37804)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=37804)[0m f1_macro: 0.26918111472118056
[2m[36m(func pid=37804)[0m f1_weighted: 0.3094949495861742
[2m[36m(func pid=37804)[0m f1_per_class: [0.351, 0.378, 0.08, 0.213, 0.155, 0.175, 0.415, 0.346, 0.275, 0.305]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.22667910447761194
[2m[36m(func pid=37854)[0m top5: 0.7882462686567164
[2m[36m(func pid=37854)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.11026189914058926
[2m[36m(func pid=37854)[0m f1_weighted: 0.20447801996524598
[2m[36m(func pid=37854)[0m f1_per_class: [0.038, 0.244, 0.0, 0.471, 0.055, 0.248, 0.0, 0.0, 0.046, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4983 | Steps: 4 | Val loss: 2.1273 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.6704 | Steps: 4 | Val loss: 1.8479 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5206 | Steps: 4 | Val loss: 2.0581 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.9361 | Steps: 4 | Val loss: 2.3657 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 14:45:20 (running for 00:11:46.04)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.498 |      0.228 |                   42 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.744 |      0.278 |                   18 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.34  |      0.269 |                   18 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.072 |      0.11  |                   18 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.27005597014925375
[2m[36m(func pid=32295)[0m top5: 0.769589552238806
[2m[36m(func pid=32295)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=32295)[0m f1_macro: 0.22792550611150553
[2m[36m(func pid=32295)[0m f1_weighted: 0.2888934902722318
[2m[36m(func pid=32295)[0m f1_per_class: [0.183, 0.338, 0.355, 0.351, 0.067, 0.356, 0.233, 0.231, 0.047, 0.119]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.345615671641791
[2m[36m(func pid=37733)[0m top5: 0.8582089552238806
[2m[36m(func pid=37733)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=37733)[0m f1_macro: 0.3198448362286336
[2m[36m(func pid=37733)[0m f1_weighted: 0.3920371902423325
[2m[36m(func pid=37733)[0m f1_per_class: [0.338, 0.246, 0.338, 0.471, 0.055, 0.327, 0.455, 0.426, 0.222, 0.319]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.24207089552238806
[2m[36m(func pid=37854)[0m top5: 0.8027052238805971
[2m[36m(func pid=37854)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=37854)[0m f1_macro: 0.13383774956158287
[2m[36m(func pid=37854)[0m f1_weighted: 0.1959912743750255
[2m[36m(func pid=37854)[0m f1_per_class: [0.079, 0.0, 0.0, 0.549, 0.053, 0.194, 0.0, 0.261, 0.06, 0.143]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.3843283582089552
[2m[36m(func pid=37804)[0m top5: 0.8535447761194029
[2m[36m(func pid=37804)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=37804)[0m f1_macro: 0.3112682792440673
[2m[36m(func pid=37804)[0m f1_weighted: 0.38681129466284975
[2m[36m(func pid=37804)[0m f1_per_class: [0.41, 0.456, 0.372, 0.447, 0.144, 0.07, 0.461, 0.265, 0.254, 0.233]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5649 | Steps: 4 | Val loss: 2.1304 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7116 | Steps: 4 | Val loss: 1.7704 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4092 | Steps: 4 | Val loss: 1.8358 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8415 | Steps: 4 | Val loss: 3.2426 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 14:45:25 (running for 00:11:51.49)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.565 |      0.232 |                   43 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.67  |      0.32  |                   19 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.936 |      0.311 |                   19 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.521 |      0.134 |                   19 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.27005597014925375
[2m[36m(func pid=32295)[0m top5: 0.7625932835820896
[2m[36m(func pid=32295)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=32295)[0m f1_macro: 0.23205705058223441
[2m[36m(func pid=32295)[0m f1_weighted: 0.2897835366792423
[2m[36m(func pid=32295)[0m f1_per_class: [0.202, 0.329, 0.361, 0.367, 0.048, 0.35, 0.226, 0.242, 0.017, 0.178]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.3917910447761194
[2m[36m(func pid=37733)[0m top5: 0.8759328358208955
[2m[36m(func pid=37733)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=37733)[0m f1_macro: 0.34319104675511963
[2m[36m(func pid=37733)[0m f1_weighted: 0.42808331849833337
[2m[36m(func pid=37733)[0m f1_per_class: [0.372, 0.292, 0.329, 0.499, 0.076, 0.377, 0.496, 0.438, 0.272, 0.282]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.32649253731343286
[2m[36m(func pid=37854)[0m top5: 0.7957089552238806
[2m[36m(func pid=37854)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=37854)[0m f1_macro: 0.24595058784658114
[2m[36m(func pid=37854)[0m f1_weighted: 0.3217292493572052
[2m[36m(func pid=37854)[0m f1_per_class: [0.043, 0.272, 0.87, 0.529, 0.059, 0.0, 0.337, 0.35, 0.0, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.18330223880597016
[2m[36m(func pid=37804)[0m top5: 0.8292910447761194
[2m[36m(func pid=37804)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=37804)[0m f1_macro: 0.1725985953198534
[2m[36m(func pid=37804)[0m f1_weighted: 0.16525169564536088
[2m[36m(func pid=37804)[0m f1_per_class: [0.359, 0.216, 0.0, 0.19, 0.055, 0.142, 0.092, 0.295, 0.098, 0.279]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5799 | Steps: 4 | Val loss: 2.1224 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.7164 | Steps: 4 | Val loss: 1.7383 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4405 | Steps: 4 | Val loss: 2.0971 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2489 | Steps: 4 | Val loss: 3.1924 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:45:31 (running for 00:11:56.89)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.58  |      0.225 |                   44 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.712 |      0.343 |                   20 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.841 |      0.173 |                   20 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.409 |      0.246 |                   20 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2593283582089552
[2m[36m(func pid=32295)[0m top5: 0.7658582089552238
[2m[36m(func pid=32295)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=32295)[0m f1_macro: 0.22530288764293332
[2m[36m(func pid=32295)[0m f1_weighted: 0.2838642546754911
[2m[36m(func pid=32295)[0m f1_per_class: [0.183, 0.331, 0.407, 0.309, 0.053, 0.348, 0.267, 0.215, 0.034, 0.105]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.40531716417910446
[2m[36m(func pid=37733)[0m top5: 0.8838619402985075
[2m[36m(func pid=37733)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=37733)[0m f1_macro: 0.34668688694663874
[2m[36m(func pid=37733)[0m f1_weighted: 0.4288223025454645
[2m[36m(func pid=37733)[0m f1_per_class: [0.337, 0.266, 0.381, 0.547, 0.08, 0.419, 0.457, 0.448, 0.208, 0.324]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.25652985074626866
[2m[36m(func pid=37854)[0m top5: 0.7439365671641791
[2m[36m(func pid=37854)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=37854)[0m f1_macro: 0.16691180034055159
[2m[36m(func pid=37854)[0m f1_weighted: 0.21200924684966455
[2m[36m(func pid=37854)[0m f1_per_class: [0.154, 0.445, 0.185, 0.09, 0.057, 0.0, 0.278, 0.343, 0.069, 0.049]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.19449626865671643
[2m[36m(func pid=37804)[0m top5: 0.7751865671641791
[2m[36m(func pid=37804)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=37804)[0m f1_macro: 0.16895810639162254
[2m[36m(func pid=37804)[0m f1_weighted: 0.2247618213455152
[2m[36m(func pid=37804)[0m f1_per_class: [0.132, 0.254, 0.089, 0.094, 0.053, 0.231, 0.359, 0.184, 0.174, 0.119]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5359 | Steps: 4 | Val loss: 2.1137 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5570 | Steps: 4 | Val loss: 1.7363 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5180 | Steps: 4 | Val loss: 1.9830 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.4612 | Steps: 4 | Val loss: 2.7885 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:45:36 (running for 00:12:02.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.536 |      0.235 |                   45 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.716 |      0.347 |                   21 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.249 |      0.169 |                   21 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.44  |      0.167 |                   21 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.271455223880597
[2m[36m(func pid=32295)[0m top5: 0.769589552238806
[2m[36m(func pid=32295)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=32295)[0m f1_macro: 0.2350261734131313
[2m[36m(func pid=32295)[0m f1_weighted: 0.292166871045316
[2m[36m(func pid=32295)[0m f1_per_class: [0.184, 0.345, 0.393, 0.342, 0.058, 0.358, 0.249, 0.226, 0.019, 0.175]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.39598880597014924
[2m[36m(func pid=37733)[0m top5: 0.8861940298507462
[2m[36m(func pid=37733)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=37733)[0m f1_macro: 0.3471413046985049
[2m[36m(func pid=37733)[0m f1_weighted: 0.415443495876843
[2m[36m(func pid=37733)[0m f1_per_class: [0.39, 0.238, 0.358, 0.555, 0.069, 0.421, 0.41, 0.467, 0.236, 0.327]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.16277985074626866
[2m[36m(func pid=37854)[0m top5: 0.7388059701492538
[2m[36m(func pid=37854)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=37854)[0m f1_macro: 0.17201141275931195
[2m[36m(func pid=37854)[0m f1_weighted: 0.09450529151232666
[2m[36m(func pid=37854)[0m f1_per_class: [0.228, 0.316, 0.585, 0.0, 0.056, 0.101, 0.003, 0.267, 0.078, 0.085]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.25699626865671643
[2m[36m(func pid=37804)[0m top5: 0.7882462686567164
[2m[36m(func pid=37804)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=37804)[0m f1_macro: 0.20277773097396037
[2m[36m(func pid=37804)[0m f1_weighted: 0.263149737489981
[2m[36m(func pid=37804)[0m f1_per_class: [0.129, 0.33, 0.069, 0.167, 0.066, 0.133, 0.38, 0.307, 0.221, 0.225]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5077 | Steps: 4 | Val loss: 2.1093 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.5449 | Steps: 4 | Val loss: 1.7451 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.1156 | Steps: 4 | Val loss: 2.1150 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:45:42 (running for 00:12:07.76)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.508 |      0.238 |                   46 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.557 |      0.347 |                   22 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.461 |      0.203 |                   22 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.518 |      0.172 |                   22 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.27611940298507465
[2m[36m(func pid=32295)[0m top5: 0.7831156716417911
[2m[36m(func pid=32295)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=32295)[0m f1_macro: 0.2376180673822066
[2m[36m(func pid=32295)[0m f1_weighted: 0.2992342896775943
[2m[36m(func pid=32295)[0m f1_per_class: [0.177, 0.329, 0.373, 0.36, 0.055, 0.382, 0.25, 0.254, 0.038, 0.158]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.2268 | Steps: 4 | Val loss: 3.0084 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=37733)[0m top1: 0.3880597014925373
[2m[36m(func pid=37733)[0m top5: 0.8791977611940298
[2m[36m(func pid=37733)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=37733)[0m f1_macro: 0.33411712993624265
[2m[36m(func pid=37733)[0m f1_weighted: 0.4084894247445194
[2m[36m(func pid=37733)[0m f1_per_class: [0.374, 0.274, 0.218, 0.556, 0.075, 0.398, 0.376, 0.445, 0.286, 0.339]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.1417910447761194
[2m[36m(func pid=37854)[0m top5: 0.6851679104477612
[2m[36m(func pid=37854)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.15041373548464848
[2m[36m(func pid=37854)[0m f1_weighted: 0.08838536821918272
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.268, 0.632, 0.0, 0.043, 0.147, 0.0, 0.308, 0.107, 0.0]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.2630597014925373
[2m[36m(func pid=37804)[0m top5: 0.7411380597014925
[2m[36m(func pid=37804)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=37804)[0m f1_macro: 0.1998156176889762
[2m[36m(func pid=37804)[0m f1_weighted: 0.26813019876600613
[2m[36m(func pid=37804)[0m f1_per_class: [0.423, 0.366, 0.0, 0.169, 0.08, 0.016, 0.417, 0.276, 0.176, 0.076]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4905 | Steps: 4 | Val loss: 2.1035 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3945 | Steps: 4 | Val loss: 1.7548 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6356 | Steps: 4 | Val loss: 2.2362 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=32295)[0m top1: 0.279384328358209
[2m[36m(func pid=32295)[0m top5: 0.7807835820895522
[2m[36m(func pid=32295)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=32295)[0m f1_macro: 0.24053806773351516
[2m[36m(func pid=32295)[0m f1_weighted: 0.3044798601238505
[2m[36m(func pid=32295)[0m f1_per_class: [0.184, 0.335, 0.373, 0.37, 0.059, 0.336, 0.27, 0.245, 0.068, 0.167]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0930 | Steps: 4 | Val loss: 3.9703 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:45:47 (running for 00:12:13.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.49  |      0.241 |                   47 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.545 |      0.334 |                   23 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.227 |      0.2   |                   23 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  3.116 |      0.15  |                   23 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m top1: 0.37173507462686567
[2m[36m(func pid=37733)[0m top5: 0.8736007462686567
[2m[36m(func pid=37733)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=37733)[0m f1_macro: 0.33832446742693145
[2m[36m(func pid=37733)[0m f1_weighted: 0.3994969203827708
[2m[36m(func pid=37733)[0m f1_per_class: [0.504, 0.334, 0.186, 0.506, 0.074, 0.36, 0.359, 0.492, 0.266, 0.302]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.13759328358208955
[2m[36m(func pid=37854)[0m top5: 0.6688432835820896
[2m[36m(func pid=37854)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=37854)[0m f1_macro: 0.08909109631606567
[2m[36m(func pid=37854)[0m f1_weighted: 0.0811917020309923
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.285, 0.0, 0.0, 0.042, 0.038, 0.0, 0.428, 0.072, 0.026]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.14132462686567165
[2m[36m(func pid=37804)[0m top5: 0.7952425373134329
[2m[36m(func pid=37804)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=37804)[0m f1_macro: 0.11833705537009728
[2m[36m(func pid=37804)[0m f1_weighted: 0.117496879415307
[2m[36m(func pid=37804)[0m f1_per_class: [0.0, 0.283, 0.051, 0.105, 0.066, 0.054, 0.034, 0.3, 0.076, 0.214]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.5170 | Steps: 4 | Val loss: 2.1004 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.3269 | Steps: 4 | Val loss: 1.7582 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.2849 | Steps: 4 | Val loss: 2.0813 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:45:53 (running for 00:12:18.68)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.517 |      0.24  |                   48 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.395 |      0.338 |                   24 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  2.093 |      0.118 |                   24 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.636 |      0.089 |                   24 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2826492537313433
[2m[36m(func pid=32295)[0m top5: 0.7901119402985075
[2m[36m(func pid=32295)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=32295)[0m f1_macro: 0.24012694833036335
[2m[36m(func pid=32295)[0m f1_weighted: 0.30821595365210797
[2m[36m(func pid=32295)[0m f1_per_class: [0.19, 0.319, 0.344, 0.392, 0.054, 0.344, 0.27, 0.237, 0.068, 0.185]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5558 | Steps: 4 | Val loss: 4.2789 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=37733)[0m top1: 0.3726679104477612
[2m[36m(func pid=37733)[0m top5: 0.871268656716418
[2m[36m(func pid=37733)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=37733)[0m f1_macro: 0.34758850564104943
[2m[36m(func pid=37733)[0m f1_weighted: 0.4025147556831738
[2m[36m(func pid=37733)[0m f1_per_class: [0.556, 0.381, 0.233, 0.48, 0.075, 0.373, 0.361, 0.459, 0.295, 0.263]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.2583955223880597
[2m[36m(func pid=37854)[0m top5: 0.7038246268656716
[2m[36m(func pid=37854)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=37854)[0m f1_macro: 0.14139905510555578
[2m[36m(func pid=37854)[0m f1_weighted: 0.18127708025007996
[2m[36m(func pid=37854)[0m f1_per_class: [0.183, 0.454, 0.0, 0.249, 0.0, 0.035, 0.0, 0.404, 0.071, 0.019]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.1166044776119403
[2m[36m(func pid=37804)[0m top5: 0.7164179104477612
[2m[36m(func pid=37804)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=37804)[0m f1_macro: 0.11122862337877346
[2m[36m(func pid=37804)[0m f1_weighted: 0.08991862169932929
[2m[36m(func pid=37804)[0m f1_per_class: [0.0, 0.116, 0.041, 0.086, 0.09, 0.061, 0.048, 0.271, 0.164, 0.235]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4708 | Steps: 4 | Val loss: 2.1006 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.4427 | Steps: 4 | Val loss: 1.7462 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4393 | Steps: 4 | Val loss: 2.0353 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:45:58 (running for 00:12:24.30)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.471 |      0.245 |                   49 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.327 |      0.348 |                   25 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.556 |      0.111 |                   25 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.285 |      0.141 |                   25 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2826492537313433
[2m[36m(func pid=32295)[0m top5: 0.7915111940298507
[2m[36m(func pid=32295)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=32295)[0m f1_macro: 0.24497506501595573
[2m[36m(func pid=32295)[0m f1_weighted: 0.3069912273981123
[2m[36m(func pid=32295)[0m f1_per_class: [0.201, 0.305, 0.349, 0.393, 0.052, 0.354, 0.262, 0.26, 0.068, 0.206]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8136 | Steps: 4 | Val loss: 5.4338 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=37733)[0m top1: 0.3810634328358209
[2m[36m(func pid=37733)[0m top5: 0.8773320895522388
[2m[36m(func pid=37733)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=37733)[0m f1_macro: 0.3450691875355291
[2m[36m(func pid=37733)[0m f1_weighted: 0.411747144275737
[2m[36m(func pid=37733)[0m f1_per_class: [0.552, 0.402, 0.197, 0.48, 0.08, 0.353, 0.385, 0.495, 0.283, 0.224]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.1921641791044776
[2m[36m(func pid=37854)[0m top5: 0.6805037313432836
[2m[36m(func pid=37854)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=37854)[0m f1_macro: 0.12310445967089971
[2m[36m(func pid=37854)[0m f1_weighted: 0.1352895101490805
[2m[36m(func pid=37854)[0m f1_per_class: [0.088, 0.427, 0.0, 0.064, 0.0, 0.129, 0.0, 0.423, 0.085, 0.015]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.09888059701492537
[2m[36m(func pid=37804)[0m top5: 0.6389925373134329
[2m[36m(func pid=37804)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=37804)[0m f1_macro: 0.15250152427326344
[2m[36m(func pid=37804)[0m f1_weighted: 0.06826720038538826
[2m[36m(func pid=37804)[0m f1_per_class: [0.043, 0.005, 0.564, 0.01, 0.027, 0.0, 0.113, 0.297, 0.176, 0.29]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4166 | Steps: 4 | Val loss: 2.0834 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3507 | Steps: 4 | Val loss: 1.7247 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9861 | Steps: 4 | Val loss: 1.9214 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8006 | Steps: 4 | Val loss: 5.0480 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:46:04 (running for 00:12:29.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.417 |      0.251 |                   50 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.443 |      0.345 |                   26 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.814 |      0.153 |                   26 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.439 |      0.123 |                   26 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2868470149253731
[2m[36m(func pid=32295)[0m top5: 0.8017723880597015
[2m[36m(func pid=32295)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=32295)[0m f1_macro: 0.2511018778461306
[2m[36m(func pid=32295)[0m f1_weighted: 0.3140200582661135
[2m[36m(func pid=32295)[0m f1_per_class: [0.213, 0.296, 0.373, 0.388, 0.052, 0.36, 0.293, 0.24, 0.083, 0.212]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.3787313432835821
[2m[36m(func pid=37733)[0m top5: 0.8782649253731343
[2m[36m(func pid=37733)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=37733)[0m f1_macro: 0.33523958363499307
[2m[36m(func pid=37733)[0m f1_weighted: 0.40668439588052674
[2m[36m(func pid=37733)[0m f1_per_class: [0.516, 0.392, 0.209, 0.503, 0.085, 0.367, 0.354, 0.493, 0.252, 0.181]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.2653917910447761
[2m[36m(func pid=37854)[0m top5: 0.7821828358208955
[2m[36m(func pid=37854)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=37854)[0m f1_macro: 0.1493429856767135
[2m[36m(func pid=37854)[0m f1_weighted: 0.24367345865579979
[2m[36m(func pid=37854)[0m f1_per_class: [0.063, 0.221, 0.0, 0.536, 0.106, 0.057, 0.081, 0.401, 0.0, 0.028]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.11520522388059702
[2m[36m(func pid=37804)[0m top5: 0.5573694029850746
[2m[36m(func pid=37804)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=37804)[0m f1_macro: 0.19468199843196893
[2m[36m(func pid=37804)[0m f1_weighted: 0.11222527244372109
[2m[36m(func pid=37804)[0m f1_per_class: [0.067, 0.146, 0.667, 0.003, 0.024, 0.021, 0.169, 0.325, 0.136, 0.388]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4524 | Steps: 4 | Val loss: 2.0868 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.2860 | Steps: 4 | Val loss: 1.6760 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5578 | Steps: 4 | Val loss: 1.9814 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:46:09 (running for 00:12:35.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.452 |      0.25  |                   51 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.351 |      0.335 |                   27 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.801 |      0.195 |                   27 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.986 |      0.149 |                   27 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.28078358208955223
[2m[36m(func pid=32295)[0m top5: 0.7980410447761194
[2m[36m(func pid=32295)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=32295)[0m f1_macro: 0.25033667422318906
[2m[36m(func pid=32295)[0m f1_weighted: 0.3123305235909746
[2m[36m(func pid=32295)[0m f1_per_class: [0.194, 0.287, 0.407, 0.36, 0.057, 0.38, 0.317, 0.229, 0.065, 0.208]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8093 | Steps: 4 | Val loss: 5.4451 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=37733)[0m top1: 0.40111940298507465
[2m[36m(func pid=37733)[0m top5: 0.8889925373134329
[2m[36m(func pid=37733)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=37733)[0m f1_macro: 0.35063711160132927
[2m[36m(func pid=37733)[0m f1_weighted: 0.4290947070875009
[2m[36m(func pid=37733)[0m f1_per_class: [0.526, 0.417, 0.253, 0.512, 0.091, 0.375, 0.406, 0.467, 0.24, 0.219]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.32136194029850745
[2m[36m(func pid=37854)[0m top5: 0.7751865671641791
[2m[36m(func pid=37854)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=37854)[0m f1_macro: 0.19968436570572715
[2m[36m(func pid=37854)[0m f1_weighted: 0.36029992080466233
[2m[36m(func pid=37854)[0m f1_per_class: [0.04, 0.17, 0.0, 0.505, 0.047, 0.209, 0.449, 0.53, 0.0, 0.046]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.14832089552238806
[2m[36m(func pid=37804)[0m top5: 0.5625
[2m[36m(func pid=37804)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=37804)[0m f1_macro: 0.10689596327188418
[2m[36m(func pid=37804)[0m f1_weighted: 0.10311090106247672
[2m[36m(func pid=37804)[0m f1_per_class: [0.207, 0.311, 0.151, 0.126, 0.025, 0.0, 0.012, 0.0, 0.122, 0.114]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4097 | Steps: 4 | Val loss: 2.0830 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.2280 | Steps: 4 | Val loss: 1.6874 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.2825 | Steps: 4 | Val loss: 2.0668 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:46:15 (running for 00:12:40.79)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.41  |      0.254 |                   52 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.286 |      0.351 |                   28 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.809 |      0.107 |                   28 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.558 |      0.2   |                   28 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.28638059701492535
[2m[36m(func pid=32295)[0m top5: 0.7999067164179104
[2m[36m(func pid=32295)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=32295)[0m f1_macro: 0.25408541808135365
[2m[36m(func pid=32295)[0m f1_weighted: 0.3184742328629014
[2m[36m(func pid=32295)[0m f1_per_class: [0.219, 0.273, 0.407, 0.394, 0.052, 0.357, 0.316, 0.235, 0.105, 0.182]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m top1: 0.3987873134328358
[2m[36m(func pid=37733)[0m top5: 0.8815298507462687
[2m[36m(func pid=37733)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=37733)[0m f1_macro: 0.35032359901948296
[2m[36m(func pid=37733)[0m f1_weighted: 0.4280536320282836
[2m[36m(func pid=37733)[0m f1_per_class: [0.504, 0.361, 0.216, 0.536, 0.089, 0.4, 0.399, 0.463, 0.309, 0.227]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7287 | Steps: 4 | Val loss: 3.8602 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=37854)[0m top1: 0.19916044776119404
[2m[36m(func pid=37854)[0m top5: 0.7714552238805971
[2m[36m(func pid=37854)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=37854)[0m f1_macro: 0.13945426639513717
[2m[36m(func pid=37854)[0m f1_weighted: 0.1956551623632115
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.259, 0.0, 0.413, 0.044, 0.218, 0.015, 0.0, 0.045, 0.4]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.24440298507462688
[2m[36m(func pid=37804)[0m top5: 0.7374067164179104
[2m[36m(func pid=37804)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=37804)[0m f1_macro: 0.20030339739584244
[2m[36m(func pid=37804)[0m f1_weighted: 0.22497411372870935
[2m[36m(func pid=37804)[0m f1_per_class: [0.234, 0.536, 0.028, 0.039, 0.11, 0.158, 0.238, 0.36, 0.139, 0.162]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3901 | Steps: 4 | Val loss: 2.0772 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.3276 | Steps: 4 | Val loss: 1.6877 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9573 | Steps: 4 | Val loss: 2.2892 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:46:20 (running for 00:12:46.18)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.39  |      0.26  |                   53 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.228 |      0.35  |                   29 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.729 |      0.2   |                   29 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.282 |      0.139 |                   29 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2966417910447761
[2m[36m(func pid=32295)[0m top5: 0.8022388059701493
[2m[36m(func pid=32295)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=32295)[0m f1_macro: 0.26006803549304014
[2m[36m(func pid=32295)[0m f1_weighted: 0.32695510830773244
[2m[36m(func pid=32295)[0m f1_per_class: [0.217, 0.297, 0.415, 0.404, 0.054, 0.341, 0.324, 0.255, 0.101, 0.193]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5840 | Steps: 4 | Val loss: 3.7631 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=37733)[0m top1: 0.3908582089552239
[2m[36m(func pid=37733)[0m top5: 0.8833955223880597
[2m[36m(func pid=37733)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=37733)[0m f1_macro: 0.3600611631012992
[2m[36m(func pid=37733)[0m f1_weighted: 0.42708759279754327
[2m[36m(func pid=37733)[0m f1_per_class: [0.574, 0.38, 0.212, 0.473, 0.082, 0.418, 0.431, 0.451, 0.328, 0.251]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.17444029850746268
[2m[36m(func pid=37854)[0m top5: 0.7444029850746269
[2m[36m(func pid=37854)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=37854)[0m f1_macro: 0.12018165081626955
[2m[36m(func pid=37854)[0m f1_weighted: 0.174214489069689
[2m[36m(func pid=37854)[0m f1_per_class: [0.0, 0.223, 0.0, 0.382, 0.04, 0.209, 0.0, 0.0, 0.045, 0.303]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.2728544776119403
[2m[36m(func pid=37804)[0m top5: 0.7845149253731343
[2m[36m(func pid=37804)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=37804)[0m f1_macro: 0.21518288934474183
[2m[36m(func pid=37804)[0m f1_weighted: 0.25198801742746224
[2m[36m(func pid=37804)[0m f1_per_class: [0.227, 0.528, 0.0, 0.251, 0.2, 0.016, 0.195, 0.27, 0.184, 0.281]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3421 | Steps: 4 | Val loss: 2.0630 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9954 | Steps: 4 | Val loss: 1.7185 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2188 | Steps: 4 | Val loss: 2.0003 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:46:26 (running for 00:12:51.73)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.342 |      0.278 |                   54 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.328 |      0.36  |                   30 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.584 |      0.215 |                   30 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.957 |      0.12  |                   30 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.30830223880597013
[2m[36m(func pid=32295)[0m top5: 0.8050373134328358
[2m[36m(func pid=32295)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=32295)[0m f1_macro: 0.278393414162378
[2m[36m(func pid=32295)[0m f1_weighted: 0.3320704989768943
[2m[36m(func pid=32295)[0m f1_per_class: [0.25, 0.319, 0.55, 0.447, 0.053, 0.317, 0.291, 0.255, 0.115, 0.188]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5493 | Steps: 4 | Val loss: 3.9810 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=37733)[0m top1: 0.38759328358208955
[2m[36m(func pid=37733)[0m top5: 0.875
[2m[36m(func pid=37733)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=37733)[0m f1_macro: 0.34991527529214583
[2m[36m(func pid=37733)[0m f1_weighted: 0.4193698615672256
[2m[36m(func pid=37733)[0m f1_per_class: [0.525, 0.404, 0.176, 0.491, 0.088, 0.398, 0.382, 0.489, 0.293, 0.251]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.21361940298507462
[2m[36m(func pid=37854)[0m top5: 0.7723880597014925
[2m[36m(func pid=37854)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=37854)[0m f1_macro: 0.22986897828203018
[2m[36m(func pid=37854)[0m f1_weighted: 0.2168681395117464
[2m[36m(func pid=37854)[0m f1_per_class: [0.148, 0.313, 0.133, 0.306, 0.05, 0.243, 0.024, 0.524, 0.07, 0.486]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.23694029850746268
[2m[36m(func pid=37804)[0m top5: 0.7709888059701493
[2m[36m(func pid=37804)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=37804)[0m f1_macro: 0.21354735523992488
[2m[36m(func pid=37804)[0m f1_weighted: 0.23487623344200353
[2m[36m(func pid=37804)[0m f1_per_class: [0.188, 0.51, 0.27, 0.19, 0.225, 0.008, 0.213, 0.282, 0.154, 0.096]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4385 | Steps: 4 | Val loss: 2.0647 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.1690 | Steps: 4 | Val loss: 1.7127 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9744 | Steps: 4 | Val loss: 2.0203 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 14:46:31 (running for 00:12:57.21)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.439 |      0.256 |                   55 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.995 |      0.35  |                   31 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.549 |      0.214 |                   31 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.219 |      0.23  |                   31 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.29384328358208955
[2m[36m(func pid=32295)[0m top5: 0.8036380597014925
[2m[36m(func pid=32295)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=32295)[0m f1_macro: 0.25611569390986905
[2m[36m(func pid=32295)[0m f1_weighted: 0.3290410700410041
[2m[36m(func pid=32295)[0m f1_per_class: [0.236, 0.298, 0.349, 0.418, 0.059, 0.274, 0.344, 0.231, 0.113, 0.239]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4701 | Steps: 4 | Val loss: 4.9793 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=37733)[0m top1: 0.38992537313432835
[2m[36m(func pid=37733)[0m top5: 0.8791977611940298
[2m[36m(func pid=37733)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=37733)[0m f1_macro: 0.34696145117707056
[2m[36m(func pid=37733)[0m f1_weighted: 0.42236470304375423
[2m[36m(func pid=37733)[0m f1_per_class: [0.504, 0.395, 0.19, 0.495, 0.092, 0.392, 0.395, 0.512, 0.278, 0.215]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.25652985074626866
[2m[36m(func pid=37854)[0m top5: 0.7756529850746269
[2m[36m(func pid=37854)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=37854)[0m f1_macro: 0.22047429707644697
[2m[36m(func pid=37854)[0m f1_weighted: 0.25590449749650335
[2m[36m(func pid=37854)[0m f1_per_class: [0.075, 0.473, 0.207, 0.27, 0.03, 0.235, 0.139, 0.347, 0.125, 0.303]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.15764925373134328
[2m[36m(func pid=37804)[0m top5: 0.722481343283582
[2m[36m(func pid=37804)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=37804)[0m f1_macro: 0.16422935308476078
[2m[36m(func pid=37804)[0m f1_weighted: 0.12728022682356077
[2m[36m(func pid=37804)[0m f1_per_class: [0.186, 0.302, 0.278, 0.067, 0.15, 0.037, 0.083, 0.268, 0.114, 0.158]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4413 | Steps: 4 | Val loss: 2.0517 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.1350 | Steps: 4 | Val loss: 1.7302 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3823 | Steps: 4 | Val loss: 2.8231 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:46:36 (running for 00:13:02.55)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.441 |      0.259 |                   56 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.169 |      0.347 |                   32 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.47  |      0.164 |                   32 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.974 |      0.22  |                   32 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2989738805970149
[2m[36m(func pid=32295)[0m top5: 0.804570895522388
[2m[36m(func pid=32295)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=32295)[0m f1_macro: 0.2589227462233656
[2m[36m(func pid=32295)[0m f1_weighted: 0.3311113650847489
[2m[36m(func pid=32295)[0m f1_per_class: [0.226, 0.272, 0.369, 0.441, 0.065, 0.288, 0.341, 0.213, 0.124, 0.25]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7918 | Steps: 4 | Val loss: 3.4911 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=37733)[0m top1: 0.3726679104477612
[2m[36m(func pid=37733)[0m top5: 0.8759328358208955
[2m[36m(func pid=37733)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=37733)[0m f1_macro: 0.33870842794308387
[2m[36m(func pid=37733)[0m f1_weighted: 0.3972293480720482
[2m[36m(func pid=37733)[0m f1_per_class: [0.504, 0.424, 0.197, 0.434, 0.091, 0.343, 0.371, 0.49, 0.314, 0.22]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.34281716417910446
[2m[36m(func pid=37854)[0m top5: 0.7430037313432836
[2m[36m(func pid=37854)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=37854)[0m f1_macro: 0.20440116302589942
[2m[36m(func pid=37854)[0m f1_weighted: 0.3704272965558206
[2m[36m(func pid=37854)[0m f1_per_class: [0.049, 0.431, 0.02, 0.307, 0.043, 0.078, 0.591, 0.355, 0.084, 0.086]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.2949 | Steps: 4 | Val loss: 2.0544 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=37804)[0m top1: 0.3138992537313433
[2m[36m(func pid=37804)[0m top5: 0.8129664179104478
[2m[36m(func pid=37804)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=37804)[0m f1_macro: 0.2759274055781633
[2m[36m(func pid=37804)[0m f1_weighted: 0.3128464093245358
[2m[36m(func pid=37804)[0m f1_per_class: [0.308, 0.308, 0.421, 0.166, 0.148, 0.054, 0.556, 0.415, 0.148, 0.236]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.1742 | Steps: 4 | Val loss: 1.6932 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.0383 | Steps: 4 | Val loss: 2.5380 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 14:46:42 (running for 00:13:07.95)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.295 |      0.264 |                   57 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.135 |      0.339 |                   33 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.792 |      0.276 |                   33 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.382 |      0.204 |                   33 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2873134328358209
[2m[36m(func pid=32295)[0m top5: 0.8073694029850746
[2m[36m(func pid=32295)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=32295)[0m f1_macro: 0.2635974719252789
[2m[36m(func pid=32295)[0m f1_weighted: 0.3144651287133548
[2m[36m(func pid=32295)[0m f1_per_class: [0.231, 0.261, 0.468, 0.424, 0.061, 0.29, 0.302, 0.238, 0.103, 0.258]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7839 | Steps: 4 | Val loss: 3.2974 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=37733)[0m top1: 0.37919776119402987
[2m[36m(func pid=37733)[0m top5: 0.8819962686567164
[2m[36m(func pid=37733)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=37733)[0m f1_macro: 0.3431332240919408
[2m[36m(func pid=37733)[0m f1_weighted: 0.3982100248181884
[2m[36m(func pid=37733)[0m f1_per_class: [0.496, 0.428, 0.222, 0.443, 0.102, 0.383, 0.353, 0.469, 0.3, 0.235]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.2957089552238806
[2m[36m(func pid=37854)[0m top5: 0.6833022388059702
[2m[36m(func pid=37854)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=37854)[0m f1_macro: 0.15091051018195323
[2m[36m(func pid=37854)[0m f1_weighted: 0.2876444713609632
[2m[36m(func pid=37854)[0m f1_per_class: [0.086, 0.479, 0.0, 0.281, 0.0, 0.0, 0.404, 0.0, 0.076, 0.183]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.4290 | Steps: 4 | Val loss: 2.0581 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37804)[0m top1: 0.3353544776119403
[2m[36m(func pid=37804)[0m top5: 0.757929104477612
[2m[36m(func pid=37804)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=37804)[0m f1_macro: 0.2841258772856947
[2m[36m(func pid=37804)[0m f1_weighted: 0.3085045065989325
[2m[36m(func pid=37804)[0m f1_per_class: [0.36, 0.335, 0.727, 0.095, 0.092, 0.098, 0.625, 0.115, 0.145, 0.248]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2003 | Steps: 4 | Val loss: 1.7150 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.0647 | Steps: 4 | Val loss: 2.3603 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:46:47 (running for 00:13:13.27)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.429 |      0.271 |                   58 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.174 |      0.343 |                   34 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.784 |      0.284 |                   34 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.038 |      0.151 |                   34 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.28451492537313433
[2m[36m(func pid=32295)[0m top5: 0.8031716417910447
[2m[36m(func pid=32295)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=32295)[0m f1_macro: 0.27148805620555644
[2m[36m(func pid=32295)[0m f1_weighted: 0.3079987766095935
[2m[36m(func pid=32295)[0m f1_per_class: [0.313, 0.278, 0.489, 0.419, 0.05, 0.298, 0.263, 0.264, 0.085, 0.256]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.9157 | Steps: 4 | Val loss: 2.4775 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=37733)[0m top1: 0.38526119402985076
[2m[36m(func pid=37733)[0m top5: 0.8731343283582089
[2m[36m(func pid=37733)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=37733)[0m f1_macro: 0.3388740826924673
[2m[36m(func pid=37733)[0m f1_weighted: 0.4069101320681348
[2m[36m(func pid=37733)[0m f1_per_class: [0.393, 0.443, 0.186, 0.478, 0.093, 0.376, 0.347, 0.475, 0.316, 0.282]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.26865671641791045
[2m[36m(func pid=37854)[0m top5: 0.691231343283582
[2m[36m(func pid=37854)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=37854)[0m f1_macro: 0.1809671749341608
[2m[36m(func pid=37854)[0m f1_weighted: 0.26843187180047745
[2m[36m(func pid=37854)[0m f1_per_class: [0.128, 0.513, 0.0, 0.277, 0.0, 0.0, 0.252, 0.348, 0.085, 0.207]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.43097014925373134
[2m[36m(func pid=37804)[0m top5: 0.8549440298507462
[2m[36m(func pid=37804)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=37804)[0m f1_macro: 0.36146707354178664
[2m[36m(func pid=37804)[0m f1_weighted: 0.44338604242209767
[2m[36m(func pid=37804)[0m f1_per_class: [0.241, 0.504, 0.462, 0.325, 0.118, 0.357, 0.611, 0.374, 0.254, 0.368]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.4118 | Steps: 4 | Val loss: 2.0549 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0485 | Steps: 4 | Val loss: 1.7203 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1709 | Steps: 4 | Val loss: 1.9619 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:46:53 (running for 00:13:18.81)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.412 |      0.263 |                   59 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.2   |      0.339 |                   35 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.916 |      0.361 |                   35 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.065 |      0.181 |                   35 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2826492537313433
[2m[36m(func pid=32295)[0m top5: 0.8041044776119403
[2m[36m(func pid=32295)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=32295)[0m f1_macro: 0.26322169497813286
[2m[36m(func pid=32295)[0m f1_weighted: 0.30640597376085504
[2m[36m(func pid=32295)[0m f1_per_class: [0.254, 0.258, 0.478, 0.408, 0.059, 0.298, 0.281, 0.278, 0.089, 0.229]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0101 | Steps: 4 | Val loss: 2.9946 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=37733)[0m top1: 0.37173507462686567
[2m[36m(func pid=37733)[0m top5: 0.8736007462686567
[2m[36m(func pid=37733)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=37733)[0m f1_macro: 0.33449709598930905
[2m[36m(func pid=37733)[0m f1_weighted: 0.3839252183178409
[2m[36m(func pid=37733)[0m f1_per_class: [0.365, 0.42, 0.276, 0.474, 0.087, 0.401, 0.284, 0.463, 0.272, 0.304]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.30363805970149255
[2m[36m(func pid=37854)[0m top5: 0.8106343283582089
[2m[36m(func pid=37854)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=37854)[0m f1_macro: 0.2765055898925547
[2m[36m(func pid=37854)[0m f1_weighted: 0.27953657370296175
[2m[36m(func pid=37854)[0m f1_per_class: [0.186, 0.547, 0.571, 0.364, 0.0, 0.142, 0.112, 0.325, 0.112, 0.407]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3612 | Steps: 4 | Val loss: 2.0487 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=37804)[0m top1: 0.36847014925373134
[2m[36m(func pid=37804)[0m top5: 0.8526119402985075
[2m[36m(func pid=37804)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=37804)[0m f1_macro: 0.30536274598654894
[2m[36m(func pid=37804)[0m f1_weighted: 0.3864681949089046
[2m[36m(func pid=37804)[0m f1_per_class: [0.268, 0.497, 0.1, 0.497, 0.072, 0.278, 0.306, 0.321, 0.261, 0.453]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.0408 | Steps: 4 | Val loss: 1.6546 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3661 | Steps: 4 | Val loss: 1.9304 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:46:58 (running for 00:13:24.17)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.361 |      0.268 |                   60 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.048 |      0.334 |                   36 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.01  |      0.305 |                   36 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.171 |      0.277 |                   36 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.283115671641791
[2m[36m(func pid=32295)[0m top5: 0.8111007462686567
[2m[36m(func pid=32295)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=32295)[0m f1_macro: 0.2679280846379996
[2m[36m(func pid=32295)[0m f1_weighted: 0.3049901692614107
[2m[36m(func pid=32295)[0m f1_per_class: [0.274, 0.265, 0.468, 0.407, 0.057, 0.346, 0.254, 0.281, 0.077, 0.25]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.9457 | Steps: 4 | Val loss: 3.5482 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=37733)[0m top1: 0.39505597014925375
[2m[36m(func pid=37733)[0m top5: 0.8885261194029851
[2m[36m(func pid=37733)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=37733)[0m f1_macro: 0.35718066214227306
[2m[36m(func pid=37733)[0m f1_weighted: 0.40958817025081434
[2m[36m(func pid=37733)[0m f1_per_class: [0.431, 0.452, 0.264, 0.483, 0.097, 0.403, 0.331, 0.459, 0.33, 0.323]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m top1: 0.3353544776119403
[2m[36m(func pid=37854)[0m top5: 0.8125
[2m[36m(func pid=37854)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=37854)[0m f1_macro: 0.2822042522795495
[2m[36m(func pid=37854)[0m f1_weighted: 0.2953563091616091
[2m[36m(func pid=37854)[0m f1_per_class: [0.171, 0.517, 0.733, 0.427, 0.0, 0.311, 0.061, 0.357, 0.102, 0.143]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3738 | Steps: 4 | Val loss: 2.0415 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37804)[0m top1: 0.3087686567164179
[2m[36m(func pid=37804)[0m top5: 0.8582089552238806
[2m[36m(func pid=37804)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=37804)[0m f1_macro: 0.2431062235945017
[2m[36m(func pid=37804)[0m f1_weighted: 0.2935305694241747
[2m[36m(func pid=37804)[0m f1_per_class: [0.378, 0.46, 0.0, 0.476, 0.067, 0.098, 0.107, 0.315, 0.271, 0.258]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.1500 | Steps: 4 | Val loss: 1.9656 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0481 | Steps: 4 | Val loss: 1.7424 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 14:47:03 (running for 00:13:29.46)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.374 |      0.273 |                   61 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.041 |      0.357 |                   37 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.946 |      0.243 |                   37 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.366 |      0.282 |                   37 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.2994402985074627
[2m[36m(func pid=32295)[0m top5: 0.8134328358208955
[2m[36m(func pid=32295)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=32295)[0m f1_macro: 0.2729243933031241
[2m[36m(func pid=32295)[0m f1_weighted: 0.32126821210363576
[2m[36m(func pid=32295)[0m f1_per_class: [0.247, 0.294, 0.468, 0.443, 0.063, 0.348, 0.259, 0.276, 0.086, 0.244]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1292 | Steps: 4 | Val loss: 3.1007 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=37854)[0m top1: 0.24813432835820895
[2m[36m(func pid=37854)[0m top5: 0.8013059701492538
[2m[36m(func pid=37854)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=37854)[0m f1_macro: 0.23248649090943846
[2m[36m(func pid=37854)[0m f1_weighted: 0.24408399760175517
[2m[36m(func pid=37854)[0m f1_per_class: [0.086, 0.252, 0.55, 0.404, 0.054, 0.296, 0.075, 0.382, 0.084, 0.143]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m top1: 0.36240671641791045
[2m[36m(func pid=37733)[0m top5: 0.882929104477612
[2m[36m(func pid=37733)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=37733)[0m f1_macro: 0.34855978363528833
[2m[36m(func pid=37733)[0m f1_weighted: 0.37617631347757885
[2m[36m(func pid=37733)[0m f1_per_class: [0.481, 0.448, 0.296, 0.373, 0.072, 0.37, 0.335, 0.444, 0.341, 0.326]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3285 | Steps: 4 | Val loss: 2.0411 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=37804)[0m top1: 0.29757462686567165
[2m[36m(func pid=37804)[0m top5: 0.8782649253731343
[2m[36m(func pid=37804)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=37804)[0m f1_macro: 0.2887277872576241
[2m[36m(func pid=37804)[0m f1_weighted: 0.26667172029856456
[2m[36m(func pid=37804)[0m f1_per_class: [0.219, 0.455, 0.429, 0.167, 0.094, 0.246, 0.227, 0.436, 0.264, 0.351]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3689 | Steps: 4 | Val loss: 2.1482 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.0617 | Steps: 4 | Val loss: 1.6568 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 14:47:09 (running for 00:13:34.77)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.329 |      0.277 |                   62 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.048 |      0.349 |                   38 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.129 |      0.289 |                   38 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.15  |      0.232 |                   38 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.3101679104477612
[2m[36m(func pid=32295)[0m top5: 0.808768656716418
[2m[36m(func pid=32295)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=32295)[0m f1_macro: 0.27654346966821314
[2m[36m(func pid=32295)[0m f1_weighted: 0.32784823544562247
[2m[36m(func pid=32295)[0m f1_per_class: [0.242, 0.319, 0.436, 0.454, 0.072, 0.322, 0.262, 0.285, 0.113, 0.26]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.3294 | Steps: 4 | Val loss: 3.5760 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=37854)[0m top1: 0.18983208955223882
[2m[36m(func pid=37854)[0m top5: 0.8232276119402985
[2m[36m(func pid=37854)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=37854)[0m f1_macro: 0.17557263138540596
[2m[36m(func pid=37854)[0m f1_weighted: 0.18810644042296149
[2m[36m(func pid=37854)[0m f1_per_class: [0.078, 0.106, 0.255, 0.321, 0.116, 0.209, 0.107, 0.259, 0.091, 0.214]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4039179104477612
[2m[36m(func pid=37733)[0m top5: 0.8889925373134329
[2m[36m(func pid=37733)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=37733)[0m f1_macro: 0.36566662608838685
[2m[36m(func pid=37733)[0m f1_weighted: 0.41793612925964574
[2m[36m(func pid=37733)[0m f1_per_class: [0.456, 0.467, 0.296, 0.505, 0.102, 0.373, 0.332, 0.478, 0.372, 0.275]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3174 | Steps: 4 | Val loss: 2.0377 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=37804)[0m top1: 0.2775186567164179
[2m[36m(func pid=37804)[0m top5: 0.8106343283582089
[2m[36m(func pid=37804)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=37804)[0m f1_macro: 0.2550301166594058
[2m[36m(func pid=37804)[0m f1_weighted: 0.20497116881135577
[2m[36m(func pid=37804)[0m f1_per_class: [0.231, 0.449, 0.533, 0.01, 0.133, 0.276, 0.177, 0.487, 0.027, 0.228]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7994 | Steps: 4 | Val loss: 2.0292 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8317 | Steps: 4 | Val loss: 1.6266 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:47:14 (running for 00:13:40.07)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.317 |      0.279 |                   63 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.062 |      0.366 |                   39 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.329 |      0.255 |                   39 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.369 |      0.176 |                   39 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.3111007462686567
[2m[36m(func pid=32295)[0m top5: 0.8111007462686567
[2m[36m(func pid=32295)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=32295)[0m f1_macro: 0.27867995065600076
[2m[36m(func pid=32295)[0m f1_weighted: 0.3288027353642592
[2m[36m(func pid=32295)[0m f1_per_class: [0.224, 0.304, 0.436, 0.447, 0.071, 0.369, 0.261, 0.292, 0.123, 0.259]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6029 | Steps: 4 | Val loss: 3.2127 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=37854)[0m top1: 0.19263059701492538
[2m[36m(func pid=37854)[0m top5: 0.8152985074626866
[2m[36m(func pid=37854)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=37854)[0m f1_macro: 0.18802677837665857
[2m[36m(func pid=37854)[0m f1_weighted: 0.19568992705138352
[2m[36m(func pid=37854)[0m f1_per_class: [0.095, 0.168, 0.182, 0.237, 0.085, 0.174, 0.175, 0.301, 0.107, 0.357]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4221082089552239
[2m[36m(func pid=37733)[0m top5: 0.8927238805970149
[2m[36m(func pid=37733)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=37733)[0m f1_macro: 0.371162967118409
[2m[36m(func pid=37733)[0m f1_weighted: 0.43264104947567783
[2m[36m(func pid=37733)[0m f1_per_class: [0.504, 0.341, 0.364, 0.583, 0.098, 0.367, 0.383, 0.486, 0.326, 0.261]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3816 | Steps: 4 | Val loss: 2.0346 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=37804)[0m top1: 0.2751865671641791
[2m[36m(func pid=37804)[0m top5: 0.8572761194029851
[2m[36m(func pid=37804)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=37804)[0m f1_macro: 0.2193494701041387
[2m[36m(func pid=37804)[0m f1_weighted: 0.23800595091588342
[2m[36m(func pid=37804)[0m f1_per_class: [0.231, 0.454, 0.262, 0.066, 0.169, 0.263, 0.306, 0.084, 0.178, 0.182]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.1152 | Steps: 4 | Val loss: 2.1881 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9133 | Steps: 4 | Val loss: 1.5912 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:47:19 (running for 00:13:45.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.382 |      0.28  |                   64 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.832 |      0.371 |                   40 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.603 |      0.219 |                   40 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.799 |      0.188 |                   40 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.30970149253731344
[2m[36m(func pid=32295)[0m top5: 0.8111007462686567
[2m[36m(func pid=32295)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=32295)[0m f1_macro: 0.2795958286827712
[2m[36m(func pid=32295)[0m f1_weighted: 0.3259006182046703
[2m[36m(func pid=32295)[0m f1_per_class: [0.254, 0.29, 0.453, 0.467, 0.073, 0.379, 0.239, 0.278, 0.117, 0.246]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7967 | Steps: 4 | Val loss: 3.5913 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=37854)[0m top1: 0.19916044776119404
[2m[36m(func pid=37854)[0m top5: 0.7947761194029851
[2m[36m(func pid=37854)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=37854)[0m f1_macro: 0.173008730683417
[2m[36m(func pid=37854)[0m f1_weighted: 0.2002846194008363
[2m[36m(func pid=37854)[0m f1_per_class: [0.092, 0.354, 0.177, 0.2, 0.038, 0.186, 0.118, 0.316, 0.117, 0.131]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m top1: 0.43423507462686567
[2m[36m(func pid=37733)[0m top5: 0.9039179104477612
[2m[36m(func pid=37733)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=37733)[0m f1_macro: 0.38239245512436676
[2m[36m(func pid=37733)[0m f1_weighted: 0.45470888561197126
[2m[36m(func pid=37733)[0m f1_per_class: [0.52, 0.386, 0.304, 0.598, 0.084, 0.375, 0.408, 0.488, 0.369, 0.292]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2789 | Steps: 4 | Val loss: 2.0318 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37804)[0m top1: 0.30923507462686567
[2m[36m(func pid=37804)[0m top5: 0.8292910447761194
[2m[36m(func pid=37804)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=37804)[0m f1_macro: 0.2238703026202397
[2m[36m(func pid=37804)[0m f1_weighted: 0.3468472736576862
[2m[36m(func pid=37804)[0m f1_per_class: [0.326, 0.516, 0.071, 0.368, 0.098, 0.086, 0.422, 0.118, 0.114, 0.121]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5608 | Steps: 4 | Val loss: 2.3033 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9945 | Steps: 4 | Val loss: 1.5966 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:47:25 (running for 00:13:50.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.279 |      0.285 |                   65 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.913 |      0.382 |                   41 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.797 |      0.224 |                   41 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.115 |      0.173 |                   41 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.31203358208955223
[2m[36m(func pid=32295)[0m top5: 0.8111007462686567
[2m[36m(func pid=32295)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=32295)[0m f1_macro: 0.28460653599239927
[2m[36m(func pid=32295)[0m f1_weighted: 0.3224760665688835
[2m[36m(func pid=32295)[0m f1_per_class: [0.26, 0.292, 0.471, 0.472, 0.073, 0.394, 0.21, 0.295, 0.122, 0.257]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m top1: 0.2756529850746269
[2m[36m(func pid=37854)[0m top5: 0.7901119402985075
[2m[36m(func pid=37854)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=37854)[0m f1_macro: 0.1535058849403394
[2m[36m(func pid=37854)[0m f1_weighted: 0.3137669391588653
[2m[36m(func pid=37854)[0m f1_per_class: [0.155, 0.1, 0.0, 0.44, 0.0, 0.2, 0.481, 0.0, 0.111, 0.046]
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3820 | Steps: 4 | Val loss: 9.7428 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4337686567164179
[2m[36m(func pid=37733)[0m top5: 0.9085820895522388
[2m[36m(func pid=37733)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=37733)[0m f1_macro: 0.396026391237165
[2m[36m(func pid=37733)[0m f1_weighted: 0.46299164813421895
[2m[36m(func pid=37733)[0m f1_per_class: [0.557, 0.461, 0.293, 0.509, 0.088, 0.371, 0.467, 0.513, 0.384, 0.317]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2438 | Steps: 4 | Val loss: 2.0294 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37804)[0m top1: 0.10541044776119403
[2m[36m(func pid=37804)[0m top5: 0.42024253731343286
[2m[36m(func pid=37804)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=37804)[0m f1_macro: 0.09691352585940702
[2m[36m(func pid=37804)[0m f1_weighted: 0.09613481288669973
[2m[36m(func pid=37804)[0m f1_per_class: [0.169, 0.512, 0.041, 0.0, 0.12, 0.0, 0.0, 0.015, 0.058, 0.054]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.1073 | Steps: 4 | Val loss: 2.4934 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8776 | Steps: 4 | Val loss: 1.6210 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=32295)[0m top1: 0.32742537313432835
[2m[36m(func pid=32295)[0m top5: 0.8083022388059702
[2m[36m(func pid=32295)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=32295)[0m f1_macro: 0.29527095968160666
[2m[36m(func pid=32295)[0m f1_weighted: 0.33434124290735545
[2m[36m(func pid=32295)[0m f1_per_class: [0.279, 0.278, 0.512, 0.501, 0.075, 0.398, 0.227, 0.301, 0.116, 0.268]
== Status ==
Current time: 2024-01-07 14:47:30 (running for 00:13:56.28)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.244 |      0.295 |                   66 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.995 |      0.396 |                   42 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.382 |      0.097 |                   42 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.561 |      0.154 |                   42 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m top1: 0.18983208955223882
[2m[36m(func pid=37854)[0m top5: 0.7164179104477612
[2m[36m(func pid=37854)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=37854)[0m f1_macro: 0.1818560385776933
[2m[36m(func pid=37854)[0m f1_weighted: 0.22796786825384682
[2m[36m(func pid=37854)[0m f1_per_class: [0.108, 0.259, 0.696, 0.394, 0.0, 0.03, 0.203, 0.0, 0.084, 0.046]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8912 | Steps: 4 | Val loss: 5.0190 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37733)[0m top1: 0.42257462686567165
[2m[36m(func pid=37733)[0m top5: 0.8992537313432836
[2m[36m(func pid=37733)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=37733)[0m f1_macro: 0.3845700963182205
[2m[36m(func pid=37733)[0m f1_weighted: 0.44616835657577314
[2m[36m(func pid=37733)[0m f1_per_class: [0.486, 0.502, 0.276, 0.481, 0.088, 0.379, 0.422, 0.476, 0.382, 0.355]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3700 | Steps: 4 | Val loss: 2.0269 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=37804)[0m top1: 0.18330223880597016
[2m[36m(func pid=37804)[0m top5: 0.6096082089552238
[2m[36m(func pid=37804)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=37804)[0m f1_macro: 0.15450484558546554
[2m[36m(func pid=37804)[0m f1_weighted: 0.2002419319715176
[2m[36m(func pid=37804)[0m f1_per_class: [0.23, 0.485, 0.055, 0.032, 0.101, 0.103, 0.285, 0.015, 0.077, 0.163]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.0858 | Steps: 4 | Val loss: 1.9696 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=32295)[0m top1: 0.3255597014925373
[2m[36m(func pid=32295)[0m top5: 0.8111007462686567
[2m[36m(func pid=32295)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=32295)[0m f1_macro: 0.2931205676994781
[2m[36m(func pid=32295)[0m f1_weighted: 0.3389221882127844
[2m[36m(func pid=32295)[0m f1_per_class: [0.263, 0.268, 0.511, 0.497, 0.069, 0.402, 0.253, 0.289, 0.114, 0.265]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8397 | Steps: 4 | Val loss: 1.5931 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:47:36 (running for 00:14:01.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.37  |      0.293 |                   67 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.878 |      0.385 |                   43 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.891 |      0.155 |                   43 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.107 |      0.182 |                   43 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.33255597014925375
[2m[36m(func pid=37854)[0m top5: 0.8498134328358209
[2m[36m(func pid=37854)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=37854)[0m f1_macro: 0.3233518523537953
[2m[36m(func pid=37854)[0m f1_weighted: 0.3781810755010043
[2m[36m(func pid=37854)[0m f1_per_class: [0.102, 0.421, 0.783, 0.287, 0.033, 0.236, 0.523, 0.497, 0.134, 0.217]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7084 | Steps: 4 | Val loss: 3.3090 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=37733)[0m top1: 0.42257462686567165
[2m[36m(func pid=37733)[0m top5: 0.9057835820895522
[2m[36m(func pid=37733)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=37733)[0m f1_macro: 0.3844326617248781
[2m[36m(func pid=37733)[0m f1_weighted: 0.4351308845637932
[2m[36m(func pid=37733)[0m f1_per_class: [0.455, 0.508, 0.3, 0.537, 0.093, 0.38, 0.325, 0.481, 0.427, 0.339]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2511 | Steps: 4 | Val loss: 2.0313 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=37804)[0m top1: 0.25466417910447764
[2m[36m(func pid=37804)[0m top5: 0.6856343283582089
[2m[36m(func pid=37804)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=37804)[0m f1_macro: 0.2156207125824488
[2m[36m(func pid=37804)[0m f1_weighted: 0.24968500712211075
[2m[36m(func pid=37804)[0m f1_per_class: [0.282, 0.437, 0.071, 0.003, 0.056, 0.285, 0.373, 0.261, 0.163, 0.224]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6013 | Steps: 4 | Val loss: 1.8454 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8437 | Steps: 4 | Val loss: 1.6015 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:47:41 (running for 00:14:07.52)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.251 |      0.278 |                   68 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.84  |      0.384 |                   44 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.708 |      0.216 |                   44 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.086 |      0.323 |                   44 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.3162313432835821
[2m[36m(func pid=32295)[0m top5: 0.8083022388059702
[2m[36m(func pid=32295)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=32295)[0m f1_macro: 0.2778970937462565
[2m[36m(func pid=32295)[0m f1_weighted: 0.32918385178736115
[2m[36m(func pid=32295)[0m f1_per_class: [0.259, 0.276, 0.369, 0.482, 0.071, 0.391, 0.235, 0.291, 0.132, 0.271]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37854)[0m top1: 0.25699626865671643
[2m[36m(func pid=37854)[0m top5: 0.8871268656716418
[2m[36m(func pid=37854)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=37854)[0m f1_macro: 0.261681840527029
[2m[36m(func pid=37854)[0m f1_weighted: 0.22154603672177037
[2m[36m(func pid=37854)[0m f1_per_class: [0.166, 0.48, 0.8, 0.285, 0.107, 0.267, 0.0, 0.247, 0.117, 0.148]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6884 | Steps: 4 | Val loss: 3.0461 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=37733)[0m top1: 0.42117537313432835
[2m[36m(func pid=37733)[0m top5: 0.9071828358208955
[2m[36m(func pid=37733)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=37733)[0m f1_macro: 0.37577170946142757
[2m[36m(func pid=37733)[0m f1_weighted: 0.4469532295787172
[2m[36m(func pid=37733)[0m f1_per_class: [0.404, 0.432, 0.304, 0.555, 0.078, 0.38, 0.406, 0.46, 0.35, 0.389]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2795 | Steps: 4 | Val loss: 2.0173 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7912 | Steps: 4 | Val loss: 1.8790 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=37804)[0m top1: 0.2583955223880597
[2m[36m(func pid=37804)[0m top5: 0.7411380597014925
[2m[36m(func pid=37804)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=37804)[0m f1_macro: 0.18673135544625427
[2m[36m(func pid=37804)[0m f1_weighted: 0.23681044967823875
[2m[36m(func pid=37804)[0m f1_per_class: [0.124, 0.368, 0.0, 0.01, 0.04, 0.282, 0.39, 0.211, 0.1, 0.344]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:47:47 (running for 00:14:12.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.28  |      0.276 |                   69 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.844 |      0.376 |                   45 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.688 |      0.187 |                   45 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.601 |      0.262 |                   45 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.31576492537313433
[2m[36m(func pid=32295)[0m top5: 0.8190298507462687
[2m[36m(func pid=32295)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=32295)[0m f1_macro: 0.2759134933912115
[2m[36m(func pid=32295)[0m f1_weighted: 0.3279019379279891
[2m[36m(func pid=32295)[0m f1_per_class: [0.257, 0.295, 0.348, 0.472, 0.073, 0.397, 0.229, 0.28, 0.138, 0.271]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8855 | Steps: 4 | Val loss: 1.5338 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=37854)[0m top1: 0.27845149253731344
[2m[36m(func pid=37854)[0m top5: 0.8362873134328358
[2m[36m(func pid=37854)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=37854)[0m f1_macro: 0.2246183416067818
[2m[36m(func pid=37854)[0m f1_weighted: 0.2314909729298412
[2m[36m(func pid=37854)[0m f1_per_class: [0.176, 0.515, 0.324, 0.272, 0.04, 0.319, 0.0, 0.327, 0.126, 0.148]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7698 | Steps: 4 | Val loss: 3.0897 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=37733)[0m top1: 0.4542910447761194
[2m[36m(func pid=37733)[0m top5: 0.9193097014925373
[2m[36m(func pid=37733)[0m f1_micro: 0.4542910447761194
[2m[36m(func pid=37733)[0m f1_macro: 0.40057147781892943
[2m[36m(func pid=37733)[0m f1_weighted: 0.48611676979612334
[2m[36m(func pid=37733)[0m f1_per_class: [0.52, 0.464, 0.296, 0.557, 0.082, 0.342, 0.516, 0.495, 0.372, 0.362]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3127 | Steps: 4 | Val loss: 2.0199 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4589 | Steps: 4 | Val loss: 2.1221 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37804)[0m top1: 0.29244402985074625
[2m[36m(func pid=37804)[0m top5: 0.8754664179104478
[2m[36m(func pid=37804)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=37804)[0m f1_macro: 0.22022696120902835
[2m[36m(func pid=37804)[0m f1_weighted: 0.3136613013065331
[2m[36m(func pid=37804)[0m f1_per_class: [0.222, 0.408, 0.044, 0.318, 0.075, 0.291, 0.344, 0.069, 0.211, 0.22]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:47:52 (running for 00:14:18.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.313 |      0.269 |                   70 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.886 |      0.401 |                   46 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.77  |      0.22  |                   46 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.791 |      0.225 |                   46 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.31343283582089554
[2m[36m(func pid=32295)[0m top5: 0.8064365671641791
[2m[36m(func pid=32295)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=32295)[0m f1_macro: 0.2693234962475889
[2m[36m(func pid=32295)[0m f1_weighted: 0.3220416533193032
[2m[36m(func pid=32295)[0m f1_per_class: [0.262, 0.294, 0.296, 0.493, 0.075, 0.365, 0.202, 0.281, 0.134, 0.29]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8149 | Steps: 4 | Val loss: 1.5366 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=37854)[0m top1: 0.30177238805970147
[2m[36m(func pid=37854)[0m top5: 0.7369402985074627
[2m[36m(func pid=37854)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=37854)[0m f1_macro: 0.24784209262901088
[2m[36m(func pid=37854)[0m f1_weighted: 0.2687257036733503
[2m[36m(func pid=37854)[0m f1_per_class: [0.188, 0.562, 0.276, 0.377, 0.024, 0.229, 0.0, 0.488, 0.127, 0.207]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.0425 | Steps: 4 | Val loss: 3.0113 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=37733)[0m top1: 0.458955223880597
[2m[36m(func pid=37733)[0m top5: 0.9169776119402985
[2m[36m(func pid=37733)[0m f1_micro: 0.458955223880597
[2m[36m(func pid=37733)[0m f1_macro: 0.40866226109158676
[2m[36m(func pid=37733)[0m f1_weighted: 0.48846713197298536
[2m[36m(func pid=37733)[0m f1_per_class: [0.541, 0.466, 0.4, 0.542, 0.092, 0.319, 0.547, 0.479, 0.358, 0.344]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2207 | Steps: 4 | Val loss: 2.0187 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1890 | Steps: 4 | Val loss: 1.8925 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=37804)[0m top1: 0.32742537313432835
[2m[36m(func pid=37804)[0m top5: 0.8754664179104478
[2m[36m(func pid=37804)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=37804)[0m f1_macro: 0.25780742990227534
[2m[36m(func pid=37804)[0m f1_weighted: 0.35254146501238437
[2m[36m(func pid=37804)[0m f1_per_class: [0.235, 0.357, 0.258, 0.499, 0.063, 0.266, 0.337, 0.028, 0.296, 0.239]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:47:57 (running for 00:14:23.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.221 |      0.278 |                   71 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.815 |      0.409 |                   47 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.043 |      0.258 |                   47 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.459 |      0.248 |                   47 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.31576492537313433
[2m[36m(func pid=32295)[0m top5: 0.8120335820895522
[2m[36m(func pid=32295)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=32295)[0m f1_macro: 0.2775559448806836
[2m[36m(func pid=32295)[0m f1_weighted: 0.3250993107473689
[2m[36m(func pid=32295)[0m f1_per_class: [0.275, 0.292, 0.375, 0.494, 0.068, 0.376, 0.206, 0.303, 0.109, 0.277]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6277 | Steps: 4 | Val loss: 1.5570 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=37854)[0m top1: 0.31576492537313433
[2m[36m(func pid=37854)[0m top5: 0.808768656716418
[2m[36m(func pid=37854)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=37854)[0m f1_macro: 0.29554465919913875
[2m[36m(func pid=37854)[0m f1_weighted: 0.28558773861741016
[2m[36m(func pid=37854)[0m f1_per_class: [0.22, 0.509, 0.632, 0.456, 0.06, 0.309, 0.0, 0.317, 0.15, 0.304]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0377 | Steps: 4 | Val loss: 3.0980 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3292 | Steps: 4 | Val loss: 2.0073 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=37733)[0m top1: 0.4430970149253731
[2m[36m(func pid=37733)[0m top5: 0.9137126865671642
[2m[36m(func pid=37733)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=37733)[0m f1_macro: 0.39789678401977946
[2m[36m(func pid=37733)[0m f1_weighted: 0.46622291343755407
[2m[36m(func pid=37733)[0m f1_per_class: [0.496, 0.481, 0.429, 0.519, 0.091, 0.274, 0.509, 0.475, 0.306, 0.4]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.9237 | Steps: 4 | Val loss: 1.8866 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=37804)[0m top1: 0.3204291044776119
[2m[36m(func pid=37804)[0m top5: 0.8460820895522388
[2m[36m(func pid=37804)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=37804)[0m f1_macro: 0.3013940938244796
[2m[36m(func pid=37804)[0m f1_weighted: 0.32448576417952013
[2m[36m(func pid=37804)[0m f1_per_class: [0.352, 0.31, 0.289, 0.558, 0.043, 0.241, 0.153, 0.309, 0.294, 0.465]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:48:03 (running for 00:14:28.98)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.329 |      0.281 |                   72 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.628 |      0.398 |                   48 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.038 |      0.301 |                   48 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.189 |      0.296 |                   48 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.3111007462686567
[2m[36m(func pid=32295)[0m top5: 0.8232276119402985
[2m[36m(func pid=32295)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=32295)[0m f1_macro: 0.28117716106411555
[2m[36m(func pid=32295)[0m f1_weighted: 0.3213027289390814
[2m[36m(func pid=32295)[0m f1_per_class: [0.287, 0.286, 0.471, 0.483, 0.069, 0.403, 0.2, 0.292, 0.094, 0.228]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6244 | Steps: 4 | Val loss: 1.5696 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=37854)[0m top1: 0.292910447761194
[2m[36m(func pid=37854)[0m top5: 0.8097014925373134
[2m[36m(func pid=37854)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.27952664038535585
[2m[36m(func pid=37854)[0m f1_weighted: 0.2750773441422177
[2m[36m(func pid=37854)[0m f1_per_class: [0.226, 0.429, 0.774, 0.465, 0.0, 0.327, 0.0, 0.313, 0.175, 0.086]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2918 | Steps: 4 | Val loss: 2.8038 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2958 | Steps: 4 | Val loss: 2.0066 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=37733)[0m top1: 0.45475746268656714
[2m[36m(func pid=37733)[0m top5: 0.898320895522388
[2m[36m(func pid=37733)[0m f1_micro: 0.45475746268656714
[2m[36m(func pid=37733)[0m f1_macro: 0.4005353241251647
[2m[36m(func pid=37733)[0m f1_weighted: 0.4773181822349936
[2m[36m(func pid=37733)[0m f1_per_class: [0.512, 0.467, 0.375, 0.566, 0.099, 0.327, 0.49, 0.475, 0.302, 0.393]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8594 | Steps: 4 | Val loss: 2.0026 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=37804)[0m top1: 0.3064365671641791
[2m[36m(func pid=37804)[0m top5: 0.8348880597014925
[2m[36m(func pid=37804)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=37804)[0m f1_macro: 0.3036886920979392
[2m[36m(func pid=37804)[0m f1_weighted: 0.311159876392271
[2m[36m(func pid=37804)[0m f1_per_class: [0.289, 0.347, 0.213, 0.448, 0.061, 0.288, 0.149, 0.468, 0.246, 0.526]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:48:08 (running for 00:14:34.41)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.296 |      0.277 |                   73 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.624 |      0.401 |                   49 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.304 |                   49 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.924 |      0.28  |                   49 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.3050373134328358
[2m[36m(func pid=32295)[0m top5: 0.8185634328358209
[2m[36m(func pid=32295)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=32295)[0m f1_macro: 0.27674226966010235
[2m[36m(func pid=32295)[0m f1_weighted: 0.3148381718481563
[2m[36m(func pid=32295)[0m f1_per_class: [0.251, 0.259, 0.48, 0.47, 0.075, 0.411, 0.205, 0.28, 0.109, 0.228]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7738 | Steps: 4 | Val loss: 1.6018 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=37854)[0m top1: 0.39225746268656714
[2m[36m(func pid=37854)[0m top5: 0.792910447761194
[2m[36m(func pid=37854)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=37854)[0m f1_macro: 0.3278100819386647
[2m[36m(func pid=37854)[0m f1_weighted: 0.43877431292238267
[2m[36m(func pid=37854)[0m f1_per_class: [0.144, 0.437, 0.69, 0.457, 0.007, 0.244, 0.565, 0.459, 0.141, 0.135]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2135 | Steps: 4 | Val loss: 2.8868 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3082 | Steps: 4 | Val loss: 2.0072 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=37733)[0m top1: 0.44216417910447764
[2m[36m(func pid=37733)[0m top5: 0.8936567164179104
[2m[36m(func pid=37733)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=37733)[0m f1_macro: 0.3771283204281569
[2m[36m(func pid=37733)[0m f1_weighted: 0.4692581277153755
[2m[36m(func pid=37733)[0m f1_per_class: [0.421, 0.455, 0.25, 0.565, 0.091, 0.339, 0.476, 0.473, 0.302, 0.4]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3317 | Steps: 4 | Val loss: 1.9832 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=37804)[0m top1: 0.2994402985074627
[2m[36m(func pid=37804)[0m top5: 0.8334888059701493
[2m[36m(func pid=37804)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=37804)[0m f1_macro: 0.26186650804481204
[2m[36m(func pid=37804)[0m f1_weighted: 0.2983035789340306
[2m[36m(func pid=37804)[0m f1_per_class: [0.282, 0.45, 0.278, 0.302, 0.068, 0.332, 0.266, 0.0, 0.188, 0.453]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:48:14 (running for 00:14:39.81)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3215
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00004 | RUNNING    | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.308 |      0.274 |                   74 |
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.774 |      0.377 |                   50 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.214 |      0.262 |                   50 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.859 |      0.328 |                   50 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.314365671641791
[2m[36m(func pid=32295)[0m top5: 0.8176305970149254
[2m[36m(func pid=32295)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=32295)[0m f1_macro: 0.2744224998059105
[2m[36m(func pid=32295)[0m f1_weighted: 0.32305511822620697
[2m[36m(func pid=32295)[0m f1_per_class: [0.249, 0.305, 0.387, 0.47, 0.082, 0.4, 0.211, 0.288, 0.101, 0.252]
[2m[36m(func pid=32295)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7732 | Steps: 4 | Val loss: 1.6272 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=37854)[0m top1: 0.33908582089552236
[2m[36m(func pid=37854)[0m top5: 0.808768656716418
[2m[36m(func pid=37854)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=37854)[0m f1_macro: 0.30204046280455243
[2m[36m(func pid=37854)[0m f1_weighted: 0.3404616516929726
[2m[36m(func pid=37854)[0m f1_per_class: [0.117, 0.505, 0.769, 0.518, 0.006, 0.195, 0.174, 0.363, 0.109, 0.263]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6020 | Steps: 4 | Val loss: 2.8430 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=32295)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.3928 | Steps: 4 | Val loss: 2.0286 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=37733)[0m top1: 0.43236940298507465
[2m[36m(func pid=37733)[0m top5: 0.8964552238805971
[2m[36m(func pid=37733)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=37733)[0m f1_macro: 0.3726699563731636
[2m[36m(func pid=37733)[0m f1_weighted: 0.4578785384966407
[2m[36m(func pid=37733)[0m f1_per_class: [0.428, 0.493, 0.218, 0.514, 0.1, 0.383, 0.451, 0.439, 0.332, 0.37]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.7523 | Steps: 4 | Val loss: 2.1988 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=37804)[0m top1: 0.2775186567164179
[2m[36m(func pid=37804)[0m top5: 0.8624067164179104
[2m[36m(func pid=37804)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=37804)[0m f1_macro: 0.1950155853532874
[2m[36m(func pid=37804)[0m f1_weighted: 0.2698840177397539
[2m[36m(func pid=37804)[0m f1_per_class: [0.041, 0.48, 0.028, 0.196, 0.066, 0.153, 0.325, 0.152, 0.09, 0.419]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:48:19 (running for 00:14:45.13)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.773 |      0.373 |                   51 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.602 |      0.195 |                   51 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.332 |      0.302 |                   51 |
| train_5ae7f_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32295)[0m top1: 0.3087686567164179
[2m[36m(func pid=32295)[0m top5: 0.7989738805970149
[2m[36m(func pid=32295)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=32295)[0m f1_macro: 0.2728581448484371
[2m[36m(func pid=32295)[0m f1_weighted: 0.31851705974865646
[2m[36m(func pid=32295)[0m f1_per_class: [0.238, 0.305, 0.324, 0.461, 0.071, 0.401, 0.195, 0.315, 0.142, 0.277]
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7544 | Steps: 4 | Val loss: 1.5906 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=37854)[0m top1: 0.2971082089552239
[2m[36m(func pid=37854)[0m top5: 0.7919776119402985
[2m[36m(func pid=37854)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=37854)[0m f1_macro: 0.2931434527304234
[2m[36m(func pid=37854)[0m f1_weighted: 0.27985306974536334
[2m[36m(func pid=37854)[0m f1_per_class: [0.138, 0.536, 0.833, 0.416, 0.01, 0.168, 0.057, 0.381, 0.038, 0.353]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5454 | Steps: 4 | Val loss: 2.9796 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=37733)[0m top1: 0.44216417910447764
[2m[36m(func pid=37733)[0m top5: 0.9039179104477612
[2m[36m(func pid=37733)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=37733)[0m f1_macro: 0.36443495179558766
[2m[36m(func pid=37733)[0m f1_weighted: 0.46918436900516575
[2m[36m(func pid=37733)[0m f1_per_class: [0.371, 0.473, 0.186, 0.57, 0.101, 0.361, 0.456, 0.466, 0.342, 0.318]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3029 | Steps: 4 | Val loss: 2.3886 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=37804)[0m top1: 0.3306902985074627
[2m[36m(func pid=37804)[0m top5: 0.8572761194029851
[2m[36m(func pid=37804)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=37804)[0m f1_macro: 0.22030747893907168
[2m[36m(func pid=37804)[0m f1_weighted: 0.3313378898585008
[2m[36m(func pid=37804)[0m f1_per_class: [0.0, 0.489, 0.375, 0.5, 0.076, 0.0, 0.322, 0.028, 0.107, 0.305]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7641 | Steps: 4 | Val loss: 1.5579 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=37854)[0m top1: 0.18516791044776118
[2m[36m(func pid=37854)[0m top5: 0.784981343283582
[2m[36m(func pid=37854)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=37854)[0m f1_macro: 0.20300464388912026
[2m[36m(func pid=37854)[0m f1_weighted: 0.18716485694627583
[2m[36m(func pid=37854)[0m f1_per_class: [0.12, 0.196, 0.381, 0.391, 0.035, 0.079, 0.0, 0.418, 0.056, 0.353]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.9121 | Steps: 4 | Val loss: 2.8136 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:48:26 (running for 00:14:52.08)
Memory usage on this node: 23.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.754 |      0.364 |                   52 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.545 |      0.22  |                   52 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.303 |      0.203 |                   53 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=50960)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=50960)[0m Configuration completed!
[2m[36m(func pid=50960)[0m New optimizer parameters:
[2m[36m(func pid=50960)[0m SGD (
[2m[36m(func pid=50960)[0m Parameter Group 0
[2m[36m(func pid=50960)[0m     dampening: 0
[2m[36m(func pid=50960)[0m     differentiable: False
[2m[36m(func pid=50960)[0m     foreach: None
[2m[36m(func pid=50960)[0m     lr: 0.0001
[2m[36m(func pid=50960)[0m     maximize: False
[2m[36m(func pid=50960)[0m     momentum: 0.99
[2m[36m(func pid=50960)[0m     nesterov: False
[2m[36m(func pid=50960)[0m     weight_decay: 0.0001
[2m[36m(func pid=50960)[0m )
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4594216417910448
[2m[36m(func pid=37733)[0m top5: 0.9174440298507462
[2m[36m(func pid=37733)[0m f1_micro: 0.4594216417910448
[2m[36m(func pid=37733)[0m f1_macro: 0.37237624780293954
[2m[36m(func pid=37733)[0m f1_weighted: 0.48293186384878733
[2m[36m(func pid=37733)[0m f1_per_class: [0.382, 0.409, 0.218, 0.596, 0.116, 0.376, 0.509, 0.475, 0.312, 0.331]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.9372 | Steps: 4 | Val loss: 2.1481 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37804)[0m top1: 0.425839552238806
[2m[36m(func pid=37804)[0m top5: 0.871268656716418
[2m[36m(func pid=37804)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=37804)[0m f1_macro: 0.31989326227103043
[2m[36m(func pid=37804)[0m f1_weighted: 0.42108728364547626
[2m[36m(func pid=37804)[0m f1_per_class: [0.085, 0.476, 0.741, 0.526, 0.088, 0.038, 0.529, 0.224, 0.217, 0.275]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0021 | Steps: 4 | Val loss: 1.5406 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:48:31 (running for 00:14:57.42)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.764 |      0.372 |                   53 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.912 |      0.32  |                   53 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.937 |      0.216 |                   54 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.22994402985074627
[2m[36m(func pid=37854)[0m top5: 0.7882462686567164
[2m[36m(func pid=37854)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=37854)[0m f1_macro: 0.21585297934960063
[2m[36m(func pid=37854)[0m f1_weighted: 0.21385071265764904
[2m[36m(func pid=37854)[0m f1_per_class: [0.17, 0.157, 0.344, 0.434, 0.052, 0.277, 0.0, 0.379, 0.104, 0.242]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9374 | Steps: 4 | Val loss: 2.3232 | Batch size: 32 | lr: 0.0001 | Duration: 4.77s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7379 | Steps: 4 | Val loss: 3.2706 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=37733)[0m top1: 0.46968283582089554
[2m[36m(func pid=37733)[0m top5: 0.9202425373134329
[2m[36m(func pid=37733)[0m f1_micro: 0.46968283582089554
[2m[36m(func pid=37733)[0m f1_macro: 0.391458168656704
[2m[36m(func pid=37733)[0m f1_weighted: 0.49517190559023305
[2m[36m(func pid=37733)[0m f1_per_class: [0.455, 0.454, 0.24, 0.6, 0.117, 0.353, 0.513, 0.485, 0.395, 0.303]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.7182 | Steps: 4 | Val loss: 1.9448 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=50960)[0m top1: 0.1669776119402985
[2m[36m(func pid=50960)[0m top5: 0.5326492537313433
[2m[36m(func pid=50960)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=50960)[0m f1_macro: 0.10268259363316065
[2m[36m(func pid=50960)[0m f1_weighted: 0.1201928914252709
[2m[36m(func pid=50960)[0m f1_per_class: [0.247, 0.3, 0.0, 0.103, 0.01, 0.261, 0.009, 0.022, 0.0, 0.075]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.3148320895522388
[2m[36m(func pid=37804)[0m top5: 0.8819962686567164
[2m[36m(func pid=37804)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=37804)[0m f1_macro: 0.35958857034642067
[2m[36m(func pid=37804)[0m f1_weighted: 0.33446587043179427
[2m[36m(func pid=37804)[0m f1_per_class: [0.458, 0.539, 0.759, 0.321, 0.144, 0.364, 0.244, 0.221, 0.166, 0.381]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.8381 | Steps: 4 | Val loss: 1.5210 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:48:37 (running for 00:15:02.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  1.002 |      0.391 |                   54 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.738 |      0.36  |                   54 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.718 |      0.261 |                   55 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.937 |      0.103 |                    1 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.30130597014925375
[2m[36m(func pid=37854)[0m top5: 0.8064365671641791
[2m[36m(func pid=37854)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=37854)[0m f1_macro: 0.26139064188709626
[2m[36m(func pid=37854)[0m f1_weighted: 0.2756537347712465
[2m[36m(func pid=37854)[0m f1_per_class: [0.171, 0.426, 0.261, 0.452, 0.056, 0.374, 0.0, 0.334, 0.119, 0.421]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0000 | Steps: 4 | Val loss: 2.3423 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5951 | Steps: 4 | Val loss: 5.8265 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=37733)[0m top1: 0.4626865671641791
[2m[36m(func pid=37733)[0m top5: 0.9244402985074627
[2m[36m(func pid=37733)[0m f1_micro: 0.4626865671641791
[2m[36m(func pid=37733)[0m f1_macro: 0.3783154650348727
[2m[36m(func pid=37733)[0m f1_weighted: 0.49231139899793397
[2m[36m(func pid=37733)[0m f1_per_class: [0.487, 0.471, 0.207, 0.552, 0.123, 0.325, 0.565, 0.407, 0.394, 0.251]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.8991 | Steps: 4 | Val loss: 1.8969 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=50960)[0m top1: 0.15391791044776118
[2m[36m(func pid=50960)[0m top5: 0.5181902985074627
[2m[36m(func pid=50960)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=50960)[0m f1_macro: 0.08544009306353917
[2m[36m(func pid=50960)[0m f1_weighted: 0.11503357724940526
[2m[36m(func pid=50960)[0m f1_per_class: [0.121, 0.252, 0.0, 0.117, 0.027, 0.275, 0.012, 0.017, 0.0, 0.034]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.16884328358208955
[2m[36m(func pid=37804)[0m top5: 0.7574626865671642
[2m[36m(func pid=37804)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=37804)[0m f1_macro: 0.21074361247840617
[2m[36m(func pid=37804)[0m f1_weighted: 0.13612458074046146
[2m[36m(func pid=37804)[0m f1_per_class: [0.322, 0.427, 0.611, 0.055, 0.126, 0.16, 0.04, 0.0, 0.081, 0.286]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7784 | Steps: 4 | Val loss: 1.5046 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:48:42 (running for 00:15:08.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.838 |      0.378 |                   55 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.595 |      0.211 |                   55 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.899 |      0.271 |                   56 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  3     |      0.085 |                    2 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.3572761194029851
[2m[36m(func pid=37854)[0m top5: 0.8101679104477612
[2m[36m(func pid=37854)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=37854)[0m f1_macro: 0.2709747696922564
[2m[36m(func pid=37854)[0m f1_weighted: 0.30879545437546546
[2m[36m(func pid=37854)[0m f1_per_class: [0.142, 0.534, 0.48, 0.519, 0.0, 0.331, 0.0, 0.368, 0.154, 0.182]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4569 | Steps: 4 | Val loss: 3.6509 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9867 | Steps: 4 | Val loss: 2.3552 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=37733)[0m top1: 0.4701492537313433
[2m[36m(func pid=37733)[0m top5: 0.9319029850746269
[2m[36m(func pid=37733)[0m f1_micro: 0.47014925373134325
[2m[36m(func pid=37733)[0m f1_macro: 0.38986748074548067
[2m[36m(func pid=37733)[0m f1_weighted: 0.4976512590053314
[2m[36m(func pid=37733)[0m f1_per_class: [0.543, 0.485, 0.24, 0.542, 0.114, 0.33, 0.582, 0.344, 0.46, 0.257]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.8215 | Steps: 4 | Val loss: 1.9312 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37804)[0m top1: 0.30783582089552236
[2m[36m(func pid=37804)[0m top5: 0.8059701492537313
[2m[36m(func pid=37804)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=37804)[0m f1_macro: 0.2574969932503195
[2m[36m(func pid=37804)[0m f1_weighted: 0.3062290603476183
[2m[36m(func pid=37804)[0m f1_per_class: [0.167, 0.417, 0.667, 0.06, 0.074, 0.261, 0.57, 0.027, 0.207, 0.124]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=50960)[0m top1: 0.14412313432835822
[2m[36m(func pid=50960)[0m top5: 0.5111940298507462
[2m[36m(func pid=50960)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=50960)[0m f1_macro: 0.07612282074730498
[2m[36m(func pid=50960)[0m f1_weighted: 0.11405056490302308
[2m[36m(func pid=50960)[0m f1_per_class: [0.075, 0.228, 0.0, 0.107, 0.008, 0.276, 0.033, 0.034, 0.0, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6938 | Steps: 4 | Val loss: 1.5172 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:48:48 (running for 00:15:13.75)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.778 |      0.39  |                   56 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.457 |      0.257 |                   56 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.822 |      0.278 |                   57 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.987 |      0.076 |                    3 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.35027985074626866
[2m[36m(func pid=37854)[0m top5: 0.8027052238805971
[2m[36m(func pid=37854)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=37854)[0m f1_macro: 0.2778013476402016
[2m[36m(func pid=37854)[0m f1_weighted: 0.30503036389010835
[2m[36m(func pid=37854)[0m f1_per_class: [0.158, 0.466, 0.533, 0.542, 0.0, 0.323, 0.0, 0.393, 0.148, 0.213]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9135 | Steps: 4 | Val loss: 2.3388 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4157 | Steps: 4 | Val loss: 2.9072 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37733)[0m top1: 0.4594216417910448
[2m[36m(func pid=37733)[0m top5: 0.9239738805970149
[2m[36m(func pid=37733)[0m f1_micro: 0.4594216417910448
[2m[36m(func pid=37733)[0m f1_macro: 0.39289008089848243
[2m[36m(func pid=37733)[0m f1_weighted: 0.4880544375073388
[2m[36m(func pid=37733)[0m f1_per_class: [0.49, 0.477, 0.222, 0.521, 0.116, 0.338, 0.554, 0.467, 0.415, 0.329]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.6357 | Steps: 4 | Val loss: 2.0189 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37804)[0m top1: 0.29757462686567165
[2m[36m(func pid=37804)[0m top5: 0.8572761194029851
[2m[36m(func pid=37804)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=37804)[0m f1_macro: 0.27731984792671666
[2m[36m(func pid=37804)[0m f1_weighted: 0.3493863737560441
[2m[36m(func pid=37804)[0m f1_per_class: [0.26, 0.385, 0.72, 0.351, 0.077, 0.147, 0.488, 0.097, 0.185, 0.061]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=50960)[0m top1: 0.14039179104477612
[2m[36m(func pid=50960)[0m top5: 0.5317164179104478
[2m[36m(func pid=50960)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=50960)[0m f1_macro: 0.08311647519127349
[2m[36m(func pid=50960)[0m f1_weighted: 0.1204411254752893
[2m[36m(func pid=50960)[0m f1_per_class: [0.044, 0.2, 0.067, 0.138, 0.018, 0.259, 0.044, 0.05, 0.011, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6775 | Steps: 4 | Val loss: 1.5522 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:48:53 (running for 00:15:19.24)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.694 |      0.393 |                   57 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.416 |      0.277 |                   57 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.636 |      0.311 |                   58 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.914 |      0.083 |                    4 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.32136194029850745
[2m[36m(func pid=37854)[0m top5: 0.8199626865671642
[2m[36m(func pid=37854)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=37854)[0m f1_macro: 0.3106493795768521
[2m[36m(func pid=37854)[0m f1_weighted: 0.2850233162241384
[2m[36m(func pid=37854)[0m f1_per_class: [0.16, 0.53, 0.75, 0.415, 0.0, 0.353, 0.0, 0.343, 0.166, 0.389]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9111 | Steps: 4 | Val loss: 2.3136 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5109 | Steps: 4 | Val loss: 2.5494 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=37733)[0m top1: 0.4375
[2m[36m(func pid=37733)[0m top5: 0.9211753731343284
[2m[36m(func pid=37733)[0m f1_micro: 0.4375
[2m[36m(func pid=37733)[0m f1_macro: 0.3850895182823208
[2m[36m(func pid=37733)[0m f1_weighted: 0.4672171992524209
[2m[36m(func pid=37733)[0m f1_per_class: [0.484, 0.46, 0.175, 0.491, 0.109, 0.343, 0.516, 0.494, 0.392, 0.386]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.14738805970149255
[2m[36m(func pid=50960)[0m top5: 0.5694962686567164
[2m[36m(func pid=50960)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=50960)[0m f1_macro: 0.09241660447823638
[2m[36m(func pid=50960)[0m f1_weighted: 0.1293403651191856
[2m[36m(func pid=50960)[0m f1_per_class: [0.059, 0.207, 0.103, 0.14, 0.017, 0.269, 0.061, 0.07, 0.0, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.7298 | Steps: 4 | Val loss: 2.7037 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37804)[0m top1: 0.3885261194029851
[2m[36m(func pid=37804)[0m top5: 0.8624067164179104
[2m[36m(func pid=37804)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=37804)[0m f1_macro: 0.277057209247861
[2m[36m(func pid=37804)[0m f1_weighted: 0.4024739890465903
[2m[36m(func pid=37804)[0m f1_per_class: [0.403, 0.474, 0.468, 0.477, 0.211, 0.026, 0.567, 0.0, 0.067, 0.078]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6741 | Steps: 4 | Val loss: 1.5945 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 14:48:59 (running for 00:15:24.71)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.678 |      0.385 |                   58 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.511 |      0.277 |                   58 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.73  |      0.197 |                   59 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.911 |      0.092 |                    5 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.21455223880597016
[2m[36m(func pid=37854)[0m top5: 0.7173507462686567
[2m[36m(func pid=37854)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=37854)[0m f1_macro: 0.1973513407936221
[2m[36m(func pid=37854)[0m f1_weighted: 0.24850353239160414
[2m[36m(func pid=37854)[0m f1_per_class: [0.112, 0.415, 0.27, 0.267, 0.0, 0.247, 0.161, 0.321, 0.09, 0.089]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9000 | Steps: 4 | Val loss: 2.2854 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5994 | Steps: 4 | Val loss: 3.4116 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37733)[0m top1: 0.41511194029850745
[2m[36m(func pid=37733)[0m top5: 0.9188432835820896
[2m[36m(func pid=37733)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=37733)[0m f1_macro: 0.3624710354606612
[2m[36m(func pid=37733)[0m f1_weighted: 0.4458171077626781
[2m[36m(func pid=37733)[0m f1_per_class: [0.444, 0.425, 0.173, 0.496, 0.107, 0.316, 0.484, 0.482, 0.332, 0.367]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.16744402985074627
[2m[36m(func pid=50960)[0m top5: 0.6040111940298507
[2m[36m(func pid=50960)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=50960)[0m f1_macro: 0.11054903098512754
[2m[36m(func pid=50960)[0m f1_weighted: 0.15849018945182008
[2m[36m(func pid=50960)[0m f1_per_class: [0.079, 0.202, 0.146, 0.203, 0.017, 0.279, 0.092, 0.086, 0.0, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.8422 | Steps: 4 | Val loss: 3.0971 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=37804)[0m top1: 0.2891791044776119
[2m[36m(func pid=37804)[0m top5: 0.8050373134328358
[2m[36m(func pid=37804)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=37804)[0m f1_macro: 0.1926946678811488
[2m[36m(func pid=37804)[0m f1_weighted: 0.2847603304303382
[2m[36m(func pid=37804)[0m f1_per_class: [0.215, 0.49, 0.304, 0.201, 0.174, 0.008, 0.45, 0.0, 0.047, 0.04]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7488 | Steps: 4 | Val loss: 1.6007 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 14:49:04 (running for 00:15:30.12)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.674 |      0.362 |                   59 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.599 |      0.193 |                   59 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.842 |      0.101 |                   60 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.9   |      0.111 |                    6 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.12453358208955224
[2m[36m(func pid=37854)[0m top5: 0.5984141791044776
[2m[36m(func pid=37854)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=37854)[0m f1_macro: 0.1012807158824239
[2m[36m(func pid=37854)[0m f1_weighted: 0.1346269084608178
[2m[36m(func pid=37854)[0m f1_per_class: [0.142, 0.066, 0.087, 0.319, 0.126, 0.064, 0.064, 0.0, 0.087, 0.058]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8724 | Steps: 4 | Val loss: 2.2554 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5146 | Steps: 4 | Val loss: 3.6317 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=37733)[0m top1: 0.43796641791044777
[2m[36m(func pid=37733)[0m top5: 0.9249067164179104
[2m[36m(func pid=37733)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=37733)[0m f1_macro: 0.38334095555861586
[2m[36m(func pid=37733)[0m f1_weighted: 0.4590935877859272
[2m[36m(func pid=37733)[0m f1_per_class: [0.496, 0.379, 0.289, 0.554, 0.112, 0.378, 0.483, 0.433, 0.277, 0.432]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.18003731343283583
[2m[36m(func pid=50960)[0m top5: 0.648320895522388
[2m[36m(func pid=50960)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=50960)[0m f1_macro: 0.11980415335012544
[2m[36m(func pid=50960)[0m f1_weighted: 0.17646693867743887
[2m[36m(func pid=50960)[0m f1_per_class: [0.089, 0.221, 0.125, 0.232, 0.022, 0.292, 0.104, 0.112, 0.0, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.6053 | Steps: 4 | Val loss: 2.7361 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=37804)[0m top1: 0.27425373134328357
[2m[36m(func pid=37804)[0m top5: 0.7378731343283582
[2m[36m(func pid=37804)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=37804)[0m f1_macro: 0.17652104901998467
[2m[36m(func pid=37804)[0m f1_weighted: 0.23718084267624398
[2m[36m(func pid=37804)[0m f1_per_class: [0.13, 0.414, 0.214, 0.212, 0.179, 0.036, 0.307, 0.0, 0.121, 0.15]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6418 | Steps: 4 | Val loss: 1.5806 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 14:49:09 (running for 00:15:35.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.749 |      0.383 |                   60 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.515 |      0.177 |                   60 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.605 |      0.136 |                   61 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.872 |      0.12  |                    7 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.16977611940298507
[2m[36m(func pid=37854)[0m top5: 0.6534514925373134
[2m[36m(func pid=37854)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=37854)[0m f1_macro: 0.13639465380432592
[2m[36m(func pid=37854)[0m f1_weighted: 0.18336346351449828
[2m[36m(func pid=37854)[0m f1_per_class: [0.211, 0.076, 0.15, 0.401, 0.046, 0.05, 0.14, 0.0, 0.1, 0.19]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8624 | Steps: 4 | Val loss: 2.2386 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4567 | Steps: 4 | Val loss: 3.3556 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=37733)[0m top1: 0.439365671641791
[2m[36m(func pid=37733)[0m top5: 0.9221082089552238
[2m[36m(func pid=37733)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=37733)[0m f1_macro: 0.3900871735407069
[2m[36m(func pid=37733)[0m f1_weighted: 0.4666593536545747
[2m[36m(func pid=37733)[0m f1_per_class: [0.542, 0.386, 0.276, 0.534, 0.136, 0.343, 0.528, 0.48, 0.244, 0.432]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5147 | Steps: 4 | Val loss: 2.2088 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=50960)[0m top1: 0.18703358208955223
[2m[36m(func pid=50960)[0m top5: 0.6697761194029851
[2m[36m(func pid=50960)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=50960)[0m f1_macro: 0.13683795191898646
[2m[36m(func pid=50960)[0m f1_weighted: 0.18963228612901645
[2m[36m(func pid=50960)[0m f1_per_class: [0.077, 0.233, 0.231, 0.265, 0.026, 0.297, 0.104, 0.125, 0.011, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.33861940298507465
[2m[36m(func pid=37804)[0m top5: 0.878731343283582
[2m[36m(func pid=37804)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=37804)[0m f1_macro: 0.2681780138629613
[2m[36m(func pid=37804)[0m f1_weighted: 0.3011532978103023
[2m[36m(func pid=37804)[0m f1_per_class: [0.267, 0.533, 0.333, 0.187, 0.139, 0.057, 0.386, 0.299, 0.2, 0.282]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6116 | Steps: 4 | Val loss: 1.5718 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 14:49:15 (running for 00:15:40.82)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.642 |      0.39  |                   61 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.457 |      0.268 |                   61 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.515 |      0.204 |                   62 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.862 |      0.137 |                    8 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.30597014925373134
[2m[36m(func pid=37854)[0m top5: 0.8218283582089553
[2m[36m(func pid=37854)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=37854)[0m f1_macro: 0.20449496831240918
[2m[36m(func pid=37854)[0m f1_weighted: 0.3409938283921796
[2m[36m(func pid=37854)[0m f1_per_class: [0.206, 0.249, 0.182, 0.45, 0.059, 0.14, 0.484, 0.0, 0.153, 0.121]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7922 | Steps: 4 | Val loss: 2.2190 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.9648 | Steps: 4 | Val loss: 3.1697 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37733)[0m top1: 0.43236940298507465
[2m[36m(func pid=37733)[0m top5: 0.9281716417910447
[2m[36m(func pid=37733)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=37733)[0m f1_macro: 0.38229073731997926
[2m[36m(func pid=37733)[0m f1_weighted: 0.46030060897909625
[2m[36m(func pid=37733)[0m f1_per_class: [0.496, 0.455, 0.176, 0.494, 0.132, 0.398, 0.483, 0.495, 0.279, 0.416]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.7128 | Steps: 4 | Val loss: 2.1934 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=50960)[0m top1: 0.20708955223880596
[2m[36m(func pid=50960)[0m top5: 0.6949626865671642
[2m[36m(func pid=50960)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=50960)[0m f1_macro: 0.1572539975929534
[2m[36m(func pid=50960)[0m f1_weighted: 0.22130820373945947
[2m[36m(func pid=50960)[0m f1_per_class: [0.096, 0.245, 0.28, 0.321, 0.026, 0.285, 0.147, 0.144, 0.028, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.34281716417910446
[2m[36m(func pid=37804)[0m top5: 0.9039179104477612
[2m[36m(func pid=37804)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=37804)[0m f1_macro: 0.2814632413363263
[2m[36m(func pid=37804)[0m f1_weighted: 0.30697824116198014
[2m[36m(func pid=37804)[0m f1_per_class: [0.167, 0.519, 0.444, 0.149, 0.167, 0.191, 0.41, 0.301, 0.08, 0.386]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5089 | Steps: 4 | Val loss: 1.6304 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:49:20 (running for 00:15:46.23)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.612 |      0.382 |                   62 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.965 |      0.281 |                   62 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.713 |      0.209 |                   63 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.792 |      0.157 |                    9 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.3041044776119403
[2m[36m(func pid=37854)[0m top5: 0.8274253731343284
[2m[36m(func pid=37854)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=37854)[0m f1_macro: 0.209051483627995
[2m[36m(func pid=37854)[0m f1_weighted: 0.3286854366121255
[2m[36m(func pid=37854)[0m f1_per_class: [0.159, 0.363, 0.226, 0.277, 0.068, 0.087, 0.546, 0.076, 0.15, 0.138]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7730 | Steps: 4 | Val loss: 2.1991 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3442 | Steps: 4 | Val loss: 2.9743 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=37733)[0m top1: 0.40158582089552236
[2m[36m(func pid=37733)[0m top5: 0.9221082089552238
[2m[36m(func pid=37733)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=37733)[0m f1_macro: 0.360568859650051
[2m[36m(func pid=37733)[0m f1_weighted: 0.4234114801462454
[2m[36m(func pid=37733)[0m f1_per_class: [0.474, 0.454, 0.161, 0.432, 0.121, 0.377, 0.428, 0.474, 0.335, 0.35]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.7377 | Steps: 4 | Val loss: 2.3242 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=50960)[0m top1: 0.23460820895522388
[2m[36m(func pid=50960)[0m top5: 0.7164179104477612
[2m[36m(func pid=50960)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=50960)[0m f1_macro: 0.18426766948127724
[2m[36m(func pid=50960)[0m f1_weighted: 0.24827184775148617
[2m[36m(func pid=50960)[0m f1_per_class: [0.124, 0.267, 0.391, 0.377, 0.027, 0.301, 0.158, 0.157, 0.039, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.2989738805970149
[2m[36m(func pid=37804)[0m top5: 0.9011194029850746
[2m[36m(func pid=37804)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=37804)[0m f1_macro: 0.21867776058464
[2m[36m(func pid=37804)[0m f1_weighted: 0.3031917036562093
[2m[36m(func pid=37804)[0m f1_per_class: [0.087, 0.354, 0.265, 0.278, 0.179, 0.277, 0.39, 0.129, 0.096, 0.131]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6235 | Steps: 4 | Val loss: 1.5658 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 14:49:25 (running for 00:15:51.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.509 |      0.361 |                   63 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.344 |      0.219 |                   63 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.738 |      0.243 |                   64 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.773 |      0.184 |                   10 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.26259328358208955
[2m[36m(func pid=37854)[0m top5: 0.8022388059701493
[2m[36m(func pid=37854)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=37854)[0m f1_macro: 0.2427215789857627
[2m[36m(func pid=37854)[0m f1_weighted: 0.30151354177631035
[2m[36m(func pid=37854)[0m f1_per_class: [0.161, 0.315, 0.373, 0.298, 0.066, 0.022, 0.414, 0.454, 0.111, 0.214]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7327 | Steps: 4 | Val loss: 2.1807 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.2771 | Steps: 4 | Val loss: 2.9784 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=37733)[0m top1: 0.4216417910447761
[2m[36m(func pid=37733)[0m top5: 0.9263059701492538
[2m[36m(func pid=37733)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=37733)[0m f1_macro: 0.3749483732720983
[2m[36m(func pid=37733)[0m f1_weighted: 0.4431965323895798
[2m[36m(func pid=37733)[0m f1_per_class: [0.504, 0.453, 0.224, 0.478, 0.118, 0.373, 0.449, 0.466, 0.363, 0.321]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.1964 | Steps: 4 | Val loss: 2.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=50960)[0m top1: 0.24720149253731344
[2m[36m(func pid=50960)[0m top5: 0.7397388059701493
[2m[36m(func pid=50960)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=50960)[0m f1_macro: 0.19846985975296758
[2m[36m(func pid=50960)[0m f1_weighted: 0.26412781438739874
[2m[36m(func pid=50960)[0m f1_per_class: [0.144, 0.257, 0.435, 0.413, 0.027, 0.306, 0.175, 0.167, 0.061, 0.0]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.3045708955223881
[2m[36m(func pid=37804)[0m top5: 0.8540111940298507
[2m[36m(func pid=37804)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=37804)[0m f1_macro: 0.23905684373921837
[2m[36m(func pid=37804)[0m f1_weighted: 0.3362409896967588
[2m[36m(func pid=37804)[0m f1_per_class: [0.394, 0.339, 0.261, 0.467, 0.206, 0.092, 0.403, 0.0, 0.144, 0.084]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5836 | Steps: 4 | Val loss: 1.5044 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:49:31 (running for 00:15:56.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.624 |      0.375 |                   64 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  1.277 |      0.239 |                   64 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.196 |      0.243 |                   65 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.733 |      0.198 |                   11 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.22621268656716417
[2m[36m(func pid=37854)[0m top5: 0.7994402985074627
[2m[36m(func pid=37854)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=37854)[0m f1_macro: 0.24309829065717214
[2m[36m(func pid=37854)[0m f1_weighted: 0.23725250627168246
[2m[36m(func pid=37854)[0m f1_per_class: [0.189, 0.27, 0.545, 0.334, 0.074, 0.14, 0.149, 0.403, 0.124, 0.204]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7015 | Steps: 4 | Val loss: 2.1667 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3416 | Steps: 4 | Val loss: 3.7878 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=37733)[0m top1: 0.45149253731343286
[2m[36m(func pid=37733)[0m top5: 0.9305037313432836
[2m[36m(func pid=37733)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=37733)[0m f1_macro: 0.4223263843892268
[2m[36m(func pid=37733)[0m f1_weighted: 0.47505389626469485
[2m[36m(func pid=37733)[0m f1_per_class: [0.611, 0.466, 0.387, 0.519, 0.105, 0.371, 0.488, 0.485, 0.44, 0.352]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6961 | Steps: 4 | Val loss: 2.4212 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=50960)[0m top1: 0.2490671641791045
[2m[36m(func pid=50960)[0m top5: 0.7513992537313433
[2m[36m(func pid=50960)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=50960)[0m f1_macro: 0.20612168630142036
[2m[36m(func pid=50960)[0m f1_weighted: 0.26830931247742845
[2m[36m(func pid=50960)[0m f1_per_class: [0.142, 0.274, 0.435, 0.413, 0.03, 0.308, 0.175, 0.177, 0.066, 0.041]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.19076492537313433
[2m[36m(func pid=37804)[0m top5: 0.753731343283582
[2m[36m(func pid=37804)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=37804)[0m f1_macro: 0.22352707505079578
[2m[36m(func pid=37804)[0m f1_weighted: 0.19039016963318006
[2m[36m(func pid=37804)[0m f1_per_class: [0.351, 0.269, 0.541, 0.314, 0.109, 0.141, 0.031, 0.223, 0.192, 0.065]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:49:36 (running for 00:16:02.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.584 |      0.422 |                   65 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.342 |      0.224 |                   65 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.696 |      0.243 |                   66 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.701 |      0.206 |                   12 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.20662313432835822
[2m[36m(func pid=37854)[0m top5: 0.7924440298507462
[2m[36m(func pid=37854)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=37854)[0m f1_macro: 0.24267685341190837
[2m[36m(func pid=37854)[0m f1_weighted: 0.2518064726151329
[2m[36m(func pid=37854)[0m f1_per_class: [0.175, 0.202, 0.769, 0.369, 0.0, 0.148, 0.203, 0.449, 0.051, 0.059]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6954 | Steps: 4 | Val loss: 1.5188 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6157 | Steps: 4 | Val loss: 2.1538 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2357 | Steps: 4 | Val loss: 3.2956 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37733)[0m top1: 0.45848880597014924
[2m[36m(func pid=37733)[0m top5: 0.9225746268656716
[2m[36m(func pid=37733)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=37733)[0m f1_macro: 0.417048742798351
[2m[36m(func pid=37733)[0m f1_weighted: 0.4832747243096478
[2m[36m(func pid=37733)[0m f1_per_class: [0.514, 0.454, 0.421, 0.552, 0.096, 0.362, 0.508, 0.473, 0.378, 0.412]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.9419 | Steps: 4 | Val loss: 2.4787 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=50960)[0m top1: 0.26119402985074625
[2m[36m(func pid=50960)[0m top5: 0.7513992537313433
[2m[36m(func pid=50960)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=50960)[0m f1_macro: 0.22630639387118895
[2m[36m(func pid=50960)[0m f1_weighted: 0.2812374427829257
[2m[36m(func pid=50960)[0m f1_per_class: [0.172, 0.294, 0.55, 0.423, 0.044, 0.309, 0.193, 0.172, 0.072, 0.033]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m top1: 0.23787313432835822
[2m[36m(func pid=37804)[0m top5: 0.8031716417910447
[2m[36m(func pid=37804)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=37804)[0m f1_macro: 0.2334477023859387
[2m[36m(func pid=37804)[0m f1_weighted: 0.2325774321358192
[2m[36m(func pid=37804)[0m f1_per_class: [0.302, 0.426, 0.364, 0.247, 0.091, 0.247, 0.102, 0.264, 0.19, 0.103]
[2m[36m(func pid=37804)[0m 
== Status ==
Current time: 2024-01-07 14:49:41 (running for 00:16:07.41)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.695 |      0.417 |                   66 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.236 |      0.233 |                   66 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.942 |      0.201 |                   67 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.616 |      0.226 |                   13 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.23880597014925373
[2m[36m(func pid=37854)[0m top5: 0.7779850746268657
[2m[36m(func pid=37854)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=37854)[0m f1_macro: 0.2006455784977968
[2m[36m(func pid=37854)[0m f1_weighted: 0.2875266103560323
[2m[36m(func pid=37854)[0m f1_per_class: [0.154, 0.124, 0.118, 0.335, 0.0, 0.262, 0.357, 0.48, 0.127, 0.05]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6738 | Steps: 4 | Val loss: 1.6322 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6249 | Steps: 4 | Val loss: 2.1421 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6204 | Steps: 4 | Val loss: 3.0979 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=37733)[0m top1: 0.435634328358209
[2m[36m(func pid=37733)[0m top5: 0.9039179104477612
[2m[36m(func pid=37733)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=37733)[0m f1_macro: 0.40621927929924107
[2m[36m(func pid=37733)[0m f1_weighted: 0.46356263558207256
[2m[36m(func pid=37733)[0m f1_per_class: [0.467, 0.433, 0.49, 0.549, 0.073, 0.326, 0.485, 0.426, 0.333, 0.481]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7928 | Steps: 4 | Val loss: 3.1492 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=50960)[0m top1: 0.24766791044776118
[2m[36m(func pid=50960)[0m top5: 0.7588619402985075
[2m[36m(func pid=50960)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=50960)[0m f1_macro: 0.22455506952702847
[2m[36m(func pid=50960)[0m f1_weighted: 0.27391967953298324
[2m[36m(func pid=50960)[0m f1_per_class: [0.202, 0.304, 0.512, 0.367, 0.038, 0.299, 0.218, 0.154, 0.094, 0.059]
[2m[36m(func pid=50960)[0m 
== Status ==
Current time: 2024-01-07 14:49:46 (running for 00:16:12.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.674 |      0.406 |                   67 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.62  |      0.259 |                   67 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.942 |      0.201 |                   67 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.625 |      0.225 |                   14 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m top1: 0.29617537313432835
[2m[36m(func pid=37804)[0m top5: 0.8166977611940298
[2m[36m(func pid=37804)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=37804)[0m f1_macro: 0.25863317238768213
[2m[36m(func pid=37804)[0m f1_weighted: 0.3158769213714363
[2m[36m(func pid=37804)[0m f1_per_class: [0.316, 0.487, 0.189, 0.302, 0.096, 0.238, 0.299, 0.248, 0.182, 0.229]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.20708955223880596
[2m[36m(func pid=37854)[0m top5: 0.7602611940298507
[2m[36m(func pid=37854)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=37854)[0m f1_macro: 0.15155322068876104
[2m[36m(func pid=37854)[0m f1_weighted: 0.20950832353010104
[2m[36m(func pid=37854)[0m f1_per_class: [0.123, 0.036, 0.0, 0.282, 0.081, 0.275, 0.222, 0.375, 0.041, 0.081]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7295 | Steps: 4 | Val loss: 1.6942 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5964 | Steps: 4 | Val loss: 2.1277 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8638 | Steps: 4 | Val loss: 3.0141 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7639 | Steps: 4 | Val loss: 2.6824 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=37733)[0m top1: 0.4253731343283582
[2m[36m(func pid=37733)[0m top5: 0.8959888059701493
[2m[36m(func pid=37733)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=37733)[0m f1_macro: 0.3786999626027845
[2m[36m(func pid=37733)[0m f1_weighted: 0.45297321464010604
[2m[36m(func pid=37733)[0m f1_per_class: [0.39, 0.414, 0.333, 0.569, 0.073, 0.326, 0.444, 0.465, 0.332, 0.442]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.2630597014925373
[2m[36m(func pid=50960)[0m top5: 0.7677238805970149
[2m[36m(func pid=50960)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=50960)[0m f1_macro: 0.2306106801105372
[2m[36m(func pid=50960)[0m f1_weighted: 0.2873151611842362
[2m[36m(func pid=50960)[0m f1_per_class: [0.241, 0.319, 0.512, 0.408, 0.046, 0.278, 0.219, 0.179, 0.075, 0.029]
[2m[36m(func pid=50960)[0m 
== Status ==
Current time: 2024-01-07 14:49:52 (running for 00:16:18.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.73  |      0.379 |                   68 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.864 |      0.264 |                   68 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.793 |      0.152 |                   68 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.596 |      0.231 |                   15 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m top1: 0.37826492537313433
[2m[36m(func pid=37804)[0m top5: 0.7756529850746269
[2m[36m(func pid=37804)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=37804)[0m f1_macro: 0.2642298569813791
[2m[36m(func pid=37804)[0m f1_weighted: 0.3536898333658853
[2m[36m(func pid=37804)[0m f1_per_class: [0.286, 0.468, 0.237, 0.212, 0.186, 0.016, 0.602, 0.254, 0.233, 0.15]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.23180970149253732
[2m[36m(func pid=37854)[0m top5: 0.7868470149253731
[2m[36m(func pid=37854)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=37854)[0m f1_macro: 0.17945524380441086
[2m[36m(func pid=37854)[0m f1_weighted: 0.23561999499807995
[2m[36m(func pid=37854)[0m f1_per_class: [0.11, 0.056, 0.179, 0.287, 0.078, 0.18, 0.322, 0.358, 0.106, 0.119]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5977 | Steps: 4 | Val loss: 1.6187 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5582 | Steps: 4 | Val loss: 2.1099 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6096 | Steps: 4 | Val loss: 3.1076 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6204 | Steps: 4 | Val loss: 2.0621 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37733)[0m top1: 0.439365671641791
[2m[36m(func pid=37733)[0m top5: 0.9146455223880597
[2m[36m(func pid=37733)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=37733)[0m f1_macro: 0.38016996212585247
[2m[36m(func pid=37733)[0m f1_weighted: 0.4667768478939099
[2m[36m(func pid=37733)[0m f1_per_class: [0.431, 0.489, 0.202, 0.571, 0.102, 0.337, 0.437, 0.458, 0.378, 0.396]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.27425373134328357
[2m[36m(func pid=50960)[0m top5: 0.7714552238805971
[2m[36m(func pid=50960)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=50960)[0m f1_macro: 0.2467217640837552
[2m[36m(func pid=50960)[0m f1_weighted: 0.2924585022652147
[2m[36m(func pid=50960)[0m f1_per_class: [0.251, 0.339, 0.489, 0.421, 0.053, 0.255, 0.21, 0.199, 0.096, 0.154]
[2m[36m(func pid=50960)[0m 
== Status ==
Current time: 2024-01-07 14:49:58 (running for 00:16:23.59)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.598 |      0.38  |                   69 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.61  |      0.264 |                   69 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.764 |      0.179 |                   69 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.558 |      0.247 |                   16 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37804)[0m top1: 0.40111940298507465
[2m[36m(func pid=37804)[0m top5: 0.8232276119402985
[2m[36m(func pid=37804)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=37804)[0m f1_macro: 0.2637436214579849
[2m[36m(func pid=37804)[0m f1_weighted: 0.35232382348968105
[2m[36m(func pid=37804)[0m f1_per_class: [0.364, 0.463, 0.0, 0.121, 0.257, 0.036, 0.655, 0.365, 0.213, 0.164]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37854)[0m top1: 0.30923507462686567
[2m[36m(func pid=37854)[0m top5: 0.8470149253731343
[2m[36m(func pid=37854)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=37854)[0m f1_macro: 0.27519865375284747
[2m[36m(func pid=37854)[0m f1_weighted: 0.3150147784032108
[2m[36m(func pid=37854)[0m f1_per_class: [0.177, 0.153, 0.486, 0.481, 0.091, 0.297, 0.288, 0.354, 0.105, 0.32]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6336 | Steps: 4 | Val loss: 1.5998 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4959 | Steps: 4 | Val loss: 2.1042 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6876 | Steps: 4 | Val loss: 3.9682 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.9928 | Steps: 4 | Val loss: 2.2302 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=37733)[0m top1: 0.4468283582089552
[2m[36m(func pid=37733)[0m top5: 0.909981343283582
[2m[36m(func pid=37733)[0m f1_micro: 0.4468283582089552
[2m[36m(func pid=37733)[0m f1_macro: 0.3806283448003549
[2m[36m(func pid=37733)[0m f1_weighted: 0.48042148524518485
[2m[36m(func pid=37733)[0m f1_per_class: [0.381, 0.527, 0.162, 0.522, 0.116, 0.353, 0.506, 0.477, 0.326, 0.438]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.28031716417910446
[2m[36m(func pid=50960)[0m top5: 0.7723880597014925
[2m[36m(func pid=50960)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=50960)[0m f1_macro: 0.2654111713338999
[2m[36m(func pid=50960)[0m f1_weighted: 0.2976479950898353
[2m[36m(func pid=50960)[0m f1_per_class: [0.247, 0.364, 0.571, 0.422, 0.051, 0.198, 0.223, 0.223, 0.105, 0.25]
[2m[36m(func pid=50960)[0m 
== Status ==
Current time: 2024-01-07 14:50:03 (running for 00:16:29.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.634 |      0.381 |                   70 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.61  |      0.264 |                   69 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.993 |      0.204 |                   71 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.496 |      0.265 |                   17 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.2980410447761194
[2m[36m(func pid=37854)[0m top5: 0.8092350746268657
[2m[36m(func pid=37854)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=37854)[0m f1_macro: 0.20402730410939313
[2m[36m(func pid=37854)[0m f1_weighted: 0.3138035431125014
[2m[36m(func pid=37854)[0m f1_per_class: [0.036, 0.375, 0.222, 0.514, 0.026, 0.19, 0.197, 0.362, 0.042, 0.076]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.32369402985074625
[2m[36m(func pid=37804)[0m top5: 0.8069029850746269
[2m[36m(func pid=37804)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=37804)[0m f1_macro: 0.28356854795773273
[2m[36m(func pid=37804)[0m f1_weighted: 0.3216195815994225
[2m[36m(func pid=37804)[0m f1_per_class: [0.303, 0.413, 0.688, 0.13, 0.316, 0.097, 0.6, 0.157, 0.054, 0.079]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4987 | Steps: 4 | Val loss: 1.5428 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4214 | Steps: 4 | Val loss: 2.1040 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.9174 | Steps: 4 | Val loss: 2.1917 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=37733)[0m top1: 0.45242537313432835
[2m[36m(func pid=37733)[0m top5: 0.929570895522388
[2m[36m(func pid=37733)[0m f1_micro: 0.45242537313432835
[2m[36m(func pid=37733)[0m f1_macro: 0.3952300300499344
[2m[36m(func pid=37733)[0m f1_weighted: 0.4743631094117506
[2m[36m(func pid=37733)[0m f1_per_class: [0.471, 0.536, 0.224, 0.46, 0.118, 0.35, 0.53, 0.476, 0.341, 0.447]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4397 | Steps: 4 | Val loss: 4.5304 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=50960)[0m top1: 0.2658582089552239
[2m[36m(func pid=50960)[0m top5: 0.7691231343283582
[2m[36m(func pid=50960)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=50960)[0m f1_macro: 0.253340917745718
[2m[36m(func pid=50960)[0m f1_weighted: 0.2829406136815925
[2m[36m(func pid=50960)[0m f1_per_class: [0.243, 0.366, 0.5, 0.388, 0.059, 0.175, 0.21, 0.247, 0.106, 0.239]
[2m[36m(func pid=50960)[0m 
== Status ==
Current time: 2024-01-07 14:50:08 (running for 00:16:34.46)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.499 |      0.395 |                   71 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.688 |      0.284 |                   70 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.917 |      0.217 |                   72 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.421 |      0.253 |                   18 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.32975746268656714
[2m[36m(func pid=37854)[0m top5: 0.8120335820895522
[2m[36m(func pid=37854)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=37854)[0m f1_macro: 0.2165849110991236
[2m[36m(func pid=37854)[0m f1_weighted: 0.30676056341114083
[2m[36m(func pid=37854)[0m f1_per_class: [0.043, 0.545, 0.143, 0.444, 0.056, 0.294, 0.105, 0.317, 0.051, 0.168]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.2667910447761194
[2m[36m(func pid=37804)[0m top5: 0.8036380597014925
[2m[36m(func pid=37804)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=37804)[0m f1_macro: 0.2541173574221379
[2m[36m(func pid=37804)[0m f1_weighted: 0.27896530003775116
[2m[36m(func pid=37804)[0m f1_per_class: [0.391, 0.39, 0.4, 0.042, 0.131, 0.206, 0.468, 0.412, 0.028, 0.073]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4544 | Steps: 4 | Val loss: 1.5198 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4622 | Steps: 4 | Val loss: 2.0998 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2154 | Steps: 4 | Val loss: 2.1979 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=37733)[0m top1: 0.45242537313432835
[2m[36m(func pid=37733)[0m top5: 0.9351679104477612
[2m[36m(func pid=37733)[0m f1_micro: 0.45242537313432835
[2m[36m(func pid=37733)[0m f1_macro: 0.39355018508900835
[2m[36m(func pid=37733)[0m f1_weighted: 0.4711366597364684
[2m[36m(func pid=37733)[0m f1_per_class: [0.489, 0.512, 0.238, 0.431, 0.124, 0.36, 0.555, 0.487, 0.337, 0.404]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3860 | Steps: 4 | Val loss: 5.2342 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=50960)[0m top1: 0.25699626865671643
[2m[36m(func pid=50960)[0m top5: 0.7677238805970149
[2m[36m(func pid=50960)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=50960)[0m f1_macro: 0.248222798463195
[2m[36m(func pid=50960)[0m f1_weighted: 0.267904371148355
[2m[36m(func pid=50960)[0m f1_per_class: [0.235, 0.372, 0.471, 0.377, 0.074, 0.149, 0.174, 0.262, 0.104, 0.264]
[2m[36m(func pid=50960)[0m 
== Status ==
Current time: 2024-01-07 14:50:14 (running for 00:16:39.84)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.454 |      0.394 |                   72 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.44  |      0.254 |                   71 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.215 |      0.272 |                   73 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.462 |      0.248 |                   19 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.3628731343283582
[2m[36m(func pid=37854)[0m top5: 0.8171641791044776
[2m[36m(func pid=37854)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=37854)[0m f1_macro: 0.27236184028423477
[2m[36m(func pid=37854)[0m f1_weighted: 0.339550838649587
[2m[36m(func pid=37854)[0m f1_per_class: [0.094, 0.558, 0.545, 0.458, 0.041, 0.262, 0.184, 0.376, 0.063, 0.143]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.19542910447761194
[2m[36m(func pid=37804)[0m top5: 0.800839552238806
[2m[36m(func pid=37804)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=37804)[0m f1_macro: 0.16754937300267064
[2m[36m(func pid=37804)[0m f1_weighted: 0.17425872953168725
[2m[36m(func pid=37804)[0m f1_per_class: [0.35, 0.389, 0.0, 0.052, 0.098, 0.17, 0.144, 0.345, 0.055, 0.073]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5017 | Steps: 4 | Val loss: 1.4646 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3394 | Steps: 4 | Val loss: 2.0734 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2601 | Steps: 4 | Val loss: 2.3118 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=37733)[0m top1: 0.46595149253731344
[2m[36m(func pid=37733)[0m top5: 0.9379664179104478
[2m[36m(func pid=37733)[0m f1_micro: 0.46595149253731344
[2m[36m(func pid=37733)[0m f1_macro: 0.40692676406323225
[2m[36m(func pid=37733)[0m f1_weighted: 0.48256506551411427
[2m[36m(func pid=37733)[0m f1_per_class: [0.454, 0.513, 0.264, 0.474, 0.136, 0.397, 0.528, 0.514, 0.393, 0.397]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.2756529850746269
[2m[36m(func pid=50960)[0m top5: 0.777518656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=50960)[0m f1_macro: 0.26052545831793145
[2m[36m(func pid=50960)[0m f1_weighted: 0.2829498124073221
[2m[36m(func pid=50960)[0m f1_per_class: [0.243, 0.397, 0.48, 0.397, 0.074, 0.162, 0.179, 0.279, 0.123, 0.269]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3406 | Steps: 4 | Val loss: 4.6209 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:50:19 (running for 00:16:45.35)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.303
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.502 |      0.407 |                   73 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.386 |      0.168 |                   72 |
| train_5ae7f_00007 | RUNNING    | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  2.26  |      0.301 |                   74 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.339 |      0.261 |                   20 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.3689365671641791
[2m[36m(func pid=37854)[0m top5: 0.8106343283582089
[2m[36m(func pid=37854)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=37854)[0m f1_macro: 0.3006286481771957
[2m[36m(func pid=37854)[0m f1_weighted: 0.36980699787026067
[2m[36m(func pid=37854)[0m f1_per_class: [0.178, 0.496, 0.476, 0.422, 0.075, 0.429, 0.27, 0.467, 0.045, 0.148]
[2m[36m(func pid=37854)[0m 
[2m[36m(func pid=37804)[0m top1: 0.20569029850746268
[2m[36m(func pid=37804)[0m top5: 0.7798507462686567
[2m[36m(func pid=37804)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=37804)[0m f1_macro: 0.202595452670234
[2m[36m(func pid=37804)[0m f1_weighted: 0.1927484852720905
[2m[36m(func pid=37804)[0m f1_per_class: [0.222, 0.424, 0.327, 0.057, 0.046, 0.158, 0.169, 0.409, 0.107, 0.107]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4813 | Steps: 4 | Val loss: 1.4785 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4207 | Steps: 4 | Val loss: 2.0457 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37854)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8936 | Steps: 4 | Val loss: 2.1804 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=37733)[0m top1: 0.478544776119403
[2m[36m(func pid=37733)[0m top5: 0.9323694029850746
[2m[36m(func pid=37733)[0m f1_micro: 0.478544776119403
[2m[36m(func pid=37733)[0m f1_macro: 0.4180963292037421
[2m[36m(func pid=37733)[0m f1_weighted: 0.4966265267702443
[2m[36m(func pid=37733)[0m f1_per_class: [0.467, 0.478, 0.407, 0.552, 0.149, 0.376, 0.537, 0.487, 0.351, 0.377]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.27705223880597013
[2m[36m(func pid=50960)[0m top5: 0.7863805970149254
[2m[36m(func pid=50960)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=50960)[0m f1_macro: 0.2759494161865007
[2m[36m(func pid=50960)[0m f1_weighted: 0.2832467066349022
[2m[36m(func pid=50960)[0m f1_per_class: [0.34, 0.373, 0.571, 0.387, 0.065, 0.106, 0.217, 0.263, 0.142, 0.294]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9312 | Steps: 4 | Val loss: 4.2929 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 14:50:25 (running for 00:16:50.87)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.30174999999999996
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.481 |      0.418 |                   74 |
| train_5ae7f_00006 | RUNNING    | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.341 |      0.203 |                   73 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.421 |      0.276 |                   21 |
| train_5ae7f_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37854)[0m top1: 0.32322761194029853
[2m[36m(func pid=37854)[0m top5: 0.7980410447761194
[2m[36m(func pid=37854)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=37854)[0m f1_macro: 0.29780459838139617
[2m[36m(func pid=37854)[0m f1_weighted: 0.3229671901005077
[2m[36m(func pid=37854)[0m f1_per_class: [0.182, 0.356, 0.593, 0.347, 0.122, 0.398, 0.279, 0.381, 0.105, 0.214]
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3816 | Steps: 4 | Val loss: 1.4887 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=37804)[0m top1: 0.23274253731343283
[2m[36m(func pid=37804)[0m top5: 0.8255597014925373
[2m[36m(func pid=37804)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=37804)[0m f1_macro: 0.22408664571927234
[2m[36m(func pid=37804)[0m f1_weighted: 0.22788337940975667
[2m[36m(func pid=37804)[0m f1_per_class: [0.319, 0.415, 0.129, 0.138, 0.038, 0.2, 0.194, 0.332, 0.233, 0.243]
[2m[36m(func pid=37804)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3161 | Steps: 4 | Val loss: 2.0465 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=37733)[0m top1: 0.48740671641791045
[2m[36m(func pid=37733)[0m top5: 0.9323694029850746
[2m[36m(func pid=37733)[0m f1_micro: 0.48740671641791045
[2m[36m(func pid=37733)[0m f1_macro: 0.4207069783931532
[2m[36m(func pid=37733)[0m f1_weighted: 0.5064813327916287
[2m[36m(func pid=37733)[0m f1_per_class: [0.493, 0.491, 0.316, 0.603, 0.122, 0.359, 0.517, 0.472, 0.405, 0.43]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.2756529850746269
[2m[36m(func pid=50960)[0m top5: 0.7793843283582089
[2m[36m(func pid=50960)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=50960)[0m f1_macro: 0.26236338295314987
[2m[36m(func pid=50960)[0m f1_weighted: 0.28150648313999127
[2m[36m(func pid=50960)[0m f1_per_class: [0.284, 0.378, 0.48, 0.405, 0.076, 0.116, 0.191, 0.284, 0.132, 0.278]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37804)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2921 | Steps: 4 | Val loss: 3.9998 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2482 | Steps: 4 | Val loss: 2.0354 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.5215 | Steps: 4 | Val loss: 1.4566 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=37804)[0m top1: 0.3376865671641791
[2m[36m(func pid=37804)[0m top5: 0.8819962686567164
[2m[36m(func pid=37804)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=37804)[0m f1_macro: 0.31221317709034135
[2m[36m(func pid=37804)[0m f1_weighted: 0.33866016178276126
[2m[36m(func pid=37804)[0m f1_per_class: [0.231, 0.436, 0.19, 0.23, 0.076, 0.445, 0.358, 0.393, 0.178, 0.585]
== Status ==
Current time: 2024-01-07 14:50:31 (running for 00:16:57.50)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.382 |      0.421 |                   75 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.316 |      0.262 |                   22 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=56441)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=56441)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=56441)[0m Configuration completed!
[2m[36m(func pid=56441)[0m New optimizer parameters:
[2m[36m(func pid=56441)[0m SGD (
[2m[36m(func pid=56441)[0m Parameter Group 0
[2m[36m(func pid=56441)[0m     dampening: 0
[2m[36m(func pid=56441)[0m     differentiable: False
[2m[36m(func pid=56441)[0m     foreach: None
[2m[36m(func pid=56441)[0m     lr: 0.001
[2m[36m(func pid=56441)[0m     maximize: False
[2m[36m(func pid=56441)[0m     momentum: 0.99
[2m[36m(func pid=56441)[0m     nesterov: False
[2m[36m(func pid=56441)[0m     weight_decay: 0.0001
[2m[36m(func pid=56441)[0m )
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4743470149253731
[2m[36m(func pid=37733)[0m top5: 0.9375
[2m[36m(func pid=37733)[0m f1_micro: 0.4743470149253731
[2m[36m(func pid=37733)[0m f1_macro: 0.41906947119903004
[2m[36m(func pid=37733)[0m f1_weighted: 0.4893943689450408
[2m[36m(func pid=37733)[0m f1_per_class: [0.467, 0.501, 0.264, 0.571, 0.14, 0.387, 0.469, 0.487, 0.417, 0.489]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.2635261194029851
[2m[36m(func pid=50960)[0m top5: 0.7877798507462687
[2m[36m(func pid=50960)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=50960)[0m f1_macro: 0.24681767373959743
[2m[36m(func pid=50960)[0m f1_weighted: 0.2596358288483867
[2m[36m(func pid=50960)[0m f1_per_class: [0.261, 0.359, 0.436, 0.394, 0.089, 0.089, 0.15, 0.296, 0.136, 0.26]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3785 | Steps: 4 | Val loss: 1.5603 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.2412 | Steps: 4 | Val loss: 2.0107 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9478 | Steps: 4 | Val loss: 2.3660 | Batch size: 32 | lr: 0.001 | Duration: 4.93s
[2m[36m(func pid=37733)[0m top1: 0.4468283582089552
[2m[36m(func pid=37733)[0m top5: 0.9216417910447762
[2m[36m(func pid=37733)[0m f1_micro: 0.4468283582089552
[2m[36m(func pid=37733)[0m f1_macro: 0.40637040644874106
[2m[36m(func pid=37733)[0m f1_weighted: 0.46472402448801703
[2m[36m(func pid=37733)[0m f1_per_class: [0.509, 0.506, 0.226, 0.495, 0.115, 0.374, 0.456, 0.487, 0.446, 0.449]
[2m[36m(func pid=50960)[0m top1: 0.28218283582089554
[2m[36m(func pid=50960)[0m top5: 0.7957089552238806
[2m[36m(func pid=50960)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=50960)[0m f1_macro: 0.25419814835137067
[2m[36m(func pid=50960)[0m f1_weighted: 0.27658701955546916
[2m[36m(func pid=50960)[0m f1_per_class: [0.293, 0.367, 0.387, 0.446, 0.095, 0.068, 0.156, 0.302, 0.143, 0.283]
[2m[36m(func pid=56441)[0m top1: 0.1357276119402985
[2m[36m(func pid=56441)[0m top5: 0.4916044776119403
[2m[36m(func pid=56441)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=56441)[0m f1_macro: 0.08566585480876464
[2m[36m(func pid=56441)[0m f1_weighted: 0.10035270606432027
[2m[36m(func pid=56441)[0m f1_per_class: [0.135, 0.259, 0.0, 0.082, 0.0, 0.228, 0.003, 0.024, 0.016, 0.109]
== Status ==
Current time: 2024-01-07 14:50:39 (running for 00:17:04.77)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.521 |      0.419 |                   76 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.248 |      0.247 |                   23 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=57031)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=57031)[0m Configuration completed!
[2m[36m(func pid=57031)[0m New optimizer parameters:
[2m[36m(func pid=57031)[0m SGD (
[2m[36m(func pid=57031)[0m Parameter Group 0
[2m[36m(func pid=57031)[0m     dampening: 0
[2m[36m(func pid=57031)[0m     differentiable: False
[2m[36m(func pid=57031)[0m     foreach: None
[2m[36m(func pid=57031)[0m     lr: 0.01
[2m[36m(func pid=57031)[0m     maximize: False
[2m[36m(func pid=57031)[0m     momentum: 0.99
[2m[36m(func pid=57031)[0m     nesterov: False
[2m[36m(func pid=57031)[0m     weight_decay: 0.0001
[2m[36m(func pid=57031)[0m )
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.5204 | Steps: 4 | Val loss: 1.5681 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.2054 | Steps: 4 | Val loss: 1.9935 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8581 | Steps: 4 | Val loss: 2.3486 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9447 | Steps: 4 | Val loss: 2.4042 | Batch size: 32 | lr: 0.01 | Duration: 4.72s
== Status ==
Current time: 2024-01-07 14:50:47 (running for 00:17:12.96)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.379 |      0.406 |                   77 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.241 |      0.254 |                   24 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.948 |      0.086 |                    1 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m top1: 0.4435634328358209
[2m[36m(func pid=37733)[0m top5: 0.9253731343283582
[2m[36m(func pid=37733)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=37733)[0m f1_macro: 0.40125921441538354
[2m[36m(func pid=37733)[0m f1_weighted: 0.4639465766855459
[2m[36m(func pid=37733)[0m f1_per_class: [0.55, 0.486, 0.211, 0.514, 0.115, 0.354, 0.455, 0.481, 0.464, 0.383]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m top1: 0.29244402985074625
[2m[36m(func pid=50960)[0m top5: 0.7919776119402985
[2m[36m(func pid=50960)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=50960)[0m f1_macro: 0.2594751274455622
[2m[36m(func pid=50960)[0m f1_weighted: 0.2815915342230084
[2m[36m(func pid=50960)[0m f1_per_class: [0.308, 0.396, 0.414, 0.465, 0.093, 0.053, 0.143, 0.305, 0.139, 0.279]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.1609141791044776
[2m[36m(func pid=56441)[0m top5: 0.5181902985074627
[2m[36m(func pid=56441)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=56441)[0m f1_macro: 0.09644213031797337
[2m[36m(func pid=56441)[0m f1_weighted: 0.1226147679467919
[2m[36m(func pid=56441)[0m f1_per_class: [0.142, 0.265, 0.05, 0.075, 0.0, 0.299, 0.057, 0.016, 0.012, 0.049]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.10494402985074627
[2m[36m(func pid=57031)[0m top5: 0.47201492537313433
[2m[36m(func pid=57031)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=57031)[0m f1_macro: 0.1291672089513596
[2m[36m(func pid=57031)[0m f1_weighted: 0.10026913128366469
[2m[36m(func pid=57031)[0m f1_per_class: [0.095, 0.049, 0.444, 0.127, 0.121, 0.1, 0.094, 0.163, 0.033, 0.065]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0836 | Steps: 4 | Val loss: 1.9854 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2717 | Steps: 4 | Val loss: 1.6128 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8293 | Steps: 4 | Val loss: 2.3185 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.5730 | Steps: 4 | Val loss: 2.0960 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:50:53 (running for 00:17:18.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.52  |      0.401 |                   78 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.084 |      0.268 |                   26 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.858 |      0.096 |                    2 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.945 |      0.129 |                    1 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.30223880597014924
[2m[36m(func pid=50960)[0m top5: 0.792910447761194
[2m[36m(func pid=50960)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=50960)[0m f1_macro: 0.2678666545196192
[2m[36m(func pid=50960)[0m f1_weighted: 0.29087842539076375
[2m[36m(func pid=50960)[0m f1_per_class: [0.316, 0.434, 0.421, 0.463, 0.091, 0.062, 0.147, 0.315, 0.148, 0.281]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m top1: 0.435634328358209
[2m[36m(func pid=37733)[0m top5: 0.9193097014925373
[2m[36m(func pid=37733)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=37733)[0m f1_macro: 0.3872591672947639
[2m[36m(func pid=37733)[0m f1_weighted: 0.46071423000029543
[2m[36m(func pid=37733)[0m f1_per_class: [0.481, 0.481, 0.183, 0.518, 0.114, 0.343, 0.457, 0.481, 0.418, 0.396]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.1791044776119403
[2m[36m(func pid=56441)[0m top5: 0.5555037313432836
[2m[36m(func pid=56441)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=56441)[0m f1_macro: 0.11800134899712678
[2m[36m(func pid=56441)[0m f1_weighted: 0.1294416294244819
[2m[36m(func pid=56441)[0m f1_per_class: [0.136, 0.297, 0.25, 0.056, 0.0, 0.323, 0.062, 0.045, 0.011, 0.0]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.332089552238806
[2m[36m(func pid=57031)[0m top5: 0.7066231343283582
[2m[36m(func pid=57031)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=57031)[0m f1_macro: 0.13692809545717421
[2m[36m(func pid=57031)[0m f1_weighted: 0.2680683508702071
[2m[36m(func pid=57031)[0m f1_per_class: [0.136, 0.0, 0.0, 0.486, 0.206, 0.0, 0.423, 0.0, 0.027, 0.091]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.1873 | Steps: 4 | Val loss: 1.9594 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.5351 | Steps: 4 | Val loss: 1.5758 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7836 | Steps: 4 | Val loss: 2.2115 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.2520 | Steps: 4 | Val loss: 1.9952 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=50960)[0m top1: 0.29757462686567165
[2m[36m(func pid=50960)[0m top5: 0.8036380597014925
[2m[36m(func pid=50960)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=50960)[0m f1_macro: 0.2571140313659478
[2m[36m(func pid=50960)[0m f1_weighted: 0.2877846476299056
[2m[36m(func pid=50960)[0m f1_per_class: [0.287, 0.421, 0.387, 0.448, 0.108, 0.054, 0.166, 0.317, 0.142, 0.241]
== Status ==
Current time: 2024-01-07 14:50:58 (running for 00:17:24.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.272 |      0.387 |                   79 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.187 |      0.257 |                   27 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.829 |      0.118 |                    3 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.573 |      0.137 |                    2 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4631529850746269
[2m[36m(func pid=37733)[0m top5: 0.9188432835820896
[2m[36m(func pid=37733)[0m f1_micro: 0.4631529850746269
[2m[36m(func pid=37733)[0m f1_macro: 0.39843195866921277
[2m[36m(func pid=37733)[0m f1_weighted: 0.4917819438741634
[2m[36m(func pid=37733)[0m f1_per_class: [0.471, 0.501, 0.215, 0.575, 0.104, 0.327, 0.502, 0.489, 0.424, 0.378]
[2m[36m(func pid=56441)[0m top1: 0.22901119402985073
[2m[36m(func pid=56441)[0m top5: 0.6464552238805971
[2m[36m(func pid=56441)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=56441)[0m f1_macro: 0.1841594655724008
[2m[36m(func pid=56441)[0m f1_weighted: 0.1453550362504455
[2m[36m(func pid=56441)[0m f1_per_class: [0.24, 0.36, 0.625, 0.049, 0.022, 0.345, 0.065, 0.0, 0.018, 0.118]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=57031)[0m top1: 0.35867537313432835
[2m[36m(func pid=57031)[0m top5: 0.7737873134328358
[2m[36m(func pid=57031)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=57031)[0m f1_macro: 0.2825946268264675
[2m[36m(func pid=57031)[0m f1_weighted: 0.3243378016152077
[2m[36m(func pid=57031)[0m f1_per_class: [0.243, 0.005, 0.75, 0.564, 0.164, 0.269, 0.39, 0.03, 0.088, 0.323]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0676 | Steps: 4 | Val loss: 1.9470 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.6975 | Steps: 4 | Val loss: 1.6160 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6159 | Steps: 4 | Val loss: 2.1493 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.6786 | Steps: 4 | Val loss: 2.1045 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:51:04 (running for 00:17:29.63)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.535 |      0.398 |                   80 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.068 |      0.27  |                   28 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.784 |      0.184 |                    4 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.252 |      0.283 |                    3 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.3138992537313433
[2m[36m(func pid=50960)[0m top5: 0.808768656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=50960)[0m f1_macro: 0.26968031639388707
[2m[36m(func pid=50960)[0m f1_weighted: 0.30616588699078234
[2m[36m(func pid=50960)[0m f1_per_class: [0.278, 0.439, 0.4, 0.46, 0.107, 0.063, 0.195, 0.333, 0.173, 0.248]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.2523320895522388
[2m[36m(func pid=56441)[0m top5: 0.6986940298507462
[2m[36m(func pid=56441)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=56441)[0m f1_macro: 0.2073705959043736
[2m[36m(func pid=56441)[0m f1_weighted: 0.15915457531450675
[2m[36m(func pid=56441)[0m f1_per_class: [0.361, 0.383, 0.537, 0.091, 0.0, 0.354, 0.043, 0.0, 0.024, 0.282]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4710820895522388
[2m[36m(func pid=37733)[0m top5: 0.9085820895522388
[2m[36m(func pid=37733)[0m f1_micro: 0.4710820895522388
[2m[36m(func pid=37733)[0m f1_macro: 0.40049397188855346
[2m[36m(func pid=37733)[0m f1_weighted: 0.4966784587618204
[2m[36m(func pid=37733)[0m f1_per_class: [0.476, 0.504, 0.381, 0.599, 0.108, 0.303, 0.522, 0.412, 0.361, 0.339]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=57031)[0m top1: 0.24766791044776118
[2m[36m(func pid=57031)[0m top5: 0.8176305970149254
[2m[36m(func pid=57031)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=57031)[0m f1_macro: 0.22825163635317353
[2m[36m(func pid=57031)[0m f1_weighted: 0.23399086267304073
[2m[36m(func pid=57031)[0m f1_per_class: [0.283, 0.063, 0.048, 0.444, 0.134, 0.447, 0.042, 0.339, 0.154, 0.328]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0325 | Steps: 4 | Val loss: 1.9085 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5276 | Steps: 4 | Val loss: 2.0539 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3788 | Steps: 4 | Val loss: 1.6808 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.2379 | Steps: 4 | Val loss: 2.0506 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:51:09 (running for 00:17:35.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.698 |      0.4   |                   81 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.032 |      0.295 |                   29 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.616 |      0.207 |                    5 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.679 |      0.228 |                    4 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.32975746268656714
[2m[36m(func pid=50960)[0m top5: 0.8218283582089553
[2m[36m(func pid=50960)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=50960)[0m f1_macro: 0.2951618343052895
[2m[36m(func pid=50960)[0m f1_weighted: 0.3243629796771942
[2m[36m(func pid=50960)[0m f1_per_class: [0.324, 0.439, 0.558, 0.474, 0.113, 0.061, 0.237, 0.341, 0.166, 0.238]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.2677238805970149
[2m[36m(func pid=56441)[0m top5: 0.7873134328358209
[2m[36m(func pid=56441)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=56441)[0m f1_macro: 0.23593901756463714
[2m[36m(func pid=56441)[0m f1_weighted: 0.19904792894587847
[2m[36m(func pid=56441)[0m f1_per_class: [0.415, 0.385, 0.462, 0.174, 0.087, 0.353, 0.083, 0.026, 0.064, 0.311]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.44076492537313433
[2m[36m(func pid=37733)[0m top5: 0.8955223880597015
[2m[36m(func pid=37733)[0m f1_micro: 0.44076492537313433
[2m[36m(func pid=37733)[0m f1_macro: 0.3781384697240685
[2m[36m(func pid=37733)[0m f1_weighted: 0.46424219870220645
[2m[36m(func pid=37733)[0m f1_per_class: [0.449, 0.524, 0.421, 0.567, 0.101, 0.254, 0.482, 0.274, 0.308, 0.4]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=57031)[0m top1: 0.27705223880597013
[2m[36m(func pid=57031)[0m top5: 0.8423507462686567
[2m[36m(func pid=57031)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=57031)[0m f1_macro: 0.23257354359983698
[2m[36m(func pid=57031)[0m f1_weighted: 0.2751843679760409
[2m[36m(func pid=57031)[0m f1_per_class: [0.475, 0.299, 0.078, 0.513, 0.2, 0.069, 0.133, 0.261, 0.124, 0.172]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0366 | Steps: 4 | Val loss: 1.8953 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.2873 | Steps: 4 | Val loss: 1.9640 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.6462 | Steps: 4 | Val loss: 1.6586 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3903 | Steps: 4 | Val loss: 2.1564 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:51:14 (running for 00:17:40.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.379 |      0.378 |                   82 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.037 |      0.293 |                   30 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.528 |      0.236 |                    6 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.238 |      0.233 |                    5 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.33861940298507465
[2m[36m(func pid=50960)[0m top5: 0.8246268656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=50960)[0m f1_macro: 0.2929668137952243
[2m[36m(func pid=50960)[0m f1_weighted: 0.33385797371855025
[2m[36m(func pid=50960)[0m f1_per_class: [0.327, 0.455, 0.462, 0.477, 0.117, 0.069, 0.253, 0.349, 0.176, 0.245]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.29617537313432835
[2m[36m(func pid=56441)[0m top5: 0.8414179104477612
[2m[36m(func pid=56441)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=56441)[0m f1_macro: 0.2595469622116226
[2m[36m(func pid=56441)[0m f1_weighted: 0.2643523655914483
[2m[36m(func pid=56441)[0m f1_per_class: [0.562, 0.351, 0.348, 0.321, 0.09, 0.393, 0.156, 0.055, 0.094, 0.227]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4351679104477612
[2m[36m(func pid=37733)[0m top5: 0.9155783582089553
[2m[36m(func pid=37733)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=37733)[0m f1_macro: 0.3865211992580954
[2m[36m(func pid=37733)[0m f1_weighted: 0.45124159262822033
[2m[36m(func pid=37733)[0m f1_per_class: [0.472, 0.535, 0.333, 0.444, 0.087, 0.172, 0.544, 0.416, 0.318, 0.543]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=57031)[0m top1: 0.3400186567164179
[2m[36m(func pid=57031)[0m top5: 0.8498134328358209
[2m[36m(func pid=57031)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=57031)[0m f1_macro: 0.2595041501750005
[2m[36m(func pid=57031)[0m f1_weighted: 0.34448330751095113
[2m[36m(func pid=57031)[0m f1_per_class: [0.163, 0.407, 0.421, 0.522, 0.154, 0.0, 0.363, 0.087, 0.132, 0.345]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0155 | Steps: 4 | Val loss: 1.8936 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2379 | Steps: 4 | Val loss: 1.8936 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4135 | Steps: 4 | Val loss: 1.6220 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4334 | Steps: 4 | Val loss: 2.2229 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:51:20 (running for 00:17:45.87)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.646 |      0.387 |                   83 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  2.015 |      0.286 |                   31 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.287 |      0.26  |                    7 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.39  |      0.26  |                    6 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.33861940298507465
[2m[36m(func pid=50960)[0m top5: 0.8166977611940298
[2m[36m(func pid=50960)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=50960)[0m f1_macro: 0.2862012896764388
[2m[36m(func pid=50960)[0m f1_weighted: 0.3387186812163827
[2m[36m(func pid=50960)[0m f1_per_class: [0.27, 0.458, 0.414, 0.483, 0.11, 0.062, 0.268, 0.349, 0.192, 0.258]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3512126865671642
[2m[36m(func pid=56441)[0m top5: 0.8619402985074627
[2m[36m(func pid=56441)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=56441)[0m f1_macro: 0.2822819928429049
[2m[36m(func pid=56441)[0m f1_weighted: 0.3437177789466489
[2m[36m(func pid=56441)[0m f1_per_class: [0.466, 0.336, 0.198, 0.446, 0.124, 0.439, 0.283, 0.127, 0.164, 0.241]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4300373134328358
[2m[36m(func pid=37733)[0m top5: 0.9193097014925373
[2m[36m(func pid=37733)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=37733)[0m f1_macro: 0.3742248349631928
[2m[36m(func pid=37733)[0m f1_weighted: 0.44807755282229395
[2m[36m(func pid=37733)[0m f1_per_class: [0.397, 0.511, 0.229, 0.437, 0.106, 0.246, 0.527, 0.438, 0.354, 0.5]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=57031)[0m top1: 0.38199626865671643
[2m[36m(func pid=57031)[0m top5: 0.8843283582089553
[2m[36m(func pid=57031)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=57031)[0m f1_macro: 0.2508996782259413
[2m[36m(func pid=57031)[0m f1_weighted: 0.36821954523613853
[2m[36m(func pid=57031)[0m f1_per_class: [0.122, 0.36, 0.438, 0.392, 0.04, 0.036, 0.594, 0.0, 0.178, 0.347]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.7622 | Steps: 4 | Val loss: 1.8796 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.0411 | Steps: 4 | Val loss: 1.8133 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2886 | Steps: 4 | Val loss: 1.6134 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.6941 | Steps: 4 | Val loss: 2.4808 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 14:51:25 (running for 00:17:51.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.414 |      0.374 |                   84 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.762 |      0.292 |                   32 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.238 |      0.282 |                    8 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.433 |      0.251 |                    7 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.33675373134328357
[2m[36m(func pid=50960)[0m top5: 0.8260261194029851
[2m[36m(func pid=50960)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=50960)[0m f1_macro: 0.2924211878950976
[2m[36m(func pid=50960)[0m f1_weighted: 0.34277248201950916
[2m[36m(func pid=50960)[0m f1_per_class: [0.285, 0.43, 0.471, 0.474, 0.092, 0.076, 0.297, 0.358, 0.195, 0.247]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m top1: 0.427705223880597
[2m[36m(func pid=37733)[0m top5: 0.9123134328358209
[2m[36m(func pid=37733)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=37733)[0m f1_macro: 0.38040635609548046
[2m[36m(func pid=37733)[0m f1_weighted: 0.4429688468467889
[2m[36m(func pid=37733)[0m f1_per_class: [0.435, 0.492, 0.195, 0.516, 0.117, 0.382, 0.387, 0.439, 0.424, 0.418]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.38013059701492535
[2m[36m(func pid=56441)[0m top5: 0.8605410447761194
[2m[36m(func pid=56441)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=56441)[0m f1_macro: 0.3025917818553555
[2m[36m(func pid=56441)[0m f1_weighted: 0.35213465415604583
[2m[36m(func pid=56441)[0m f1_per_class: [0.462, 0.252, 0.216, 0.557, 0.133, 0.443, 0.208, 0.341, 0.198, 0.215]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.2891791044776119
[2m[36m(func pid=57031)[0m top5: 0.8861940298507462
[2m[36m(func pid=57031)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=57031)[0m f1_macro: 0.2621768895171369
[2m[36m(func pid=57031)[0m f1_weighted: 0.3139367036051503
[2m[36m(func pid=57031)[0m f1_per_class: [0.343, 0.165, 0.15, 0.339, 0.208, 0.438, 0.362, 0.29, 0.142, 0.184]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8637 | Steps: 4 | Val loss: 1.8509 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.5424 | Steps: 4 | Val loss: 1.5715 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9336 | Steps: 4 | Val loss: 1.8047 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0921 | Steps: 4 | Val loss: 4.2988 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:51:30 (running for 00:17:56.52)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.289 |      0.38  |                   85 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.864 |      0.29  |                   33 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  2.041 |      0.303 |                    9 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.694 |      0.262 |                    8 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.33722014925373134
[2m[36m(func pid=50960)[0m top5: 0.8348880597014925
[2m[36m(func pid=50960)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=50960)[0m f1_macro: 0.2901429090487008
[2m[36m(func pid=50960)[0m f1_weighted: 0.3471378502816983
[2m[36m(func pid=50960)[0m f1_per_class: [0.301, 0.451, 0.436, 0.439, 0.088, 0.083, 0.334, 0.343, 0.185, 0.241]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4449626865671642
[2m[36m(func pid=37733)[0m top5: 0.9146455223880597
[2m[36m(func pid=37733)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=37733)[0m f1_macro: 0.38754582178080754
[2m[36m(func pid=37733)[0m f1_weighted: 0.46096849290425534
[2m[36m(func pid=37733)[0m f1_per_class: [0.339, 0.512, 0.333, 0.525, 0.146, 0.389, 0.436, 0.452, 0.34, 0.404]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.37593283582089554
[2m[36m(func pid=56441)[0m top5: 0.8558768656716418
[2m[36m(func pid=56441)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=56441)[0m f1_macro: 0.2987199567614566
[2m[36m(func pid=56441)[0m f1_weighted: 0.34013890661202684
[2m[36m(func pid=56441)[0m f1_per_class: [0.369, 0.209, 0.185, 0.576, 0.147, 0.456, 0.153, 0.446, 0.23, 0.217]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.14039179104477612
[2m[36m(func pid=57031)[0m top5: 0.7131529850746269
[2m[36m(func pid=57031)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=57031)[0m f1_macro: 0.12841802511665656
[2m[36m(func pid=57031)[0m f1_weighted: 0.12309648422819788
[2m[36m(func pid=57031)[0m f1_per_class: [0.179, 0.117, 0.046, 0.244, 0.131, 0.03, 0.009, 0.308, 0.164, 0.056]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.7930 | Steps: 4 | Val loss: 1.8353 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.5922 | Steps: 4 | Val loss: 1.5826 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.8129 | Steps: 4 | Val loss: 1.7819 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1313 | Steps: 4 | Val loss: 8.6839 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:51:36 (running for 00:18:01.90)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.542 |      0.388 |                   86 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.793 |      0.289 |                   34 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.934 |      0.299 |                   10 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.092 |      0.128 |                    9 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.3460820895522388
[2m[36m(func pid=50960)[0m top5: 0.8390858208955224
[2m[36m(func pid=50960)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=50960)[0m f1_macro: 0.288632127647173
[2m[36m(func pid=50960)[0m f1_weighted: 0.3557428117995875
[2m[36m(func pid=50960)[0m f1_per_class: [0.275, 0.462, 0.369, 0.422, 0.105, 0.083, 0.371, 0.343, 0.216, 0.24]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3666044776119403
[2m[36m(func pid=56441)[0m top5: 0.8540111940298507
[2m[36m(func pid=56441)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=56441)[0m f1_macro: 0.3045709655218606
[2m[36m(func pid=56441)[0m f1_weighted: 0.3498738267914452
[2m[36m(func pid=56441)[0m f1_per_class: [0.374, 0.318, 0.203, 0.566, 0.127, 0.419, 0.153, 0.381, 0.284, 0.221]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4398320895522388
[2m[36m(func pid=37733)[0m top5: 0.9123134328358209
[2m[36m(func pid=37733)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=37733)[0m f1_macro: 0.39580158549121935
[2m[36m(func pid=37733)[0m f1_weighted: 0.4588504440483705
[2m[36m(func pid=37733)[0m f1_per_class: [0.36, 0.497, 0.511, 0.481, 0.135, 0.393, 0.481, 0.445, 0.286, 0.37]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=57031)[0m top1: 0.06529850746268656
[2m[36m(func pid=57031)[0m top5: 0.5461753731343284
[2m[36m(func pid=57031)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=57031)[0m f1_macro: 0.07112905441211623
[2m[36m(func pid=57031)[0m f1_weighted: 0.048465375643649965
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.011, 0.02, 0.068, 0.173, 0.0, 0.012, 0.344, 0.084, 0.0]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.6748 | Steps: 4 | Val loss: 1.8075 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3301 | Steps: 4 | Val loss: 1.6328 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.1288 | Steps: 4 | Val loss: 6.0943 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.4480 | Steps: 4 | Val loss: 1.7786 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 14:51:41 (running for 00:18:07.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.592 |      0.396 |                   87 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.675 |      0.319 |                   35 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.813 |      0.305 |                   11 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.131 |      0.071 |                   10 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.36800373134328357
[2m[36m(func pid=50960)[0m top5: 0.847481343283582
[2m[36m(func pid=50960)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=50960)[0m f1_macro: 0.3185589550870697
[2m[36m(func pid=50960)[0m f1_weighted: 0.3807855482128427
[2m[36m(func pid=50960)[0m f1_per_class: [0.311, 0.472, 0.49, 0.427, 0.107, 0.168, 0.405, 0.36, 0.201, 0.244]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.23460820895522388
[2m[36m(func pid=57031)[0m top5: 0.7910447761194029
[2m[36m(func pid=57031)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=57031)[0m f1_macro: 0.15714567113959016
[2m[36m(func pid=57031)[0m f1_weighted: 0.24785016636196616
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.34, 0.011, 0.128, 0.198, 0.0, 0.43, 0.347, 0.119, 0.0]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=37733)[0m top1: 0.42350746268656714
[2m[36m(func pid=37733)[0m top5: 0.9071828358208955
[2m[36m(func pid=37733)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=37733)[0m f1_macro: 0.382184120569442
[2m[36m(func pid=37733)[0m f1_weighted: 0.4457386376087174
[2m[36m(func pid=37733)[0m f1_per_class: [0.371, 0.471, 0.533, 0.492, 0.134, 0.346, 0.47, 0.411, 0.246, 0.346]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.36100746268656714
[2m[36m(func pid=56441)[0m top5: 0.8512126865671642
[2m[36m(func pid=56441)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=56441)[0m f1_macro: 0.31432784069517916
[2m[36m(func pid=56441)[0m f1_weighted: 0.3439791114666549
[2m[36m(func pid=56441)[0m f1_per_class: [0.418, 0.404, 0.282, 0.542, 0.12, 0.41, 0.108, 0.368, 0.281, 0.21]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8798 | Steps: 4 | Val loss: 1.7919 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9020 | Steps: 4 | Val loss: 15.1588 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.4870 | Steps: 4 | Val loss: 1.7667 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3735 | Steps: 4 | Val loss: 1.6386 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 14:51:47 (running for 00:18:12.86)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.33  |      0.382 |                   88 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.88  |      0.32  |                   36 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.448 |      0.314 |                   12 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.129 |      0.157 |                   11 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.37966417910447764
[2m[36m(func pid=50960)[0m top5: 0.8502798507462687
[2m[36m(func pid=50960)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=50960)[0m f1_macro: 0.3201875394191654
[2m[36m(func pid=50960)[0m f1_weighted: 0.3917570512499872
[2m[36m(func pid=50960)[0m f1_per_class: [0.333, 0.48, 0.387, 0.447, 0.106, 0.192, 0.406, 0.372, 0.216, 0.262]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.06063432835820896
[2m[36m(func pid=57031)[0m top5: 0.5041977611940298
[2m[36m(func pid=57031)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=57031)[0m f1_macro: 0.061226159576080005
[2m[36m(func pid=57031)[0m f1_weighted: 0.07277710711077418
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.026, 0.021, 0.178, 0.154, 0.024, 0.034, 0.0, 0.111, 0.063]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=37733)[0m top1: 0.447294776119403
[2m[36m(func pid=37733)[0m top5: 0.9113805970149254
[2m[36m(func pid=37733)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=37733)[0m f1_macro: 0.3855484497357028
[2m[36m(func pid=37733)[0m f1_weighted: 0.4643648291508723
[2m[36m(func pid=37733)[0m f1_per_class: [0.418, 0.481, 0.364, 0.57, 0.184, 0.348, 0.446, 0.435, 0.256, 0.354]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.34654850746268656
[2m[36m(func pid=56441)[0m top5: 0.8502798507462687
[2m[36m(func pid=56441)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=56441)[0m f1_macro: 0.3188303962226106
[2m[36m(func pid=56441)[0m f1_weighted: 0.3309042876070779
[2m[36m(func pid=56441)[0m f1_per_class: [0.397, 0.444, 0.414, 0.506, 0.105, 0.365, 0.091, 0.371, 0.268, 0.227]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6308 | Steps: 4 | Val loss: 1.7913 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.2741 | Steps: 4 | Val loss: 17.8774 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.4049 | Steps: 4 | Val loss: 1.6952 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.2151 | Steps: 4 | Val loss: 1.7333 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:51:52 (running for 00:18:18.29)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.373 |      0.386 |                   89 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.631 |      0.304 |                   37 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.487 |      0.319 |                   13 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.902 |      0.061 |                   12 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.36613805970149255
[2m[36m(func pid=50960)[0m top5: 0.8526119402985075
[2m[36m(func pid=50960)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=50960)[0m f1_macro: 0.3039418643966438
[2m[36m(func pid=50960)[0m f1_weighted: 0.3867916047326189
[2m[36m(func pid=50960)[0m f1_per_class: [0.277, 0.478, 0.343, 0.426, 0.114, 0.194, 0.42, 0.352, 0.2, 0.236]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.052705223880597014
[2m[36m(func pid=57031)[0m top5: 0.30597014925373134
[2m[36m(func pid=57031)[0m f1_micro: 0.05270522388059702
[2m[36m(func pid=57031)[0m f1_macro: 0.06946539956990141
[2m[36m(func pid=57031)[0m f1_weighted: 0.04175201643237562
[2m[36m(func pid=57031)[0m f1_per_class: [0.09, 0.183, 0.0, 0.003, 0.308, 0.031, 0.0, 0.0, 0.038, 0.042]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3498134328358209
[2m[36m(func pid=56441)[0m top5: 0.8652052238805971
[2m[36m(func pid=56441)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=56441)[0m f1_macro: 0.33431088984770285
[2m[36m(func pid=56441)[0m f1_weighted: 0.3453290629396107
[2m[36m(func pid=56441)[0m f1_per_class: [0.408, 0.436, 0.511, 0.491, 0.089, 0.346, 0.158, 0.396, 0.275, 0.235]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4319029850746269
[2m[36m(func pid=37733)[0m top5: 0.9169776119402985
[2m[36m(func pid=37733)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=37733)[0m f1_macro: 0.36664040242893065
[2m[36m(func pid=37733)[0m f1_weighted: 0.45203037005676266
[2m[36m(func pid=37733)[0m f1_per_class: [0.5, 0.497, 0.24, 0.49, 0.129, 0.246, 0.521, 0.335, 0.279, 0.429]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6646 | Steps: 4 | Val loss: 1.7801 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9868 | Steps: 4 | Val loss: 25.0675 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.1861 | Steps: 4 | Val loss: 1.7952 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.7285 | Steps: 4 | Val loss: 1.7786 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 14:51:58 (running for 00:18:23.66)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.405 |      0.367 |                   90 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.665 |      0.304 |                   38 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.215 |      0.334 |                   14 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  3.274 |      0.069 |                   13 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.3605410447761194
[2m[36m(func pid=50960)[0m top5: 0.8591417910447762
[2m[36m(func pid=50960)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=50960)[0m f1_macro: 0.3035109249000006
[2m[36m(func pid=50960)[0m f1_weighted: 0.38221210448305554
[2m[36m(func pid=50960)[0m f1_per_class: [0.256, 0.479, 0.329, 0.391, 0.119, 0.232, 0.42, 0.366, 0.215, 0.228]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.07929104477611941
[2m[36m(func pid=57031)[0m top5: 0.5890858208955224
[2m[36m(func pid=57031)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=57031)[0m f1_macro: 0.046500161631890205
[2m[36m(func pid=57031)[0m f1_weighted: 0.08297580636917655
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.218, 0.034, 0.035, 0.0, 0.0, 0.112, 0.0, 0.066, 0.0]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.32882462686567165
[2m[36m(func pid=56441)[0m top5: 0.8736007462686567
[2m[36m(func pid=56441)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=56441)[0m f1_macro: 0.3319824778999718
[2m[36m(func pid=56441)[0m f1_weighted: 0.3271623134733669
[2m[36m(func pid=56441)[0m f1_per_class: [0.423, 0.417, 0.522, 0.405, 0.095, 0.353, 0.184, 0.386, 0.295, 0.241]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.39972014925373134
[2m[36m(func pid=37733)[0m top5: 0.9165111940298507
[2m[36m(func pid=37733)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=37733)[0m f1_macro: 0.3492020430887617
[2m[36m(func pid=37733)[0m f1_weighted: 0.4191732190719649
[2m[36m(func pid=37733)[0m f1_per_class: [0.489, 0.48, 0.258, 0.41, 0.088, 0.255, 0.498, 0.317, 0.29, 0.408]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.5821 | Steps: 4 | Val loss: 1.7414 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.1563 | Steps: 4 | Val loss: 95.4890 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.3760 | Steps: 4 | Val loss: 1.8516 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.5339 | Steps: 4 | Val loss: 1.6800 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:52:03 (running for 00:18:29.32)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.729 |      0.349 |                   91 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.582 |      0.33  |                   39 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.186 |      0.332 |                   15 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.987 |      0.047 |                   14 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.37593283582089554
[2m[36m(func pid=50960)[0m top5: 0.871268656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=50960)[0m f1_macro: 0.32964962835584233
[2m[36m(func pid=50960)[0m f1_weighted: 0.3972677212059252
[2m[36m(func pid=50960)[0m f1_per_class: [0.414, 0.484, 0.421, 0.458, 0.088, 0.253, 0.388, 0.359, 0.188, 0.243]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.02378731343283582
[2m[36m(func pid=57031)[0m top5: 0.3218283582089552
[2m[36m(func pid=57031)[0m f1_micro: 0.02378731343283582
[2m[36m(func pid=57031)[0m f1_macro: 0.011838602075890211
[2m[36m(func pid=57031)[0m f1_weighted: 0.019859835964693037
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0, 0.026, 0.03]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.31156716417910446
[2m[36m(func pid=56441)[0m top5: 0.882929104477612
[2m[36m(func pid=56441)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=56441)[0m f1_macro: 0.33407823213145116
[2m[36m(func pid=56441)[0m f1_weighted: 0.29808425509813946
[2m[36m(func pid=56441)[0m f1_per_class: [0.464, 0.413, 0.6, 0.245, 0.099, 0.365, 0.226, 0.412, 0.283, 0.235]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.42257462686567165
[2m[36m(func pid=37733)[0m top5: 0.9197761194029851
[2m[36m(func pid=37733)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=37733)[0m f1_macro: 0.37515464720558056
[2m[36m(func pid=37733)[0m f1_weighted: 0.4476747939687785
[2m[36m(func pid=37733)[0m f1_per_class: [0.429, 0.454, 0.176, 0.467, 0.107, 0.365, 0.475, 0.466, 0.409, 0.404]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5645 | Steps: 4 | Val loss: 1.7189 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 4.2656 | Steps: 4 | Val loss: 54.9025 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.1904 | Steps: 4 | Val loss: 1.8201 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3876 | Steps: 4 | Val loss: 1.6865 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 14:52:09 (running for 00:18:34.74)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.534 |      0.375 |                   92 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.564 |      0.356 |                   40 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.376 |      0.334 |                   16 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  4.156 |      0.012 |                   15 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.3885261194029851
[2m[36m(func pid=50960)[0m top5: 0.8796641791044776
[2m[36m(func pid=50960)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=50960)[0m f1_macro: 0.3555589944760201
[2m[36m(func pid=50960)[0m f1_weighted: 0.4121433343167569
[2m[36m(func pid=50960)[0m f1_per_class: [0.522, 0.483, 0.49, 0.457, 0.085, 0.299, 0.411, 0.374, 0.187, 0.249]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.013526119402985074
[2m[36m(func pid=57031)[0m top5: 0.30363805970149255
[2m[36m(func pid=57031)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=57031)[0m f1_macro: 0.007116688012524084
[2m[36m(func pid=57031)[0m f1_weighted: 0.008148087923011416
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.009, 0.01, 0.0, 0.025]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3423507462686567
[2m[36m(func pid=56441)[0m top5: 0.8973880597014925
[2m[36m(func pid=56441)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=56441)[0m f1_macro: 0.355191160108396
[2m[36m(func pid=56441)[0m f1_weighted: 0.347271800212313
[2m[36m(func pid=56441)[0m f1_per_class: [0.463, 0.406, 0.632, 0.205, 0.092, 0.362, 0.422, 0.459, 0.307, 0.204]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.42723880597014924
[2m[36m(func pid=37733)[0m top5: 0.9029850746268657
[2m[36m(func pid=37733)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=37733)[0m f1_macro: 0.3693901707513003
[2m[36m(func pid=37733)[0m f1_weighted: 0.45347235402372754
[2m[36m(func pid=37733)[0m f1_per_class: [0.39, 0.476, 0.153, 0.488, 0.131, 0.396, 0.454, 0.472, 0.42, 0.313]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.6277 | Steps: 4 | Val loss: 1.6993 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5256 | Steps: 4 | Val loss: 40.7718 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8577 | Steps: 4 | Val loss: 1.7349 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3083 | Steps: 4 | Val loss: 1.7267 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:52:14 (running for 00:18:40.07)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.388 |      0.369 |                   93 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.628 |      0.362 |                   41 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.19  |      0.355 |                   17 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  4.266 |      0.007 |                   16 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.38572761194029853
[2m[36m(func pid=50960)[0m top5: 0.8908582089552238
[2m[36m(func pid=50960)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=50960)[0m f1_macro: 0.3615443521707182
[2m[36m(func pid=50960)[0m f1_weighted: 0.4149084629756284
[2m[36m(func pid=50960)[0m f1_per_class: [0.558, 0.459, 0.5, 0.47, 0.072, 0.338, 0.405, 0.376, 0.175, 0.263]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.10587686567164178
[2m[36m(func pid=57031)[0m top5: 0.5027985074626866
[2m[36m(func pid=57031)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=57031)[0m f1_macro: 0.060898507977207104
[2m[36m(func pid=57031)[0m f1_weighted: 0.1010054440336192
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.307, 0.002, 0.0, 0.0, 0.0, 0.154, 0.013, 0.0, 0.133]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=37733)[0m top1: 0.427705223880597
[2m[36m(func pid=37733)[0m top5: 0.8973880597014925
[2m[36m(func pid=37733)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=37733)[0m f1_macro: 0.3619439382462796
[2m[36m(func pid=37733)[0m f1_weighted: 0.4564522990546212
[2m[36m(func pid=37733)[0m f1_per_class: [0.365, 0.469, 0.136, 0.554, 0.136, 0.398, 0.416, 0.445, 0.391, 0.309]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.37220149253731344
[2m[36m(func pid=56441)[0m top5: 0.9127798507462687
[2m[36m(func pid=56441)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=56441)[0m f1_macro: 0.384611928848643
[2m[36m(func pid=56441)[0m f1_weighted: 0.3782370247978229
[2m[36m(func pid=56441)[0m f1_per_class: [0.477, 0.432, 0.727, 0.218, 0.1, 0.378, 0.484, 0.491, 0.293, 0.247]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.6261 | Steps: 4 | Val loss: 1.6930 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.1313 | Steps: 4 | Val loss: 37.9722 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3362 | Steps: 4 | Val loss: 1.8325 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8339 | Steps: 4 | Val loss: 1.7319 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:52:20 (running for 00:18:45.60)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.308 |      0.362 |                   94 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.626 |      0.361 |                   42 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.858 |      0.385 |                   18 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.526 |      0.061 |                   17 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.3941231343283582
[2m[36m(func pid=50960)[0m top5: 0.8922574626865671
[2m[36m(func pid=50960)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=50960)[0m f1_macro: 0.36112518616189954
[2m[36m(func pid=50960)[0m f1_weighted: 0.42070250482305344
[2m[36m(func pid=50960)[0m f1_per_class: [0.533, 0.477, 0.436, 0.473, 0.071, 0.345, 0.402, 0.406, 0.2, 0.268]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.1296641791044776
[2m[36m(func pid=57031)[0m top5: 0.5998134328358209
[2m[36m(func pid=57031)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=57031)[0m f1_macro: 0.07898706285781751
[2m[36m(func pid=57031)[0m f1_weighted: 0.12486912165396438
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.181, 0.0, 0.003, 0.0, 0.0, 0.278, 0.108, 0.06, 0.16]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=37733)[0m top1: 0.40718283582089554
[2m[36m(func pid=37733)[0m top5: 0.8745335820895522
[2m[36m(func pid=37733)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=37733)[0m f1_macro: 0.33435281094164293
[2m[36m(func pid=37733)[0m f1_weighted: 0.4355747919117088
[2m[36m(func pid=37733)[0m f1_per_class: [0.344, 0.421, 0.181, 0.563, 0.112, 0.361, 0.402, 0.402, 0.297, 0.261]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=56441)[0m top1: 0.4006529850746269
[2m[36m(func pid=56441)[0m top5: 0.9160447761194029
[2m[36m(func pid=56441)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=56441)[0m f1_macro: 0.3970356330028619
[2m[36m(func pid=56441)[0m f1_weighted: 0.4264809889917461
[2m[36m(func pid=56441)[0m f1_per_class: [0.509, 0.446, 0.558, 0.367, 0.092, 0.405, 0.482, 0.514, 0.293, 0.305]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4496 | Steps: 4 | Val loss: 1.6680 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1177 | Steps: 4 | Val loss: 37.9223 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4287 | Steps: 4 | Val loss: 1.8939 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9103 | Steps: 4 | Val loss: 1.6544 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:52:25 (running for 00:18:51.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.336 |      0.334 |                   95 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.45  |      0.367 |                   43 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.834 |      0.397 |                   19 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.131 |      0.079 |                   18 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.41277985074626866
[2m[36m(func pid=50960)[0m top5: 0.8899253731343284
[2m[36m(func pid=50960)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=50960)[0m f1_macro: 0.3669243366775856
[2m[36m(func pid=50960)[0m f1_weighted: 0.4347712873909884
[2m[36m(func pid=50960)[0m f1_per_class: [0.505, 0.515, 0.393, 0.476, 0.083, 0.372, 0.413, 0.416, 0.222, 0.275]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.12686567164179105
[2m[36m(func pid=57031)[0m top5: 0.6459888059701493
[2m[36m(func pid=57031)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=57031)[0m f1_macro: 0.08785822097922742
[2m[36m(func pid=57031)[0m f1_weighted: 0.13632013887498326
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.0, 0.0, 0.208, 0.0, 0.0, 0.224, 0.103, 0.072, 0.271]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.4351679104477612
[2m[36m(func pid=56441)[0m top5: 0.9272388059701493
[2m[36m(func pid=56441)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=56441)[0m f1_macro: 0.42457514976471555
[2m[36m(func pid=56441)[0m f1_weighted: 0.46168373198971097
[2m[36m(func pid=56441)[0m f1_per_class: [0.539, 0.459, 0.615, 0.428, 0.096, 0.404, 0.526, 0.511, 0.345, 0.322]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.39598880597014924
[2m[36m(func pid=37733)[0m top5: 0.8600746268656716
[2m[36m(func pid=37733)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=37733)[0m f1_macro: 0.3200829777028987
[2m[36m(func pid=37733)[0m f1_weighted: 0.4161948722766203
[2m[36m(func pid=37733)[0m f1_per_class: [0.361, 0.382, 0.205, 0.588, 0.136, 0.331, 0.362, 0.347, 0.25, 0.239]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.4171 | Steps: 4 | Val loss: 1.6571 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.9542 | Steps: 4 | Val loss: 28.1006 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.4310 | Steps: 4 | Val loss: 2.0251 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7346 | Steps: 4 | Val loss: 1.7585 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 14:52:30 (running for 00:18:56.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.429 |      0.32  |                   96 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.417 |      0.372 |                   44 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.91  |      0.425 |                   20 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.118 |      0.088 |                   19 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.40904850746268656
[2m[36m(func pid=50960)[0m top5: 0.8922574626865671
[2m[36m(func pid=50960)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=50960)[0m f1_macro: 0.3716964284292743
[2m[36m(func pid=50960)[0m f1_weighted: 0.43148073467247067
[2m[36m(func pid=50960)[0m f1_per_class: [0.519, 0.507, 0.453, 0.474, 0.079, 0.379, 0.404, 0.417, 0.216, 0.27]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.1310634328358209
[2m[36m(func pid=57031)[0m top5: 0.6226679104477612
[2m[36m(func pid=57031)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=57031)[0m f1_macro: 0.08911732536530761
[2m[36m(func pid=57031)[0m f1_weighted: 0.11084234791894375
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.0, 0.0, 0.282, 0.108, 0.0, 0.057, 0.17, 0.061, 0.213]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.43097014925373134
[2m[36m(func pid=56441)[0m top5: 0.9239738805970149
[2m[36m(func pid=56441)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=56441)[0m f1_macro: 0.4039974101818339
[2m[36m(func pid=56441)[0m f1_weighted: 0.46157001972912076
[2m[36m(func pid=56441)[0m f1_per_class: [0.48, 0.456, 0.511, 0.441, 0.095, 0.408, 0.53, 0.466, 0.335, 0.319]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.3675373134328358
[2m[36m(func pid=37733)[0m top5: 0.8502798507462687
[2m[36m(func pid=37733)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=37733)[0m f1_macro: 0.29728622779142955
[2m[36m(func pid=37733)[0m f1_weighted: 0.3853309218451614
[2m[36m(func pid=37733)[0m f1_per_class: [0.354, 0.322, 0.176, 0.569, 0.119, 0.316, 0.33, 0.296, 0.215, 0.275]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5202 | Steps: 4 | Val loss: 1.6494 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.1033 | Steps: 4 | Val loss: 20.6682 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.7503 | Steps: 4 | Val loss: 1.8890 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3839 | Steps: 4 | Val loss: 1.9966 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 14:52:36 (running for 00:19:01.97)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.431 |      0.297 |                   97 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.52  |      0.376 |                   45 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.735 |      0.404 |                   21 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.954 |      0.089 |                   20 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.4076492537313433
[2m[36m(func pid=50960)[0m top5: 0.8880597014925373
[2m[36m(func pid=50960)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=50960)[0m f1_macro: 0.37623428339236936
[2m[36m(func pid=50960)[0m f1_weighted: 0.42637419283519895
[2m[36m(func pid=50960)[0m f1_per_class: [0.509, 0.505, 0.522, 0.467, 0.084, 0.363, 0.395, 0.426, 0.242, 0.25]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.134794776119403
[2m[36m(func pid=57031)[0m top5: 0.5923507462686567
[2m[36m(func pid=57031)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=57031)[0m f1_macro: 0.09072242088260501
[2m[36m(func pid=57031)[0m f1_weighted: 0.10422535496078908
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.116, 0.0, 0.218, 0.143, 0.0, 0.006, 0.307, 0.064, 0.053]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3656716417910448
[2m[36m(func pid=56441)[0m top5: 0.9085820895522388
[2m[36m(func pid=56441)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=56441)[0m f1_macro: 0.3322886761229024
[2m[36m(func pid=56441)[0m f1_weighted: 0.3978923258589002
[2m[36m(func pid=56441)[0m f1_per_class: [0.381, 0.389, 0.231, 0.333, 0.082, 0.396, 0.48, 0.464, 0.311, 0.257]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4001865671641791
[2m[36m(func pid=37733)[0m top5: 0.8955223880597015
[2m[36m(func pid=37733)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=37733)[0m f1_macro: 0.34138260946870885
[2m[36m(func pid=37733)[0m f1_weighted: 0.436403860989882
[2m[36m(func pid=37733)[0m f1_per_class: [0.386, 0.308, 0.117, 0.555, 0.1, 0.355, 0.47, 0.405, 0.318, 0.4]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5243 | Steps: 4 | Val loss: 1.6405 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.0301 | Steps: 4 | Val loss: 16.9698 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5834 | Steps: 4 | Val loss: 2.2221 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.4323 | Steps: 4 | Val loss: 1.6537 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:52:41 (running for 00:19:07.32)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.75  |      0.341 |                   98 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.524 |      0.372 |                   46 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.384 |      0.332 |                   22 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  3.103 |      0.091 |                   21 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.4197761194029851
[2m[36m(func pid=50960)[0m top5: 0.8871268656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=50960)[0m f1_macro: 0.3722794317356623
[2m[36m(func pid=50960)[0m f1_weighted: 0.44150699055440384
[2m[36m(func pid=50960)[0m f1_per_class: [0.52, 0.524, 0.387, 0.502, 0.084, 0.378, 0.399, 0.415, 0.262, 0.251]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.12406716417910447
[2m[36m(func pid=57031)[0m top5: 0.6944962686567164
[2m[36m(func pid=57031)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=57031)[0m f1_macro: 0.08033304238771305
[2m[36m(func pid=57031)[0m f1_weighted: 0.0984319545956812
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.222, 0.0, 0.133, 0.0, 0.024, 0.0, 0.301, 0.078, 0.046]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.31343283582089554
[2m[36m(func pid=56441)[0m top5: 0.9029850746268657
[2m[36m(func pid=56441)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=56441)[0m f1_macro: 0.3038471082911395
[2m[36m(func pid=56441)[0m f1_weighted: 0.3381148479436272
[2m[36m(func pid=56441)[0m f1_per_class: [0.435, 0.348, 0.22, 0.294, 0.073, 0.378, 0.352, 0.417, 0.32, 0.2]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4398320895522388
[2m[36m(func pid=37733)[0m top5: 0.9235074626865671
[2m[36m(func pid=37733)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=37733)[0m f1_macro: 0.3819066328605636
[2m[36m(func pid=37733)[0m f1_weighted: 0.47173408112614085
[2m[36m(func pid=37733)[0m f1_per_class: [0.472, 0.403, 0.137, 0.558, 0.115, 0.346, 0.507, 0.446, 0.426, 0.409]
[2m[36m(func pid=37733)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.2350 | Steps: 4 | Val loss: 1.6446 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6553 | Steps: 4 | Val loss: 26.7383 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8795 | Steps: 4 | Val loss: 2.7495 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:52:47 (running for 00:19:12.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00005 | RUNNING    | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.432 |      0.382 |                   99 |
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.235 |      0.37  |                   47 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.583 |      0.304 |                   23 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.03  |      0.08  |                   22 |
| train_5ae7f_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37733)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.4010 | Steps: 4 | Val loss: 1.6540 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=50960)[0m top1: 0.41744402985074625
[2m[36m(func pid=50960)[0m top5: 0.8875932835820896
[2m[36m(func pid=50960)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=50960)[0m f1_macro: 0.36968382399665883
[2m[36m(func pid=50960)[0m f1_weighted: 0.44157256524381655
[2m[36m(func pid=50960)[0m f1_per_class: [0.475, 0.521, 0.4, 0.509, 0.085, 0.364, 0.4, 0.421, 0.26, 0.261]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.09561567164179105
[2m[36m(func pid=57031)[0m top5: 0.6697761194029851
[2m[36m(func pid=57031)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=57031)[0m f1_macro: 0.07438443615031964
[2m[36m(func pid=57031)[0m f1_weighted: 0.06950536190516032
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.135, 0.0, 0.0, 0.0, 0.236, 0.003, 0.28, 0.052, 0.038]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.24207089552238806
[2m[36m(func pid=56441)[0m top5: 0.8208955223880597
[2m[36m(func pid=56441)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=56441)[0m f1_macro: 0.22454690515322104
[2m[36m(func pid=56441)[0m f1_weighted: 0.2707121674672899
[2m[36m(func pid=56441)[0m f1_per_class: [0.364, 0.373, 0.17, 0.288, 0.083, 0.271, 0.233, 0.127, 0.25, 0.086]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=37733)[0m top1: 0.4361007462686567
[2m[36m(func pid=37733)[0m top5: 0.9314365671641791
[2m[36m(func pid=37733)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=37733)[0m f1_macro: 0.39471190909656934
[2m[36m(func pid=37733)[0m f1_weighted: 0.46095006584918374
[2m[36m(func pid=37733)[0m f1_per_class: [0.509, 0.412, 0.215, 0.53, 0.109, 0.326, 0.495, 0.445, 0.392, 0.513]
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.3862 | Steps: 4 | Val loss: 1.6143 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8637 | Steps: 4 | Val loss: 26.9172 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7849 | Steps: 4 | Val loss: 2.7768 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=50960)[0m top1: 0.4239738805970149
[2m[36m(func pid=50960)[0m top5: 0.8964552238805971
[2m[36m(func pid=50960)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=50960)[0m f1_macro: 0.36838785626230275
[2m[36m(func pid=50960)[0m f1_weighted: 0.4467731160283016
[2m[36m(func pid=50960)[0m f1_per_class: [0.436, 0.514, 0.343, 0.505, 0.086, 0.397, 0.412, 0.444, 0.255, 0.292]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.06110074626865672
[2m[36m(func pid=57031)[0m top5: 0.6305970149253731
[2m[36m(func pid=57031)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=57031)[0m f1_macro: 0.04409750640693038
[2m[36m(func pid=57031)[0m f1_weighted: 0.08083633911571772
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.021, 0.0, 0.0, 0.0, 0.141, 0.198, 0.0, 0.052, 0.028]
[2m[36m(func pid=56441)[0m top1: 0.23087686567164178
[2m[36m(func pid=56441)[0m top5: 0.7901119402985075
[2m[36m(func pid=56441)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=56441)[0m f1_macro: 0.21309629162046634
[2m[36m(func pid=56441)[0m f1_weighted: 0.254544348700393
[2m[36m(func pid=56441)[0m f1_per_class: [0.424, 0.313, 0.204, 0.322, 0.102, 0.189, 0.221, 0.11, 0.165, 0.082]
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.4065 | Steps: 4 | Val loss: 1.6025 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=50960)[0m top1: 0.42490671641791045
[2m[36m(func pid=50960)[0m top5: 0.9001865671641791
[2m[36m(func pid=50960)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=50960)[0m f1_macro: 0.37851540667572114
[2m[36m(func pid=50960)[0m f1_weighted: 0.44859529646248697
[2m[36m(func pid=50960)[0m f1_per_class: [0.508, 0.51, 0.358, 0.506, 0.079, 0.412, 0.407, 0.447, 0.279, 0.28]
== Status ==
Current time: 2024-01-07 14:52:52 (running for 00:19:18.35)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.386 |      0.368 |                   48 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.879 |      0.225 |                   24 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.655 |      0.074 |                   23 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 14:52:59 (running for 00:19:24.80)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.386 |      0.368 |                   48 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.785 |      0.213 |                   25 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.655 |      0.074 |                   23 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=62931)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=62931)[0m Configuration completed!
[2m[36m(func pid=62931)[0m New optimizer parameters:
[2m[36m(func pid=62931)[0m SGD (
[2m[36m(func pid=62931)[0m Parameter Group 0
[2m[36m(func pid=62931)[0m     dampening: 0
[2m[36m(func pid=62931)[0m     differentiable: False
[2m[36m(func pid=62931)[0m     foreach: None
[2m[36m(func pid=62931)[0m     lr: 0.1
[2m[36m(func pid=62931)[0m     maximize: False
[2m[36m(func pid=62931)[0m     momentum: 0.99
[2m[36m(func pid=62931)[0m     nesterov: False
[2m[36m(func pid=62931)[0m     weight_decay: 0.0001
[2m[36m(func pid=62931)[0m )
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2546 | Steps: 4 | Val loss: 1.5839 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4519 | Steps: 4 | Val loss: 24.4333 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5206 | Steps: 4 | Val loss: 2.1525 | Batch size: 32 | lr: 0.001 | Duration: 3.29s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.8743 | Steps: 4 | Val loss: 3.4646 | Batch size: 32 | lr: 0.1 | Duration: 4.77s
== Status ==
Current time: 2024-01-07 14:53:04 (running for 00:19:29.81)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.406 |      0.379 |                   49 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.785 |      0.213 |                   25 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.864 |      0.044 |                   24 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.42677238805970147
[2m[36m(func pid=50960)[0m top5: 0.902518656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=50960)[0m f1_macro: 0.38204953972730327
[2m[36m(func pid=50960)[0m f1_weighted: 0.44849709149120337
[2m[36m(func pid=50960)[0m f1_per_class: [0.504, 0.515, 0.358, 0.515, 0.079, 0.415, 0.394, 0.437, 0.28, 0.323]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.07789179104477612
[2m[36m(func pid=57031)[0m top5: 0.6697761194029851
[2m[36m(func pid=57031)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=57031)[0m f1_macro: 0.050940276709726985
[2m[36m(func pid=57031)[0m f1_weighted: 0.0997536490844911
[2m[36m(func pid=57031)[0m f1_per_class: [0.0, 0.005, 0.0, 0.007, 0.0, 0.199, 0.245, 0.0, 0.025, 0.029]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3278917910447761
[2m[36m(func pid=56441)[0m top5: 0.8773320895522388
[2m[36m(func pid=56441)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=56441)[0m f1_macro: 0.28978195448079147
[2m[36m(func pid=56441)[0m f1_weighted: 0.3652029389766943
[2m[36m(func pid=56441)[0m f1_per_class: [0.354, 0.392, 0.282, 0.381, 0.115, 0.282, 0.403, 0.376, 0.181, 0.132]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.025652985074626867
[2m[36m(func pid=62931)[0m top5: 0.5844216417910447
[2m[36m(func pid=62931)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=62931)[0m f1_macro: 0.010588237093643548
[2m[36m(func pid=62931)[0m f1_weighted: 0.0050080166211034435
[2m[36m(func pid=62931)[0m f1_per_class: [0.059, 0.0, 0.0, 0.0, 0.0, 0.023, 0.003, 0.0, 0.0, 0.021]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.9899 | Steps: 4 | Val loss: 28.2106 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.2449 | Steps: 4 | Val loss: 1.5729 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3413 | Steps: 4 | Val loss: 1.9476 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.2323 | Steps: 4 | Val loss: 10.1793 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=57031)[0m top1: 0.03731343283582089
[2m[36m(func pid=57031)[0m top5: 0.6534514925373134
[2m[36m(func pid=57031)[0m f1_micro: 0.03731343283582089
[2m[36m(func pid=57031)[0m f1_macro: 0.03192113532123721
[2m[36m(func pid=57031)[0m f1_weighted: 0.03355608757310988
[2m[36m(func pid=57031)[0m f1_per_class: [0.032, 0.0, 0.0, 0.026, 0.0, 0.211, 0.0, 0.021, 0.0, 0.029]
[2m[36m(func pid=57031)[0m 
== Status ==
Current time: 2024-01-07 14:53:10 (running for 00:19:35.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.255 |      0.382 |                   50 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.521 |      0.29  |                   26 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.99  |      0.032 |                   26 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.874 |      0.011 |                    1 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.43236940298507465
[2m[36m(func pid=50960)[0m top5: 0.9034514925373134
[2m[36m(func pid=50960)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=50960)[0m f1_macro: 0.38915405736598685
[2m[36m(func pid=50960)[0m f1_weighted: 0.45165783235532114
[2m[36m(func pid=50960)[0m f1_per_class: [0.512, 0.517, 0.358, 0.53, 0.079, 0.415, 0.384, 0.457, 0.284, 0.356]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.38152985074626866
[2m[36m(func pid=56441)[0m top5: 0.9146455223880597
[2m[36m(func pid=56441)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=56441)[0m f1_macro: 0.3253683401303754
[2m[36m(func pid=56441)[0m f1_weighted: 0.4150359478671008
[2m[36m(func pid=56441)[0m f1_per_class: [0.305, 0.372, 0.253, 0.455, 0.116, 0.352, 0.462, 0.466, 0.23, 0.242]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.16744402985074627
[2m[36m(func pid=62931)[0m top5: 0.498134328358209
[2m[36m(func pid=62931)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=62931)[0m f1_macro: 0.04998023495772933
[2m[36m(func pid=62931)[0m f1_weighted: 0.05372878211496565
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.29, 0.186, 0.0, 0.0, 0.024, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6022 | Steps: 4 | Val loss: 23.7169 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1521 | Steps: 4 | Val loss: 1.5620 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4760 | Steps: 4 | Val loss: 2.1104 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 4.1263 | Steps: 4 | Val loss: 2077.8372 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:53:15 (running for 00:19:40.91)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.245 |      0.389 |                   51 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.341 |      0.325 |                   27 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.602 |      0.051 |                   27 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.232 |      0.05  |                    2 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.049906716417910446
[2m[36m(func pid=57031)[0m top5: 0.6077425373134329
[2m[36m(func pid=57031)[0m f1_micro: 0.04990671641791045
[2m[36m(func pid=57031)[0m f1_macro: 0.050754492711666044
[2m[36m(func pid=57031)[0m f1_weighted: 0.046105961878243625
[2m[36m(func pid=57031)[0m f1_per_class: [0.146, 0.0, 0.0, 0.045, 0.0, 0.254, 0.0, 0.0, 0.036, 0.026]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.43470149253731344
[2m[36m(func pid=50960)[0m top5: 0.9011194029850746
[2m[36m(func pid=50960)[0m f1_micro: 0.43470149253731344
[2m[36m(func pid=50960)[0m f1_macro: 0.39119968611664113
[2m[36m(func pid=50960)[0m f1_weighted: 0.4524002913712647
[2m[36m(func pid=50960)[0m f1_per_class: [0.481, 0.514, 0.429, 0.529, 0.084, 0.424, 0.386, 0.463, 0.282, 0.321]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3675373134328358
[2m[36m(func pid=56441)[0m top5: 0.909981343283582
[2m[36m(func pid=56441)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=56441)[0m f1_macro: 0.3222739827013644
[2m[36m(func pid=56441)[0m f1_weighted: 0.39844694209789644
[2m[36m(func pid=56441)[0m f1_per_class: [0.263, 0.34, 0.289, 0.446, 0.13, 0.391, 0.421, 0.47, 0.208, 0.264]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.0960820895522388
[2m[36m(func pid=62931)[0m top5: 0.47994402985074625
[2m[36m(func pid=62931)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=62931)[0m f1_macro: 0.021575757575757575
[2m[36m(func pid=62931)[0m f1_weighted: 0.02140886476707372
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.176, 0.0, 0.0, 0.04, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9904 | Steps: 4 | Val loss: 2.1337 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.1613 | Steps: 4 | Val loss: 1.5636 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8399 | Steps: 4 | Val loss: 22.6151 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 4.1500 | Steps: 4 | Val loss: 39223.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 14:53:20 (running for 00:19:46.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.152 |      0.391 |                   52 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.99  |      0.325 |                   29 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.602 |      0.051 |                   27 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  4.126 |      0.022 |                    3 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.43656716417910446
[2m[36m(func pid=50960)[0m top5: 0.8987873134328358
[2m[36m(func pid=50960)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=50960)[0m f1_macro: 0.3940648805202035
[2m[36m(func pid=50960)[0m f1_weighted: 0.45420315372525355
[2m[36m(func pid=50960)[0m f1_per_class: [0.468, 0.518, 0.462, 0.532, 0.087, 0.412, 0.386, 0.485, 0.299, 0.292]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.36847014925373134
[2m[36m(func pid=56441)[0m top5: 0.9127798507462687
[2m[36m(func pid=56441)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=56441)[0m f1_macro: 0.3251578658713089
[2m[36m(func pid=56441)[0m f1_weighted: 0.4022770203541333
[2m[36m(func pid=56441)[0m f1_per_class: [0.289, 0.384, 0.276, 0.417, 0.112, 0.379, 0.442, 0.437, 0.232, 0.284]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.0853544776119403
[2m[36m(func pid=57031)[0m top5: 0.5499067164179104
[2m[36m(func pid=57031)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=57031)[0m f1_macro: 0.0631638244379796
[2m[36m(func pid=57031)[0m f1_weighted: 0.09626129306568378
[2m[36m(func pid=57031)[0m f1_per_class: [0.105, 0.0, 0.0, 0.092, 0.0, 0.296, 0.115, 0.0, 0.0, 0.024]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m top1: 0.17210820895522388
[2m[36m(func pid=62931)[0m top5: 0.5774253731343284
[2m[36m(func pid=62931)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=62931)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=62931)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4202 | Steps: 4 | Val loss: 2.5469 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.1223 | Steps: 4 | Val loss: 1.5617 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7979 | Steps: 4 | Val loss: 20.8736 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.6353 | Steps: 4 | Val loss: 5432.4272 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:53:26 (running for 00:19:51.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.161 |      0.394 |                   53 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.42  |      0.216 |                   30 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.84  |      0.063 |                   28 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  4.15  |      0.029 |                    4 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=56441)[0m top1: 0.27658582089552236
[2m[36m(func pid=56441)[0m top5: 0.8689365671641791
[2m[36m(func pid=56441)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=56441)[0m f1_macro: 0.216472977457562
[2m[36m(func pid=56441)[0m f1_weighted: 0.28889800763480583
[2m[36m(func pid=56441)[0m f1_per_class: [0.249, 0.415, 0.189, 0.293, 0.133, 0.24, 0.312, 0.0, 0.187, 0.145]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.07789179104477612
[2m[36m(func pid=57031)[0m top5: 0.49580223880597013
[2m[36m(func pid=57031)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=57031)[0m f1_macro: 0.0677329693472614
[2m[36m(func pid=57031)[0m f1_weighted: 0.08837424905545055
[2m[36m(func pid=57031)[0m f1_per_class: [0.088, 0.011, 0.0, 0.206, 0.149, 0.132, 0.035, 0.0, 0.0, 0.057]
[2m[36m(func pid=50960)[0m top1: 0.43703358208955223
[2m[36m(func pid=50960)[0m top5: 0.8997201492537313
[2m[36m(func pid=50960)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=50960)[0m f1_macro: 0.3894335603474726
[2m[36m(func pid=50960)[0m f1_weighted: 0.4559462728280383
[2m[36m(func pid=50960)[0m f1_per_class: [0.451, 0.51, 0.436, 0.53, 0.092, 0.426, 0.397, 0.479, 0.297, 0.276]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m top1: 0.0065298507462686565
[2m[36m(func pid=62931)[0m top5: 0.3666044776119403
[2m[36m(func pid=62931)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=62931)[0m f1_macro: 0.00174558007767306
[2m[36m(func pid=62931)[0m f1_weighted: 0.0010009621625022272
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3312 | Steps: 4 | Val loss: 20.4251 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3161 | Steps: 4 | Val loss: 1.5543 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9003 | Steps: 4 | Val loss: 2.3899 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.5444 | Steps: 4 | Val loss: 350.6184 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:53:31 (running for 00:19:57.28)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.122 |      0.389 |                   54 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.42  |      0.216 |                   30 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.331 |      0.065 |                   30 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.635 |      0.002 |                    5 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.4300373134328358
[2m[36m(func pid=50960)[0m top5: 0.902518656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=50960)[0m f1_macro: 0.39115393367220846
[2m[36m(func pid=50960)[0m f1_weighted: 0.44620417630530596
[2m[36m(func pid=50960)[0m f1_per_class: [0.474, 0.499, 0.471, 0.523, 0.098, 0.435, 0.376, 0.462, 0.29, 0.286]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.33255597014925375
[2m[36m(func pid=56441)[0m top5: 0.9057835820895522
[2m[36m(func pid=56441)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=56441)[0m f1_macro: 0.2783062594528341
[2m[36m(func pid=56441)[0m f1_weighted: 0.3495940587439708
[2m[36m(func pid=56441)[0m f1_per_class: [0.259, 0.415, 0.196, 0.292, 0.124, 0.361, 0.425, 0.163, 0.261, 0.288]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.0960820895522388
[2m[36m(func pid=57031)[0m top5: 0.43283582089552236
[2m[36m(func pid=57031)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=57031)[0m f1_macro: 0.06510229728894917
[2m[36m(func pid=57031)[0m f1_weighted: 0.09219215566810185
[2m[36m(func pid=57031)[0m f1_per_class: [0.101, 0.016, 0.023, 0.301, 0.0, 0.0, 0.0, 0.0, 0.034, 0.176]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m top1: 0.2658582089552239
[2m[36m(func pid=62931)[0m top5: 0.46921641791044777
[2m[36m(func pid=62931)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=62931)[0m f1_macro: 0.045099728493266426
[2m[36m(func pid=62931)[0m f1_weighted: 0.12198392656338404
[2m[36m(func pid=62931)[0m f1_per_class: [0.012, 0.005, 0.0, 0.433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6341 | Steps: 4 | Val loss: 13.7957 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.0040 | Steps: 4 | Val loss: 1.5482 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6477 | Steps: 4 | Val loss: 2.5809 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 5.6372 | Steps: 4 | Val loss: 41.1206 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:53:37 (running for 00:20:02.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.316 |      0.391 |                   55 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.9   |      0.278 |                   31 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.634 |      0.071 |                   31 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  5.544 |      0.045 |                    6 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.4361007462686567
[2m[36m(func pid=50960)[0m top5: 0.9006529850746269
[2m[36m(func pid=50960)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=50960)[0m f1_macro: 0.39112277343701674
[2m[36m(func pid=50960)[0m f1_weighted: 0.450493834634614
[2m[36m(func pid=50960)[0m f1_per_class: [0.44, 0.521, 0.444, 0.525, 0.098, 0.419, 0.378, 0.472, 0.313, 0.3]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m top1: 0.12126865671641791
[2m[36m(func pid=57031)[0m top5: 0.49673507462686567
[2m[36m(func pid=57031)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=57031)[0m f1_macro: 0.0708418086550458
[2m[36m(func pid=57031)[0m f1_weighted: 0.10240047223179399
[2m[36m(func pid=57031)[0m f1_per_class: [0.108, 0.016, 0.027, 0.332, 0.0, 0.008, 0.0, 0.0, 0.05, 0.167]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.29151119402985076
[2m[36m(func pid=56441)[0m top5: 0.8997201492537313
[2m[36m(func pid=56441)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=56441)[0m f1_macro: 0.2626641452445989
[2m[36m(func pid=56441)[0m f1_weighted: 0.29762010470154066
[2m[36m(func pid=56441)[0m f1_per_class: [0.198, 0.397, 0.024, 0.225, 0.123, 0.404, 0.269, 0.392, 0.259, 0.337]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.03404850746268657
[2m[36m(func pid=62931)[0m top5: 0.5326492537313433
[2m[36m(func pid=62931)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=62931)[0m f1_macro: 0.023578010783893134
[2m[36m(func pid=62931)[0m f1_weighted: 0.015276238921569252
[2m[36m(func pid=62931)[0m f1_per_class: [0.039, 0.061, 0.074, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.9001 | Steps: 4 | Val loss: 9.3928 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.2125 | Steps: 4 | Val loss: 1.5580 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1946 | Steps: 4 | Val loss: 2.9906 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.2537 | Steps: 4 | Val loss: 500.5762 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 14:53:42 (running for 00:20:07.94)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.004 |      0.391 |                   56 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.648 |      0.263 |                   32 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.9   |      0.117 |                   32 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  5.637 |      0.024 |                    7 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.13805970149253732
[2m[36m(func pid=57031)[0m top5: 0.6543843283582089
[2m[36m(func pid=57031)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=57031)[0m f1_macro: 0.1168442286040442
[2m[36m(func pid=57031)[0m f1_weighted: 0.12304960783244343
[2m[36m(func pid=57031)[0m f1_per_class: [0.081, 0.037, 0.225, 0.3, 0.073, 0.114, 0.0, 0.263, 0.012, 0.062]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.4398320895522388
[2m[36m(func pid=50960)[0m top5: 0.8955223880597015
[2m[36m(func pid=50960)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=50960)[0m f1_macro: 0.3947085643809
[2m[36m(func pid=50960)[0m f1_weighted: 0.4549766264703343
[2m[36m(func pid=50960)[0m f1_per_class: [0.451, 0.528, 0.393, 0.534, 0.091, 0.417, 0.374, 0.485, 0.36, 0.313]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.24300373134328357
[2m[36m(func pid=56441)[0m top5: 0.878731343283582
[2m[36m(func pid=56441)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=56441)[0m f1_macro: 0.23470464875428818
[2m[36m(func pid=56441)[0m f1_weighted: 0.23458435017555765
[2m[36m(func pid=56441)[0m f1_per_class: [0.219, 0.347, 0.085, 0.167, 0.114, 0.348, 0.172, 0.334, 0.256, 0.304]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.024720149253731342
[2m[36m(func pid=62931)[0m top5: 0.5354477611940298
[2m[36m(func pid=62931)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=62931)[0m f1_macro: 0.022490085141489046
[2m[36m(func pid=62931)[0m f1_weighted: 0.023492944127390128
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.123, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.017]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.0482 | Steps: 4 | Val loss: 12.8479 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0073 | Steps: 4 | Val loss: 1.5796 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5454 | Steps: 4 | Val loss: 2.9210 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.2017 | Steps: 4 | Val loss: 452.1296 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 14:53:47 (running for 00:20:13.30)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.213 |      0.395 |                   57 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.195 |      0.235 |                   33 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  3.048 |      0.056 |                   33 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.254 |      0.022 |                    8 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.08488805970149253
[2m[36m(func pid=57031)[0m top5: 0.5359141791044776
[2m[36m(func pid=57031)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=57031)[0m f1_macro: 0.05638970743855572
[2m[36m(func pid=57031)[0m f1_weighted: 0.08055762623681413
[2m[36m(func pid=57031)[0m f1_per_class: [0.069, 0.071, 0.0, 0.227, 0.074, 0.0, 0.0, 0.0, 0.08, 0.043]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.4351679104477612
[2m[36m(func pid=50960)[0m top5: 0.8899253731343284
[2m[36m(func pid=50960)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=50960)[0m f1_macro: 0.3894700477014228
[2m[36m(func pid=50960)[0m f1_weighted: 0.4523853879002187
[2m[36m(func pid=50960)[0m f1_per_class: [0.43, 0.535, 0.348, 0.527, 0.081, 0.419, 0.368, 0.498, 0.348, 0.341]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.27611940298507465
[2m[36m(func pid=56441)[0m top5: 0.9011194029850746
[2m[36m(func pid=56441)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=56441)[0m f1_macro: 0.26745431138175263
[2m[36m(func pid=56441)[0m f1_weighted: 0.27825611340338474
[2m[36m(func pid=56441)[0m f1_per_class: [0.326, 0.341, 0.203, 0.254, 0.12, 0.362, 0.232, 0.313, 0.24, 0.283]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.036380597014925374
[2m[36m(func pid=62931)[0m top5: 0.5816231343283582
[2m[36m(func pid=62931)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=62931)[0m f1_macro: 0.01880115423182958
[2m[36m(func pid=62931)[0m f1_weighted: 0.018420241801627556
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.086, 0.013]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1302 | Steps: 4 | Val loss: 10.7133 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0990 | Steps: 4 | Val loss: 1.5593 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5532 | Steps: 4 | Val loss: 2.4502 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.6094 | Steps: 4 | Val loss: 309.5171 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=57031)[0m top1: 0.07975746268656717
[2m[36m(func pid=57031)[0m top5: 0.5914179104477612
[2m[36m(func pid=57031)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=57031)[0m f1_macro: 0.05701553747449606
[2m[36m(func pid=57031)[0m f1_weighted: 0.08081973485343678
[2m[36m(func pid=57031)[0m f1_per_class: [0.068, 0.109, 0.0, 0.204, 0.059, 0.0, 0.0, 0.0, 0.082, 0.048]
[2m[36m(func pid=57031)[0m 
== Status ==
Current time: 2024-01-07 14:53:53 (running for 00:20:18.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.007 |      0.389 |                   58 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.545 |      0.267 |                   34 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.13  |      0.057 |                   34 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.202 |      0.019 |                    9 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m top1: 0.4314365671641791
[2m[36m(func pid=50960)[0m top5: 0.8969216417910447
[2m[36m(func pid=50960)[0m f1_micro: 0.4314365671641791
[2m[36m(func pid=50960)[0m f1_macro: 0.3861033467503416
[2m[36m(func pid=50960)[0m f1_weighted: 0.4450550338109759
[2m[36m(func pid=50960)[0m f1_per_class: [0.453, 0.531, 0.364, 0.522, 0.091, 0.415, 0.355, 0.478, 0.351, 0.302]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.35867537313432835
[2m[36m(func pid=56441)[0m top5: 0.925839552238806
[2m[36m(func pid=56441)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=56441)[0m f1_macro: 0.34782176600387554
[2m[36m(func pid=56441)[0m f1_weighted: 0.3719268424056635
[2m[36m(func pid=56441)[0m f1_per_class: [0.508, 0.389, 0.387, 0.357, 0.133, 0.416, 0.368, 0.387, 0.263, 0.27]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.05037313432835821
[2m[36m(func pid=62931)[0m top5: 0.4118470149253731
[2m[36m(func pid=62931)[0m f1_micro: 0.05037313432835821
[2m[36m(func pid=62931)[0m f1_macro: 0.02792951647191197
[2m[36m(func pid=62931)[0m f1_weighted: 0.04494255511671498
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.209, 0.025, 0.031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2128 | Steps: 4 | Val loss: 7.3227 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.9828 | Steps: 4 | Val loss: 1.5780 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5714 | Steps: 4 | Val loss: 2.2593 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.2867 | Steps: 4 | Val loss: 116.9686 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 14:53:58 (running for 00:20:24.03)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.099 |      0.386 |                   59 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.553 |      0.348 |                   35 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.213 |      0.067 |                   35 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.609 |      0.028 |                   10 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.0914179104477612
[2m[36m(func pid=57031)[0m top5: 0.7033582089552238
[2m[36m(func pid=57031)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=57031)[0m f1_macro: 0.06666770294107191
[2m[36m(func pid=57031)[0m f1_weighted: 0.10418974861036907
[2m[36m(func pid=57031)[0m f1_per_class: [0.067, 0.12, 0.0, 0.127, 0.055, 0.0, 0.144, 0.0, 0.069, 0.084]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.4291044776119403
[2m[36m(func pid=50960)[0m top5: 0.8992537313432836
[2m[36m(func pid=50960)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=50960)[0m f1_macro: 0.38390605479515294
[2m[36m(func pid=50960)[0m f1_weighted: 0.44508339443889977
[2m[36m(func pid=50960)[0m f1_per_class: [0.492, 0.534, 0.324, 0.526, 0.085, 0.41, 0.351, 0.475, 0.34, 0.302]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.41744402985074625
[2m[36m(func pid=56441)[0m top5: 0.941231343283582
[2m[36m(func pid=56441)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=56441)[0m f1_macro: 0.411179663163549
[2m[36m(func pid=56441)[0m f1_weighted: 0.43869391544407393
[2m[36m(func pid=56441)[0m f1_per_class: [0.494, 0.426, 0.714, 0.439, 0.139, 0.443, 0.47, 0.416, 0.278, 0.292]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.08675373134328358
[2m[36m(func pid=62931)[0m top5: 0.527518656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=62931)[0m f1_macro: 0.03424851802289911
[2m[36m(func pid=62931)[0m f1_weighted: 0.08073989637450989
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.021, 0.028, 0.275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.8594 | Steps: 4 | Val loss: 7.3184 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.1634 | Steps: 4 | Val loss: 1.5487 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2918 | Steps: 4 | Val loss: 2.1955 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7365 | Steps: 4 | Val loss: 39.8126 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:54:03 (running for 00:20:29.31)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.983 |      0.384 |                   60 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.571 |      0.411 |                   36 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.859 |      0.097 |                   36 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.287 |      0.034 |                   11 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.14598880597014927
[2m[36m(func pid=57031)[0m top5: 0.7234141791044776
[2m[36m(func pid=57031)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=57031)[0m f1_macro: 0.09705242979544877
[2m[36m(func pid=57031)[0m f1_weighted: 0.14806828994043747
[2m[36m(func pid=57031)[0m f1_per_class: [0.051, 0.156, 0.0, 0.013, 0.086, 0.0, 0.362, 0.065, 0.059, 0.177]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.435634328358209
[2m[36m(func pid=50960)[0m top5: 0.9095149253731343
[2m[36m(func pid=50960)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=50960)[0m f1_macro: 0.39406444196602874
[2m[36m(func pid=50960)[0m f1_weighted: 0.45309348675734556
[2m[36m(func pid=50960)[0m f1_per_class: [0.545, 0.53, 0.343, 0.531, 0.086, 0.415, 0.369, 0.477, 0.347, 0.298]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.46548507462686567
[2m[36m(func pid=56441)[0m top5: 0.9472947761194029
[2m[36m(func pid=56441)[0m f1_micro: 0.4654850746268657
[2m[36m(func pid=56441)[0m f1_macro: 0.43144863175596776
[2m[36m(func pid=56441)[0m f1_weighted: 0.48457664185043886
[2m[36m(func pid=56441)[0m f1_per_class: [0.382, 0.449, 0.833, 0.489, 0.17, 0.404, 0.586, 0.398, 0.263, 0.339]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.14225746268656717
[2m[36m(func pid=62931)[0m top5: 0.6259328358208955
[2m[36m(func pid=62931)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=62931)[0m f1_macro: 0.03515861480949593
[2m[36m(func pid=62931)[0m f1_weighted: 0.0810939261151642
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.035, 0.289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4026 | Steps: 4 | Val loss: 7.7047 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.0910 | Steps: 4 | Val loss: 1.5758 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.1535 | Steps: 4 | Val loss: 17.9241 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3495 | Steps: 4 | Val loss: 2.2486 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 14:54:09 (running for 00:20:34.59)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.163 |      0.394 |                   61 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.292 |      0.431 |                   37 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.403 |      0.096 |                   37 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.737 |      0.035 |                   12 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.11567164179104478
[2m[36m(func pid=57031)[0m top5: 0.7290111940298507
[2m[36m(func pid=57031)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=57031)[0m f1_macro: 0.09645988348271006
[2m[36m(func pid=57031)[0m f1_weighted: 0.08987109606645037
[2m[36m(func pid=57031)[0m f1_per_class: [0.049, 0.175, 0.0, 0.0, 0.147, 0.0, 0.142, 0.218, 0.0, 0.234]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.43330223880597013
[2m[36m(func pid=50960)[0m top5: 0.8987873134328358
[2m[36m(func pid=50960)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=50960)[0m f1_macro: 0.3883979854742393
[2m[36m(func pid=50960)[0m f1_weighted: 0.4535423014730078
[2m[36m(func pid=50960)[0m f1_per_class: [0.5, 0.521, 0.32, 0.537, 0.081, 0.414, 0.371, 0.478, 0.366, 0.296]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m top1: 0.47294776119402987
[2m[36m(func pid=56441)[0m top5: 0.9444962686567164
[2m[36m(func pid=56441)[0m f1_micro: 0.47294776119402987
[2m[36m(func pid=56441)[0m f1_macro: 0.41413468429866934
[2m[36m(func pid=56441)[0m f1_weighted: 0.4730330456196415
[2m[36m(func pid=56441)[0m f1_per_class: [0.389, 0.417, 0.828, 0.455, 0.164, 0.285, 0.664, 0.292, 0.248, 0.4]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m top1: 0.17583955223880596
[2m[36m(func pid=62931)[0m top5: 0.6632462686567164
[2m[36m(func pid=62931)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=62931)[0m f1_macro: 0.047895855842833596
[2m[36m(func pid=62931)[0m f1_weighted: 0.0929265544249192
[2m[36m(func pid=62931)[0m f1_per_class: [0.103, 0.0, 0.051, 0.324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7442 | Steps: 4 | Val loss: 7.7843 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.1845 | Steps: 4 | Val loss: 1.5456 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8974 | Steps: 4 | Val loss: 2.5708 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9046 | Steps: 4 | Val loss: 10.5072 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 14:54:14 (running for 00:20:40.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.091 |      0.388 |                   62 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.349 |      0.414 |                   38 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.744 |      0.102 |                   38 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.154 |      0.048 |                   13 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.12126865671641791
[2m[36m(func pid=57031)[0m top5: 0.7490671641791045
[2m[36m(func pid=57031)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=57031)[0m f1_macro: 0.10191604203057514
[2m[36m(func pid=57031)[0m f1_weighted: 0.1008948978855511
[2m[36m(func pid=57031)[0m f1_per_class: [0.042, 0.183, 0.0, 0.072, 0.151, 0.016, 0.099, 0.229, 0.0, 0.227]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.43843283582089554
[2m[36m(func pid=50960)[0m top5: 0.9039179104477612
[2m[36m(func pid=50960)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=50960)[0m f1_macro: 0.3937590221292987
[2m[36m(func pid=50960)[0m f1_weighted: 0.4571515350107876
[2m[36m(func pid=50960)[0m f1_per_class: [0.545, 0.522, 0.343, 0.549, 0.097, 0.408, 0.375, 0.465, 0.352, 0.282]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.3087686567164179
[2m[36m(func pid=62931)[0m top5: 0.6553171641791045
[2m[36m(func pid=62931)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=62931)[0m f1_macro: 0.11390512624148474
[2m[36m(func pid=62931)[0m f1_weighted: 0.2563856498154107
[2m[36m(func pid=62931)[0m f1_per_class: [0.089, 0.0, 0.176, 0.355, 0.0, 0.0, 0.518, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.44822761194029853
[2m[36m(func pid=56441)[0m top5: 0.9291044776119403
[2m[36m(func pid=56441)[0m f1_micro: 0.44822761194029853
[2m[36m(func pid=56441)[0m f1_macro: 0.38800285672339957
[2m[36m(func pid=56441)[0m f1_weighted: 0.42849958530802645
[2m[36m(func pid=56441)[0m f1_per_class: [0.585, 0.382, 0.774, 0.424, 0.156, 0.038, 0.654, 0.286, 0.196, 0.386]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.9223 | Steps: 4 | Val loss: 6.2867 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3541 | Steps: 4 | Val loss: 1.5908 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.2110 | Steps: 4 | Val loss: 8.7908 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1988 | Steps: 4 | Val loss: 2.9147 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 14:54:19 (running for 00:20:45.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.185 |      0.394 |                   63 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.897 |      0.388 |                   39 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.922 |      0.125 |                   39 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.905 |      0.114 |                   14 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.1525186567164179
[2m[36m(func pid=57031)[0m top5: 0.757929104477612
[2m[36m(func pid=57031)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=57031)[0m f1_macro: 0.12466286366885089
[2m[36m(func pid=57031)[0m f1_weighted: 0.14105442788450562
[2m[36m(func pid=57031)[0m f1_per_class: [0.068, 0.236, 0.0, 0.146, 0.15, 0.086, 0.102, 0.249, 0.0, 0.209]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.425839552238806
[2m[36m(func pid=50960)[0m top5: 0.894589552238806
[2m[36m(func pid=50960)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=50960)[0m f1_macro: 0.38580906493995115
[2m[36m(func pid=50960)[0m f1_weighted: 0.4443878182571671
[2m[36m(func pid=50960)[0m f1_per_class: [0.5, 0.515, 0.369, 0.535, 0.087, 0.407, 0.352, 0.481, 0.324, 0.288]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.3694029850746269
[2m[36m(func pid=62931)[0m top5: 0.7056902985074627
[2m[36m(func pid=62931)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=62931)[0m f1_macro: 0.118569968824116
[2m[36m(func pid=62931)[0m f1_weighted: 0.29845025650795737
[2m[36m(func pid=62931)[0m f1_per_class: [0.028, 0.0, 0.0, 0.494, 0.048, 0.0, 0.526, 0.0, 0.09, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.4048507462686567
[2m[36m(func pid=56441)[0m top5: 0.9160447761194029
[2m[36m(func pid=56441)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=56441)[0m f1_macro: 0.3584334863526936
[2m[36m(func pid=56441)[0m f1_weighted: 0.3896298471053516
[2m[36m(func pid=56441)[0m f1_per_class: [0.532, 0.407, 0.632, 0.366, 0.132, 0.0, 0.581, 0.298, 0.196, 0.441]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6787 | Steps: 4 | Val loss: 5.5611 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8249 | Steps: 4 | Val loss: 1.5553 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7665 | Steps: 4 | Val loss: 8.2546 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9240 | Steps: 4 | Val loss: 3.0448 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=57031)[0m top1: 0.16791044776119404
[2m[36m(func pid=57031)[0m top5: 0.7560634328358209
[2m[36m(func pid=57031)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=57031)[0m f1_macro: 0.12892340758184467
[2m[36m(func pid=57031)[0m f1_weighted: 0.16061830300657004
[2m[36m(func pid=57031)[0m f1_per_class: [0.073, 0.269, 0.0, 0.183, 0.136, 0.037, 0.13, 0.267, 0.0, 0.194]
== Status ==
Current time: 2024-01-07 14:54:25 (running for 00:20:50.62)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.354 |      0.386 |                   64 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.199 |      0.358 |                   40 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.679 |      0.129 |                   40 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.211 |      0.119 |                   15 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.44263059701492535
[2m[36m(func pid=50960)[0m top5: 0.8987873134328358
[2m[36m(func pid=50960)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=50960)[0m f1_macro: 0.40088071657905877
[2m[36m(func pid=50960)[0m f1_weighted: 0.4633451682289636
[2m[36m(func pid=50960)[0m f1_per_class: [0.517, 0.524, 0.381, 0.543, 0.093, 0.407, 0.396, 0.488, 0.361, 0.299]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.3474813432835821
[2m[36m(func pid=62931)[0m top5: 0.6576492537313433
[2m[36m(func pid=62931)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=62931)[0m f1_macro: 0.11086060337837185
[2m[36m(func pid=62931)[0m f1_weighted: 0.2949245386910517
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.493, 0.024, 0.0, 0.52, 0.0, 0.073, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3871268656716418
[2m[36m(func pid=56441)[0m top5: 0.9043843283582089
[2m[36m(func pid=56441)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=56441)[0m f1_macro: 0.3516423935918511
[2m[36m(func pid=56441)[0m f1_weighted: 0.3791602645237852
[2m[36m(func pid=56441)[0m f1_per_class: [0.525, 0.415, 0.585, 0.366, 0.093, 0.0, 0.536, 0.327, 0.19, 0.478]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4386 | Steps: 4 | Val loss: 4.7321 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.9324 | Steps: 4 | Val loss: 1.5306 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.9827 | Steps: 4 | Val loss: 6.8413 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7662 | Steps: 4 | Val loss: 3.0890 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:54:30 (running for 00:20:55.87)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.825 |      0.401 |                   65 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.924 |      0.352 |                   41 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.439 |      0.133 |                   41 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.767 |      0.111 |                   16 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.1730410447761194
[2m[36m(func pid=57031)[0m top5: 0.7346082089552238
[2m[36m(func pid=57031)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=57031)[0m f1_macro: 0.13294358658800193
[2m[36m(func pid=57031)[0m f1_weighted: 0.15792550328449256
[2m[36m(func pid=57031)[0m f1_per_class: [0.083, 0.269, 0.0, 0.058, 0.14, 0.0, 0.236, 0.313, 0.061, 0.169]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m top1: 0.4510261194029851
[2m[36m(func pid=50960)[0m top5: 0.9085820895522388
[2m[36m(func pid=50960)[0m f1_micro: 0.4510261194029851
[2m[36m(func pid=50960)[0m f1_macro: 0.40187081921875184
[2m[36m(func pid=50960)[0m f1_weighted: 0.47210821153525434
[2m[36m(func pid=50960)[0m f1_per_class: [0.517, 0.531, 0.375, 0.545, 0.106, 0.408, 0.422, 0.48, 0.347, 0.288]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.21455223880597016
[2m[36m(func pid=62931)[0m top5: 0.6427238805970149
[2m[36m(func pid=62931)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=62931)[0m f1_macro: 0.0846997514173656
[2m[36m(func pid=62931)[0m f1_weighted: 0.16185752953054777
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.446, 0.025, 0.306, 0.0, 0.0, 0.07, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.3521455223880597
[2m[36m(func pid=56441)[0m top5: 0.8899253731343284
[2m[36m(func pid=56441)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=56441)[0m f1_macro: 0.33105290401195525
[2m[36m(func pid=56441)[0m f1_weighted: 0.343402818847056
[2m[36m(func pid=56441)[0m f1_per_class: [0.4, 0.391, 0.615, 0.327, 0.16, 0.008, 0.458, 0.411, 0.201, 0.34]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3013 | Steps: 4 | Val loss: 4.5458 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.9096 | Steps: 4 | Val loss: 1.4935 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8892 | Steps: 4 | Val loss: 6.3268 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 14:54:35 (running for 00:21:01.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.932 |      0.402 |                   66 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.766 |      0.331 |                   42 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.301 |      0.149 |                   42 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.983 |      0.085 |                   17 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.19309701492537312
[2m[36m(func pid=57031)[0m top5: 0.7285447761194029
[2m[36m(func pid=57031)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=57031)[0m f1_macro: 0.1492303700672926
[2m[36m(func pid=57031)[0m f1_weighted: 0.1840907490086884
[2m[36m(func pid=57031)[0m f1_per_class: [0.109, 0.286, 0.0, 0.0, 0.106, 0.0, 0.345, 0.427, 0.08, 0.14]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7170 | Steps: 4 | Val loss: 3.5237 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=50960)[0m top1: 0.46175373134328357
[2m[36m(func pid=50960)[0m top5: 0.9151119402985075
[2m[36m(func pid=50960)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=50960)[0m f1_macro: 0.40478556615773
[2m[36m(func pid=50960)[0m f1_weighted: 0.4804409128548967
[2m[36m(func pid=50960)[0m f1_per_class: [0.5, 0.528, 0.381, 0.546, 0.134, 0.414, 0.451, 0.465, 0.354, 0.275]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.23880597014925373
[2m[36m(func pid=62931)[0m top5: 0.683768656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=62931)[0m f1_macro: 0.07847848537005161
[2m[36m(func pid=62931)[0m f1_weighted: 0.16902647905053048
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.482, 0.0, 0.303, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.24207089552238806
[2m[36m(func pid=56441)[0m top5: 0.7336753731343284
[2m[36m(func pid=56441)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=56441)[0m f1_macro: 0.22127760833054352
[2m[36m(func pid=56441)[0m f1_weighted: 0.25515411816614786
[2m[36m(func pid=56441)[0m f1_per_class: [0.324, 0.332, 0.212, 0.24, 0.088, 0.098, 0.281, 0.382, 0.117, 0.138]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2974 | Steps: 4 | Val loss: 5.2085 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0234 | Steps: 4 | Val loss: 1.5451 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6089 | Steps: 4 | Val loss: 4.2322 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:54:40 (running for 00:21:06.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.91  |      0.405 |                   67 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.717 |      0.221 |                   43 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.297 |      0.145 |                   43 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.889 |      0.078 |                   18 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.17024253731343283
[2m[36m(func pid=57031)[0m top5: 0.738339552238806
[2m[36m(func pid=57031)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=57031)[0m f1_macro: 0.14507991796560263
[2m[36m(func pid=57031)[0m f1_weighted: 0.15915163741751137
[2m[36m(func pid=57031)[0m f1_per_class: [0.123, 0.309, 0.0, 0.0, 0.113, 0.0, 0.241, 0.456, 0.089, 0.12]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3104 | Steps: 4 | Val loss: 3.7071 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=50960)[0m top1: 0.45802238805970147
[2m[36m(func pid=50960)[0m top5: 0.902518656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.45802238805970147
[2m[36m(func pid=50960)[0m f1_macro: 0.3963781319924208
[2m[36m(func pid=50960)[0m f1_weighted: 0.4822014375974775
[2m[36m(func pid=50960)[0m f1_per_class: [0.5, 0.535, 0.364, 0.553, 0.106, 0.381, 0.467, 0.437, 0.345, 0.277]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.25886194029850745
[2m[36m(func pid=62931)[0m top5: 0.715018656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=62931)[0m f1_macro: 0.0868649314302278
[2m[36m(func pid=62931)[0m f1_weighted: 0.1802821559788809
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.024, 0.0, 0.503, 0.03, 0.312, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.22014925373134328
[2m[36m(func pid=56441)[0m top5: 0.7061567164179104
[2m[36m(func pid=56441)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=56441)[0m f1_macro: 0.211734510918218
[2m[36m(func pid=56441)[0m f1_weighted: 0.24299574034764573
[2m[36m(func pid=56441)[0m f1_per_class: [0.342, 0.316, 0.115, 0.102, 0.067, 0.159, 0.355, 0.368, 0.146, 0.146]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7148 | Steps: 4 | Val loss: 5.0208 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0430 | Steps: 4 | Val loss: 1.5693 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:54:46 (running for 00:21:11.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.023 |      0.396 |                   68 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.31  |      0.212 |                   44 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.715 |      0.117 |                   44 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.609 |      0.087 |                   19 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.12779850746268656
[2m[36m(func pid=57031)[0m top5: 0.7262126865671642
[2m[36m(func pid=57031)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=57031)[0m f1_macro: 0.11736189140429365
[2m[36m(func pid=57031)[0m f1_weighted: 0.08724357069118358
[2m[36m(func pid=57031)[0m f1_per_class: [0.147, 0.301, 0.0, 0.013, 0.154, 0.0, 0.0, 0.418, 0.085, 0.056]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6647 | Steps: 4 | Val loss: 3.7973 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1985 | Steps: 4 | Val loss: 3.7793 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=50960)[0m top1: 0.4608208955223881
[2m[36m(func pid=50960)[0m top5: 0.9015858208955224
[2m[36m(func pid=50960)[0m f1_micro: 0.4608208955223881
[2m[36m(func pid=50960)[0m f1_macro: 0.3930094886796979
[2m[36m(func pid=50960)[0m f1_weighted: 0.4881395448649922
[2m[36m(func pid=50960)[0m f1_per_class: [0.475, 0.531, 0.364, 0.556, 0.121, 0.385, 0.488, 0.449, 0.322, 0.239]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.2593283582089552
[2m[36m(func pid=62931)[0m top5: 0.715018656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=62931)[0m f1_macro: 0.09825303857071953
[2m[36m(func pid=62931)[0m f1_weighted: 0.19776659714399805
[2m[36m(func pid=62931)[0m f1_per_class: [0.026, 0.161, 0.0, 0.478, 0.0, 0.318, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.2019589552238806
[2m[36m(func pid=56441)[0m top5: 0.746268656716418
[2m[36m(func pid=56441)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=56441)[0m f1_macro: 0.19588660524848653
[2m[36m(func pid=56441)[0m f1_weighted: 0.19173763991847548
[2m[36m(func pid=56441)[0m f1_per_class: [0.299, 0.339, 0.073, 0.013, 0.069, 0.165, 0.252, 0.333, 0.21, 0.204]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7287 | Steps: 4 | Val loss: 4.7269 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.0038 | Steps: 4 | Val loss: 1.5874 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 14:54:51 (running for 00:21:17.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.043 |      0.393 |                   69 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.199 |      0.196 |                   45 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.729 |      0.114 |                   45 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.665 |      0.098 |                   20 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.12639925373134328
[2m[36m(func pid=57031)[0m top5: 0.7374067164179104
[2m[36m(func pid=57031)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=57031)[0m f1_macro: 0.11387235601437436
[2m[36m(func pid=57031)[0m f1_weighted: 0.09007524229324715
[2m[36m(func pid=57031)[0m f1_per_class: [0.12, 0.266, 0.0, 0.054, 0.181, 0.0, 0.0, 0.375, 0.091, 0.051]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4708 | Steps: 4 | Val loss: 3.7190 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8672 | Steps: 4 | Val loss: 4.4831 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=50960)[0m top1: 0.45848880597014924
[2m[36m(func pid=50960)[0m top5: 0.9006529850746269
[2m[36m(func pid=50960)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=50960)[0m f1_macro: 0.38574465770086486
[2m[36m(func pid=50960)[0m f1_weighted: 0.48697330955039736
[2m[36m(func pid=50960)[0m f1_per_class: [0.48, 0.534, 0.338, 0.557, 0.103, 0.369, 0.494, 0.41, 0.341, 0.232]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.17630597014925373
[2m[36m(func pid=62931)[0m top5: 0.6847014925373134
[2m[36m(func pid=62931)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=62931)[0m f1_macro: 0.07683130825719642
[2m[36m(func pid=62931)[0m f1_weighted: 0.13957995209741017
[2m[36m(func pid=62931)[0m f1_per_class: [0.065, 0.063, 0.0, 0.329, 0.0, 0.311, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.18003731343283583
[2m[36m(func pid=56441)[0m top5: 0.7210820895522388
[2m[36m(func pid=56441)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=56441)[0m f1_macro: 0.1821386932645512
[2m[36m(func pid=56441)[0m f1_weighted: 0.15731980726773914
[2m[36m(func pid=56441)[0m f1_per_class: [0.294, 0.386, 0.077, 0.029, 0.062, 0.137, 0.114, 0.277, 0.229, 0.217]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.8696 | Steps: 4 | Val loss: 4.2235 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8090 | Steps: 4 | Val loss: 1.6227 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4857 | Steps: 4 | Val loss: 3.9418 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 14:54:57 (running for 00:21:22.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.004 |      0.386 |                   70 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.867 |      0.182 |                   46 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.87  |      0.115 |                   46 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.471 |      0.077 |                   21 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.1296641791044776
[2m[36m(func pid=57031)[0m top5: 0.7756529850746269
[2m[36m(func pid=57031)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=57031)[0m f1_macro: 0.11520484514820395
[2m[36m(func pid=57031)[0m f1_weighted: 0.10264977151032897
[2m[36m(func pid=57031)[0m f1_per_class: [0.12, 0.19, 0.013, 0.144, 0.165, 0.023, 0.0, 0.336, 0.101, 0.06]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5548 | Steps: 4 | Val loss: 4.2375 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=50960)[0m top1: 0.4510261194029851
[2m[36m(func pid=50960)[0m top5: 0.8931902985074627
[2m[36m(func pid=50960)[0m f1_micro: 0.4510261194029851
[2m[36m(func pid=50960)[0m f1_macro: 0.3867113343029557
[2m[36m(func pid=50960)[0m f1_weighted: 0.4826484698131503
[2m[36m(func pid=50960)[0m f1_per_class: [0.509, 0.529, 0.353, 0.552, 0.094, 0.371, 0.484, 0.422, 0.317, 0.235]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.10774253731343283
[2m[36m(func pid=62931)[0m top5: 0.6842350746268657
[2m[36m(func pid=62931)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=62931)[0m f1_macro: 0.039166458836265086
[2m[36m(func pid=62931)[0m f1_weighted: 0.037512088174996405
[2m[36m(func pid=62931)[0m f1_per_class: [0.059, 0.0, 0.016, 0.0, 0.0, 0.317, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.2234141791044776
[2m[36m(func pid=56441)[0m top5: 0.7159514925373134
[2m[36m(func pid=56441)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=56441)[0m f1_macro: 0.21503213208561117
[2m[36m(func pid=56441)[0m f1_weighted: 0.19897749092172373
[2m[36m(func pid=56441)[0m f1_per_class: [0.382, 0.446, 0.111, 0.036, 0.062, 0.294, 0.143, 0.315, 0.218, 0.143]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.8224 | Steps: 4 | Val loss: 4.1959 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.0961 | Steps: 4 | Val loss: 1.6235 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 14:55:02 (running for 00:21:28.07)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.809 |      0.387 |                   71 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.555 |      0.215 |                   47 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.822 |      0.144 |                   47 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.486 |      0.039 |                   22 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.1515858208955224
[2m[36m(func pid=57031)[0m top5: 0.792910447761194
[2m[36m(func pid=57031)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=57031)[0m f1_macro: 0.14389202136519924
[2m[36m(func pid=57031)[0m f1_weighted: 0.1341163687510702
[2m[36m(func pid=57031)[0m f1_per_class: [0.083, 0.157, 0.041, 0.245, 0.168, 0.077, 0.0, 0.359, 0.108, 0.2]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6424 | Steps: 4 | Val loss: 3.5835 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2477 | Steps: 4 | Val loss: 4.1006 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=50960)[0m top1: 0.4458955223880597
[2m[36m(func pid=50960)[0m top5: 0.8978544776119403
[2m[36m(func pid=50960)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=50960)[0m f1_macro: 0.38685137132602576
[2m[36m(func pid=50960)[0m f1_weighted: 0.4780852761860707
[2m[36m(func pid=50960)[0m f1_per_class: [0.525, 0.51, 0.333, 0.56, 0.086, 0.358, 0.472, 0.438, 0.332, 0.254]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.13292910447761194
[2m[36m(func pid=62931)[0m top5: 0.6875
[2m[36m(func pid=62931)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=62931)[0m f1_macro: 0.0734004940058984
[2m[36m(func pid=62931)[0m f1_weighted: 0.07577246180090752
[2m[36m(func pid=62931)[0m f1_per_class: [0.083, 0.201, 0.081, 0.0, 0.0, 0.339, 0.0, 0.0, 0.0, 0.031]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.2737873134328358
[2m[36m(func pid=56441)[0m top5: 0.7178171641791045
[2m[36m(func pid=56441)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=56441)[0m f1_macro: 0.2396226732346852
[2m[36m(func pid=56441)[0m f1_weighted: 0.2504330422591842
[2m[36m(func pid=56441)[0m f1_per_class: [0.253, 0.451, 0.186, 0.01, 0.076, 0.371, 0.298, 0.367, 0.296, 0.09]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.9497 | Steps: 4 | Val loss: 3.4352 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7892 | Steps: 4 | Val loss: 1.6221 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:55:07 (running for 00:21:33.46)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  1.096 |      0.387 |                   72 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.248 |      0.24  |                   48 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.95  |      0.161 |                   48 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.642 |      0.073 |                   23 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.1884328358208955
[2m[36m(func pid=57031)[0m top5: 0.8264925373134329
[2m[36m(func pid=57031)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=57031)[0m f1_macro: 0.1610740047870207
[2m[36m(func pid=57031)[0m f1_weighted: 0.16958567834188704
[2m[36m(func pid=57031)[0m f1_per_class: [0.114, 0.156, 0.074, 0.331, 0.131, 0.178, 0.006, 0.305, 0.137, 0.178]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4994 | Steps: 4 | Val loss: 3.3929 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7852 | Steps: 4 | Val loss: 4.2224 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=50960)[0m top1: 0.44449626865671643
[2m[36m(func pid=50960)[0m top5: 0.90625
[2m[36m(func pid=50960)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=50960)[0m f1_macro: 0.3906521340711052
[2m[36m(func pid=50960)[0m f1_weighted: 0.47687389509663647
[2m[36m(func pid=50960)[0m f1_per_class: [0.478, 0.521, 0.324, 0.556, 0.076, 0.365, 0.459, 0.45, 0.34, 0.339]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.13759328358208955
[2m[36m(func pid=62931)[0m top5: 0.6557835820895522
[2m[36m(func pid=62931)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=62931)[0m f1_macro: 0.06857191550454197
[2m[36m(func pid=62931)[0m f1_weighted: 0.06912091886303005
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.309, 0.115, 0.01, 0.0, 0.0, 0.0, 0.208, 0.0, 0.044]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.2873134328358209
[2m[36m(func pid=56441)[0m top5: 0.7299440298507462
[2m[36m(func pid=56441)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=56441)[0m f1_macro: 0.22363213885174332
[2m[36m(func pid=56441)[0m f1_weighted: 0.26642705279837914
[2m[36m(func pid=56441)[0m f1_per_class: [0.14, 0.444, 0.0, 0.013, 0.072, 0.392, 0.344, 0.407, 0.338, 0.088]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.1299 | Steps: 4 | Val loss: 3.6287 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7120 | Steps: 4 | Val loss: 1.5997 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:55:13 (running for 00:21:38.75)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.789 |      0.391 |                   73 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.785 |      0.224 |                   49 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  3.13  |      0.181 |                   49 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.499 |      0.069 |                   24 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.22154850746268656
[2m[36m(func pid=57031)[0m top5: 0.8330223880597015
[2m[36m(func pid=57031)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=57031)[0m f1_macro: 0.18066279255428302
[2m[36m(func pid=57031)[0m f1_weighted: 0.221538158407356
[2m[36m(func pid=57031)[0m f1_per_class: [0.108, 0.186, 0.043, 0.366, 0.121, 0.19, 0.123, 0.326, 0.125, 0.219]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4729 | Steps: 4 | Val loss: 3.7223 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7672 | Steps: 4 | Val loss: 4.2043 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=50960)[0m top1: 0.44869402985074625
[2m[36m(func pid=50960)[0m top5: 0.9071828358208955
[2m[36m(func pid=50960)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=50960)[0m f1_macro: 0.39450197271548065
[2m[36m(func pid=50960)[0m f1_weighted: 0.4797655599800386
[2m[36m(func pid=50960)[0m f1_per_class: [0.463, 0.487, 0.348, 0.567, 0.086, 0.394, 0.465, 0.452, 0.357, 0.326]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.1525186567164179
[2m[36m(func pid=62931)[0m top5: 0.6604477611940298
[2m[36m(func pid=62931)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=62931)[0m f1_macro: 0.09323238749400367
[2m[36m(func pid=62931)[0m f1_weighted: 0.13063253803572422
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.231, 0.067, 0.275, 0.043, 0.0, 0.0, 0.212, 0.0, 0.104]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.23460820895522388
[2m[36m(func pid=56441)[0m top5: 0.7761194029850746
[2m[36m(func pid=56441)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=56441)[0m f1_macro: 0.24580527246993383
[2m[36m(func pid=56441)[0m f1_weighted: 0.2174604759275211
[2m[36m(func pid=56441)[0m f1_per_class: [0.248, 0.397, 0.516, 0.063, 0.057, 0.297, 0.206, 0.313, 0.252, 0.109]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.4371 | Steps: 4 | Val loss: 3.1201 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8878 | Steps: 4 | Val loss: 1.6107 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 14:55:18 (running for 00:21:44.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.32825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.712 |      0.395 |                   74 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.767 |      0.246 |                   50 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.437 |      0.238 |                   50 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.473 |      0.093 |                   25 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.314365671641791
[2m[36m(func pid=57031)[0m top5: 0.851679104477612
[2m[36m(func pid=57031)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=57031)[0m f1_macro: 0.23775419500536388
[2m[36m(func pid=57031)[0m f1_weighted: 0.335371326939091
[2m[36m(func pid=57031)[0m f1_per_class: [0.137, 0.209, 0.0, 0.383, 0.089, 0.19, 0.444, 0.437, 0.182, 0.306]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.0840 | Steps: 4 | Val loss: 3.9277 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3900 | Steps: 4 | Val loss: 4.1783 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=50960)[0m top1: 0.44029850746268656
[2m[36m(func pid=50960)[0m top5: 0.9039179104477612
[2m[36m(func pid=50960)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=50960)[0m f1_macro: 0.39600236466041544
[2m[36m(func pid=50960)[0m f1_weighted: 0.46812787208542495
[2m[36m(func pid=50960)[0m f1_per_class: [0.437, 0.468, 0.375, 0.565, 0.086, 0.391, 0.43, 0.496, 0.378, 0.333]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.1814365671641791
[2m[36m(func pid=62931)[0m top5: 0.6865671641791045
[2m[36m(func pid=62931)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=62931)[0m f1_macro: 0.09973586663416321
[2m[36m(func pid=62931)[0m f1_weighted: 0.15923974505526975
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.136, 0.077, 0.429, 0.039, 0.0, 0.0, 0.242, 0.022, 0.051]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.23041044776119404
[2m[36m(func pid=56441)[0m top5: 0.7826492537313433
[2m[36m(func pid=56441)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=56441)[0m f1_macro: 0.20550914674449036
[2m[36m(func pid=56441)[0m f1_weighted: 0.23572239232280204
[2m[36m(func pid=56441)[0m f1_per_class: [0.281, 0.338, 0.086, 0.107, 0.065, 0.176, 0.319, 0.295, 0.184, 0.204]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6480 | Steps: 4 | Val loss: 3.2160 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.9934 | Steps: 4 | Val loss: 1.5862 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:55:24 (running for 00:21:49.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.888 |      0.396 |                   75 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.39  |      0.206 |                   51 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.648 |      0.257 |                   51 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.084 |      0.1   |                   26 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.34701492537313433
[2m[36m(func pid=57031)[0m top5: 0.8689365671641791
[2m[36m(func pid=57031)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=57031)[0m f1_macro: 0.2570261657083203
[2m[36m(func pid=57031)[0m f1_weighted: 0.3647658093980902
[2m[36m(func pid=57031)[0m f1_per_class: [0.168, 0.239, 0.0, 0.34, 0.087, 0.184, 0.555, 0.513, 0.125, 0.358]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3609 | Steps: 4 | Val loss: 3.5377 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6418 | Steps: 4 | Val loss: 5.7316 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=50960)[0m top1: 0.45149253731343286
[2m[36m(func pid=50960)[0m top5: 0.9085820895522388
[2m[36m(func pid=50960)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=50960)[0m f1_macro: 0.409938459763503
[2m[36m(func pid=50960)[0m f1_weighted: 0.4766056570770294
[2m[36m(func pid=50960)[0m f1_per_class: [0.471, 0.482, 0.407, 0.577, 0.088, 0.396, 0.43, 0.513, 0.385, 0.35]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.12126865671641791
[2m[36m(func pid=62931)[0m top5: 0.683768656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=62931)[0m f1_macro: 0.07647375360328727
[2m[36m(func pid=62931)[0m f1_weighted: 0.13279710965379127
[2m[36m(func pid=62931)[0m f1_per_class: [0.22, 0.06, 0.0, 0.417, 0.022, 0.0, 0.0, 0.0, 0.046, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.1791044776119403
[2m[36m(func pid=56441)[0m top5: 0.7346082089552238
[2m[36m(func pid=56441)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=56441)[0m f1_macro: 0.1706876716058573
[2m[36m(func pid=56441)[0m f1_weighted: 0.18062116605098708
[2m[36m(func pid=56441)[0m f1_per_class: [0.159, 0.254, 0.051, 0.101, 0.108, 0.089, 0.241, 0.246, 0.137, 0.32]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1456 | Steps: 4 | Val loss: 3.6528 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.7099 | Steps: 4 | Val loss: 1.5534 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:55:29 (running for 00:21:55.20)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.993 |      0.41  |                   76 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.642 |      0.171 |                   52 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.146 |      0.237 |                   52 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.361 |      0.076 |                   27 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.30736940298507465
[2m[36m(func pid=57031)[0m top5: 0.8689365671641791
[2m[36m(func pid=57031)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=57031)[0m f1_macro: 0.2373613523057196
[2m[36m(func pid=57031)[0m f1_weighted: 0.32305339278373363
[2m[36m(func pid=57031)[0m f1_per_class: [0.167, 0.271, 0.0, 0.29, 0.09, 0.131, 0.48, 0.453, 0.071, 0.421]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.8852 | Steps: 4 | Val loss: 3.0788 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.8765 | Steps: 4 | Val loss: 6.0959 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=50960)[0m top1: 0.46222014925373134
[2m[36m(func pid=50960)[0m top5: 0.917910447761194
[2m[36m(func pid=50960)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=50960)[0m f1_macro: 0.4136459792363385
[2m[36m(func pid=50960)[0m f1_weighted: 0.4889170409061088
[2m[36m(func pid=50960)[0m f1_per_class: [0.464, 0.488, 0.407, 0.578, 0.093, 0.39, 0.468, 0.528, 0.377, 0.344]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.26399253731343286
[2m[36m(func pid=62931)[0m top5: 0.6707089552238806
[2m[36m(func pid=62931)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=62931)[0m f1_macro: 0.1522287566352086
[2m[36m(func pid=62931)[0m f1_weighted: 0.26949222404479267
[2m[36m(func pid=62931)[0m f1_per_class: [0.24, 0.033, 0.264, 0.398, 0.028, 0.0, 0.482, 0.0, 0.078, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.1525186567164179
[2m[36m(func pid=56441)[0m top5: 0.7448694029850746
[2m[36m(func pid=56441)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=56441)[0m f1_macro: 0.14769169109825833
[2m[36m(func pid=56441)[0m f1_weighted: 0.1457661442884995
[2m[36m(func pid=56441)[0m f1_per_class: [0.021, 0.19, 0.054, 0.08, 0.168, 0.076, 0.199, 0.217, 0.118, 0.353]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4073 | Steps: 4 | Val loss: 3.4020 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.8919 | Steps: 4 | Val loss: 1.5257 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:55:35 (running for 00:22:00.70)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.71  |      0.414 |                   77 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.876 |      0.148 |                   53 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.407 |      0.209 |                   53 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.885 |      0.152 |                   28 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.25046641791044777
[2m[36m(func pid=57031)[0m top5: 0.8810634328358209
[2m[36m(func pid=57031)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=57031)[0m f1_macro: 0.2085392117556096
[2m[36m(func pid=57031)[0m f1_weighted: 0.2617100117415511
[2m[36m(func pid=57031)[0m f1_per_class: [0.214, 0.278, 0.0, 0.317, 0.084, 0.165, 0.255, 0.33, 0.056, 0.386]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.1896 | Steps: 4 | Val loss: 3.8384 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7993 | Steps: 4 | Val loss: 4.6223 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=50960)[0m top1: 0.46595149253731344
[2m[36m(func pid=50960)[0m top5: 0.9263059701492538
[2m[36m(func pid=50960)[0m f1_micro: 0.46595149253731344
[2m[36m(func pid=50960)[0m f1_macro: 0.4151268276166135
[2m[36m(func pid=50960)[0m f1_weighted: 0.49255703699404785
[2m[36m(func pid=50960)[0m f1_per_class: [0.492, 0.516, 0.381, 0.558, 0.088, 0.396, 0.481, 0.511, 0.377, 0.35]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.35634328358208955
[2m[36m(func pid=62931)[0m top5: 0.7182835820895522
[2m[36m(func pid=62931)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=62931)[0m f1_macro: 0.13416809150959286
[2m[36m(func pid=62931)[0m f1_weighted: 0.3053448561798061
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.068, 0.538, 0.047, 0.0, 0.51, 0.0, 0.021, 0.158]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.23041044776119404
[2m[36m(func pid=56441)[0m top5: 0.8101679104477612
[2m[36m(func pid=56441)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=56441)[0m f1_macro: 0.1926483324785449
[2m[36m(func pid=56441)[0m f1_weighted: 0.25661994770023233
[2m[36m(func pid=56441)[0m f1_per_class: [0.017, 0.257, 0.059, 0.21, 0.137, 0.137, 0.381, 0.234, 0.173, 0.321]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5747 | Steps: 4 | Val loss: 3.9355 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.5039 | Steps: 4 | Val loss: 1.5091 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:55:40 (running for 00:22:06.05)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.892 |      0.415 |                   78 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.799 |      0.193 |                   54 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.575 |      0.196 |                   54 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.19  |      0.134 |                   29 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.2271455223880597
[2m[36m(func pid=57031)[0m top5: 0.8675373134328358
[2m[36m(func pid=57031)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=57031)[0m f1_macro: 0.19608030337114693
[2m[36m(func pid=57031)[0m f1_weighted: 0.22871993679724428
[2m[36m(func pid=57031)[0m f1_per_class: [0.222, 0.268, 0.0, 0.299, 0.092, 0.183, 0.162, 0.32, 0.059, 0.356]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.4987 | Steps: 4 | Val loss: 5.3853 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4938 | Steps: 4 | Val loss: 3.1633 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=50960)[0m top1: 0.447294776119403
[2m[36m(func pid=50960)[0m top5: 0.9305037313432836
[2m[36m(func pid=50960)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=50960)[0m f1_macro: 0.4083995660391371
[2m[36m(func pid=50960)[0m f1_weighted: 0.47269486802031413
[2m[36m(func pid=50960)[0m f1_per_class: [0.477, 0.5, 0.4, 0.506, 0.094, 0.416, 0.471, 0.486, 0.378, 0.357]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.3969216417910448
[2m[36m(func pid=62931)[0m top5: 0.7290111940298507
[2m[36m(func pid=62931)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=62931)[0m f1_macro: 0.14182033330120247
[2m[36m(func pid=62931)[0m f1_weighted: 0.31281877195222774
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.19, 0.558, 0.049, 0.022, 0.508, 0.0, 0.028, 0.062]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=56441)[0m top1: 0.37220149253731344
[2m[36m(func pid=56441)[0m top5: 0.875
[2m[36m(func pid=56441)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=56441)[0m f1_macro: 0.2883335366085821
[2m[36m(func pid=56441)[0m f1_weighted: 0.4182799184464461
[2m[36m(func pid=56441)[0m f1_per_class: [0.061, 0.375, 0.098, 0.461, 0.112, 0.38, 0.507, 0.246, 0.291, 0.353]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.6729 | Steps: 4 | Val loss: 3.5153 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.7611 | Steps: 4 | Val loss: 1.5590 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:55:45 (running for 00:22:11.48)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.504 |      0.408 |                   79 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.494 |      0.288 |                   55 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.673 |      0.203 |                   55 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.499 |      0.142 |                   30 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.23507462686567165
[2m[36m(func pid=57031)[0m top5: 0.8805970149253731
[2m[36m(func pid=57031)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=57031)[0m f1_macro: 0.20291662679124886
[2m[36m(func pid=57031)[0m f1_weighted: 0.23711076089594613
[2m[36m(func pid=57031)[0m f1_per_class: [0.195, 0.278, 0.0, 0.331, 0.101, 0.223, 0.14, 0.322, 0.048, 0.39]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.3925 | Steps: 4 | Val loss: 4.3978 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1022 | Steps: 4 | Val loss: 3.1719 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=50960)[0m top1: 0.425839552238806
[2m[36m(func pid=50960)[0m top5: 0.9230410447761194
[2m[36m(func pid=50960)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=50960)[0m f1_macro: 0.37634940020441127
[2m[36m(func pid=50960)[0m f1_weighted: 0.45710347476278346
[2m[36m(func pid=50960)[0m f1_per_class: [0.395, 0.496, 0.293, 0.51, 0.086, 0.4, 0.442, 0.457, 0.335, 0.351]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.39925373134328357
[2m[36m(func pid=62931)[0m top5: 0.7229477611940298
[2m[36m(func pid=62931)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=62931)[0m f1_macro: 0.1597929359729396
[2m[36m(func pid=62931)[0m f1_weighted: 0.3508834218324694
[2m[36m(func pid=62931)[0m f1_per_class: [0.038, 0.223, 0.171, 0.546, 0.0, 0.038, 0.512, 0.0, 0.027, 0.042]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5437 | Steps: 4 | Val loss: 3.4296 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=56441)[0m top1: 0.3712686567164179
[2m[36m(func pid=56441)[0m top5: 0.878731343283582
[2m[36m(func pid=56441)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=56441)[0m f1_macro: 0.29436185637496526
[2m[36m(func pid=56441)[0m f1_weighted: 0.37700813604040384
[2m[36m(func pid=56441)[0m f1_per_class: [0.064, 0.382, 0.226, 0.534, 0.124, 0.387, 0.29, 0.209, 0.348, 0.378]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.6810 | Steps: 4 | Val loss: 1.5946 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=57031)[0m top1: 0.2490671641791045
[2m[36m(func pid=57031)[0m top5: 0.8927238805970149
[2m[36m(func pid=57031)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=57031)[0m f1_macro: 0.21025307301343346
[2m[36m(func pid=57031)[0m f1_weighted: 0.24566642412261033
[2m[36m(func pid=57031)[0m f1_per_class: [0.222, 0.355, 0.0, 0.356, 0.127, 0.243, 0.097, 0.296, 0.056, 0.351]
[2m[36m(func pid=57031)[0m 
== Status ==
Current time: 2024-01-07 14:55:51 (running for 00:22:16.91)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.761 |      0.376 |                   80 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.102 |      0.294 |                   56 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.544 |      0.21  |                   56 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.393 |      0.16  |                   31 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6296 | Steps: 4 | Val loss: 3.9853 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7854 | Steps: 4 | Val loss: 3.2289 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=50960)[0m top1: 0.42490671641791045
[2m[36m(func pid=50960)[0m top5: 0.914179104477612
[2m[36m(func pid=50960)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=50960)[0m f1_macro: 0.37991845516866146
[2m[36m(func pid=50960)[0m f1_weighted: 0.45800480352446554
[2m[36m(func pid=50960)[0m f1_per_class: [0.421, 0.516, 0.289, 0.504, 0.078, 0.4, 0.434, 0.468, 0.347, 0.342]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=62931)[0m top1: 0.23180970149253732
[2m[36m(func pid=62931)[0m top5: 0.746268656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=62931)[0m f1_macro: 0.13190937418568263
[2m[36m(func pid=62931)[0m f1_weighted: 0.195304047407392
[2m[36m(func pid=62931)[0m f1_per_class: [0.066, 0.178, 0.348, 0.0, 0.0, 0.046, 0.511, 0.0, 0.077, 0.093]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5333 | Steps: 4 | Val loss: 3.1632 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=56441)[0m top1: 0.365205223880597
[2m[36m(func pid=56441)[0m top5: 0.8838619402985075
[2m[36m(func pid=56441)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=56441)[0m f1_macro: 0.27954966806739073
[2m[36m(func pid=56441)[0m f1_weighted: 0.3524646513695672
[2m[36m(func pid=56441)[0m f1_per_class: [0.06, 0.344, 0.369, 0.54, 0.124, 0.367, 0.242, 0.179, 0.327, 0.242]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.5771 | Steps: 4 | Val loss: 1.5235 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:55:56 (running for 00:22:22.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.681 |      0.38  |                   81 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.785 |      0.28  |                   57 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.533 |      0.225 |                   57 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.63  |      0.132 |                   32 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.27705223880597013
[2m[36m(func pid=57031)[0m top5: 0.8908582089552238
[2m[36m(func pid=57031)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=57031)[0m f1_macro: 0.22510391005869385
[2m[36m(func pid=57031)[0m f1_weighted: 0.2896748106767792
[2m[36m(func pid=57031)[0m f1_per_class: [0.171, 0.375, 0.0, 0.331, 0.153, 0.314, 0.221, 0.363, 0.072, 0.25]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.0124 | Steps: 4 | Val loss: 3.3701 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=50960)[0m top1: 0.4458955223880597
[2m[36m(func pid=50960)[0m top5: 0.9253731343283582
[2m[36m(func pid=50960)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=50960)[0m f1_macro: 0.3934379667430871
[2m[36m(func pid=50960)[0m f1_weighted: 0.4721926323823166
[2m[36m(func pid=50960)[0m f1_per_class: [0.416, 0.531, 0.353, 0.507, 0.101, 0.405, 0.467, 0.466, 0.356, 0.333]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8235 | Steps: 4 | Val loss: 2.7241 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
[2m[36m(func pid=62931)[0m top1: 0.11054104477611941
[2m[36m(func pid=62931)[0m top5: 0.7052238805970149
[2m[36m(func pid=62931)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=62931)[0m f1_macro: 0.04946363328621143
[2m[36m(func pid=62931)[0m f1_weighted: 0.0405564920043392
[2m[36m(func pid=62931)[0m f1_per_class: [0.056, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.07, 0.049]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3902 | Steps: 4 | Val loss: 3.2029 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56441)[0m top1: 0.3969216417910448
[2m[36m(func pid=56441)[0m top5: 0.8861940298507462
[2m[36m(func pid=56441)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=56441)[0m f1_macro: 0.3324592296472263
[2m[36m(func pid=56441)[0m f1_weighted: 0.41271866771075916
[2m[36m(func pid=56441)[0m f1_per_class: [0.24, 0.383, 0.226, 0.464, 0.139, 0.407, 0.421, 0.439, 0.257, 0.348]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.6057 | Steps: 4 | Val loss: 1.5552 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:56:01 (running for 00:22:27.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.577 |      0.393 |                   82 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.824 |      0.332 |                   58 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.39  |      0.242 |                   58 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.012 |      0.049 |                   33 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.29850746268656714
[2m[36m(func pid=57031)[0m top5: 0.8880597014925373
[2m[36m(func pid=57031)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=57031)[0m f1_macro: 0.24223728162097755
[2m[36m(func pid=57031)[0m f1_weighted: 0.3284881807735939
[2m[36m(func pid=57031)[0m f1_per_class: [0.145, 0.363, 0.0, 0.286, 0.171, 0.323, 0.381, 0.465, 0.061, 0.229]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5545 | Steps: 4 | Val loss: 2.7537 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=50960)[0m top1: 0.4291044776119403
[2m[36m(func pid=50960)[0m top5: 0.9197761194029851
[2m[36m(func pid=50960)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=50960)[0m f1_macro: 0.3883723247249748
[2m[36m(func pid=50960)[0m f1_weighted: 0.45358504589533416
[2m[36m(func pid=50960)[0m f1_per_class: [0.416, 0.527, 0.381, 0.48, 0.094, 0.393, 0.439, 0.452, 0.351, 0.353]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4721 | Steps: 4 | Val loss: 3.4060 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=62931)[0m top1: 0.11240671641791045
[2m[36m(func pid=62931)[0m top5: 0.7038246268656716
[2m[36m(func pid=62931)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=62931)[0m f1_macro: 0.07823340503493952
[2m[36m(func pid=62931)[0m f1_weighted: 0.04283930873747029
[2m[36m(func pid=62931)[0m f1_per_class: [0.06, 0.0, 0.316, 0.003, 0.0, 0.321, 0.0, 0.0, 0.054, 0.028]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4615 | Steps: 4 | Val loss: 3.2771 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=56441)[0m top1: 0.3591417910447761
[2m[36m(func pid=56441)[0m top5: 0.851679104477612
[2m[36m(func pid=56441)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=56441)[0m f1_macro: 0.32005569681493296
[2m[36m(func pid=56441)[0m f1_weighted: 0.3632603381931422
[2m[36m(func pid=56441)[0m f1_per_class: [0.377, 0.414, 0.104, 0.403, 0.147, 0.377, 0.296, 0.431, 0.298, 0.354]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.6375 | Steps: 4 | Val loss: 1.5460 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:56:07 (running for 00:22:33.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.606 |      0.388 |                   83 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.472 |      0.32  |                   59 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.462 |      0.235 |                   59 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.555 |      0.078 |                   34 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.29151119402985076
[2m[36m(func pid=57031)[0m top5: 0.8852611940298507
[2m[36m(func pid=57031)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=57031)[0m f1_macro: 0.2352878534155604
[2m[36m(func pid=57031)[0m f1_weighted: 0.33041746786759363
[2m[36m(func pid=57031)[0m f1_per_class: [0.132, 0.388, 0.054, 0.274, 0.18, 0.298, 0.412, 0.369, 0.077, 0.168]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.7997 | Steps: 4 | Val loss: 3.1583 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=50960)[0m top1: 0.4388992537313433
[2m[36m(func pid=50960)[0m top5: 0.9244402985074627
[2m[36m(func pid=50960)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=50960)[0m f1_macro: 0.3928850532557181
[2m[36m(func pid=50960)[0m f1_weighted: 0.45971891346640875
[2m[36m(func pid=50960)[0m f1_per_class: [0.431, 0.542, 0.32, 0.47, 0.101, 0.395, 0.454, 0.456, 0.384, 0.376]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7204 | Steps: 4 | Val loss: 5.6868 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=62931)[0m top1: 0.30550373134328357
[2m[36m(func pid=62931)[0m top5: 0.7238805970149254
[2m[36m(func pid=62931)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=62931)[0m f1_macro: 0.12136211161297221
[2m[36m(func pid=62931)[0m f1_weighted: 0.20157345289053716
[2m[36m(func pid=62931)[0m f1_per_class: [0.231, 0.0, 0.0, 0.552, 0.0, 0.353, 0.0, 0.0, 0.078, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5614 | Steps: 4 | Val loss: 3.3432 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=56441)[0m top1: 0.2555970149253731
[2m[36m(func pid=56441)[0m top5: 0.6767723880597015
[2m[36m(func pid=56441)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=56441)[0m f1_macro: 0.2258847221313907
[2m[36m(func pid=56441)[0m f1_weighted: 0.23386261160136673
[2m[36m(func pid=56441)[0m f1_per_class: [0.288, 0.427, 0.063, 0.309, 0.148, 0.338, 0.016, 0.238, 0.222, 0.21]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.6020 | Steps: 4 | Val loss: 1.5500 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:56:12 (running for 00:22:38.35)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.637 |      0.393 |                   84 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.72  |      0.226 |                   60 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.561 |      0.22  |                   60 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.8   |      0.121 |                   35 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.2789179104477612
[2m[36m(func pid=57031)[0m top5: 0.8763992537313433
[2m[36m(func pid=57031)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=57031)[0m f1_macro: 0.2202653657654731
[2m[36m(func pid=57031)[0m f1_weighted: 0.31448038957183294
[2m[36m(func pid=57031)[0m f1_per_class: [0.141, 0.384, 0.119, 0.307, 0.182, 0.256, 0.382, 0.153, 0.127, 0.153]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7181 | Steps: 4 | Val loss: 2.5384 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=50960)[0m top1: 0.44029850746268656
[2m[36m(func pid=50960)[0m top5: 0.9207089552238806
[2m[36m(func pid=50960)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=50960)[0m f1_macro: 0.40283448983568243
[2m[36m(func pid=50960)[0m f1_weighted: 0.4589222944099937
[2m[36m(func pid=50960)[0m f1_per_class: [0.47, 0.549, 0.381, 0.477, 0.083, 0.387, 0.438, 0.459, 0.38, 0.404]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6892 | Steps: 4 | Val loss: 6.7970 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=62931)[0m top1: 0.19916044776119404
[2m[36m(func pid=62931)[0m top5: 0.7803171641791045
[2m[36m(func pid=62931)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=62931)[0m f1_macro: 0.06990948460848193
[2m[36m(func pid=62931)[0m f1_weighted: 0.15205611485508744
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.105, 0.536, 0.0, 0.0, 0.0, 0.0, 0.058, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.9282 | Steps: 4 | Val loss: 2.5998 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=56441)[0m top1: 0.19169776119402984
[2m[36m(func pid=56441)[0m top5: 0.648320895522388
[2m[36m(func pid=56441)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=56441)[0m f1_macro: 0.17223860178210507
[2m[36m(func pid=56441)[0m f1_weighted: 0.18024428898235664
[2m[36m(func pid=56441)[0m f1_per_class: [0.206, 0.281, 0.039, 0.268, 0.15, 0.323, 0.006, 0.06, 0.227, 0.163]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.7663 | Steps: 4 | Val loss: 1.5559 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 14:56:18 (running for 00:22:43.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.602 |      0.403 |                   85 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.689 |      0.172 |                   61 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.928 |      0.275 |                   61 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.718 |      0.07  |                   36 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.3572761194029851
[2m[36m(func pid=57031)[0m top5: 0.9085820895522388
[2m[36m(func pid=57031)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=57031)[0m f1_macro: 0.27460319875791517
[2m[36m(func pid=57031)[0m f1_weighted: 0.389618339195643
[2m[36m(func pid=57031)[0m f1_per_class: [0.172, 0.355, 0.122, 0.346, 0.11, 0.26, 0.56, 0.368, 0.172, 0.282]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7376 | Steps: 4 | Val loss: 3.1135 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=50960)[0m top1: 0.4398320895522388
[2m[36m(func pid=50960)[0m top5: 0.9193097014925373
[2m[36m(func pid=50960)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=50960)[0m f1_macro: 0.39843317858086585
[2m[36m(func pid=50960)[0m f1_weighted: 0.46085965324431605
[2m[36m(func pid=50960)[0m f1_per_class: [0.481, 0.539, 0.387, 0.47, 0.094, 0.38, 0.467, 0.435, 0.371, 0.361]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.8483 | Steps: 4 | Val loss: 4.4376 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=62931)[0m top1: 0.10494402985074627
[2m[36m(func pid=62931)[0m top5: 0.7728544776119403
[2m[36m(func pid=62931)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=62931)[0m f1_macro: 0.04404830766705636
[2m[36m(func pid=62931)[0m f1_weighted: 0.10203327299960664
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.005, 0.025, 0.355, 0.0, 0.0, 0.0, 0.0, 0.055, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2188 | Steps: 4 | Val loss: 2.1471 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=56441)[0m top1: 0.28078358208955223
[2m[36m(func pid=56441)[0m top5: 0.8582089552238806
[2m[36m(func pid=56441)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=56441)[0m f1_macro: 0.2626919137580376
[2m[36m(func pid=56441)[0m f1_weighted: 0.3087443784372116
[2m[36m(func pid=56441)[0m f1_per_class: [0.196, 0.264, 0.046, 0.169, 0.128, 0.32, 0.452, 0.495, 0.206, 0.351]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.6395 | Steps: 4 | Val loss: 1.5826 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=57031)[0m top1: 0.3829291044776119
[2m[36m(func pid=57031)[0m top5: 0.9104477611940298
[2m[36m(func pid=57031)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=57031)[0m f1_macro: 0.29204138198463875
[2m[36m(func pid=57031)[0m f1_weighted: 0.4033350227405463
[2m[36m(func pid=57031)[0m f1_per_class: [0.219, 0.369, 0.156, 0.419, 0.088, 0.16, 0.548, 0.464, 0.117, 0.381]
[2m[36m(func pid=57031)[0m 
== Status ==
Current time: 2024-01-07 14:56:23 (running for 00:22:49.44)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.766 |      0.398 |                   86 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.848 |      0.263 |                   62 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.219 |      0.292 |                   62 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.738 |      0.044 |                   37 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 4.2834 | Steps: 4 | Val loss: 3.3272 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=50960)[0m top1: 0.44263059701492535
[2m[36m(func pid=50960)[0m top5: 0.9183768656716418
[2m[36m(func pid=50960)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=50960)[0m f1_macro: 0.39514668338515035
[2m[36m(func pid=50960)[0m f1_weighted: 0.46150502863809834
[2m[36m(func pid=50960)[0m f1_per_class: [0.434, 0.548, 0.333, 0.468, 0.098, 0.382, 0.461, 0.462, 0.389, 0.376]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0050 | Steps: 4 | Val loss: 4.2987 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=62931)[0m top1: 0.17630597014925373
[2m[36m(func pid=62931)[0m top5: 0.7919776119402985
[2m[36m(func pid=62931)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=62931)[0m f1_macro: 0.09226438102446317
[2m[36m(func pid=62931)[0m f1_weighted: 0.14292873782412793
[2m[36m(func pid=62931)[0m f1_per_class: [0.131, 0.056, 0.004, 0.409, 0.0, 0.0, 0.0, 0.241, 0.083, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.5888 | Steps: 4 | Val loss: 2.2008 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=56441)[0m top1: 0.3204291044776119
[2m[36m(func pid=56441)[0m top5: 0.8302238805970149
[2m[36m(func pid=56441)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=56441)[0m f1_macro: 0.26726509617960587
[2m[36m(func pid=56441)[0m f1_weighted: 0.34488573171960546
[2m[36m(func pid=56441)[0m f1_per_class: [0.298, 0.279, 0.043, 0.204, 0.14, 0.212, 0.582, 0.435, 0.194, 0.286]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.7516 | Steps: 4 | Val loss: 1.5721 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:56:29 (running for 00:22:54.61)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.639 |      0.395 |                   87 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.005 |      0.267 |                   63 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.589 |      0.259 |                   63 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  4.283 |      0.092 |                   38 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.31669776119402987
[2m[36m(func pid=57031)[0m top5: 0.9146455223880597
[2m[36m(func pid=57031)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=57031)[0m f1_macro: 0.2585114237631879
[2m[36m(func pid=57031)[0m f1_weighted: 0.3245150845460579
[2m[36m(func pid=57031)[0m f1_per_class: [0.318, 0.351, 0.173, 0.458, 0.08, 0.068, 0.316, 0.331, 0.077, 0.414]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.9970 | Steps: 4 | Val loss: 4.0826 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=50960)[0m top1: 0.44776119402985076
[2m[36m(func pid=50960)[0m top5: 0.917910447761194
[2m[36m(func pid=50960)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=50960)[0m f1_macro: 0.3840116959617914
[2m[36m(func pid=50960)[0m f1_weighted: 0.46781577966550364
[2m[36m(func pid=50960)[0m f1_per_class: [0.395, 0.546, 0.264, 0.469, 0.115, 0.368, 0.494, 0.461, 0.373, 0.355]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5153 | Steps: 4 | Val loss: 4.3303 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=62931)[0m top1: 0.16371268656716417
[2m[36m(func pid=62931)[0m top5: 0.7434701492537313
[2m[36m(func pid=62931)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=62931)[0m f1_macro: 0.0913076057110291
[2m[36m(func pid=62931)[0m f1_weighted: 0.13146734830083118
[2m[36m(func pid=62931)[0m f1_per_class: [0.179, 0.016, 0.0, 0.397, 0.038, 0.0, 0.0, 0.193, 0.09, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.9223 | Steps: 4 | Val loss: 2.2183 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=56441)[0m top1: 0.28824626865671643
[2m[36m(func pid=56441)[0m top5: 0.8414179104477612
[2m[36m(func pid=56441)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=56441)[0m f1_macro: 0.25673394893611634
[2m[36m(func pid=56441)[0m f1_weighted: 0.3302109418629679
[2m[36m(func pid=56441)[0m f1_per_class: [0.217, 0.265, 0.039, 0.194, 0.132, 0.315, 0.52, 0.413, 0.22, 0.254]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.5313 | Steps: 4 | Val loss: 1.5790 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 14:56:34 (running for 00:22:59.96)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.752 |      0.384 |                   88 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.515 |      0.257 |                   64 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.922 |      0.263 |                   64 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.997 |      0.091 |                   39 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.3101679104477612
[2m[36m(func pid=57031)[0m top5: 0.9123134328358209
[2m[36m(func pid=57031)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=57031)[0m f1_macro: 0.26291924306861103
[2m[36m(func pid=57031)[0m f1_weighted: 0.3191250058579538
[2m[36m(func pid=57031)[0m f1_per_class: [0.364, 0.431, 0.157, 0.46, 0.069, 0.046, 0.255, 0.324, 0.082, 0.442]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.9589 | Steps: 4 | Val loss: 3.4692 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=50960)[0m top1: 0.45009328358208955
[2m[36m(func pid=50960)[0m top5: 0.9151119402985075
[2m[36m(func pid=50960)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=50960)[0m f1_macro: 0.38587963569066386
[2m[36m(func pid=50960)[0m f1_weighted: 0.4755664914482832
[2m[36m(func pid=50960)[0m f1_per_class: [0.476, 0.536, 0.273, 0.479, 0.109, 0.372, 0.518, 0.443, 0.339, 0.314]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5983 | Steps: 4 | Val loss: 4.7158 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=62931)[0m top1: 0.0708955223880597
[2m[36m(func pid=62931)[0m top5: 0.7751865671641791
[2m[36m(func pid=62931)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=62931)[0m f1_macro: 0.03417596102164
[2m[36m(func pid=62931)[0m f1_weighted: 0.02921305135082354
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.055, 0.021, 0.0, 0.0, 0.197, 0.068, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6903 | Steps: 4 | Val loss: 2.2971 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=56441)[0m top1: 0.2635261194029851
[2m[36m(func pid=56441)[0m top5: 0.8442164179104478
[2m[36m(func pid=56441)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=56441)[0m f1_macro: 0.24698550261223912
[2m[36m(func pid=56441)[0m f1_weighted: 0.2971456844603286
[2m[36m(func pid=56441)[0m f1_per_class: [0.151, 0.268, 0.044, 0.283, 0.19, 0.431, 0.28, 0.456, 0.178, 0.188]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.5979 | Steps: 4 | Val loss: 1.5407 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 14:56:39 (running for 00:23:05.19)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.531 |      0.386 |                   89 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.598 |      0.247 |                   65 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.69  |      0.251 |                   65 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.959 |      0.034 |                   40 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.2943097014925373
[2m[36m(func pid=57031)[0m top5: 0.902518656716418
[2m[36m(func pid=57031)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=57031)[0m f1_macro: 0.2506540240817833
[2m[36m(func pid=57031)[0m f1_weighted: 0.3006712175221926
[2m[36m(func pid=57031)[0m f1_per_class: [0.301, 0.457, 0.127, 0.398, 0.074, 0.023, 0.244, 0.337, 0.113, 0.434]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.8401 | Steps: 4 | Val loss: 3.6198 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=50960)[0m top1: 0.4664179104477612
[2m[36m(func pid=50960)[0m top5: 0.9207089552238806
[2m[36m(func pid=50960)[0m f1_micro: 0.4664179104477612
[2m[36m(func pid=50960)[0m f1_macro: 0.39467679846908893
[2m[36m(func pid=50960)[0m f1_weighted: 0.4913912891600691
[2m[36m(func pid=50960)[0m f1_per_class: [0.444, 0.548, 0.286, 0.489, 0.12, 0.389, 0.545, 0.463, 0.343, 0.318]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4777 | Steps: 4 | Val loss: 4.8317 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=62931)[0m top1: 0.06809701492537314
[2m[36m(func pid=62931)[0m top5: 0.769589552238806
[2m[36m(func pid=62931)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=62931)[0m f1_macro: 0.03971819268800357
[2m[36m(func pid=62931)[0m f1_weighted: 0.030234970114928577
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.049, 0.027, 0.0, 0.0, 0.245, 0.065, 0.013]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0310 | Steps: 4 | Val loss: 2.3860 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=56441)[0m top1: 0.27005597014925375
[2m[36m(func pid=56441)[0m top5: 0.7910447761194029
[2m[36m(func pid=56441)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=56441)[0m f1_macro: 0.2380183332713875
[2m[36m(func pid=56441)[0m f1_weighted: 0.2876537130028955
[2m[36m(func pid=56441)[0m f1_per_class: [0.185, 0.357, 0.066, 0.349, 0.164, 0.407, 0.163, 0.348, 0.205, 0.137]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.5721 | Steps: 4 | Val loss: 1.5849 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 14:56:45 (running for 00:23:10.62)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.395 |                   90 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.478 |      0.238 |                   66 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.031 |      0.245 |                   66 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.84  |      0.04  |                   41 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.28031716417910446
[2m[36m(func pid=57031)[0m top5: 0.9001865671641791
[2m[36m(func pid=57031)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=57031)[0m f1_macro: 0.2450712753212426
[2m[36m(func pid=57031)[0m f1_weighted: 0.2848316659965791
[2m[36m(func pid=57031)[0m f1_per_class: [0.278, 0.427, 0.151, 0.326, 0.071, 0.086, 0.249, 0.345, 0.15, 0.367]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.0140 | Steps: 4 | Val loss: 3.9530 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=50960)[0m top1: 0.4608208955223881
[2m[36m(func pid=50960)[0m top5: 0.9127798507462687
[2m[36m(func pid=50960)[0m f1_micro: 0.4608208955223881
[2m[36m(func pid=50960)[0m f1_macro: 0.38477197766853805
[2m[36m(func pid=50960)[0m f1_weighted: 0.4861537745400165
[2m[36m(func pid=50960)[0m f1_per_class: [0.455, 0.542, 0.276, 0.48, 0.119, 0.368, 0.557, 0.419, 0.345, 0.288]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5839 | Steps: 4 | Val loss: 5.0334 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=62931)[0m top1: 0.06576492537313433
[2m[36m(func pid=62931)[0m top5: 0.7024253731343284
[2m[36m(func pid=62931)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=62931)[0m f1_macro: 0.036056259395474535
[2m[36m(func pid=62931)[0m f1_weighted: 0.03355339975366278
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.0, 0.064, 0.028, 0.0, 0.0, 0.269, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.7964 | Steps: 4 | Val loss: 2.2944 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=56441)[0m top1: 0.2891791044776119
[2m[36m(func pid=56441)[0m top5: 0.7770522388059702
[2m[36m(func pid=56441)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=56441)[0m f1_macro: 0.22664342502280732
[2m[36m(func pid=56441)[0m f1_weighted: 0.2811872768521447
[2m[36m(func pid=56441)[0m f1_per_class: [0.267, 0.392, 0.253, 0.451, 0.15, 0.33, 0.116, 0.014, 0.151, 0.142]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.7267 | Steps: 4 | Val loss: 1.5592 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 14:56:50 (running for 00:23:15.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.385 |                   91 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.584 |      0.227 |                   67 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.796 |      0.281 |                   67 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.014 |      0.036 |                   42 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.3138992537313433
[2m[36m(func pid=57031)[0m top5: 0.8955223880597015
[2m[36m(func pid=57031)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=57031)[0m f1_macro: 0.28147796465258024
[2m[36m(func pid=57031)[0m f1_weighted: 0.32655156391916723
[2m[36m(func pid=57031)[0m f1_per_class: [0.263, 0.439, 0.164, 0.318, 0.072, 0.24, 0.31, 0.4, 0.253, 0.356]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7424 | Steps: 4 | Val loss: 3.1088 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=50960)[0m top1: 0.4631529850746269
[2m[36m(func pid=50960)[0m top5: 0.9174440298507462
[2m[36m(func pid=50960)[0m f1_micro: 0.4631529850746269
[2m[36m(func pid=50960)[0m f1_macro: 0.39678120313909454
[2m[36m(func pid=50960)[0m f1_weighted: 0.48693435453578043
[2m[36m(func pid=50960)[0m f1_per_class: [0.468, 0.518, 0.348, 0.507, 0.112, 0.381, 0.539, 0.398, 0.385, 0.311]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0519 | Steps: 4 | Val loss: 5.7905 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4338 | Steps: 4 | Val loss: 2.2588 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=62931)[0m top1: 0.06063432835820896
[2m[36m(func pid=62931)[0m top5: 0.6469216417910447
[2m[36m(func pid=62931)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=62931)[0m f1_macro: 0.03245105682046275
[2m[36m(func pid=62931)[0m f1_weighted: 0.013813301020230467
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.048, 0.0, 0.023, 0.0, 0.0, 0.225, 0.0, 0.028]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.7716 | Steps: 4 | Val loss: 1.5195 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 14:56:55 (running for 00:23:20.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.727 |      0.397 |                   92 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  1.052 |      0.187 |                   68 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.796 |      0.281 |                   67 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.742 |      0.032 |                   43 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=56441)[0m top1: 0.2644589552238806
[2m[36m(func pid=56441)[0m top5: 0.761660447761194
[2m[36m(func pid=56441)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=56441)[0m f1_macro: 0.18703505736377163
[2m[36m(func pid=56441)[0m f1_weighted: 0.26097457497849036
[2m[36m(func pid=56441)[0m f1_per_class: [0.156, 0.381, 0.111, 0.45, 0.117, 0.315, 0.08, 0.0, 0.117, 0.145]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.314365671641791
[2m[36m(func pid=57031)[0m top5: 0.8917910447761194
[2m[36m(func pid=57031)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=57031)[0m f1_macro: 0.26685861351132945
[2m[36m(func pid=57031)[0m f1_weighted: 0.32628667642784037
[2m[36m(func pid=57031)[0m f1_per_class: [0.259, 0.43, 0.088, 0.321, 0.075, 0.295, 0.297, 0.413, 0.208, 0.282]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7748 | Steps: 4 | Val loss: 3.4328 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=50960)[0m top1: 0.4748134328358209
[2m[36m(func pid=50960)[0m top5: 0.9235074626865671
[2m[36m(func pid=50960)[0m f1_micro: 0.4748134328358209
[2m[36m(func pid=50960)[0m f1_macro: 0.41711845258640523
[2m[36m(func pid=50960)[0m f1_weighted: 0.496876932569219
[2m[36m(func pid=50960)[0m f1_per_class: [0.535, 0.529, 0.4, 0.507, 0.112, 0.389, 0.554, 0.402, 0.413, 0.331]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5109 | Steps: 4 | Val loss: 5.7502 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.5010 | Steps: 4 | Val loss: 2.3798 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=62931)[0m top1: 0.1814365671641791
[2m[36m(func pid=62931)[0m top5: 0.570429104477612
[2m[36m(func pid=62931)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=62931)[0m f1_macro: 0.07642760709980792
[2m[36m(func pid=62931)[0m f1_weighted: 0.16409033579060664
[2m[36m(func pid=62931)[0m f1_per_class: [0.04, 0.0, 0.043, 0.0, 0.029, 0.072, 0.504, 0.077, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.6755 | Steps: 4 | Val loss: 1.5365 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:57:00 (running for 00:23:26.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.772 |      0.417 |                   93 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.511 |      0.234 |                   69 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.434 |      0.267 |                   68 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.775 |      0.076 |                   44 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=56441)[0m top1: 0.2635261194029851
[2m[36m(func pid=56441)[0m top5: 0.773320895522388
[2m[36m(func pid=56441)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=56441)[0m f1_macro: 0.23350594757898607
[2m[36m(func pid=56441)[0m f1_weighted: 0.26084345001825815
[2m[36m(func pid=56441)[0m f1_per_class: [0.196, 0.362, 0.533, 0.437, 0.106, 0.302, 0.091, 0.016, 0.122, 0.17]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.29151119402985076
[2m[36m(func pid=57031)[0m top5: 0.8894589552238806
[2m[36m(func pid=57031)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=57031)[0m f1_macro: 0.2453730671953883
[2m[36m(func pid=57031)[0m f1_weighted: 0.2902264029762893
[2m[36m(func pid=57031)[0m f1_per_class: [0.294, 0.415, 0.0, 0.311, 0.084, 0.274, 0.207, 0.396, 0.179, 0.294]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.1208 | Steps: 4 | Val loss: 3.1431 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=50960)[0m top1: 0.47294776119402987
[2m[36m(func pid=50960)[0m top5: 0.9244402985074627
[2m[36m(func pid=50960)[0m f1_micro: 0.47294776119402987
[2m[36m(func pid=50960)[0m f1_macro: 0.42182197219540785
[2m[36m(func pid=50960)[0m f1_weighted: 0.49534743900557604
[2m[36m(func pid=50960)[0m f1_per_class: [0.519, 0.525, 0.421, 0.514, 0.103, 0.402, 0.541, 0.392, 0.406, 0.396]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7855 | Steps: 4 | Val loss: 4.9427 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.8773 | Steps: 4 | Val loss: 2.3571 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=62931)[0m top1: 0.20009328358208955
[2m[36m(func pid=62931)[0m top5: 0.5648320895522388
[2m[36m(func pid=62931)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=62931)[0m f1_macro: 0.06630350258895434
[2m[36m(func pid=62931)[0m f1_weighted: 0.16034977476656967
[2m[36m(func pid=62931)[0m f1_per_class: [0.053, 0.0, 0.045, 0.0, 0.0, 0.051, 0.514, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.4441 | Steps: 4 | Val loss: 1.5610 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=57031)[0m top1: 0.271455223880597
[2m[36m(func pid=57031)[0m top5: 0.8950559701492538
[2m[36m(func pid=57031)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=57031)[0m f1_macro: 0.24030246970669156
[2m[36m(func pid=57031)[0m f1_weighted: 0.25507186777235863
[2m[36m(func pid=57031)[0m f1_per_class: [0.293, 0.375, 0.0, 0.356, 0.084, 0.297, 0.059, 0.392, 0.197, 0.351]
[2m[36m(func pid=57031)[0m 
== Status ==
Current time: 2024-01-07 14:57:06 (running for 00:23:31.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.676 |      0.422 |                   94 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.511 |      0.234 |                   69 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.877 |      0.24  |                   70 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.121 |      0.066 |                   45 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=56441)[0m top1: 0.30597014925373134
[2m[36m(func pid=56441)[0m top5: 0.8036380597014925
[2m[36m(func pid=56441)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=56441)[0m f1_macro: 0.2824685869048453
[2m[36m(func pid=56441)[0m f1_weighted: 0.32208465712510864
[2m[36m(func pid=56441)[0m f1_per_class: [0.228, 0.362, 0.533, 0.417, 0.118, 0.349, 0.25, 0.217, 0.169, 0.182]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.0586 | Steps: 4 | Val loss: 2.6153 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=50960)[0m top1: 0.458955223880597
[2m[36m(func pid=50960)[0m top5: 0.9239738805970149
[2m[36m(func pid=50960)[0m f1_micro: 0.458955223880597
[2m[36m(func pid=50960)[0m f1_macro: 0.4020731055626519
[2m[36m(func pid=50960)[0m f1_weighted: 0.4827435267693016
[2m[36m(func pid=50960)[0m f1_per_class: [0.482, 0.52, 0.364, 0.495, 0.108, 0.381, 0.53, 0.4, 0.418, 0.323]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7888 | Steps: 4 | Val loss: 4.2618 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3954 | Steps: 4 | Val loss: 2.3583 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=62931)[0m top1: 0.21875
[2m[36m(func pid=62931)[0m top5: 0.6711753731343284
[2m[36m(func pid=62931)[0m f1_micro: 0.21875
[2m[36m(func pid=62931)[0m f1_macro: 0.07222266581972972
[2m[36m(func pid=62931)[0m f1_weighted: 0.1676669166275259
[2m[36m(func pid=62931)[0m f1_per_class: [0.052, 0.0, 0.027, 0.0, 0.0, 0.05, 0.533, 0.0, 0.061, 0.0]
[2m[36m(func pid=62931)[0m 
== Status ==
Current time: 2024-01-07 14:57:11 (running for 00:23:37.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.444 |      0.402 |                   95 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.785 |      0.282 |                   70 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.395 |      0.23  |                   71 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.059 |      0.072 |                   46 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.6609 | Steps: 4 | Val loss: 1.5427 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=56441)[0m top1: 0.30597014925373134
[2m[36m(func pid=56441)[0m top5: 0.8488805970149254
[2m[36m(func pid=56441)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=56441)[0m f1_macro: 0.2824403284838818
[2m[36m(func pid=56441)[0m f1_weighted: 0.3272818437743542
[2m[36m(func pid=56441)[0m f1_per_class: [0.168, 0.321, 0.353, 0.368, 0.121, 0.347, 0.3, 0.405, 0.243, 0.198]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=57031)[0m top1: 0.27005597014925375
[2m[36m(func pid=57031)[0m top5: 0.8955223880597015
[2m[36m(func pid=57031)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=57031)[0m f1_macro: 0.23016891361430117
[2m[36m(func pid=57031)[0m f1_weighted: 0.25887408519068084
[2m[36m(func pid=57031)[0m f1_per_class: [0.225, 0.366, 0.0, 0.398, 0.084, 0.279, 0.045, 0.442, 0.163, 0.3]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.1858 | Steps: 4 | Val loss: 2.2256 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=50960)[0m top1: 0.4608208955223881
[2m[36m(func pid=50960)[0m top5: 0.9286380597014925
[2m[36m(func pid=50960)[0m f1_micro: 0.4608208955223881
[2m[36m(func pid=50960)[0m f1_macro: 0.4082152241201772
[2m[36m(func pid=50960)[0m f1_weighted: 0.4825494983224265
[2m[36m(func pid=50960)[0m f1_per_class: [0.496, 0.527, 0.387, 0.496, 0.115, 0.375, 0.52, 0.423, 0.427, 0.316]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4063 | Steps: 4 | Val loss: 2.3871 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7211 | Steps: 4 | Val loss: 4.7320 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=62931)[0m top1: 0.251865671641791
[2m[36m(func pid=62931)[0m top5: 0.8134328358208955
[2m[36m(func pid=62931)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=62931)[0m f1_macro: 0.09390692562168634
[2m[36m(func pid=62931)[0m f1_weighted: 0.16870682452890573
[2m[36m(func pid=62931)[0m f1_per_class: [0.046, 0.0, 0.29, 0.01, 0.0, 0.031, 0.529, 0.032, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
== Status ==
Current time: 2024-01-07 14:57:17 (running for 00:23:42.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.661 |      0.408 |                   96 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.789 |      0.282 |                   71 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.406 |      0.31  |                   72 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.186 |      0.094 |                   47 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.6350 | Steps: 4 | Val loss: 1.5633 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=57031)[0m top1: 0.2681902985074627
[2m[36m(func pid=57031)[0m top5: 0.8931902985074627
[2m[36m(func pid=57031)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=57031)[0m f1_macro: 0.30972424072775995
[2m[36m(func pid=57031)[0m f1_weighted: 0.2686957340993998
[2m[36m(func pid=57031)[0m f1_per_class: [0.247, 0.359, 0.833, 0.415, 0.075, 0.242, 0.059, 0.476, 0.14, 0.25]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.27798507462686567
[2m[36m(func pid=56441)[0m top5: 0.8227611940298507
[2m[36m(func pid=56441)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=56441)[0m f1_macro: 0.2728535039507148
[2m[36m(func pid=56441)[0m f1_weighted: 0.2976894887003335
[2m[36m(func pid=56441)[0m f1_per_class: [0.092, 0.329, 0.25, 0.214, 0.125, 0.412, 0.305, 0.435, 0.329, 0.236]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 3.1379 | Steps: 4 | Val loss: 2.8934 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=50960)[0m top1: 0.45755597014925375
[2m[36m(func pid=50960)[0m top5: 0.9370335820895522
[2m[36m(func pid=50960)[0m f1_micro: 0.45755597014925375
[2m[36m(func pid=50960)[0m f1_macro: 0.4185197577866976
[2m[36m(func pid=50960)[0m f1_weighted: 0.48088122194960725
[2m[36m(func pid=50960)[0m f1_per_class: [0.555, 0.52, 0.407, 0.509, 0.11, 0.394, 0.489, 0.453, 0.421, 0.328]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.4430 | Steps: 4 | Val loss: 2.5398 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8032 | Steps: 4 | Val loss: 5.5173 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=62931)[0m top1: 0.04664179104477612
[2m[36m(func pid=62931)[0m top5: 0.7798507462686567
[2m[36m(func pid=62931)[0m f1_micro: 0.04664179104477612
[2m[36m(func pid=62931)[0m f1_macro: 0.05155581311644741
[2m[36m(func pid=62931)[0m f1_weighted: 0.04080468060697177
[2m[36m(func pid=62931)[0m f1_per_class: [0.061, 0.0, 0.22, 0.11, 0.0, 0.0, 0.012, 0.026, 0.07, 0.017]
[2m[36m(func pid=62931)[0m 
== Status ==
Current time: 2024-01-07 14:57:22 (running for 00:23:48.05)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.635 |      0.419 |                   97 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.721 |      0.273 |                   72 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.443 |      0.281 |                   73 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.138 |      0.052 |                   48 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.269589552238806
[2m[36m(func pid=57031)[0m top5: 0.8931902985074627
[2m[36m(func pid=57031)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=57031)[0m f1_macro: 0.28132510448285675
[2m[36m(func pid=57031)[0m f1_weighted: 0.29276010671655955
[2m[36m(func pid=57031)[0m f1_per_class: [0.219, 0.31, 0.579, 0.439, 0.072, 0.246, 0.159, 0.471, 0.093, 0.225]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=56441)[0m top1: 0.24300373134328357
[2m[36m(func pid=56441)[0m top5: 0.7751865671641791
[2m[36m(func pid=56441)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=56441)[0m f1_macro: 0.23466861856167925
[2m[36m(func pid=56441)[0m f1_weighted: 0.26435639092160246
[2m[36m(func pid=56441)[0m f1_per_class: [0.095, 0.345, 0.164, 0.11, 0.082, 0.352, 0.313, 0.435, 0.311, 0.138]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.7932 | Steps: 4 | Val loss: 1.5377 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7259 | Steps: 4 | Val loss: 3.9868 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=50960)[0m top1: 0.44869402985074625
[2m[36m(func pid=50960)[0m top5: 0.9388992537313433
[2m[36m(func pid=50960)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=50960)[0m f1_macro: 0.4132509138761907
[2m[36m(func pid=50960)[0m f1_weighted: 0.46907448007680014
[2m[36m(func pid=50960)[0m f1_per_class: [0.529, 0.51, 0.429, 0.519, 0.126, 0.399, 0.455, 0.406, 0.405, 0.356]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.7208 | Steps: 4 | Val loss: 2.2440 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3307 | Steps: 4 | Val loss: 6.2073 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=62931)[0m top1: 0.027985074626865673
[2m[36m(func pid=62931)[0m top5: 0.539179104477612
[2m[36m(func pid=62931)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=62931)[0m f1_macro: 0.03566486430853379
[2m[36m(func pid=62931)[0m f1_weighted: 0.004145342862306615
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.016]
[2m[36m(func pid=62931)[0m 
== Status ==
Current time: 2024-01-07 14:57:27 (running for 00:23:53.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.793 |      0.413 |                   98 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.803 |      0.235 |                   73 |
| train_5ae7f_00010 | RUNNING    | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  1.721 |      0.324 |                   74 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.726 |      0.036 |                   49 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.341884328358209
[2m[36m(func pid=57031)[0m top5: 0.8978544776119403
[2m[36m(func pid=57031)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=57031)[0m f1_macro: 0.3239017167368752
[2m[36m(func pid=57031)[0m f1_weighted: 0.378987455080916
[2m[36m(func pid=57031)[0m f1_per_class: [0.22, 0.285, 0.556, 0.483, 0.074, 0.31, 0.389, 0.478, 0.127, 0.319]
[2m[36m(func pid=57031)[0m 
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.5905 | Steps: 4 | Val loss: 1.5534 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=56441)[0m top1: 0.22434701492537312
[2m[36m(func pid=56441)[0m top5: 0.7360074626865671
[2m[36m(func pid=56441)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=56441)[0m f1_macro: 0.214699585149134
[2m[36m(func pid=56441)[0m f1_weighted: 0.24461779418339435
[2m[36m(func pid=56441)[0m f1_per_class: [0.097, 0.334, 0.143, 0.071, 0.074, 0.311, 0.326, 0.349, 0.274, 0.168]
[2m[36m(func pid=56441)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.8455 | Steps: 4 | Val loss: 3.9611 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=50960)[0m top1: 0.44822761194029853
[2m[36m(func pid=50960)[0m top5: 0.9370335820895522
[2m[36m(func pid=50960)[0m f1_micro: 0.44822761194029853
[2m[36m(func pid=50960)[0m f1_macro: 0.4057621981277946
[2m[36m(func pid=50960)[0m f1_weighted: 0.46924796303169
[2m[36m(func pid=50960)[0m f1_per_class: [0.504, 0.526, 0.387, 0.505, 0.122, 0.392, 0.463, 0.43, 0.385, 0.344]
[2m[36m(func pid=50960)[0m 
[2m[36m(func pid=57031)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1759 | Steps: 4 | Val loss: 1.9869 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=56441)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8805 | Steps: 4 | Val loss: 5.8230 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=62931)[0m top1: 0.027052238805970148
[2m[36m(func pid=62931)[0m top5: 0.5289179104477612
[2m[36m(func pid=62931)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=62931)[0m f1_macro: 0.026418879771514896
[2m[36m(func pid=62931)[0m f1_weighted: 0.0035684565977622067
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.069, 0.016]
[2m[36m(func pid=62931)[0m 
== Status ==
Current time: 2024-01-07 14:57:33 (running for 00:23:58.76)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3695
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 3 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00008 | RUNNING    | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.59  |      0.406 |                   99 |
| train_5ae7f_00009 | RUNNING    | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.331 |      0.215 |                   74 |
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.845 |      0.026 |                   50 |
| train_5ae7f_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING)


[2m[36m(func pid=57031)[0m top1: 0.416044776119403
[2m[36m(func pid=57031)[0m top5: 0.9020522388059702
[2m[36m(func pid=57031)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=57031)[0m f1_macro: 0.34710904558803823
[2m[36m(func pid=57031)[0m f1_weighted: 0.44412382803598627
[2m[36m(func pid=57031)[0m f1_per_class: [0.242, 0.322, 0.524, 0.505, 0.083, 0.352, 0.552, 0.463, 0.12, 0.308]
[2m[36m(func pid=50960)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.5978 | Steps: 4 | Val loss: 1.6278 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=56441)[0m top1: 0.2868470149253731
[2m[36m(func pid=56441)[0m top5: 0.742070895522388
[2m[36m(func pid=56441)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=56441)[0m f1_macro: 0.25070564415266877
[2m[36m(func pid=56441)[0m f1_weighted: 0.31719078767650183
[2m[36m(func pid=56441)[0m f1_per_class: [0.113, 0.366, 0.185, 0.094, 0.066, 0.312, 0.525, 0.347, 0.286, 0.214]
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.1289 | Steps: 4 | Val loss: 3.0022 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=50960)[0m top1: 0.43050373134328357
[2m[36m(func pid=50960)[0m top5: 0.9277052238805971
[2m[36m(func pid=50960)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=50960)[0m f1_macro: 0.3955504676236192
[2m[36m(func pid=50960)[0m f1_weighted: 0.4474220682599967
[2m[36m(func pid=50960)[0m f1_per_class: [0.535, 0.53, 0.343, 0.488, 0.114, 0.382, 0.408, 0.413, 0.392, 0.351]
[2m[36m(func pid=62931)[0m top1: 0.060167910447761194
[2m[36m(func pid=62931)[0m top5: 0.6851679104477612
[2m[36m(func pid=62931)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=62931)[0m f1_macro: 0.051409790185935486
[2m[36m(func pid=62931)[0m f1_weighted: 0.06796571232292113
[2m[36m(func pid=62931)[0m f1_per_class: [0.037, 0.063, 0.131, 0.189, 0.0, 0.0, 0.0, 0.0, 0.073, 0.02]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6892 | Steps: 4 | Val loss: 2.5670 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=75166)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75166)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=75166)[0m Configuration completed!
[2m[36m(func pid=75166)[0m New optimizer parameters:
[2m[36m(func pid=75166)[0m SGD (
[2m[36m(func pid=75166)[0m Parameter Group 0
[2m[36m(func pid=75166)[0m     dampening: 0
[2m[36m(func pid=75166)[0m     differentiable: False
[2m[36m(func pid=75166)[0m     foreach: None
[2m[36m(func pid=75166)[0m     lr: 0.0001
[2m[36m(func pid=75166)[0m     maximize: False
[2m[36m(func pid=75166)[0m     momentum: 0.9
[2m[36m(func pid=75166)[0m     nesterov: False
[2m[36m(func pid=75166)[0m     weight_decay: 0.0001
[2m[36m(func pid=75166)[0m )
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m top1: 0.034981343283582086
[2m[36m(func pid=62931)[0m top5: 0.7056902985074627
[2m[36m(func pid=62931)[0m f1_micro: 0.034981343283582086
[2m[36m(func pid=62931)[0m f1_macro: 0.023526751667012992
[2m[36m(func pid=62931)[0m f1_weighted: 0.032272049349728216
[2m[36m(func pid=62931)[0m f1_per_class: [0.07, 0.035, 0.0, 0.085, 0.0, 0.0, 0.0, 0.0, 0.023, 0.022]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75223)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75223)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=75223)[0m Configuration completed!
[2m[36m(func pid=75223)[0m New optimizer parameters:
[2m[36m(func pid=75223)[0m SGD (
[2m[36m(func pid=75223)[0m Parameter Group 0
[2m[36m(func pid=75223)[0m     dampening: 0
[2m[36m(func pid=75223)[0m     differentiable: False
[2m[36m(func pid=75223)[0m     foreach: None
[2m[36m(func pid=75223)[0m     lr: 0.001
[2m[36m(func pid=75223)[0m     maximize: False
[2m[36m(func pid=75223)[0m     momentum: 0.9
[2m[36m(func pid=75223)[0m     nesterov: False
[2m[36m(func pid=75223)[0m     weight_decay: 0.0001
[2m[36m(func pid=75223)[0m )
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.9624 | Steps: 4 | Val loss: 2.9531 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 14:57:42 (running for 00:24:07.90)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.689 |      0.024 |                   52 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75300)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=75300)[0m Configuration completed!
[2m[36m(func pid=75300)[0m New optimizer parameters:
[2m[36m(func pid=75300)[0m SGD (
[2m[36m(func pid=75300)[0m Parameter Group 0
[2m[36m(func pid=75300)[0m     dampening: 0
[2m[36m(func pid=75300)[0m     differentiable: False
[2m[36m(func pid=75300)[0m     foreach: None
[2m[36m(func pid=75300)[0m     lr: 0.01
[2m[36m(func pid=75300)[0m     maximize: False
[2m[36m(func pid=75300)[0m     momentum: 0.9
[2m[36m(func pid=75300)[0m     nesterov: False
[2m[36m(func pid=75300)[0m     weight_decay: 0.0001
[2m[36m(func pid=75300)[0m )
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9538 | Steps: 4 | Val loss: 2.3441 | Batch size: 32 | lr: 0.0001 | Duration: 4.80s
== Status ==
Current time: 2024-01-07 14:57:47 (running for 00:24:13.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.962 |      0.043 |                   53 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.06949626865671642
[2m[36m(func pid=62931)[0m top5: 0.457089552238806
[2m[36m(func pid=62931)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=62931)[0m f1_macro: 0.042916657685257514
[2m[36m(func pid=62931)[0m f1_weighted: 0.013307556095824406
[2m[36m(func pid=62931)[0m f1_per_class: [0.062, 0.0, 0.157, 0.0, 0.0, 0.0, 0.0, 0.167, 0.044, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9739 | Steps: 4 | Val loss: 2.4165 | Batch size: 32 | lr: 0.001 | Duration: 4.97s
[2m[36m(func pid=75166)[0m top1: 0.17117537313432835
[2m[36m(func pid=75166)[0m top5: 0.5083955223880597
[2m[36m(func pid=75166)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=75166)[0m f1_macro: 0.1060965329602449
[2m[36m(func pid=75166)[0m f1_weighted: 0.1217182918326275
[2m[36m(func pid=75166)[0m f1_per_class: [0.219, 0.313, 0.0, 0.09, 0.011, 0.272, 0.012, 0.041, 0.0, 0.103]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.8147 | Steps: 4 | Val loss: 3.1601 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9282 | Steps: 4 | Val loss: 2.4017 | Batch size: 32 | lr: 0.01 | Duration: 4.94s
[2m[36m(func pid=75223)[0m top1: 0.11473880597014925
[2m[36m(func pid=75223)[0m top5: 0.435634328358209
[2m[36m(func pid=75223)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=75223)[0m f1_macro: 0.06690295407729188
[2m[36m(func pid=75223)[0m f1_weighted: 0.08305137394648122
[2m[36m(func pid=75223)[0m f1_per_class: [0.104, 0.175, 0.0, 0.07, 0.0, 0.248, 0.003, 0.022, 0.012, 0.036]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9796 | Steps: 4 | Val loss: 2.3993 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 14:57:53 (running for 00:24:18.95)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.815 |      0.049 |                   54 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.954 |      0.106 |                    1 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.974 |      0.067 |                    1 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.07462686567164178
[2m[36m(func pid=62931)[0m top5: 0.527518656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=62931)[0m f1_macro: 0.0487699978231523
[2m[36m(func pid=62931)[0m f1_weighted: 0.014158995398768541
[2m[36m(func pid=62931)[0m f1_per_class: [0.062, 0.0, 0.226, 0.0, 0.0, 0.0, 0.0, 0.199, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.10307835820895522
[2m[36m(func pid=75300)[0m top5: 0.4762126865671642
[2m[36m(func pid=75300)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=75300)[0m f1_macro: 0.11276618565431043
[2m[36m(func pid=75300)[0m f1_weighted: 0.07884106470566568
[2m[36m(func pid=75300)[0m f1_per_class: [0.107, 0.133, 0.293, 0.051, 0.057, 0.177, 0.012, 0.208, 0.027, 0.063]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.13759328358208955
[2m[36m(func pid=75166)[0m top5: 0.4710820895522388
[2m[36m(func pid=75166)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=75166)[0m f1_macro: 0.07025330940582565
[2m[36m(func pid=75166)[0m f1_weighted: 0.09879007339237658
[2m[36m(func pid=75166)[0m f1_per_class: [0.075, 0.227, 0.0, 0.084, 0.0, 0.27, 0.006, 0.027, 0.012, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9335 | Steps: 4 | Val loss: 2.3891 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.8313 | Steps: 4 | Val loss: 3.0050 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6253 | Steps: 4 | Val loss: 2.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9601 | Steps: 4 | Val loss: 2.3720 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=75223)[0m top1: 0.11147388059701492
[2m[36m(func pid=75223)[0m top5: 0.48274253731343286
[2m[36m(func pid=75223)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=75223)[0m f1_macro: 0.06986762881176088
[2m[36m(func pid=75223)[0m f1_weighted: 0.08467927754162217
[2m[36m(func pid=75223)[0m f1_per_class: [0.076, 0.166, 0.012, 0.07, 0.018, 0.257, 0.006, 0.049, 0.022, 0.024]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 14:57:58 (running for 00:24:24.12)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.831 |      0.047 |                   55 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.98  |      0.07  |                    2 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.934 |      0.07  |                    2 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  2.928 |      0.113 |                    1 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.07509328358208955
[2m[36m(func pid=62931)[0m top5: 0.7728544776119403
[2m[36m(func pid=62931)[0m f1_micro: 0.07509328358208955
[2m[36m(func pid=62931)[0m f1_macro: 0.047220462706820475
[2m[36m(func pid=62931)[0m f1_weighted: 0.025957660670818627
[2m[36m(func pid=62931)[0m f1_per_class: [0.052, 0.046, 0.103, 0.0, 0.029, 0.0, 0.009, 0.234, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.16884328358208955
[2m[36m(func pid=75300)[0m top5: 0.597481343283582
[2m[36m(func pid=75300)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=75300)[0m f1_macro: 0.1009033593290285
[2m[36m(func pid=75300)[0m f1_weighted: 0.1902271627476926
[2m[36m(func pid=75300)[0m f1_per_class: [0.084, 0.116, 0.048, 0.147, 0.1, 0.016, 0.415, 0.0, 0.0, 0.082]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.14458955223880596
[2m[36m(func pid=75166)[0m top5: 0.5004664179104478
[2m[36m(func pid=75166)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=75166)[0m f1_macro: 0.07592371511188119
[2m[36m(func pid=75166)[0m f1_weighted: 0.11782912461299161
[2m[36m(func pid=75166)[0m f1_per_class: [0.065, 0.2, 0.0, 0.103, 0.0, 0.269, 0.066, 0.035, 0.021, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8709 | Steps: 4 | Val loss: 2.3398 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.9698 | Steps: 4 | Val loss: 2.8436 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.1919 | Steps: 4 | Val loss: 2.0024 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9383 | Steps: 4 | Val loss: 2.3576 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=75223)[0m top1: 0.13852611940298507
[2m[36m(func pid=75223)[0m top5: 0.5382462686567164
[2m[36m(func pid=75223)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=75223)[0m f1_macro: 0.101102807245666
[2m[36m(func pid=75223)[0m f1_weighted: 0.13159247116002504
[2m[36m(func pid=75223)[0m f1_per_class: [0.113, 0.175, 0.056, 0.096, 0.048, 0.29, 0.107, 0.103, 0.024, 0.0]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 14:58:03 (running for 00:24:29.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.97  |      0.021 |                   56 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.96  |      0.076 |                    3 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.871 |      0.101 |                    3 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  2.625 |      0.101 |                    2 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.0457089552238806
[2m[36m(func pid=62931)[0m top5: 0.777518656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.0457089552238806
[2m[36m(func pid=62931)[0m f1_macro: 0.021133608198128706
[2m[36m(func pid=62931)[0m f1_weighted: 0.030084106690737033
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.173, 0.026, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.35027985074626866
[2m[36m(func pid=75300)[0m top5: 0.7458022388059702
[2m[36m(func pid=75300)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=75300)[0m f1_macro: 0.2330376635017815
[2m[36m(func pid=75300)[0m f1_weighted: 0.31306746629216303
[2m[36m(func pid=75300)[0m f1_per_class: [0.149, 0.027, 0.353, 0.51, 0.298, 0.008, 0.47, 0.223, 0.069, 0.224]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.1515858208955224
[2m[36m(func pid=75166)[0m top5: 0.5181902985074627
[2m[36m(func pid=75166)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=75166)[0m f1_macro: 0.08421849283756813
[2m[36m(func pid=75166)[0m f1_weighted: 0.1358893363793756
[2m[36m(func pid=75166)[0m f1_per_class: [0.045, 0.201, 0.0, 0.112, 0.022, 0.26, 0.116, 0.066, 0.02, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7242 | Steps: 4 | Val loss: 2.2919 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.9349 | Steps: 4 | Val loss: 2.8723 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0575 | Steps: 4 | Val loss: 1.8618 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9169 | Steps: 4 | Val loss: 2.3314 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:58:09 (running for 00:24:34.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.97  |      0.021 |                   56 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.938 |      0.084 |                    4 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.724 |      0.129 |                    4 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  2.192 |      0.233 |                    3 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.1571828358208955
[2m[36m(func pid=75223)[0m top5: 0.6077425373134329
[2m[36m(func pid=75223)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=75223)[0m f1_macro: 0.129479812605057
[2m[36m(func pid=75223)[0m f1_weighted: 0.1669296995892545
[2m[36m(func pid=75223)[0m f1_per_class: [0.116, 0.178, 0.143, 0.158, 0.06, 0.309, 0.143, 0.175, 0.012, 0.0]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=62931)[0m top1: 0.010261194029850746
[2m[36m(func pid=62931)[0m top5: 0.7504664179104478
[2m[36m(func pid=62931)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=62931)[0m f1_macro: 0.007894875261707105
[2m[36m(func pid=62931)[0m f1_weighted: 0.0016742728567817123
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.023, 0.0, 0.0, 0.0, 0.043, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.31343283582089554
[2m[36m(func pid=75300)[0m top5: 0.8218283582089553
[2m[36m(func pid=75300)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=75300)[0m f1_macro: 0.23437605534247868
[2m[36m(func pid=75300)[0m f1_weighted: 0.21045363330328137
[2m[36m(func pid=75300)[0m f1_per_class: [0.235, 0.078, 0.471, 0.565, 0.278, 0.047, 0.0, 0.303, 0.12, 0.248]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.1525186567164179
[2m[36m(func pid=75166)[0m top5: 0.539179104477612
[2m[36m(func pid=75166)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=75166)[0m f1_macro: 0.0832690954358343
[2m[36m(func pid=75166)[0m f1_weighted: 0.13584067569744518
[2m[36m(func pid=75166)[0m f1_per_class: [0.033, 0.202, 0.0, 0.124, 0.009, 0.27, 0.101, 0.073, 0.021, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6926 | Steps: 4 | Val loss: 2.6614 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6462 | Steps: 4 | Val loss: 2.2362 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.6679 | Steps: 4 | Val loss: 1.8640 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9418 | Steps: 4 | Val loss: 2.3195 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=62931)[0m top1: 0.052705223880597014
[2m[36m(func pid=62931)[0m top5: 0.7425373134328358
[2m[36m(func pid=62931)[0m f1_micro: 0.05270522388059702
[2m[36m(func pid=62931)[0m f1_macro: 0.034694782334699505
[2m[36m(func pid=62931)[0m f1_weighted: 0.06509325779956274
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.148, 0.034, 0.14, 0.024, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 14:58:14 (running for 00:24:40.56)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.693 |      0.035 |                   58 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.917 |      0.083 |                    5 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.724 |      0.129 |                    4 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  2.057 |      0.234 |                    4 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75223)[0m top1: 0.1791044776119403
[2m[36m(func pid=75223)[0m top5: 0.6595149253731343
[2m[36m(func pid=75223)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=75223)[0m f1_macro: 0.16062438827727027
[2m[36m(func pid=75223)[0m f1_weighted: 0.18944065883119912
[2m[36m(func pid=75223)[0m f1_per_class: [0.128, 0.189, 0.216, 0.249, 0.068, 0.313, 0.101, 0.264, 0.039, 0.04]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m top1: 0.2989738805970149
[2m[36m(func pid=75300)[0m top5: 0.8227611940298507
[2m[36m(func pid=75300)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=75300)[0m f1_macro: 0.3113160782903516
[2m[36m(func pid=75300)[0m f1_weighted: 0.2419698406791999
[2m[36m(func pid=75300)[0m f1_per_class: [0.337, 0.447, 0.632, 0.275, 0.121, 0.377, 0.006, 0.349, 0.236, 0.333]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.15298507462686567
[2m[36m(func pid=75166)[0m top5: 0.5541044776119403
[2m[36m(func pid=75166)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=75166)[0m f1_macro: 0.09241469989550488
[2m[36m(func pid=75166)[0m f1_weighted: 0.1399606425557872
[2m[36m(func pid=75166)[0m f1_per_class: [0.039, 0.215, 0.043, 0.124, 0.023, 0.268, 0.103, 0.086, 0.023, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.8356 | Steps: 4 | Val loss: 2.6453 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.5584 | Steps: 4 | Val loss: 1.8764 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5605 | Steps: 4 | Val loss: 2.2025 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9247 | Steps: 4 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:58:20 (running for 00:24:46.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.836 |      0.049 |                   59 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.942 |      0.092 |                    6 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.646 |      0.161 |                    5 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.668 |      0.311 |                    5 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.07042910447761194
[2m[36m(func pid=62931)[0m top5: 0.7173507462686567
[2m[36m(func pid=62931)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=62931)[0m f1_macro: 0.04930477387706724
[2m[36m(func pid=62931)[0m f1_weighted: 0.08439422118037955
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.185, 0.053, 0.178, 0.023, 0.008, 0.0, 0.0, 0.046, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.3605410447761194
[2m[36m(func pid=75300)[0m top5: 0.8218283582089553
[2m[36m(func pid=75300)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=75300)[0m f1_macro: 0.27138048937558223
[2m[36m(func pid=75300)[0m f1_weighted: 0.339539178421287
[2m[36m(func pid=75300)[0m f1_per_class: [0.261, 0.451, 0.089, 0.072, 0.227, 0.435, 0.569, 0.052, 0.249, 0.309]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.197294776119403
[2m[36m(func pid=75223)[0m top5: 0.6907649253731343
[2m[36m(func pid=75223)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=75223)[0m f1_macro: 0.1875332576129975
[2m[36m(func pid=75223)[0m f1_weighted: 0.20693911561192918
[2m[36m(func pid=75223)[0m f1_per_class: [0.136, 0.173, 0.361, 0.326, 0.062, 0.315, 0.085, 0.29, 0.056, 0.071]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.16511194029850745
[2m[36m(func pid=75166)[0m top5: 0.5718283582089553
[2m[36m(func pid=75166)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=75166)[0m f1_macro: 0.097566847287075
[2m[36m(func pid=75166)[0m f1_weighted: 0.15150515441574586
[2m[36m(func pid=75166)[0m f1_per_class: [0.028, 0.237, 0.073, 0.159, 0.008, 0.281, 0.095, 0.071, 0.024, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.9039 | Steps: 4 | Val loss: 2.7060 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1943 | Steps: 4 | Val loss: 2.0472 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5091 | Steps: 4 | Val loss: 2.1628 | Batch size: 32 | lr: 0.001 | Duration: 3.33s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8950 | Steps: 4 | Val loss: 2.2978 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:58:25 (running for 00:24:51.40)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.904 |      0.126 |                   60 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.925 |      0.098 |                    7 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.56  |      0.188 |                    6 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.558 |      0.271 |                    6 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.20382462686567165
[2m[36m(func pid=62931)[0m top5: 0.5783582089552238
[2m[36m(func pid=62931)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=62931)[0m f1_macro: 0.1256259563626586
[2m[36m(func pid=62931)[0m f1_weighted: 0.17426051582624696
[2m[36m(func pid=62931)[0m f1_per_class: [0.163, 0.175, 0.102, 0.356, 0.051, 0.333, 0.0, 0.0, 0.077, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.28218283582089554
[2m[36m(func pid=75300)[0m top5: 0.7364738805970149
[2m[36m(func pid=75300)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=75300)[0m f1_macro: 0.23243366521929837
[2m[36m(func pid=75300)[0m f1_weighted: 0.25802637515218285
[2m[36m(func pid=75300)[0m f1_per_class: [0.525, 0.21, 0.105, 0.01, 0.104, 0.317, 0.497, 0.316, 0.08, 0.162]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.21875
[2m[36m(func pid=75223)[0m top5: 0.7280783582089553
[2m[36m(func pid=75223)[0m f1_micro: 0.21875
[2m[36m(func pid=75223)[0m f1_macro: 0.21408138791009237
[2m[36m(func pid=75223)[0m f1_weighted: 0.22669889866251486
[2m[36m(func pid=75223)[0m f1_per_class: [0.137, 0.17, 0.393, 0.362, 0.06, 0.37, 0.083, 0.335, 0.086, 0.145]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.1646455223880597
[2m[36m(func pid=75166)[0m top5: 0.5811567164179104
[2m[36m(func pid=75166)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=75166)[0m f1_macro: 0.09803607432133608
[2m[36m(func pid=75166)[0m f1_weighted: 0.152239401439098
[2m[36m(func pid=75166)[0m f1_per_class: [0.014, 0.241, 0.078, 0.17, 0.016, 0.273, 0.086, 0.088, 0.013, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6551 | Steps: 4 | Val loss: 2.7602 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9625 | Steps: 4 | Val loss: 2.0140 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3863 | Steps: 4 | Val loss: 2.1149 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8110 | Steps: 4 | Val loss: 2.2772 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:58:31 (running for 00:24:56.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.655 |      0.101 |                   61 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.895 |      0.098 |                    8 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.509 |      0.214 |                    7 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.194 |      0.232 |                    7 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.27238805970149255
[2m[36m(func pid=62931)[0m top5: 0.5699626865671642
[2m[36m(func pid=62931)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=62931)[0m f1_macro: 0.10111831947912749
[2m[36m(func pid=62931)[0m f1_weighted: 0.18435517323671385
[2m[36m(func pid=62931)[0m f1_per_class: [0.025, 0.0, 0.109, 0.52, 0.0, 0.324, 0.0, 0.0, 0.033, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.27472014925373134
[2m[36m(func pid=75300)[0m top5: 0.8292910447761194
[2m[36m(func pid=75300)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=75300)[0m f1_macro: 0.2961668221837935
[2m[36m(func pid=75300)[0m f1_weighted: 0.2691746737477987
[2m[36m(func pid=75300)[0m f1_per_class: [0.649, 0.156, 0.468, 0.084, 0.077, 0.351, 0.436, 0.412, 0.212, 0.118]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.23880597014925373
[2m[36m(func pid=75223)[0m top5: 0.7555970149253731
[2m[36m(func pid=75223)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=75223)[0m f1_macro: 0.23102129490282208
[2m[36m(func pid=75223)[0m f1_weighted: 0.24180911016436923
[2m[36m(func pid=75223)[0m f1_per_class: [0.147, 0.189, 0.458, 0.416, 0.071, 0.314, 0.093, 0.301, 0.1, 0.22]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.1837686567164179
[2m[36m(func pid=75166)[0m top5: 0.6086753731343284
[2m[36m(func pid=75166)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=75166)[0m f1_macro: 0.1152891538814919
[2m[36m(func pid=75166)[0m f1_weighted: 0.17064382074182632
[2m[36m(func pid=75166)[0m f1_per_class: [0.026, 0.271, 0.16, 0.194, 0.016, 0.299, 0.096, 0.089, 0.0, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.0870 | Steps: 4 | Val loss: 3.0860 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.2042 | Steps: 4 | Val loss: 2.2094 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3331 | Steps: 4 | Val loss: 2.0615 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8081 | Steps: 4 | Val loss: 2.2723 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 14:58:36 (running for 00:25:02.00)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.087 |      0.11  |                   62 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.811 |      0.115 |                    9 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.386 |      0.231 |                    8 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.963 |      0.296 |                    8 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.2658582089552239
[2m[36m(func pid=62931)[0m top5: 0.5139925373134329
[2m[36m(func pid=62931)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=62931)[0m f1_macro: 0.10987580212505658
[2m[36m(func pid=62931)[0m f1_weighted: 0.16772507326052605
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.0, 0.274, 0.539, 0.0, 0.008, 0.0, 0.232, 0.047, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.22388059701492538
[2m[36m(func pid=75300)[0m top5: 0.8759328358208955
[2m[36m(func pid=75300)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=75300)[0m f1_macro: 0.1608737457732296
[2m[36m(func pid=75300)[0m f1_weighted: 0.23583485253867767
[2m[36m(func pid=75300)[0m f1_per_class: [0.351, 0.27, 0.0, 0.104, 0.099, 0.152, 0.439, 0.0, 0.099, 0.096]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.18003731343283583
[2m[36m(func pid=75166)[0m top5: 0.6012126865671642
[2m[36m(func pid=75166)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=75166)[0m f1_macro: 0.11756445711156398
[2m[36m(func pid=75166)[0m f1_weighted: 0.17029562278685859
[2m[36m(func pid=75166)[0m f1_per_class: [0.037, 0.256, 0.175, 0.226, 0.016, 0.295, 0.075, 0.08, 0.014, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.27845149253731344
[2m[36m(func pid=75223)[0m top5: 0.7845149253731343
[2m[36m(func pid=75223)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=75223)[0m f1_macro: 0.2535593573190689
[2m[36m(func pid=75223)[0m f1_weighted: 0.2874145221543973
[2m[36m(func pid=75223)[0m f1_per_class: [0.192, 0.278, 0.386, 0.432, 0.077, 0.333, 0.167, 0.305, 0.12, 0.246]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1011 | Steps: 4 | Val loss: 2.1980 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.4329 | Steps: 4 | Val loss: 2.6147 | Batch size: 32 | lr: 0.1 | Duration: 3.35s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8397 | Steps: 4 | Val loss: 2.2658 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 14:58:42 (running for 00:25:07.59)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.087 |      0.11  |                   62 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.808 |      0.118 |                   10 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.333 |      0.254 |                    9 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.101 |      0.287 |                   10 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.28218283582089554
[2m[36m(func pid=75300)[0m top5: 0.8236940298507462
[2m[36m(func pid=75300)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=75300)[0m f1_macro: 0.2871803447219381
[2m[36m(func pid=75300)[0m f1_weighted: 0.2872636398408281
[2m[36m(func pid=75300)[0m f1_per_class: [0.511, 0.322, 0.529, 0.176, 0.083, 0.31, 0.379, 0.194, 0.211, 0.157]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.08395522388059702
[2m[36m(func pid=62931)[0m top5: 0.5606343283582089
[2m[36m(func pid=62931)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=62931)[0m f1_macro: 0.08584712445486119
[2m[36m(func pid=62931)[0m f1_weighted: 0.03502505555941738
[2m[36m(func pid=62931)[0m f1_per_class: [0.064, 0.0, 0.45, 0.059, 0.0, 0.0, 0.0, 0.206, 0.08, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3183 | Steps: 4 | Val loss: 2.0211 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=75166)[0m top1: 0.17817164179104478
[2m[36m(func pid=75166)[0m top5: 0.6138059701492538
[2m[36m(func pid=75166)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=75166)[0m f1_macro: 0.11503249669295505
[2m[36m(func pid=75166)[0m f1_weighted: 0.17111915053593788
[2m[36m(func pid=75166)[0m f1_per_class: [0.035, 0.254, 0.156, 0.227, 0.025, 0.29, 0.082, 0.082, 0.0, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.2943097014925373
[2m[36m(func pid=75223)[0m top5: 0.8027052238805971
[2m[36m(func pid=75223)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=75223)[0m f1_macro: 0.2635235539052806
[2m[36m(func pid=75223)[0m f1_weighted: 0.31184786162756184
[2m[36m(func pid=75223)[0m f1_per_class: [0.225, 0.296, 0.407, 0.41, 0.086, 0.336, 0.255, 0.329, 0.1, 0.191]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.9069 | Steps: 4 | Val loss: 2.8779 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3994 | Steps: 4 | Val loss: 2.5496 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 14:58:47 (running for 00:25:13.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.433 |      0.086 |                   63 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.84  |      0.115 |                   11 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.318 |      0.264 |                   10 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.907 |      0.144 |                   11 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=62931)[0m top1: 0.08115671641791045
[2m[36m(func pid=62931)[0m top5: 0.726679104477612
[2m[36m(func pid=62931)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=62931)[0m f1_macro: 0.049215717794322725
[2m[36m(func pid=62931)[0m f1_weighted: 0.04069885680299076
[2m[36m(func pid=62931)[0m f1_per_class: [0.047, 0.005, 0.064, 0.043, 0.0, 0.0, 0.038, 0.217, 0.078, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m top1: 0.12313432835820895
[2m[36m(func pid=75300)[0m top5: 0.7201492537313433
[2m[36m(func pid=75300)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=75300)[0m f1_macro: 0.1441740009309513
[2m[36m(func pid=75300)[0m f1_weighted: 0.11833418928703668
[2m[36m(func pid=75300)[0m f1_per_class: [0.0, 0.178, 0.024, 0.016, 0.088, 0.289, 0.077, 0.33, 0.107, 0.333]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8310 | Steps: 4 | Val loss: 2.2606 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2129 | Steps: 4 | Val loss: 1.9643 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=75166)[0m top1: 0.1865671641791045
[2m[36m(func pid=75166)[0m top5: 0.6222014925373134
[2m[36m(func pid=75166)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=75166)[0m f1_macro: 0.12887891434385373
[2m[36m(func pid=75166)[0m f1_weighted: 0.1757251730343049
[2m[36m(func pid=75166)[0m f1_per_class: [0.042, 0.257, 0.259, 0.23, 0.024, 0.307, 0.083, 0.086, 0.0, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7719 | Steps: 4 | Val loss: 2.5166 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6454 | Steps: 4 | Val loss: 2.4004 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=75223)[0m top1: 0.3460820895522388
[2m[36m(func pid=75223)[0m top5: 0.8246268656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=75223)[0m f1_macro: 0.3044986433742521
[2m[36m(func pid=75223)[0m f1_weighted: 0.35817023479363025
[2m[36m(func pid=75223)[0m f1_per_class: [0.309, 0.316, 0.511, 0.495, 0.088, 0.368, 0.29, 0.354, 0.13, 0.183]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 14:58:53 (running for 00:25:18.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.399 |      0.049 |                   64 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.831 |      0.129 |                   12 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.213 |      0.304 |                   11 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.772 |      0.213 |                   12 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2332089552238806
[2m[36m(func pid=75300)[0m top5: 0.8446828358208955
[2m[36m(func pid=75300)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=75300)[0m f1_macro: 0.2125325605307929
[2m[36m(func pid=75300)[0m f1_weighted: 0.2265127114223323
[2m[36m(func pid=75300)[0m f1_per_class: [0.242, 0.364, 0.056, 0.14, 0.131, 0.38, 0.163, 0.336, 0.158, 0.155]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8750 | Steps: 4 | Val loss: 2.2535 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=62931)[0m top1: 0.12126865671641791
[2m[36m(func pid=62931)[0m top5: 0.7644589552238806
[2m[36m(func pid=62931)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=62931)[0m f1_macro: 0.059273579797638684
[2m[36m(func pid=62931)[0m f1_weighted: 0.09755388586937878
[2m[36m(func pid=62931)[0m f1_per_class: [0.019, 0.011, 0.051, 0.07, 0.0, 0.0, 0.208, 0.234, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0709 | Steps: 4 | Val loss: 1.9130 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=75166)[0m top1: 0.18470149253731344
[2m[36m(func pid=75166)[0m top5: 0.6352611940298507
[2m[36m(func pid=75166)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=75166)[0m f1_macro: 0.13506678550155612
[2m[36m(func pid=75166)[0m f1_weighted: 0.17885933157293168
[2m[36m(func pid=75166)[0m f1_per_class: [0.048, 0.237, 0.286, 0.267, 0.029, 0.29, 0.074, 0.08, 0.014, 0.024]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5986 | Steps: 4 | Val loss: 2.9787 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.5025 | Steps: 4 | Val loss: 2.2315 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=75223)[0m top1: 0.36613805970149255
[2m[36m(func pid=75223)[0m top5: 0.8442164179104478
[2m[36m(func pid=75223)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=75223)[0m f1_macro: 0.3029925581822056
[2m[36m(func pid=75223)[0m f1_weighted: 0.36930310792270876
[2m[36m(func pid=75223)[0m f1_per_class: [0.357, 0.315, 0.49, 0.535, 0.119, 0.318, 0.315, 0.357, 0.059, 0.165]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 14:58:58 (running for 00:25:24.14)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.645 |      0.059 |                   65 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.875 |      0.135 |                   13 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.071 |      0.303 |                   12 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.599 |      0.202 |                   13 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.21361940298507462
[2m[36m(func pid=75300)[0m top5: 0.7807835820895522
[2m[36m(func pid=75300)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=75300)[0m f1_macro: 0.20165885989778407
[2m[36m(func pid=75300)[0m f1_weighted: 0.16814249404916073
[2m[36m(func pid=75300)[0m f1_per_class: [0.33, 0.427, 0.138, 0.1, 0.094, 0.325, 0.022, 0.097, 0.164, 0.319]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.22014925373134328
[2m[36m(func pid=62931)[0m top5: 0.8157649253731343
[2m[36m(func pid=62931)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=62931)[0m f1_macro: 0.10701499056137305
[2m[36m(func pid=62931)[0m f1_weighted: 0.19890327598875646
[2m[36m(func pid=62931)[0m f1_per_class: [0.081, 0.016, 0.103, 0.316, 0.0, 0.0, 0.307, 0.248, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8292 | Steps: 4 | Val loss: 2.2465 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0440 | Steps: 4 | Val loss: 1.8586 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=75166)[0m top1: 0.1935634328358209
[2m[36m(func pid=75166)[0m top5: 0.6324626865671642
[2m[36m(func pid=75166)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=75166)[0m f1_macro: 0.13434408052077695
[2m[36m(func pid=75166)[0m f1_weighted: 0.18709116189339597
[2m[36m(func pid=75166)[0m f1_per_class: [0.038, 0.255, 0.258, 0.284, 0.015, 0.306, 0.072, 0.079, 0.015, 0.022]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8707 | Steps: 4 | Val loss: 2.3547 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.4877 | Steps: 4 | Val loss: 2.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=75223)[0m top1: 0.37919776119402987
[2m[36m(func pid=75223)[0m top5: 0.8558768656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=75223)[0m f1_macro: 0.3112733002987441
[2m[36m(func pid=75223)[0m f1_weighted: 0.37973374049108616
[2m[36m(func pid=75223)[0m f1_per_class: [0.351, 0.288, 0.545, 0.539, 0.126, 0.302, 0.366, 0.349, 0.083, 0.163]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 14:59:03 (running for 00:25:29.40)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.502 |      0.107 |                   66 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.829 |      0.134 |                   14 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.044 |      0.311 |                   13 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.871 |      0.186 |                   14 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.25046641791044777
[2m[36m(func pid=75300)[0m top5: 0.7971082089552238
[2m[36m(func pid=75300)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=75300)[0m f1_macro: 0.18637429955068377
[2m[36m(func pid=75300)[0m f1_weighted: 0.2383059972865104
[2m[36m(func pid=75300)[0m f1_per_class: [0.255, 0.337, 0.029, 0.081, 0.174, 0.275, 0.369, 0.046, 0.159, 0.139]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.4025186567164179
[2m[36m(func pid=62931)[0m top5: 0.8199626865671642
[2m[36m(func pid=62931)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=62931)[0m f1_macro: 0.12383934163804382
[2m[36m(func pid=62931)[0m f1_weighted: 0.3170932691285151
[2m[36m(func pid=62931)[0m f1_per_class: [0.112, 0.088, 0.0, 0.517, 0.0, 0.0, 0.522, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7764 | Steps: 4 | Val loss: 2.2463 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.1444 | Steps: 4 | Val loss: 1.8347 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=75166)[0m top1: 0.197294776119403
[2m[36m(func pid=75166)[0m top5: 0.6343283582089553
[2m[36m(func pid=75166)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=75166)[0m f1_macro: 0.13782382729012366
[2m[36m(func pid=75166)[0m f1_weighted: 0.18823043351504457
[2m[36m(func pid=75166)[0m f1_per_class: [0.041, 0.255, 0.271, 0.264, 0.014, 0.324, 0.085, 0.087, 0.015, 0.022]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3262 | Steps: 4 | Val loss: 1.9000 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5595 | Steps: 4 | Val loss: 2.2025 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=75223)[0m top1: 0.39132462686567165
[2m[36m(func pid=75223)[0m top5: 0.8647388059701493
[2m[36m(func pid=75223)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=75223)[0m f1_macro: 0.31500655635320196
[2m[36m(func pid=75223)[0m f1_weighted: 0.39813783350539406
[2m[36m(func pid=75223)[0m f1_per_class: [0.365, 0.254, 0.545, 0.55, 0.122, 0.279, 0.442, 0.368, 0.083, 0.142]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 14:59:09 (running for 00:25:34.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.488 |      0.124 |                   67 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.776 |      0.138 |                   15 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  2.144 |      0.315 |                   14 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.326 |      0.239 |                   15 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.33955223880597013
[2m[36m(func pid=75300)[0m top5: 0.8875932835820896
[2m[36m(func pid=75300)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=75300)[0m f1_macro: 0.23892002787087954
[2m[36m(func pid=75300)[0m f1_weighted: 0.3667907295099241
[2m[36m(func pid=75300)[0m f1_per_class: [0.348, 0.322, 0.0, 0.345, 0.107, 0.219, 0.572, 0.09, 0.092, 0.294]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.41277985074626866
[2m[36m(func pid=62931)[0m top5: 0.7756529850746269
[2m[36m(func pid=62931)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=62931)[0m f1_macro: 0.15193879765425655
[2m[36m(func pid=62931)[0m f1_weighted: 0.3190559947688838
[2m[36m(func pid=62931)[0m f1_per_class: [0.0, 0.084, 0.387, 0.53, 0.0, 0.0, 0.518, 0.0, 0.0, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8398 | Steps: 4 | Val loss: 2.2322 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8629 | Steps: 4 | Val loss: 1.8343 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6530 | Steps: 4 | Val loss: 1.7295 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=75166)[0m top1: 0.20522388059701493
[2m[36m(func pid=75166)[0m top5: 0.6553171641791045
[2m[36m(func pid=75166)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=75166)[0m f1_macro: 0.14678802024159054
[2m[36m(func pid=75166)[0m f1_weighted: 0.1996050346712576
[2m[36m(func pid=75166)[0m f1_per_class: [0.062, 0.262, 0.291, 0.283, 0.016, 0.318, 0.099, 0.101, 0.016, 0.019]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.7990 | Steps: 4 | Val loss: 2.3860 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 14:59:14 (running for 00:25:39.84)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.559 |      0.152 |                   68 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.84  |      0.147 |                   16 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.863 |      0.301 |                   15 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.326 |      0.239 |                   15 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.36800373134328357
[2m[36m(func pid=75223)[0m top5: 0.863339552238806
[2m[36m(func pid=75223)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=75223)[0m f1_macro: 0.30135122750027443
[2m[36m(func pid=75223)[0m f1_weighted: 0.3770069189669694
[2m[36m(func pid=75223)[0m f1_per_class: [0.358, 0.252, 0.453, 0.536, 0.101, 0.274, 0.38, 0.403, 0.104, 0.152]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m top1: 0.447294776119403
[2m[36m(func pid=75300)[0m top5: 0.9160447761194029
[2m[36m(func pid=75300)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=75300)[0m f1_macro: 0.34094901522096793
[2m[36m(func pid=75300)[0m f1_weighted: 0.4488436530960541
[2m[36m(func pid=75300)[0m f1_per_class: [0.434, 0.38, 0.49, 0.47, 0.089, 0.295, 0.645, 0.062, 0.17, 0.373]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.3344216417910448
[2m[36m(func pid=62931)[0m top5: 0.7583955223880597
[2m[36m(func pid=62931)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=62931)[0m f1_macro: 0.14933387017573088
[2m[36m(func pid=62931)[0m f1_weighted: 0.29433044512019596
[2m[36m(func pid=62931)[0m f1_per_class: [0.049, 0.249, 0.267, 0.326, 0.0, 0.015, 0.516, 0.0, 0.071, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8115 | Steps: 4 | Val loss: 2.2258 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7348 | Steps: 4 | Val loss: 2.1376 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9732 | Steps: 4 | Val loss: 1.7952 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=75166)[0m top1: 0.2150186567164179
[2m[36m(func pid=75166)[0m top5: 0.6511194029850746
[2m[36m(func pid=75166)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=75166)[0m f1_macro: 0.15419307279034244
[2m[36m(func pid=75166)[0m f1_weighted: 0.21151047552372887
[2m[36m(func pid=75166)[0m f1_per_class: [0.067, 0.257, 0.31, 0.32, 0.023, 0.329, 0.104, 0.091, 0.015, 0.025]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6085 | Steps: 4 | Val loss: 2.5395 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 14:59:19 (running for 00:25:45.49)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.799 |      0.149 |                   69 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.811 |      0.154 |                   17 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.863 |      0.301 |                   15 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.735 |      0.293 |                   17 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.34841417910447764
[2m[36m(func pid=75300)[0m top5: 0.8642723880597015
[2m[36m(func pid=75300)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=75300)[0m f1_macro: 0.2930297114908248
[2m[36m(func pid=75300)[0m f1_weighted: 0.3697144806969305
[2m[36m(func pid=75300)[0m f1_per_class: [0.198, 0.22, 0.49, 0.448, 0.089, 0.197, 0.493, 0.396, 0.098, 0.301]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.34654850746268656
[2m[36m(func pid=75223)[0m top5: 0.8717350746268657
[2m[36m(func pid=75223)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=75223)[0m f1_macro: 0.29068734893410336
[2m[36m(func pid=75223)[0m f1_weighted: 0.34223546646912045
[2m[36m(func pid=75223)[0m f1_per_class: [0.408, 0.243, 0.381, 0.552, 0.091, 0.343, 0.231, 0.366, 0.114, 0.178]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7370 | Steps: 4 | Val loss: 2.2242 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=62931)[0m top1: 0.09281716417910447
[2m[36m(func pid=62931)[0m top5: 0.715018656716418
[2m[36m(func pid=62931)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=62931)[0m f1_macro: 0.09426366730423344
[2m[36m(func pid=62931)[0m f1_weighted: 0.043461616651682904
[2m[36m(func pid=62931)[0m f1_per_class: [0.094, 0.011, 0.468, 0.0, 0.0, 0.0, 0.072, 0.218, 0.08, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2998 | Steps: 4 | Val loss: 2.2029 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=75166)[0m top1: 0.21548507462686567
[2m[36m(func pid=75166)[0m top5: 0.6585820895522388
[2m[36m(func pid=75166)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=75166)[0m f1_macro: 0.16067713664624728
[2m[36m(func pid=75166)[0m f1_weighted: 0.21662661544642356
[2m[36m(func pid=75166)[0m f1_per_class: [0.103, 0.266, 0.31, 0.319, 0.03, 0.325, 0.113, 0.103, 0.012, 0.025]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9508 | Steps: 4 | Val loss: 1.7970 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6805 | Steps: 4 | Val loss: 2.7761 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 14:59:25 (running for 00:25:50.91)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.609 |      0.094 |                   70 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.737 |      0.161 |                   18 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.973 |      0.291 |                   16 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.3   |      0.29  |                   18 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3596082089552239
[2m[36m(func pid=75300)[0m top5: 0.8694029850746269
[2m[36m(func pid=75300)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=75300)[0m f1_macro: 0.28958799036987826
[2m[36m(func pid=75300)[0m f1_weighted: 0.3574054271691702
[2m[36m(func pid=75300)[0m f1_per_class: [0.4, 0.407, 0.156, 0.515, 0.168, 0.172, 0.288, 0.322, 0.164, 0.303]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.07975746268656717
[2m[36m(func pid=62931)[0m top5: 0.628731343283582
[2m[36m(func pid=62931)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=62931)[0m f1_macro: 0.07379437586592076
[2m[36m(func pid=62931)[0m f1_weighted: 0.023897084574631286
[2m[36m(func pid=62931)[0m f1_per_class: [0.087, 0.005, 0.333, 0.0, 0.0, 0.0, 0.012, 0.226, 0.075, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75223)[0m top1: 0.34328358208955223
[2m[36m(func pid=75223)[0m top5: 0.8736007462686567
[2m[36m(func pid=75223)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=75223)[0m f1_macro: 0.28582582426773734
[2m[36m(func pid=75223)[0m f1_weighted: 0.33855797036756474
[2m[36m(func pid=75223)[0m f1_per_class: [0.361, 0.316, 0.304, 0.541, 0.084, 0.376, 0.174, 0.376, 0.131, 0.194]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8045 | Steps: 4 | Val loss: 2.2267 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4869 | Steps: 4 | Val loss: 2.2339 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=75166)[0m top1: 0.2126865671641791
[2m[36m(func pid=75166)[0m top5: 0.6557835820895522
[2m[36m(func pid=75166)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=75166)[0m f1_macro: 0.16408249777835082
[2m[36m(func pid=75166)[0m f1_weighted: 0.2144839065067015
[2m[36m(func pid=75166)[0m f1_per_class: [0.122, 0.256, 0.327, 0.33, 0.04, 0.31, 0.101, 0.123, 0.033, 0.0]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6615 | Steps: 4 | Val loss: 2.6514 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8749 | Steps: 4 | Val loss: 1.7939 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 14:59:30 (running for 00:25:56.24)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.68  |      0.074 |                   71 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.804 |      0.164 |                   19 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.951 |      0.286 |                   17 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.487 |      0.301 |                   19 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.34654850746268656
[2m[36m(func pid=75300)[0m top5: 0.8577425373134329
[2m[36m(func pid=75300)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=75300)[0m f1_macro: 0.30125283639341505
[2m[36m(func pid=75300)[0m f1_weighted: 0.36430898606557566
[2m[36m(func pid=75300)[0m f1_per_class: [0.406, 0.447, 0.097, 0.311, 0.154, 0.288, 0.435, 0.297, 0.198, 0.381]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.14272388059701493
[2m[36m(func pid=62931)[0m top5: 0.6814365671641791
[2m[36m(func pid=62931)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=62931)[0m f1_macro: 0.09635870786346973
[2m[36m(func pid=62931)[0m f1_weighted: 0.1429022688976541
[2m[36m(func pid=62931)[0m f1_per_class: [0.051, 0.0, 0.125, 0.448, 0.028, 0.0, 0.0, 0.224, 0.088, 0.0]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7616 | Steps: 4 | Val loss: 2.2185 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=75223)[0m top1: 0.33115671641791045
[2m[36m(func pid=75223)[0m top5: 0.8675373134328358
[2m[36m(func pid=75223)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=75223)[0m f1_macro: 0.28358391020280566
[2m[36m(func pid=75223)[0m f1_weighted: 0.3191257833482383
[2m[36m(func pid=75223)[0m f1_per_class: [0.364, 0.33, 0.296, 0.526, 0.091, 0.396, 0.107, 0.357, 0.17, 0.199]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3497 | Steps: 4 | Val loss: 3.5762 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=75166)[0m top1: 0.2178171641791045
[2m[36m(func pid=75166)[0m top5: 0.667910447761194
[2m[36m(func pid=75166)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=75166)[0m f1_macro: 0.17154854998330496
[2m[36m(func pid=75166)[0m f1_weighted: 0.2219325089476378
[2m[36m(func pid=75166)[0m f1_per_class: [0.141, 0.265, 0.316, 0.335, 0.038, 0.31, 0.113, 0.127, 0.024, 0.048]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 3.0425 | Steps: 4 | Val loss: 2.5826 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7281 | Steps: 4 | Val loss: 1.8214 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 14:59:36 (running for 00:26:01.80)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.661 |      0.096 |                   72 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.762 |      0.172 |                   20 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.875 |      0.284 |                   18 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.35  |      0.13  |                   20 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.134794776119403
[2m[36m(func pid=75300)[0m top5: 0.5760261194029851
[2m[36m(func pid=75300)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=75300)[0m f1_macro: 0.13012535995324434
[2m[36m(func pid=75300)[0m f1_weighted: 0.12420724376570984
[2m[36m(func pid=75300)[0m f1_per_class: [0.194, 0.277, 0.045, 0.148, 0.2, 0.111, 0.022, 0.089, 0.127, 0.089]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.16044776119402984
[2m[36m(func pid=62931)[0m top5: 0.6982276119402985
[2m[36m(func pid=62931)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=62931)[0m f1_macro: 0.09577895809802696
[2m[36m(func pid=62931)[0m f1_weighted: 0.13453015289574483
[2m[36m(func pid=62931)[0m f1_per_class: [0.076, 0.0, 0.143, 0.416, 0.044, 0.0, 0.0, 0.268, 0.0, 0.011]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7505 | Steps: 4 | Val loss: 2.2136 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=75223)[0m top1: 0.32322761194029853
[2m[36m(func pid=75223)[0m top5: 0.8563432835820896
[2m[36m(func pid=75223)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=75223)[0m f1_macro: 0.279323496100337
[2m[36m(func pid=75223)[0m f1_weighted: 0.30995191471709554
[2m[36m(func pid=75223)[0m f1_per_class: [0.326, 0.376, 0.245, 0.486, 0.091, 0.434, 0.071, 0.37, 0.185, 0.209]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5449 | Steps: 4 | Val loss: 2.6494 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=75166)[0m top1: 0.22154850746268656
[2m[36m(func pid=75166)[0m top5: 0.6809701492537313
[2m[36m(func pid=75166)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=75166)[0m f1_macro: 0.17255038892531144
[2m[36m(func pid=75166)[0m f1_weighted: 0.22845021113250324
[2m[36m(func pid=75166)[0m f1_per_class: [0.153, 0.275, 0.323, 0.343, 0.035, 0.303, 0.125, 0.131, 0.013, 0.026]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6285 | Steps: 4 | Val loss: 2.5936 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.8955 | Steps: 4 | Val loss: 1.7818 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 14:59:41 (running for 00:26:07.16)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  3.043 |      0.096 |                   73 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.75  |      0.173 |                   21 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.728 |      0.279 |                   19 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.545 |      0.214 |                   21 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.261660447761194
[2m[36m(func pid=75300)[0m top5: 0.7430037313432836
[2m[36m(func pid=75300)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=75300)[0m f1_macro: 0.2140735478195231
[2m[36m(func pid=75300)[0m f1_weighted: 0.26975174959293813
[2m[36m(func pid=75300)[0m f1_per_class: [0.356, 0.342, 0.142, 0.442, 0.27, 0.129, 0.175, 0.105, 0.111, 0.07]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.06716417910447761
[2m[36m(func pid=62931)[0m top5: 0.6026119402985075
[2m[36m(func pid=62931)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=62931)[0m f1_macro: 0.09102083294388878
[2m[36m(func pid=62931)[0m f1_weighted: 0.029810663404083466
[2m[36m(func pid=62931)[0m f1_per_class: [0.14, 0.011, 0.462, 0.037, 0.004, 0.0, 0.0, 0.192, 0.0, 0.065]
[2m[36m(func pid=62931)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7593 | Steps: 4 | Val loss: 2.2058 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=75223)[0m top1: 0.33861940298507465
[2m[36m(func pid=75223)[0m top5: 0.8791977611940298
[2m[36m(func pid=75223)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=75223)[0m f1_macro: 0.3065210339611677
[2m[36m(func pid=75223)[0m f1_weighted: 0.3282045626871468
[2m[36m(func pid=75223)[0m f1_per_class: [0.397, 0.403, 0.375, 0.472, 0.094, 0.424, 0.124, 0.378, 0.197, 0.201]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4493 | Steps: 4 | Val loss: 2.4691 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=75166)[0m top1: 0.23134328358208955
[2m[36m(func pid=75166)[0m top5: 0.6921641791044776
[2m[36m(func pid=75166)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=75166)[0m f1_macro: 0.17653059707518187
[2m[36m(func pid=75166)[0m f1_weighted: 0.23380814038418524
[2m[36m(func pid=75166)[0m f1_per_class: [0.169, 0.276, 0.313, 0.352, 0.037, 0.333, 0.123, 0.122, 0.014, 0.027]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=62931)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.9182 | Steps: 4 | Val loss: 2.5194 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.7895 | Steps: 4 | Val loss: 1.7353 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 14:59:46 (running for 00:26:12.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00011 | RUNNING    | 192.168.7.53:62931 | 0.1    |       0.99 |         0.0001 |  2.629 |      0.091 |                   74 |
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.759 |      0.177 |                   22 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.895 |      0.307 |                   20 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.449 |      0.29  |                   22 |
| train_5ae7f_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.333955223880597
[2m[36m(func pid=75300)[0m top5: 0.8386194029850746
[2m[36m(func pid=75300)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=75300)[0m f1_macro: 0.29048927759035925
[2m[36m(func pid=75300)[0m f1_weighted: 0.32049023205195726
[2m[36m(func pid=75300)[0m f1_per_class: [0.323, 0.469, 0.264, 0.366, 0.2, 0.318, 0.204, 0.374, 0.219, 0.168]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=62931)[0m top1: 0.06996268656716417
[2m[36m(func pid=62931)[0m top5: 0.6529850746268657
[2m[36m(func pid=62931)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=62931)[0m f1_macro: 0.09850255126856122
[2m[36m(func pid=62931)[0m f1_weighted: 0.020975045549206185
[2m[36m(func pid=62931)[0m f1_per_class: [0.107, 0.016, 0.585, 0.0, 0.012, 0.0, 0.0, 0.203, 0.0, 0.062]
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6996 | Steps: 4 | Val loss: 2.1952 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=75223)[0m top1: 0.3689365671641791
[2m[36m(func pid=75223)[0m top5: 0.9011194029850746
[2m[36m(func pid=75223)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=75223)[0m f1_macro: 0.33883000160641713
[2m[36m(func pid=75223)[0m f1_weighted: 0.3714127668351249
[2m[36m(func pid=75223)[0m f1_per_class: [0.475, 0.399, 0.421, 0.498, 0.099, 0.424, 0.23, 0.405, 0.246, 0.191]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.0480 | Steps: 4 | Val loss: 3.1452 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=75166)[0m top1: 0.23460820895522388
[2m[36m(func pid=75166)[0m top5: 0.7000932835820896
[2m[36m(func pid=75166)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=75166)[0m f1_macro: 0.1937984294947065
[2m[36m(func pid=75166)[0m f1_weighted: 0.23747748225788257
[2m[36m(func pid=75166)[0m f1_per_class: [0.155, 0.275, 0.408, 0.32, 0.033, 0.363, 0.146, 0.148, 0.025, 0.065]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.27472014925373134
[2m[36m(func pid=75300)[0m top5: 0.8190298507462687
[2m[36m(func pid=75300)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=75300)[0m f1_macro: 0.2140717450061905
[2m[36m(func pid=75300)[0m f1_weighted: 0.26179146727484365
[2m[36m(func pid=75300)[0m f1_per_class: [0.246, 0.462, 0.153, 0.448, 0.292, 0.118, 0.1, 0.0, 0.097, 0.225]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5353 | Steps: 4 | Val loss: 1.7217 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7377 | Steps: 4 | Val loss: 2.1916 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=75223)[0m top1: 0.40158582089552236
[2m[36m(func pid=75223)[0m top5: 0.9071828358208955
[2m[36m(func pid=75223)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=75223)[0m f1_macro: 0.3572287017927108
[2m[36m(func pid=75223)[0m f1_weighted: 0.4136817596968887
[2m[36m(func pid=75223)[0m f1_per_class: [0.464, 0.434, 0.421, 0.538, 0.111, 0.395, 0.318, 0.432, 0.269, 0.191]
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.2492 | Steps: 4 | Val loss: 2.5994 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=75166)[0m top1: 0.23787313432835822
[2m[36m(func pid=75166)[0m top5: 0.7028917910447762
[2m[36m(func pid=75166)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=75166)[0m f1_macro: 0.19497341981793587
[2m[36m(func pid=75166)[0m f1_weighted: 0.24466063086741788
[2m[36m(func pid=75166)[0m f1_per_class: [0.158, 0.257, 0.4, 0.355, 0.03, 0.347, 0.147, 0.184, 0.023, 0.048]
== Status ==
Current time: 2024-01-07 14:59:52 (running for 00:26:17.82)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.7   |      0.194 |                   23 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.789 |      0.339 |                   21 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.048 |      0.214 |                   23 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=81458)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=81458)[0m Configuration completed!
[2m[36m(func pid=81458)[0m New optimizer parameters:
[2m[36m(func pid=81458)[0m SGD (
[2m[36m(func pid=81458)[0m Parameter Group 0
[2m[36m(func pid=81458)[0m     dampening: 0
[2m[36m(func pid=81458)[0m     differentiable: False
[2m[36m(func pid=81458)[0m     foreach: None
[2m[36m(func pid=81458)[0m     lr: 0.1
[2m[36m(func pid=81458)[0m     maximize: False
[2m[36m(func pid=81458)[0m     momentum: 0.9
[2m[36m(func pid=81458)[0m     nesterov: False
[2m[36m(func pid=81458)[0m     weight_decay: 0.0001
[2m[36m(func pid=81458)[0m )
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 14:59:57 (running for 00:26:22.97)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.738 |      0.195 |                   24 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.535 |      0.357 |                   22 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.249 |      0.186 |                   24 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.27798507462686567
[2m[36m(func pid=75300)[0m top5: 0.8292910447761194
[2m[36m(func pid=75300)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=75300)[0m f1_macro: 0.18613022847637786
[2m[36m(func pid=75300)[0m f1_weighted: 0.2759090320203039
[2m[36m(func pid=75300)[0m f1_per_class: [0.043, 0.335, 0.0, 0.178, 0.333, 0.186, 0.453, 0.059, 0.116, 0.159]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7216 | Steps: 4 | Val loss: 2.1827 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.4795 | Steps: 4 | Val loss: 1.7051 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.0476 | Steps: 4 | Val loss: 3.1908 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.7608 | Steps: 4 | Val loss: 3.2398 | Batch size: 32 | lr: 0.1 | Duration: 4.98s
[2m[36m(func pid=75166)[0m top1: 0.22527985074626866
[2m[36m(func pid=75166)[0m top5: 0.7182835820895522
[2m[36m(func pid=75166)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=75166)[0m f1_macro: 0.19429793319665842
[2m[36m(func pid=75166)[0m f1_weighted: 0.23432789502635407
[2m[36m(func pid=75166)[0m f1_per_class: [0.195, 0.231, 0.357, 0.338, 0.045, 0.326, 0.142, 0.204, 0.047, 0.058]
[2m[36m(func pid=75166)[0m 
== Status ==
Current time: 2024-01-07 15:00:02 (running for 00:26:28.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.722 |      0.194 |                   25 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.535 |      0.357 |                   22 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.048 |      0.142 |                   25 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.40111940298507465
[2m[36m(func pid=75223)[0m top5: 0.9090485074626866
[2m[36m(func pid=75223)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=75223)[0m f1_macro: 0.3521673733840467
[2m[36m(func pid=75223)[0m f1_weighted: 0.41255853578376367
[2m[36m(func pid=75223)[0m f1_per_class: [0.443, 0.423, 0.387, 0.542, 0.112, 0.387, 0.322, 0.42, 0.282, 0.204]
[2m[36m(func pid=75300)[0m top1: 0.26259328358208955
[2m[36m(func pid=75300)[0m top5: 0.7672574626865671
[2m[36m(func pid=75300)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=75300)[0m f1_macro: 0.14176278078043006
[2m[36m(func pid=75300)[0m f1_weighted: 0.2499045829222347
[2m[36m(func pid=75300)[0m f1_per_class: [0.081, 0.029, 0.0, 0.162, 0.061, 0.068, 0.586, 0.159, 0.129, 0.142]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.2537313432835821
[2m[36m(func pid=81458)[0m top5: 0.6455223880597015
[2m[36m(func pid=81458)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=81458)[0m f1_macro: 0.06975820322469678
[2m[36m(func pid=81458)[0m f1_weighted: 0.17463358993176203
[2m[36m(func pid=81458)[0m f1_per_class: [0.061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58, 0.0, 0.0, 0.057]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6671 | Steps: 4 | Val loss: 2.1773 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8012 | Steps: 4 | Val loss: 3.2051 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.5650 | Steps: 4 | Val loss: 1.6972 | Batch size: 32 | lr: 0.001 | Duration: 3.29s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.3899 | Steps: 4 | Val loss: 4.1129 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:00:07 (running for 00:26:33.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.667 |      0.204 |                   26 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.48  |      0.352 |                   23 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.048 |      0.142 |                   25 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.761 |      0.07  |                    1 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m top1: 0.23880597014925373
[2m[36m(func pid=75166)[0m top5: 0.7318097014925373
[2m[36m(func pid=75166)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=75166)[0m f1_macro: 0.20353411902998877
[2m[36m(func pid=75166)[0m f1_weighted: 0.24788630387587043
[2m[36m(func pid=75166)[0m f1_per_class: [0.178, 0.241, 0.392, 0.357, 0.05, 0.344, 0.156, 0.216, 0.047, 0.055]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.2392723880597015
[2m[36m(func pid=75300)[0m top5: 0.840018656716418
[2m[36m(func pid=75300)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=75300)[0m f1_macro: 0.18988310195419267
[2m[36m(func pid=75300)[0m f1_weighted: 0.26990769197914105
[2m[36m(func pid=75300)[0m f1_per_class: [0.092, 0.032, 0.035, 0.208, 0.101, 0.147, 0.526, 0.383, 0.161, 0.213]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.40158582089552236
[2m[36m(func pid=75223)[0m top5: 0.9085820895522388
[2m[36m(func pid=75223)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=75223)[0m f1_macro: 0.36459352881532164
[2m[36m(func pid=75223)[0m f1_weighted: 0.41110522689132906
[2m[36m(func pid=75223)[0m f1_per_class: [0.443, 0.408, 0.375, 0.526, 0.092, 0.433, 0.31, 0.468, 0.299, 0.292]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.024720149253731342
[2m[36m(func pid=81458)[0m top5: 0.542910447761194
[2m[36m(func pid=81458)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=81458)[0m f1_macro: 0.011292934826659166
[2m[36m(func pid=81458)[0m f1_weighted: 0.0037654963551279263
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.076, 0.029]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7559 | Steps: 4 | Val loss: 2.1805 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7132 | Steps: 4 | Val loss: 3.5589 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5268 | Steps: 4 | Val loss: 1.7570 | Batch size: 32 | lr: 0.001 | Duration: 3.37s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.8212 | Steps: 4 | Val loss: 6.3227 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=75166)[0m top1: 0.23227611940298507
[2m[36m(func pid=75166)[0m top5: 0.7262126865671642
[2m[36m(func pid=75166)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=75166)[0m f1_macro: 0.1968694672461085
[2m[36m(func pid=75166)[0m f1_weighted: 0.23612549709494504
[2m[36m(func pid=75166)[0m f1_per_class: [0.197, 0.218, 0.377, 0.38, 0.043, 0.339, 0.111, 0.218, 0.022, 0.062]
[2m[36m(func pid=75166)[0m 
== Status ==
Current time: 2024-01-07 15:00:13 (running for 00:26:38.81)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.756 |      0.197 |                   27 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.565 |      0.365 |                   24 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.801 |      0.19  |                   26 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.39  |      0.011 |                    2 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.14832089552238806
[2m[36m(func pid=75300)[0m top5: 0.7546641791044776
[2m[36m(func pid=75300)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=75300)[0m f1_macro: 0.14518552961755876
[2m[36m(func pid=75300)[0m f1_weighted: 0.16779880047520515
[2m[36m(func pid=75300)[0m f1_per_class: [0.267, 0.205, 0.032, 0.153, 0.088, 0.125, 0.206, 0.0, 0.158, 0.219]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.3596082089552239
[2m[36m(func pid=75223)[0m top5: 0.882929104477612
[2m[36m(func pid=75223)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=75223)[0m f1_macro: 0.3387372393894907
[2m[36m(func pid=75223)[0m f1_weighted: 0.37646219763433536
[2m[36m(func pid=75223)[0m f1_per_class: [0.452, 0.401, 0.296, 0.432, 0.087, 0.426, 0.301, 0.426, 0.265, 0.303]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.13292910447761194
[2m[36m(func pid=81458)[0m top5: 0.7723880597014925
[2m[36m(func pid=81458)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=81458)[0m f1_macro: 0.07691836638560459
[2m[36m(func pid=81458)[0m f1_weighted: 0.12517523170102662
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.472, 0.232, 0.0, 0.066, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6925 | Steps: 4 | Val loss: 2.1780 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6022 | Steps: 4 | Val loss: 2.7713 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3449 | Steps: 4 | Val loss: 1.7507 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.5199 | Steps: 4 | Val loss: 1006.2053 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:00:18 (running for 00:26:44.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.692 |      0.208 |                   28 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.527 |      0.339 |                   25 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.713 |      0.145 |                   27 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.821 |      0.077 |                    3 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m top1: 0.23694029850746268
[2m[36m(func pid=75166)[0m top5: 0.7201492537313433
[2m[36m(func pid=75166)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=75166)[0m f1_macro: 0.20759927328187028
[2m[36m(func pid=75166)[0m f1_weighted: 0.23891514670363623
[2m[36m(func pid=75166)[0m f1_per_class: [0.191, 0.268, 0.465, 0.355, 0.036, 0.353, 0.108, 0.213, 0.037, 0.05]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.2775186567164179
[2m[36m(func pid=75300)[0m top5: 0.8460820895522388
[2m[36m(func pid=75300)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=75300)[0m f1_macro: 0.2542744710698393
[2m[36m(func pid=75300)[0m f1_weighted: 0.2883817058288247
[2m[36m(func pid=75300)[0m f1_per_class: [0.218, 0.325, 0.167, 0.202, 0.1, 0.233, 0.355, 0.431, 0.313, 0.2]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.3712686567164179
[2m[36m(func pid=75223)[0m top5: 0.8801305970149254
[2m[36m(func pid=75223)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=75223)[0m f1_macro: 0.3546261888665988
[2m[36m(func pid=75223)[0m f1_weighted: 0.3804749615209398
[2m[36m(func pid=75223)[0m f1_per_class: [0.472, 0.461, 0.343, 0.415, 0.095, 0.432, 0.281, 0.482, 0.243, 0.322]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.17210820895522388
[2m[36m(func pid=81458)[0m top5: 0.6501865671641791
[2m[36m(func pid=81458)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=81458)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=81458)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3415 | Steps: 4 | Val loss: 2.2926 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6756 | Steps: 4 | Val loss: 2.1695 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.2996 | Steps: 4 | Val loss: 1.7162 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.1817 | Steps: 4 | Val loss: 4019.6130 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 15:00:24 (running for 00:26:49.84)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.692 |      0.208 |                   28 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.345 |      0.355 |                   26 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.342 |      0.291 |                   29 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.52  |      0.029 |                    4 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3400186567164179
[2m[36m(func pid=75300)[0m top5: 0.8838619402985075
[2m[36m(func pid=75300)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=75300)[0m f1_macro: 0.2910633404950237
[2m[36m(func pid=75300)[0m f1_weighted: 0.3466281375946751
[2m[36m(func pid=75300)[0m f1_per_class: [0.376, 0.31, 0.308, 0.305, 0.015, 0.275, 0.448, 0.444, 0.131, 0.299]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.23367537313432835
[2m[36m(func pid=75166)[0m top5: 0.7392723880597015
[2m[36m(func pid=75166)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=75166)[0m f1_macro: 0.2155258357129141
[2m[36m(func pid=75166)[0m f1_weighted: 0.23560217137361156
[2m[36m(func pid=75166)[0m f1_per_class: [0.167, 0.304, 0.476, 0.285, 0.044, 0.348, 0.14, 0.214, 0.057, 0.121]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.027052238805970148
[2m[36m(func pid=81458)[0m top5: 0.4710820895522388
[2m[36m(func pid=81458)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=81458)[0m f1_macro: 0.014201326504911427
[2m[36m(func pid=81458)[0m f1_weighted: 0.012390985630992045
[2m[36m(func pid=81458)[0m f1_per_class: [0.041, 0.0, 0.0, 0.0, 0.0, 0.101, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.384794776119403
[2m[36m(func pid=75223)[0m top5: 0.8861940298507462
[2m[36m(func pid=75223)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=75223)[0m f1_macro: 0.35176987147466654
[2m[36m(func pid=75223)[0m f1_weighted: 0.401677355695553
[2m[36m(func pid=75223)[0m f1_per_class: [0.376, 0.487, 0.393, 0.423, 0.137, 0.404, 0.356, 0.447, 0.216, 0.278]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3486 | Steps: 4 | Val loss: 1.9547 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5773 | Steps: 4 | Val loss: 2.1768 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.1818 | Steps: 4 | Val loss: 1552.6177 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.4251 | Steps: 4 | Val loss: 1.6714 | Batch size: 32 | lr: 0.001 | Duration: 3.35s
== Status ==
Current time: 2024-01-07 15:00:29 (running for 00:26:55.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.676 |      0.216 |                   29 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.3   |      0.352 |                   27 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.349 |      0.377 |                   30 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.182 |      0.014 |                    5 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.41697761194029853
[2m[36m(func pid=75300)[0m top5: 0.9015858208955224
[2m[36m(func pid=75300)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=75300)[0m f1_macro: 0.37733209389808886
[2m[36m(func pid=75300)[0m f1_weighted: 0.4267774642505914
[2m[36m(func pid=75300)[0m f1_per_class: [0.447, 0.411, 0.429, 0.401, 0.081, 0.335, 0.51, 0.491, 0.236, 0.431]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.23367537313432835
[2m[36m(func pid=75166)[0m top5: 0.7299440298507462
[2m[36m(func pid=75166)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=75166)[0m f1_macro: 0.21271171010278067
[2m[36m(func pid=75166)[0m f1_weighted: 0.23845099702766515
[2m[36m(func pid=75166)[0m f1_per_class: [0.167, 0.311, 0.458, 0.297, 0.045, 0.349, 0.136, 0.211, 0.055, 0.098]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.05363805970149254
[2m[36m(func pid=81458)[0m top5: 0.44402985074626866
[2m[36m(func pid=81458)[0m f1_micro: 0.05363805970149254
[2m[36m(func pid=81458)[0m f1_macro: 0.034871836309585784
[2m[36m(func pid=81458)[0m f1_weighted: 0.0562256559851235
[2m[36m(func pid=81458)[0m f1_per_class: [0.045, 0.0, 0.0, 0.0, 0.0, 0.082, 0.151, 0.0, 0.0, 0.07]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.396455223880597
[2m[36m(func pid=75223)[0m top5: 0.8955223880597015
[2m[36m(func pid=75223)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=75223)[0m f1_macro: 0.36073323034182014
[2m[36m(func pid=75223)[0m f1_weighted: 0.4154438741839791
[2m[36m(func pid=75223)[0m f1_per_class: [0.39, 0.46, 0.436, 0.472, 0.128, 0.401, 0.375, 0.421, 0.235, 0.29]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4160 | Steps: 4 | Val loss: 2.2863 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5908 | Steps: 4 | Val loss: 2.1802 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7453 | Steps: 4 | Val loss: 953.5400 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:00:35 (running for 00:27:00.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.577 |      0.213 |                   30 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.425 |      0.361 |                   28 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.416 |      0.369 |                   31 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.182 |      0.035 |                    6 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.37220149253731344
[2m[36m(func pid=75300)[0m top5: 0.8773320895522388
[2m[36m(func pid=75300)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=75300)[0m f1_macro: 0.3692816416761603
[2m[36m(func pid=75300)[0m f1_weighted: 0.3964489460729401
[2m[36m(func pid=75300)[0m f1_per_class: [0.413, 0.408, 0.429, 0.483, 0.067, 0.327, 0.338, 0.44, 0.325, 0.462]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.23087686567164178
[2m[36m(func pid=75166)[0m top5: 0.7243470149253731
[2m[36m(func pid=75166)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=75166)[0m f1_macro: 0.21453669442228365
[2m[36m(func pid=75166)[0m f1_weighted: 0.2338301327357313
[2m[36m(func pid=75166)[0m f1_per_class: [0.165, 0.298, 0.478, 0.289, 0.045, 0.356, 0.129, 0.227, 0.056, 0.103]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3332 | Steps: 4 | Val loss: 1.6646 | Batch size: 32 | lr: 0.001 | Duration: 3.33s
[2m[36m(func pid=81458)[0m top1: 0.03871268656716418
[2m[36m(func pid=81458)[0m top5: 0.47388059701492535
[2m[36m(func pid=81458)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=81458)[0m f1_macro: 0.026282601379774527
[2m[36m(func pid=81458)[0m f1_weighted: 0.0143995189216713
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.247, 0.0, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3255 | Steps: 4 | Val loss: 2.6296 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5863 | Steps: 4 | Val loss: 2.1784 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=75223)[0m top1: 0.394589552238806
[2m[36m(func pid=75223)[0m top5: 0.8913246268656716
[2m[36m(func pid=75223)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=75223)[0m f1_macro: 0.359619749349788
[2m[36m(func pid=75223)[0m f1_weighted: 0.4111294846072836
[2m[36m(func pid=75223)[0m f1_per_class: [0.444, 0.424, 0.4, 0.493, 0.117, 0.413, 0.346, 0.462, 0.249, 0.247]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6606 | Steps: 4 | Val loss: 172.2005 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:00:40 (running for 00:27:06.01)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.591 |      0.215 |                   31 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.333 |      0.36  |                   29 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.325 |      0.335 |                   32 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.745 |      0.026 |                    7 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3712686567164179
[2m[36m(func pid=75300)[0m top5: 0.8465485074626866
[2m[36m(func pid=75300)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=75300)[0m f1_macro: 0.3353685085141762
[2m[36m(func pid=75300)[0m f1_weighted: 0.38146337372659284
[2m[36m(func pid=75300)[0m f1_per_class: [0.421, 0.21, 0.25, 0.571, 0.071, 0.339, 0.316, 0.484, 0.298, 0.394]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.2271455223880597
[2m[36m(func pid=75166)[0m top5: 0.7238805970149254
[2m[36m(func pid=75166)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=75166)[0m f1_macro: 0.2130803627793542
[2m[36m(func pid=75166)[0m f1_weighted: 0.23139327910518817
[2m[36m(func pid=75166)[0m f1_per_class: [0.176, 0.303, 0.524, 0.3, 0.038, 0.332, 0.119, 0.224, 0.041, 0.075]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.1114 | Steps: 4 | Val loss: 1.6508 | Batch size: 32 | lr: 0.001 | Duration: 3.32s
[2m[36m(func pid=81458)[0m top1: 0.05503731343283582
[2m[36m(func pid=81458)[0m top5: 0.5578358208955224
[2m[36m(func pid=81458)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=81458)[0m f1_macro: 0.029431583102507115
[2m[36m(func pid=81458)[0m f1_weighted: 0.015114673048441227
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.0, 0.0, 0.0, 0.241, 0.032, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2255 | Steps: 4 | Val loss: 3.2613 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6072 | Steps: 4 | Val loss: 2.1769 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=75223)[0m top1: 0.4076492537313433
[2m[36m(func pid=75223)[0m top5: 0.8950559701492538
[2m[36m(func pid=75223)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=75223)[0m f1_macro: 0.37433183413560656
[2m[36m(func pid=75223)[0m f1_weighted: 0.4284012324422935
[2m[36m(func pid=75223)[0m f1_per_class: [0.459, 0.436, 0.444, 0.509, 0.088, 0.387, 0.379, 0.499, 0.3, 0.242]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:00:45 (running for 00:27:11.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.586 |      0.213 |                   32 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.111 |      0.374 |                   30 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.226 |      0.243 |                   33 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.661 |      0.029 |                    8 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.25513059701492535
[2m[36m(func pid=75300)[0m top5: 0.8199626865671642
[2m[36m(func pid=75300)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=75300)[0m f1_macro: 0.24343028186636398
[2m[36m(func pid=75300)[0m f1_weighted: 0.28994839528830274
[2m[36m(func pid=75300)[0m f1_per_class: [0.151, 0.21, 0.068, 0.216, 0.098, 0.273, 0.4, 0.483, 0.215, 0.319]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.0297 | Steps: 4 | Val loss: 24.2676 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=75166)[0m top1: 0.23740671641791045
[2m[36m(func pid=75166)[0m top5: 0.7192164179104478
[2m[36m(func pid=75166)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=75166)[0m f1_macro: 0.21108875314005143
[2m[36m(func pid=75166)[0m f1_weighted: 0.2420921768886214
[2m[36m(func pid=75166)[0m f1_per_class: [0.171, 0.307, 0.431, 0.331, 0.044, 0.336, 0.119, 0.252, 0.041, 0.08]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4195 | Steps: 4 | Val loss: 1.6569 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=81458)[0m top1: 0.12360074626865672
[2m[36m(func pid=81458)[0m top5: 0.6571828358208955
[2m[36m(func pid=81458)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=81458)[0m f1_macro: 0.06331104280738761
[2m[36m(func pid=81458)[0m f1_weighted: 0.05895512128319306
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.253, 0.092, 0.0, 0.0, 0.0, 0.0, 0.214, 0.074, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6585 | Steps: 4 | Val loss: 3.3017 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4974 | Steps: 4 | Val loss: 2.1840 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=75223)[0m top1: 0.404384328358209
[2m[36m(func pid=75223)[0m top5: 0.8913246268656716
[2m[36m(func pid=75223)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=75223)[0m f1_macro: 0.36709077097322623
[2m[36m(func pid=75223)[0m f1_weighted: 0.43061713562875487
[2m[36m(func pid=75223)[0m f1_per_class: [0.449, 0.464, 0.387, 0.483, 0.079, 0.368, 0.408, 0.469, 0.302, 0.261]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:00:51 (running for 00:27:16.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.607 |      0.211 |                   33 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.419 |      0.367 |                   31 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.658 |      0.28  |                   34 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  4.03  |      0.063 |                    9 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2966417910447761
[2m[36m(func pid=75300)[0m top5: 0.8306902985074627
[2m[36m(func pid=75300)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=75300)[0m f1_macro: 0.2801765733185837
[2m[36m(func pid=75300)[0m f1_weighted: 0.3309851170979988
[2m[36m(func pid=75300)[0m f1_per_class: [0.164, 0.252, 0.226, 0.223, 0.069, 0.31, 0.49, 0.421, 0.318, 0.33]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4184 | Steps: 4 | Val loss: 8.0565 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=75166)[0m top1: 0.2271455223880597
[2m[36m(func pid=75166)[0m top5: 0.7182835820895522
[2m[36m(func pid=75166)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=75166)[0m f1_macro: 0.2043143401209134
[2m[36m(func pid=75166)[0m f1_weighted: 0.23271349203112848
[2m[36m(func pid=75166)[0m f1_per_class: [0.169, 0.286, 0.393, 0.313, 0.044, 0.348, 0.111, 0.256, 0.041, 0.082]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2146 | Steps: 4 | Val loss: 1.6314 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
[2m[36m(func pid=81458)[0m top1: 0.1287313432835821
[2m[36m(func pid=81458)[0m top5: 0.7285447761194029
[2m[36m(func pid=81458)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=81458)[0m f1_macro: 0.07200965035662696
[2m[36m(func pid=81458)[0m f1_weighted: 0.07903838402034953
[2m[36m(func pid=81458)[0m f1_per_class: [0.099, 0.0, 0.0, 0.16, 0.048, 0.253, 0.0, 0.025, 0.0, 0.136]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5129 | Steps: 4 | Val loss: 3.8922 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5845 | Steps: 4 | Val loss: 2.1797 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 15:00:56 (running for 00:27:21.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.497 |      0.204 |                   34 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.215 |      0.378 |                   32 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.658 |      0.28  |                   34 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.418 |      0.072 |                   10 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.43283582089552236
[2m[36m(func pid=75223)[0m top5: 0.8908582089552238
[2m[36m(func pid=75223)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=75223)[0m f1_macro: 0.37800919020044893
[2m[36m(func pid=75223)[0m f1_weighted: 0.46009160320490006
[2m[36m(func pid=75223)[0m f1_per_class: [0.45, 0.492, 0.375, 0.515, 0.087, 0.381, 0.463, 0.417, 0.322, 0.278]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m top1: 0.21082089552238806
[2m[36m(func pid=75300)[0m top5: 0.7975746268656716
[2m[36m(func pid=75300)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=75300)[0m f1_macro: 0.20029410191924518
[2m[36m(func pid=75300)[0m f1_weighted: 0.19209121731015522
[2m[36m(func pid=75300)[0m f1_per_class: [0.08, 0.179, 0.324, 0.287, 0.064, 0.374, 0.03, 0.286, 0.202, 0.177]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.4612 | Steps: 4 | Val loss: 3.7797 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=75166)[0m top1: 0.2248134328358209
[2m[36m(func pid=75166)[0m top5: 0.7243470149253731
[2m[36m(func pid=75166)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=75166)[0m f1_macro: 0.208316593690274
[2m[36m(func pid=75166)[0m f1_weighted: 0.2283831770063249
[2m[36m(func pid=75166)[0m f1_per_class: [0.164, 0.284, 0.415, 0.307, 0.05, 0.339, 0.105, 0.244, 0.056, 0.118]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.15578358208955223
[2m[36m(func pid=81458)[0m top5: 0.7555970149253731
[2m[36m(func pid=81458)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=81458)[0m f1_macro: 0.09969584194512762
[2m[36m(func pid=81458)[0m f1_weighted: 0.12949485738494998
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.0, 0.071, 0.299, 0.044, 0.21, 0.018, 0.253, 0.0, 0.101]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5842 | Steps: 4 | Val loss: 3.3931 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.2578 | Steps: 4 | Val loss: 1.6320 | Batch size: 32 | lr: 0.001 | Duration: 3.35s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6225 | Steps: 4 | Val loss: 2.1618 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:01:01 (running for 00:27:27.20)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.584 |      0.208 |                   35 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.215 |      0.378 |                   32 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.584 |      0.235 |                   36 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.461 |      0.1   |                   11 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2751865671641791
[2m[36m(func pid=75300)[0m top5: 0.8302238805970149
[2m[36m(func pid=75300)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=75300)[0m f1_macro: 0.23514223500765302
[2m[36m(func pid=75300)[0m f1_weighted: 0.25949837586072094
[2m[36m(func pid=75300)[0m f1_per_class: [0.0, 0.25, 0.375, 0.407, 0.082, 0.424, 0.071, 0.347, 0.267, 0.128]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.23694029850746268
[2m[36m(func pid=75166)[0m top5: 0.7406716417910447
[2m[36m(func pid=75166)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=75166)[0m f1_macro: 0.21940862593404642
[2m[36m(func pid=75166)[0m f1_weighted: 0.24585113035443829
[2m[36m(func pid=75166)[0m f1_per_class: [0.177, 0.282, 0.468, 0.336, 0.047, 0.349, 0.136, 0.225, 0.056, 0.118]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6983 | Steps: 4 | Val loss: 2.9435 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=75223)[0m top1: 0.4239738805970149
[2m[36m(func pid=75223)[0m top5: 0.8927238805970149
[2m[36m(func pid=75223)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=75223)[0m f1_macro: 0.3613544015401814
[2m[36m(func pid=75223)[0m f1_weighted: 0.4498376311091847
[2m[36m(func pid=75223)[0m f1_per_class: [0.426, 0.498, 0.308, 0.498, 0.09, 0.414, 0.444, 0.36, 0.333, 0.244]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.3087686567164179
[2m[36m(func pid=81458)[0m top5: 0.7723880597014925
[2m[36m(func pid=81458)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=81458)[0m f1_macro: 0.1269152708131384
[2m[36m(func pid=81458)[0m f1_weighted: 0.2889635129373913
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.209, 0.0, 0.239, 0.043, 0.007, 0.61, 0.0, 0.065, 0.095]
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.0566 | Steps: 4 | Val loss: 2.8710 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5179 | Steps: 4 | Val loss: 2.1606 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.0913 | Steps: 4 | Val loss: 1.6221 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 15:01:07 (running for 00:27:32.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.623 |      0.219 |                   36 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.258 |      0.361 |                   33 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.057 |      0.243 |                   37 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.698 |      0.127 |                   12 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.28125
[2m[36m(func pid=75300)[0m top5: 0.8292910447761194
[2m[36m(func pid=75300)[0m f1_micro: 0.28125
[2m[36m(func pid=75300)[0m f1_macro: 0.24258084434356025
[2m[36m(func pid=75300)[0m f1_weighted: 0.2984766625165258
[2m[36m(func pid=75300)[0m f1_per_class: [0.233, 0.294, 0.224, 0.266, 0.1, 0.404, 0.315, 0.4, 0.073, 0.117]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.1854 | Steps: 4 | Val loss: 2.7911 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=75166)[0m top1: 0.24300373134328357
[2m[36m(func pid=75166)[0m top5: 0.7355410447761194
[2m[36m(func pid=75166)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=75166)[0m f1_macro: 0.223089159273617
[2m[36m(func pid=75166)[0m f1_weighted: 0.2549120372592542
[2m[36m(func pid=75166)[0m f1_per_class: [0.177, 0.267, 0.449, 0.347, 0.045, 0.359, 0.157, 0.234, 0.083, 0.112]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4412313432835821
[2m[36m(func pid=75223)[0m top5: 0.8936567164179104
[2m[36m(func pid=75223)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=75223)[0m f1_macro: 0.3724025735535367
[2m[36m(func pid=75223)[0m f1_weighted: 0.4660128918844352
[2m[36m(func pid=75223)[0m f1_per_class: [0.417, 0.503, 0.381, 0.525, 0.14, 0.386, 0.476, 0.4, 0.29, 0.206]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0507 | Steps: 4 | Val loss: 3.2129 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=81458)[0m top1: 0.2537313432835821
[2m[36m(func pid=81458)[0m top5: 0.6833022388059702
[2m[36m(func pid=81458)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=81458)[0m f1_macro: 0.11167461392772275
[2m[36m(func pid=81458)[0m f1_weighted: 0.24339635092431036
[2m[36m(func pid=81458)[0m f1_per_class: [0.037, 0.0, 0.101, 0.199, 0.037, 0.019, 0.614, 0.0, 0.0, 0.111]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5232 | Steps: 4 | Val loss: 2.1467 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.9334 | Steps: 4 | Val loss: 1.6172 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 15:01:12 (running for 00:27:38.13)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.518 |      0.223 |                   37 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.091 |      0.372 |                   34 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.051 |      0.236 |                   38 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.185 |      0.112 |                   13 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2943097014925373
[2m[36m(func pid=75300)[0m top5: 0.6884328358208955
[2m[36m(func pid=75300)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=75300)[0m f1_macro: 0.2355169450798611
[2m[36m(func pid=75300)[0m f1_weighted: 0.29168247226408817
[2m[36m(func pid=75300)[0m f1_per_class: [0.22, 0.309, 0.055, 0.003, 0.095, 0.294, 0.568, 0.358, 0.146, 0.308]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.26259328358208955
[2m[36m(func pid=75166)[0m top5: 0.746268656716418
[2m[36m(func pid=75166)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=75166)[0m f1_macro: 0.23142365982530205
[2m[36m(func pid=75166)[0m f1_weighted: 0.27038694601831004
[2m[36m(func pid=75166)[0m f1_per_class: [0.188, 0.304, 0.44, 0.377, 0.05, 0.367, 0.157, 0.244, 0.043, 0.144]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.0801 | Steps: 4 | Val loss: 2.5606 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=75223)[0m top1: 0.439365671641791
[2m[36m(func pid=75223)[0m top5: 0.8997201492537313
[2m[36m(func pid=75223)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=75223)[0m f1_macro: 0.3735996870496846
[2m[36m(func pid=75223)[0m f1_weighted: 0.4640057887630063
[2m[36m(func pid=75223)[0m f1_per_class: [0.474, 0.505, 0.308, 0.52, 0.134, 0.379, 0.469, 0.398, 0.321, 0.228]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.14505597014925373
[2m[36m(func pid=81458)[0m top5: 0.6399253731343284
[2m[36m(func pid=81458)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=81458)[0m f1_macro: 0.12441758038568236
[2m[36m(func pid=81458)[0m f1_weighted: 0.17558119475285563
[2m[36m(func pid=81458)[0m f1_per_class: [0.063, 0.0, 0.0, 0.045, 0.032, 0.027, 0.421, 0.548, 0.0, 0.108]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2894 | Steps: 4 | Val loss: 2.9455 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5692 | Steps: 4 | Val loss: 2.1300 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9502 | Steps: 4 | Val loss: 1.6285 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 15:01:18 (running for 00:27:43.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.523 |      0.231 |                   38 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.933 |      0.374 |                   35 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.289 |      0.231 |                   39 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  3.08  |      0.124 |                   14 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3521455223880597
[2m[36m(func pid=75300)[0m top5: 0.6972947761194029
[2m[36m(func pid=75300)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=75300)[0m f1_macro: 0.23101632464527713
[2m[36m(func pid=75300)[0m f1_weighted: 0.2886149492344222
[2m[36m(func pid=75300)[0m f1_per_class: [0.337, 0.481, 0.17, 0.003, 0.154, 0.071, 0.543, 0.408, 0.028, 0.114]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7568 | Steps: 4 | Val loss: 2.2239 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=75166)[0m top1: 0.25419776119402987
[2m[36m(func pid=75166)[0m top5: 0.7504664179104478
[2m[36m(func pid=75166)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=75166)[0m f1_macro: 0.22104709044995646
[2m[36m(func pid=75166)[0m f1_weighted: 0.26663113421075924
[2m[36m(func pid=75166)[0m f1_per_class: [0.187, 0.308, 0.361, 0.359, 0.052, 0.334, 0.173, 0.244, 0.055, 0.14]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.42677238805970147
[2m[36m(func pid=75223)[0m top5: 0.8931902985074627
[2m[36m(func pid=75223)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=75223)[0m f1_macro: 0.36881312967152696
[2m[36m(func pid=75223)[0m f1_weighted: 0.44744762736433275
[2m[36m(func pid=75223)[0m f1_per_class: [0.486, 0.491, 0.293, 0.542, 0.122, 0.371, 0.396, 0.439, 0.314, 0.234]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.08628731343283583
[2m[36m(func pid=81458)[0m top5: 0.6763059701492538
[2m[36m(func pid=81458)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=81458)[0m f1_macro: 0.10660496948342275
[2m[36m(func pid=81458)[0m f1_weighted: 0.040395406175547766
[2m[36m(func pid=81458)[0m f1_per_class: [0.088, 0.077, 0.424, 0.0, 0.034, 0.008, 0.0, 0.358, 0.0, 0.077]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6158 | Steps: 4 | Val loss: 2.3912 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5716 | Steps: 4 | Val loss: 2.1238 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8565 | Steps: 4 | Val loss: 1.5554 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 15:01:23 (running for 00:27:49.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.569 |      0.221 |                   39 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.95  |      0.369 |                   36 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.616 |      0.279 |                   40 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.757 |      0.107 |                   15 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.36427238805970147
[2m[36m(func pid=75300)[0m top5: 0.8675373134328358
[2m[36m(func pid=75300)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=75300)[0m f1_macro: 0.2788804249921792
[2m[36m(func pid=75300)[0m f1_weighted: 0.3115719748164498
[2m[36m(func pid=75300)[0m f1_per_class: [0.351, 0.458, 0.444, 0.094, 0.109, 0.023, 0.54, 0.426, 0.164, 0.178]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5075 | Steps: 4 | Val loss: 2.3802 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=75166)[0m top1: 0.2579291044776119
[2m[36m(func pid=75166)[0m top5: 0.7625932835820896
[2m[36m(func pid=75166)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=75166)[0m f1_macro: 0.228863840045533
[2m[36m(func pid=75166)[0m f1_weighted: 0.27321451515553974
[2m[36m(func pid=75166)[0m f1_per_class: [0.188, 0.299, 0.361, 0.355, 0.057, 0.342, 0.199, 0.228, 0.081, 0.18]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.43703358208955223
[2m[36m(func pid=75223)[0m top5: 0.9151119402985075
[2m[36m(func pid=75223)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=75223)[0m f1_macro: 0.39437629442972655
[2m[36m(func pid=75223)[0m f1_weighted: 0.4488266733523586
[2m[36m(func pid=75223)[0m f1_per_class: [0.566, 0.479, 0.407, 0.552, 0.115, 0.407, 0.368, 0.466, 0.343, 0.24]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.0853544776119403
[2m[36m(func pid=81458)[0m top5: 0.679570895522388
[2m[36m(func pid=81458)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=81458)[0m f1_macro: 0.07163280141371779
[2m[36m(func pid=81458)[0m f1_weighted: 0.028708985740364616
[2m[36m(func pid=81458)[0m f1_per_class: [0.086, 0.041, 0.0, 0.0, 0.051, 0.0, 0.0, 0.265, 0.049, 0.225]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4652 | Steps: 4 | Val loss: 2.0982 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5101 | Steps: 4 | Val loss: 2.1100 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3117 | Steps: 4 | Val loss: 1.5281 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=75300)[0m top1: 0.4454291044776119
[2m[36m(func pid=75300)[0m top5: 0.9085820895522388
[2m[36m(func pid=75300)[0m f1_micro: 0.4454291044776119
[2m[36m(func pid=75300)[0m f1_macro: 0.3476864629467324
[2m[36m(func pid=75300)[0m f1_weighted: 0.42999417571668985
[2m[36m(func pid=75300)[0m f1_per_class: [0.361, 0.478, 0.4, 0.379, 0.171, 0.127, 0.601, 0.441, 0.263, 0.256]
[2m[36m(func pid=75300)[0m 
== Status ==
Current time: 2024-01-07 15:01:28 (running for 00:27:54.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.572 |      0.229 |                   40 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.856 |      0.394 |                   37 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.465 |      0.348 |                   41 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.508 |      0.072 |                   16 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.9143 | Steps: 4 | Val loss: 2.5715 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=75166)[0m top1: 0.2630597014925373
[2m[36m(func pid=75166)[0m top5: 0.7756529850746269
[2m[36m(func pid=75166)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=75166)[0m f1_macro: 0.2409232421685902
[2m[36m(func pid=75166)[0m f1_weighted: 0.27989487853655887
[2m[36m(func pid=75166)[0m f1_per_class: [0.181, 0.288, 0.458, 0.374, 0.053, 0.355, 0.206, 0.21, 0.084, 0.2]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.447294776119403
[2m[36m(func pid=75223)[0m top5: 0.9183768656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=75223)[0m f1_macro: 0.3987232973822767
[2m[36m(func pid=75223)[0m f1_weighted: 0.461722930104244
[2m[36m(func pid=75223)[0m f1_per_class: [0.53, 0.489, 0.407, 0.549, 0.125, 0.405, 0.412, 0.461, 0.347, 0.262]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.07649253731343283
[2m[36m(func pid=81458)[0m top5: 0.7201492537313433
[2m[36m(func pid=81458)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=81458)[0m f1_macro: 0.047320222014813465
[2m[36m(func pid=81458)[0m f1_weighted: 0.023743227324344186
[2m[36m(func pid=81458)[0m f1_per_class: [0.08, 0.011, 0.013, 0.0, 0.069, 0.0, 0.021, 0.223, 0.0, 0.057]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5121 | Steps: 4 | Val loss: 2.4445 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5711 | Steps: 4 | Val loss: 2.1140 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:01:34 (running for 00:27:59.84)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.51  |      0.241 |                   41 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.312 |      0.399 |                   38 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.512 |      0.303 |                   42 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.914 |      0.047 |                   17 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.39925373134328357
[2m[36m(func pid=75300)[0m top5: 0.8978544776119403
[2m[36m(func pid=75300)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=75300)[0m f1_macro: 0.302931475564962
[2m[36m(func pid=75300)[0m f1_weighted: 0.4070598318336098
[2m[36m(func pid=75300)[0m f1_per_class: [0.364, 0.406, 0.214, 0.512, 0.182, 0.307, 0.449, 0.107, 0.159, 0.329]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4647 | Steps: 4 | Val loss: 2.1977 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2059 | Steps: 4 | Val loss: 1.5626 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
[2m[36m(func pid=75166)[0m top1: 0.2644589552238806
[2m[36m(func pid=75166)[0m top5: 0.7793843283582089
[2m[36m(func pid=75166)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=75166)[0m f1_macro: 0.24191849030905255
[2m[36m(func pid=75166)[0m f1_weighted: 0.27406486854323253
[2m[36m(func pid=75166)[0m f1_per_class: [0.18, 0.309, 0.431, 0.349, 0.06, 0.368, 0.186, 0.233, 0.101, 0.202]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.28638059701492535
[2m[36m(func pid=81458)[0m top5: 0.7411380597014925
[2m[36m(func pid=81458)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=81458)[0m f1_macro: 0.1568607582088491
[2m[36m(func pid=81458)[0m f1_weighted: 0.2918290027783976
[2m[36m(func pid=81458)[0m f1_per_class: [0.042, 0.0, 0.0, 0.314, 0.066, 0.0, 0.578, 0.518, 0.0, 0.05]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4375
[2m[36m(func pid=75223)[0m top5: 0.9160447761194029
[2m[36m(func pid=75223)[0m f1_micro: 0.4375
[2m[36m(func pid=75223)[0m f1_macro: 0.39357013651297323
[2m[36m(func pid=75223)[0m f1_weighted: 0.45960380604974643
[2m[36m(func pid=75223)[0m f1_per_class: [0.522, 0.492, 0.358, 0.508, 0.096, 0.393, 0.439, 0.496, 0.354, 0.276]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6592 | Steps: 4 | Val loss: 3.2026 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5110 | Steps: 4 | Val loss: 2.1197 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=75300)[0m top1: 0.3316231343283582
[2m[36m(func pid=75300)[0m top5: 0.8745335820895522
[2m[36m(func pid=75300)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=75300)[0m f1_macro: 0.2855101702233897
[2m[36m(func pid=75300)[0m f1_weighted: 0.3367182333338612
[2m[36m(func pid=75300)[0m f1_per_class: [0.223, 0.146, 0.167, 0.464, 0.169, 0.331, 0.335, 0.472, 0.185, 0.364]
== Status ==
Current time: 2024-01-07 15:01:39 (running for 00:28:05.30)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.571 |      0.242 |                   42 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.206 |      0.394 |                   39 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.659 |      0.286 |                   43 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.465 |      0.157 |                   18 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7014 | Steps: 4 | Val loss: 1.9345 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75166)[0m top1: 0.2728544776119403
[2m[36m(func pid=75166)[0m top5: 0.7700559701492538
[2m[36m(func pid=75166)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=75166)[0m f1_macro: 0.23872976758181905
[2m[36m(func pid=75166)[0m f1_weighted: 0.2814735111111888
[2m[36m(func pid=75166)[0m f1_per_class: [0.186, 0.29, 0.367, 0.41, 0.055, 0.39, 0.158, 0.233, 0.094, 0.205]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9959 | Steps: 4 | Val loss: 1.5949 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=81458)[0m top1: 0.3530783582089552
[2m[36m(func pid=81458)[0m top5: 0.7565298507462687
[2m[36m(func pid=81458)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=81458)[0m f1_macro: 0.18263998998873635
[2m[36m(func pid=81458)[0m f1_weighted: 0.3266126142634619
[2m[36m(func pid=81458)[0m f1_per_class: [0.038, 0.0, 0.289, 0.544, 0.049, 0.0, 0.563, 0.0, 0.014, 0.328]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2306 | Steps: 4 | Val loss: 4.8216 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=75223)[0m top1: 0.41651119402985076
[2m[36m(func pid=75223)[0m top5: 0.9151119402985075
[2m[36m(func pid=75223)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=75223)[0m f1_macro: 0.3844021338894702
[2m[36m(func pid=75223)[0m f1_weighted: 0.44009769170964996
[2m[36m(func pid=75223)[0m f1_per_class: [0.496, 0.472, 0.348, 0.46, 0.079, 0.408, 0.425, 0.498, 0.361, 0.296]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4685 | Steps: 4 | Val loss: 2.1229 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:01:45 (running for 00:28:10.70)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.511 |      0.239 |                   43 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.996 |      0.384 |                   40 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.231 |      0.149 |                   44 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.701 |      0.183 |                   19 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.19076492537313433
[2m[36m(func pid=75300)[0m top5: 0.7075559701492538
[2m[36m(func pid=75300)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=75300)[0m f1_macro: 0.14906063253144
[2m[36m(func pid=75300)[0m f1_weighted: 0.21518547908481928
[2m[36m(func pid=75300)[0m f1_per_class: [0.24, 0.114, 0.158, 0.408, 0.067, 0.01, 0.223, 0.039, 0.123, 0.107]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6301 | Steps: 4 | Val loss: 1.8846 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=75166)[0m top1: 0.2756529850746269
[2m[36m(func pid=75166)[0m top5: 0.7625932835820896
[2m[36m(func pid=75166)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=75166)[0m f1_macro: 0.24116298574415346
[2m[36m(func pid=75166)[0m f1_weighted: 0.2788139219510629
[2m[36m(func pid=75166)[0m f1_per_class: [0.201, 0.309, 0.4, 0.408, 0.052, 0.397, 0.132, 0.275, 0.072, 0.167]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9435 | Steps: 4 | Val loss: 1.6306 | Batch size: 32 | lr: 0.001 | Duration: 3.32s
[2m[36m(func pid=81458)[0m top1: 0.3894589552238806
[2m[36m(func pid=81458)[0m top5: 0.8138992537313433
[2m[36m(func pid=81458)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=81458)[0m f1_macro: 0.19103794083053044
[2m[36m(func pid=81458)[0m f1_weighted: 0.3480688125305868
[2m[36m(func pid=81458)[0m f1_per_class: [0.044, 0.005, 0.308, 0.556, 0.073, 0.0, 0.616, 0.0, 0.075, 0.233]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6159 | Steps: 4 | Val loss: 4.0046 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4682 | Steps: 4 | Val loss: 2.1271 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=75223)[0m top1: 0.3969216417910448
[2m[36m(func pid=75223)[0m top5: 0.9020522388059702
[2m[36m(func pid=75223)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=75223)[0m f1_macro: 0.3531095202222002
[2m[36m(func pid=75223)[0m f1_weighted: 0.42187842418539556
[2m[36m(func pid=75223)[0m f1_per_class: [0.441, 0.455, 0.238, 0.468, 0.089, 0.41, 0.39, 0.428, 0.33, 0.284]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m top1: 0.20289179104477612
[2m[36m(func pid=75300)[0m top5: 0.7751865671641791
[2m[36m(func pid=75300)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=75300)[0m f1_macro: 0.2140046676378972
[2m[36m(func pid=75300)[0m f1_weighted: 0.24172549311984484
[2m[36m(func pid=75300)[0m f1_per_class: [0.281, 0.185, 0.242, 0.229, 0.045, 0.055, 0.344, 0.391, 0.131, 0.236]
[2m[36m(func pid=75300)[0m 
== Status ==
Current time: 2024-01-07 15:01:50 (running for 00:28:16.18)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.469 |      0.241 |                   44 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.944 |      0.353 |                   41 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.616 |      0.214 |                   45 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.63  |      0.191 |                   20 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3552 | Steps: 4 | Val loss: 1.9072 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=75166)[0m top1: 0.2644589552238806
[2m[36m(func pid=75166)[0m top5: 0.7583955223880597
[2m[36m(func pid=75166)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=75166)[0m f1_macro: 0.2326901415477607
[2m[36m(func pid=75166)[0m f1_weighted: 0.2695887164564175
[2m[36m(func pid=75166)[0m f1_per_class: [0.194, 0.321, 0.361, 0.39, 0.062, 0.37, 0.123, 0.275, 0.068, 0.163]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9071 | Steps: 4 | Val loss: 1.6950 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9632 | Steps: 4 | Val loss: 3.3360 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=81458)[0m top1: 0.3148320895522388
[2m[36m(func pid=81458)[0m top5: 0.7840485074626866
[2m[36m(func pid=81458)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=81458)[0m f1_macro: 0.16858805923932207
[2m[36m(func pid=81458)[0m f1_weighted: 0.3395468173210565
[2m[36m(func pid=81458)[0m f1_per_class: [0.042, 0.252, 0.227, 0.464, 0.073, 0.0, 0.541, 0.0, 0.088, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5095 | Steps: 4 | Val loss: 2.1260 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=75223)[0m top1: 0.38013059701492535
[2m[36m(func pid=75223)[0m top5: 0.8885261194029851
[2m[36m(func pid=75223)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=75223)[0m f1_macro: 0.33586874361614116
[2m[36m(func pid=75223)[0m f1_weighted: 0.40667010290951244
[2m[36m(func pid=75223)[0m f1_per_class: [0.395, 0.458, 0.18, 0.441, 0.084, 0.411, 0.366, 0.436, 0.318, 0.269]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:01:56 (running for 00:28:21.67)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.468 |      0.233 |                   45 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.907 |      0.336 |                   42 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.963 |      0.246 |                   46 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.355 |      0.169 |                   21 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.27052238805970147
[2m[36m(func pid=75300)[0m top5: 0.7635261194029851
[2m[36m(func pid=75300)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=75300)[0m f1_macro: 0.2458272215986325
[2m[36m(func pid=75300)[0m f1_weighted: 0.3087806150759747
[2m[36m(func pid=75300)[0m f1_per_class: [0.286, 0.402, 0.133, 0.153, 0.023, 0.151, 0.479, 0.342, 0.208, 0.28]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3722 | Steps: 4 | Val loss: 2.0987 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=75166)[0m top1: 0.2579291044776119
[2m[36m(func pid=75166)[0m top5: 0.7625932835820896
[2m[36m(func pid=75166)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=75166)[0m f1_macro: 0.2288169956304728
[2m[36m(func pid=75166)[0m f1_weighted: 0.2645898180431694
[2m[36m(func pid=75166)[0m f1_per_class: [0.173, 0.343, 0.333, 0.357, 0.07, 0.336, 0.137, 0.266, 0.093, 0.18]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8849 | Steps: 4 | Val loss: 1.6401 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=81458)[0m top1: 0.24580223880597016
[2m[36m(func pid=81458)[0m top5: 0.7262126865671642
[2m[36m(func pid=81458)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=81458)[0m f1_macro: 0.20095849181216816
[2m[36m(func pid=81458)[0m f1_weighted: 0.31097453289017263
[2m[36m(func pid=81458)[0m f1_per_class: [0.116, 0.376, 0.177, 0.374, 0.035, 0.008, 0.355, 0.516, 0.052, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6646 | Steps: 4 | Val loss: 3.5945 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4401 | Steps: 4 | Val loss: 2.1178 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=75223)[0m top1: 0.38619402985074625
[2m[36m(func pid=75223)[0m top5: 0.9151119402985075
[2m[36m(func pid=75223)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=75223)[0m f1_macro: 0.34791524490816206
[2m[36m(func pid=75223)[0m f1_weighted: 0.41531688142188145
[2m[36m(func pid=75223)[0m f1_per_class: [0.429, 0.437, 0.168, 0.431, 0.086, 0.429, 0.398, 0.458, 0.362, 0.282]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:02:01 (running for 00:28:27.03)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.51  |      0.229 |                   46 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.348 |                   43 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.665 |      0.243 |                   47 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.372 |      0.201 |                   22 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2775186567164179
[2m[36m(func pid=75300)[0m top5: 0.7364738805970149
[2m[36m(func pid=75300)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=75300)[0m f1_macro: 0.24314547198843836
[2m[36m(func pid=75300)[0m f1_weighted: 0.29856214786239377
[2m[36m(func pid=75300)[0m f1_per_class: [0.114, 0.407, 0.051, 0.118, 0.06, 0.265, 0.437, 0.357, 0.183, 0.438]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0437 | Steps: 4 | Val loss: 2.3565 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=75166)[0m top1: 0.2653917910447761
[2m[36m(func pid=75166)[0m top5: 0.7667910447761194
[2m[36m(func pid=75166)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=75166)[0m f1_macro: 0.24072511229029314
[2m[36m(func pid=75166)[0m f1_weighted: 0.26970908593547044
[2m[36m(func pid=75166)[0m f1_per_class: [0.184, 0.313, 0.375, 0.386, 0.065, 0.359, 0.132, 0.27, 0.097, 0.227]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1207 | Steps: 4 | Val loss: 1.5928 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=81458)[0m top1: 0.1357276119402985
[2m[36m(func pid=81458)[0m top5: 0.6618470149253731
[2m[36m(func pid=81458)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=81458)[0m f1_macro: 0.17133877735393477
[2m[36m(func pid=81458)[0m f1_weighted: 0.14618702617092058
[2m[36m(func pid=81458)[0m f1_per_class: [0.124, 0.394, 0.4, 0.122, 0.026, 0.06, 0.0, 0.524, 0.063, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.9675 | Steps: 4 | Val loss: 4.7712 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4814 | Steps: 4 | Val loss: 2.1075 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=75223)[0m top1: 0.40578358208955223
[2m[36m(func pid=75223)[0m top5: 0.9328358208955224
[2m[36m(func pid=75223)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=75223)[0m f1_macro: 0.3754716307831241
[2m[36m(func pid=75223)[0m f1_weighted: 0.4324135356270921
[2m[36m(func pid=75223)[0m f1_per_class: [0.523, 0.445, 0.304, 0.476, 0.077, 0.416, 0.403, 0.462, 0.378, 0.271]
== Status ==
Current time: 2024-01-07 15:02:06 (running for 00:28:32.19)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.44  |      0.241 |                   47 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.121 |      0.375 |                   44 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.665 |      0.243 |                   47 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.044 |      0.171 |                   23 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m top1: 0.27005597014925375
[2m[36m(func pid=75300)[0m top5: 0.7411380597014925
[2m[36m(func pid=75300)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=75300)[0m f1_macro: 0.2362568658075169
[2m[36m(func pid=75300)[0m f1_weighted: 0.3126411393343545
[2m[36m(func pid=75300)[0m f1_per_class: [0.045, 0.329, 0.027, 0.128, 0.211, 0.354, 0.544, 0.025, 0.256, 0.444]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1668 | Steps: 4 | Val loss: 2.1744 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=75166)[0m top1: 0.2719216417910448
[2m[36m(func pid=75166)[0m top5: 0.7719216417910447
[2m[36m(func pid=75166)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=75166)[0m f1_macro: 0.2402903886336732
[2m[36m(func pid=75166)[0m f1_weighted: 0.2787007009447084
[2m[36m(func pid=75166)[0m f1_per_class: [0.191, 0.321, 0.338, 0.407, 0.063, 0.341, 0.144, 0.275, 0.087, 0.235]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8671 | Steps: 4 | Val loss: 1.6013 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3992 | Steps: 4 | Val loss: 3.3733 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=81458)[0m top1: 0.23227611940298507
[2m[36m(func pid=81458)[0m top5: 0.7140858208955224
[2m[36m(func pid=81458)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=81458)[0m f1_macro: 0.20922627488306186
[2m[36m(func pid=81458)[0m f1_weighted: 0.23547671501628795
[2m[36m(func pid=81458)[0m f1_per_class: [0.105, 0.33, 0.545, 0.493, 0.027, 0.054, 0.0, 0.49, 0.027, 0.02]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4693 | Steps: 4 | Val loss: 2.0964 | Batch size: 32 | lr: 0.0001 | Duration: 3.33s
== Status ==
Current time: 2024-01-07 15:02:12 (running for 00:28:37.77)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.481 |      0.24  |                   48 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  1.121 |      0.375 |                   44 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.399 |      0.199 |                   49 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.167 |      0.209 |                   24 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3031716417910448
[2m[36m(func pid=75300)[0m top5: 0.7569962686567164
[2m[36m(func pid=75300)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=75300)[0m f1_macro: 0.19887984728849228
[2m[36m(func pid=75300)[0m f1_weighted: 0.31932862787634847
[2m[36m(func pid=75300)[0m f1_per_class: [0.199, 0.484, 0.07, 0.167, 0.147, 0.199, 0.527, 0.012, 0.051, 0.132]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4244402985074627
[2m[36m(func pid=75223)[0m top5: 0.9132462686567164
[2m[36m(func pid=75223)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=75223)[0m f1_macro: 0.3728924168862917
[2m[36m(func pid=75223)[0m f1_weighted: 0.4471655117771618
[2m[36m(func pid=75223)[0m f1_per_class: [0.492, 0.479, 0.279, 0.534, 0.097, 0.395, 0.396, 0.423, 0.371, 0.261]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.0955 | Steps: 4 | Val loss: 2.1962 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=75166)[0m top1: 0.27705223880597013
[2m[36m(func pid=75166)[0m top5: 0.7793843283582089
[2m[36m(func pid=75166)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=75166)[0m f1_macro: 0.24395615180872127
[2m[36m(func pid=75166)[0m f1_weighted: 0.2826568816488454
[2m[36m(func pid=75166)[0m f1_per_class: [0.199, 0.318, 0.381, 0.427, 0.071, 0.357, 0.138, 0.256, 0.092, 0.202]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4930 | Steps: 4 | Val loss: 4.1072 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=81458)[0m top1: 0.29757462686567165
[2m[36m(func pid=81458)[0m top5: 0.6833022388059702
[2m[36m(func pid=81458)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=81458)[0m f1_macro: 0.19465707716105546
[2m[36m(func pid=81458)[0m f1_weighted: 0.2666972465671987
[2m[36m(func pid=81458)[0m f1_per_class: [0.098, 0.435, 0.219, 0.558, 0.025, 0.006, 0.0, 0.541, 0.0, 0.065]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9331 | Steps: 4 | Val loss: 1.9484 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4042 | Steps: 4 | Val loss: 2.0931 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:02:17 (running for 00:28:43.01)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.469 |      0.244 |                   49 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.867 |      0.373 |                   45 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.493 |      0.205 |                   50 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.096 |      0.195 |                   25 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.27705223880597013
[2m[36m(func pid=75300)[0m top5: 0.7360074626865671
[2m[36m(func pid=75300)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=75300)[0m f1_macro: 0.20532375664043476
[2m[36m(func pid=75300)[0m f1_weighted: 0.26434793500281384
[2m[36m(func pid=75300)[0m f1_per_class: [0.174, 0.469, 0.041, 0.057, 0.161, 0.407, 0.341, 0.166, 0.105, 0.132]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0136 | Steps: 4 | Val loss: 2.2534 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=75223)[0m top1: 0.32369402985074625
[2m[36m(func pid=75223)[0m top5: 0.808768656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=75223)[0m f1_macro: 0.2640985240696141
[2m[36m(func pid=75223)[0m f1_weighted: 0.3512165405180502
[2m[36m(func pid=75223)[0m f1_per_class: [0.357, 0.436, 0.132, 0.519, 0.104, 0.228, 0.238, 0.289, 0.217, 0.12]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.27705223880597013
[2m[36m(func pid=75166)[0m top5: 0.7854477611940298
[2m[36m(func pid=75166)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=75166)[0m f1_macro: 0.24676764343731716
[2m[36m(func pid=75166)[0m f1_weighted: 0.28574273094824476
[2m[36m(func pid=75166)[0m f1_per_class: [0.184, 0.309, 0.393, 0.396, 0.066, 0.387, 0.17, 0.265, 0.081, 0.216]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5704 | Steps: 4 | Val loss: 3.8782 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=81458)[0m top1: 0.2644589552238806
[2m[36m(func pid=81458)[0m top5: 0.6529850746268657
[2m[36m(func pid=81458)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=81458)[0m f1_macro: 0.18978514424769072
[2m[36m(func pid=81458)[0m f1_weighted: 0.27464397479438024
[2m[36m(func pid=81458)[0m f1_per_class: [0.073, 0.455, 0.64, 0.391, 0.019, 0.073, 0.246, 0.0, 0.0, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8291 | Steps: 4 | Val loss: 1.9456 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 15:02:22 (running for 00:28:48.18)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.404 |      0.247 |                   50 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.933 |      0.264 |                   46 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.57  |      0.192 |                   51 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.014 |      0.19  |                   26 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.25093283582089554
[2m[36m(func pid=75300)[0m top5: 0.7649253731343284
[2m[36m(func pid=75300)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=75300)[0m f1_macro: 0.19227748952226856
[2m[36m(func pid=75300)[0m f1_weighted: 0.2493114240343439
[2m[36m(func pid=75300)[0m f1_per_class: [0.199, 0.417, 0.194, 0.15, 0.12, 0.21, 0.331, 0.014, 0.124, 0.164]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4413 | Steps: 4 | Val loss: 2.0923 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0364 | Steps: 4 | Val loss: 2.2084 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=75223)[0m top1: 0.2957089552238806
[2m[36m(func pid=75223)[0m top5: 0.840018656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=75223)[0m f1_macro: 0.27000277065600786
[2m[36m(func pid=75223)[0m f1_weighted: 0.33346156527239523
[2m[36m(func pid=75223)[0m f1_per_class: [0.376, 0.407, 0.097, 0.345, 0.071, 0.298, 0.312, 0.37, 0.229, 0.195]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.27658582089552236
[2m[36m(func pid=75166)[0m top5: 0.7868470149253731
[2m[36m(func pid=75166)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=75166)[0m f1_macro: 0.24233211343142388
[2m[36m(func pid=75166)[0m f1_weighted: 0.2822238699446756
[2m[36m(func pid=75166)[0m f1_per_class: [0.18, 0.335, 0.344, 0.403, 0.083, 0.336, 0.152, 0.273, 0.11, 0.208]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0610 | Steps: 4 | Val loss: 3.3621 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=81458)[0m top1: 0.18097014925373134
[2m[36m(func pid=81458)[0m top5: 0.6856343283582089
[2m[36m(func pid=81458)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=81458)[0m f1_macro: 0.16786757191088844
[2m[36m(func pid=81458)[0m f1_weighted: 0.19768804549983415
[2m[36m(func pid=81458)[0m f1_per_class: [0.095, 0.218, 0.645, 0.041, 0.055, 0.14, 0.422, 0.0, 0.0, 0.063]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.8382 | Steps: 4 | Val loss: 1.7702 | Batch size: 32 | lr: 0.001 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 15:02:27 (running for 00:28:53.53)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.441 |      0.242 |                   51 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.829 |      0.27  |                   47 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.061 |      0.248 |                   52 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.036 |      0.168 |                   27 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.27611940298507465
[2m[36m(func pid=75300)[0m top5: 0.898320895522388
[2m[36m(func pid=75300)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=75300)[0m f1_macro: 0.24826402753401502
[2m[36m(func pid=75300)[0m f1_weighted: 0.26902369405450927
[2m[36m(func pid=75300)[0m f1_per_class: [0.382, 0.332, 0.0, 0.33, 0.089, 0.288, 0.156, 0.353, 0.234, 0.317]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4801 | Steps: 4 | Val loss: 2.1061 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0875 | Steps: 4 | Val loss: 2.8855 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=75223)[0m top1: 0.34888059701492535
[2m[36m(func pid=75223)[0m top5: 0.8959888059701493
[2m[36m(func pid=75223)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=75223)[0m f1_macro: 0.3374857691956172
[2m[36m(func pid=75223)[0m f1_weighted: 0.38744382718783543
[2m[36m(func pid=75223)[0m f1_per_class: [0.468, 0.431, 0.1, 0.319, 0.066, 0.382, 0.429, 0.457, 0.333, 0.389]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2708 | Steps: 4 | Val loss: 3.5317 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=75166)[0m top1: 0.25886194029850745
[2m[36m(func pid=75166)[0m top5: 0.773320895522388
[2m[36m(func pid=75166)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=75166)[0m f1_macro: 0.23725477104296555
[2m[36m(func pid=75166)[0m f1_weighted: 0.2707711646006996
[2m[36m(func pid=75166)[0m f1_per_class: [0.17, 0.314, 0.349, 0.342, 0.08, 0.342, 0.183, 0.274, 0.093, 0.226]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.12826492537313433
[2m[36m(func pid=81458)[0m top5: 0.5904850746268657
[2m[36m(func pid=81458)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=81458)[0m f1_macro: 0.11959506950570901
[2m[36m(func pid=81458)[0m f1_weighted: 0.0990636810353466
[2m[36m(func pid=81458)[0m f1_per_class: [0.106, 0.204, 0.0, 0.0, 0.068, 0.188, 0.05, 0.393, 0.0, 0.188]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:02:33 (running for 00:28:58.77)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.48  |      0.237 |                   52 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.838 |      0.337 |                   48 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.271 |      0.213 |                   53 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.087 |      0.12  |                   28 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.314365671641791
[2m[36m(func pid=75300)[0m top5: 0.8810634328358209
[2m[36m(func pid=75300)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=75300)[0m f1_macro: 0.21336893854449154
[2m[36m(func pid=75300)[0m f1_weighted: 0.3003875976521814
[2m[36m(func pid=75300)[0m f1_per_class: [0.176, 0.285, 0.0, 0.457, 0.07, 0.281, 0.197, 0.38, 0.168, 0.119]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7207 | Steps: 4 | Val loss: 1.5963 | Batch size: 32 | lr: 0.001 | Duration: 3.34s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4507 | Steps: 4 | Val loss: 2.1093 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3728 | Steps: 4 | Val loss: 3.6567 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=75223)[0m top1: 0.4253731343283582
[2m[36m(func pid=75223)[0m top5: 0.9146455223880597
[2m[36m(func pid=75223)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=75223)[0m f1_macro: 0.3953534363770711
[2m[36m(func pid=75223)[0m f1_weighted: 0.45501854638515676
[2m[36m(func pid=75223)[0m f1_per_class: [0.53, 0.458, 0.255, 0.456, 0.082, 0.42, 0.477, 0.502, 0.361, 0.412]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6812 | Steps: 4 | Val loss: 3.7725 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=75166)[0m top1: 0.26026119402985076
[2m[36m(func pid=75166)[0m top5: 0.7714552238805971
[2m[36m(func pid=75166)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=75166)[0m f1_macro: 0.23999006665545836
[2m[36m(func pid=75166)[0m f1_weighted: 0.2705583928648541
[2m[36m(func pid=75166)[0m f1_per_class: [0.159, 0.306, 0.367, 0.361, 0.059, 0.387, 0.151, 0.28, 0.097, 0.233]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.15391791044776118
[2m[36m(func pid=81458)[0m top5: 0.6161380597014925
[2m[36m(func pid=81458)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=81458)[0m f1_macro: 0.10289122143549918
[2m[36m(func pid=81458)[0m f1_weighted: 0.07565948233306788
[2m[36m(func pid=81458)[0m f1_per_class: [0.067, 0.052, 0.06, 0.0, 0.0, 0.308, 0.0, 0.508, 0.0, 0.034]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:02:38 (running for 00:29:04.11)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.451 |      0.24  |                   53 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.721 |      0.395 |                   49 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.681 |      0.226 |                   54 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.373 |      0.103 |                   29 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3045708955223881
[2m[36m(func pid=75300)[0m top5: 0.867070895522388
[2m[36m(func pid=75300)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=75300)[0m f1_macro: 0.2261218736653484
[2m[36m(func pid=75300)[0m f1_weighted: 0.2897336283294941
[2m[36m(func pid=75300)[0m f1_per_class: [0.198, 0.32, 0.0, 0.444, 0.087, 0.392, 0.112, 0.32, 0.224, 0.164]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8715 | Steps: 4 | Val loss: 1.5274 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3862 | Steps: 4 | Val loss: 2.0911 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3340 | Steps: 4 | Val loss: 3.2588 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.8961 | Steps: 4 | Val loss: 4.9457 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=75223)[0m top1: 0.47388059701492535
[2m[36m(func pid=75223)[0m top5: 0.9085820895522388
[2m[36m(func pid=75223)[0m f1_micro: 0.47388059701492535
[2m[36m(func pid=75223)[0m f1_macro: 0.4288735038188426
[2m[36m(func pid=75223)[0m f1_weighted: 0.49718330558651375
[2m[36m(func pid=75223)[0m f1_per_class: [0.487, 0.524, 0.49, 0.54, 0.102, 0.401, 0.51, 0.517, 0.31, 0.408]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.2775186567164179
[2m[36m(func pid=75166)[0m top5: 0.7826492537313433
[2m[36m(func pid=75166)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=75166)[0m f1_macro: 0.247871621029005
[2m[36m(func pid=75166)[0m f1_weighted: 0.2852307889409595
[2m[36m(func pid=75166)[0m f1_per_class: [0.163, 0.309, 0.344, 0.391, 0.075, 0.403, 0.162, 0.287, 0.1, 0.246]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.15438432835820895
[2m[36m(func pid=81458)[0m top5: 0.683768656716418
[2m[36m(func pid=81458)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=81458)[0m f1_macro: 0.0838813142847904
[2m[36m(func pid=81458)[0m f1_weighted: 0.06432148928873488
[2m[36m(func pid=81458)[0m f1_per_class: [0.058, 0.0, 0.0, 0.0, 0.0, 0.34, 0.0, 0.414, 0.0, 0.027]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:02:43 (running for 00:29:09.54)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.386 |      0.248 |                   54 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.872 |      0.429 |                   50 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.896 |      0.242 |                   55 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.334 |      0.084 |                   30 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.279384328358209
[2m[36m(func pid=75300)[0m top5: 0.8395522388059702
[2m[36m(func pid=75300)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=75300)[0m f1_macro: 0.24155350837354916
[2m[36m(func pid=75300)[0m f1_weighted: 0.26958298349873777
[2m[36m(func pid=75300)[0m f1_per_class: [0.214, 0.35, 0.015, 0.352, 0.088, 0.365, 0.107, 0.316, 0.328, 0.28]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4874 | Steps: 4 | Val loss: 2.0858 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.4122 | Steps: 4 | Val loss: 2.7570 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.9719 | Steps: 4 | Val loss: 1.5168 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7999 | Steps: 4 | Val loss: 4.5529 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=75166)[0m top1: 0.291044776119403
[2m[36m(func pid=75166)[0m top5: 0.7854477611940298
[2m[36m(func pid=75166)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=75166)[0m f1_macro: 0.262025888772293
[2m[36m(func pid=75166)[0m f1_weighted: 0.29258041790162626
[2m[36m(func pid=75166)[0m f1_per_class: [0.185, 0.338, 0.387, 0.404, 0.076, 0.422, 0.145, 0.293, 0.102, 0.268]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.12639925373134328
[2m[36m(func pid=81458)[0m top5: 0.6800373134328358
[2m[36m(func pid=81458)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=81458)[0m f1_macro: 0.09638581396717494
[2m[36m(func pid=81458)[0m f1_weighted: 0.13953788212683527
[2m[36m(func pid=81458)[0m f1_per_class: [0.081, 0.041, 0.0, 0.0, 0.051, 0.166, 0.328, 0.173, 0.096, 0.027]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4743470149253731
[2m[36m(func pid=75223)[0m top5: 0.909981343283582
[2m[36m(func pid=75223)[0m f1_micro: 0.4743470149253731
[2m[36m(func pid=75223)[0m f1_macro: 0.428230483245728
[2m[36m(func pid=75223)[0m f1_weighted: 0.49691459816272093
[2m[36m(func pid=75223)[0m f1_per_class: [0.495, 0.529, 0.6, 0.54, 0.118, 0.369, 0.53, 0.459, 0.291, 0.351]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:02:49 (running for 00:29:15.14)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.487 |      0.262 |                   55 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.972 |      0.428 |                   51 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.8   |      0.253 |                   56 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.412 |      0.096 |                   31 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.3148320895522388
[2m[36m(func pid=75300)[0m top5: 0.8264925373134329
[2m[36m(func pid=75300)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=75300)[0m f1_macro: 0.25333666644394015
[2m[36m(func pid=75300)[0m f1_weighted: 0.32028016858968894
[2m[36m(func pid=75300)[0m f1_per_class: [0.062, 0.373, 0.012, 0.24, 0.122, 0.412, 0.354, 0.364, 0.294, 0.301]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3477 | Steps: 4 | Val loss: 2.0796 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0975 | Steps: 4 | Val loss: 2.3698 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6836 | Steps: 4 | Val loss: 1.5882 | Batch size: 32 | lr: 0.001 | Duration: 3.38s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3801 | Steps: 4 | Val loss: 3.8959 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=75166)[0m top1: 0.2947761194029851
[2m[36m(func pid=75166)[0m top5: 0.7868470149253731
[2m[36m(func pid=75166)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=75166)[0m f1_macro: 0.26308037879303486
[2m[36m(func pid=75166)[0m f1_weighted: 0.29515616485557494
[2m[36m(func pid=75166)[0m f1_per_class: [0.194, 0.324, 0.4, 0.411, 0.076, 0.45, 0.147, 0.283, 0.099, 0.246]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.12080223880597014
[2m[36m(func pid=81458)[0m top5: 0.71875
[2m[36m(func pid=81458)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=81458)[0m f1_macro: 0.06536116642815581
[2m[36m(func pid=81458)[0m f1_weighted: 0.13322554644144216
[2m[36m(func pid=81458)[0m f1_per_class: [0.075, 0.036, 0.0, 0.023, 0.04, 0.0, 0.389, 0.0, 0.091, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4505597014925373
[2m[36m(func pid=75223)[0m top5: 0.8964552238805971
[2m[36m(func pid=75223)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=75223)[0m f1_macro: 0.39073842946522724
[2m[36m(func pid=75223)[0m f1_weighted: 0.4727405529931504
[2m[36m(func pid=75223)[0m f1_per_class: [0.455, 0.533, 0.453, 0.544, 0.125, 0.371, 0.471, 0.362, 0.268, 0.326]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:02:54 (running for 00:29:20.44)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.348 |      0.263 |                   56 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.684 |      0.391 |                   52 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.38  |      0.206 |                   57 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.098 |      0.065 |                   32 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2943097014925373
[2m[36m(func pid=75300)[0m top5: 0.832089552238806
[2m[36m(func pid=75300)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=75300)[0m f1_macro: 0.20553437138976222
[2m[36m(func pid=75300)[0m f1_weighted: 0.3286383475223217
[2m[36m(func pid=75300)[0m f1_per_class: [0.0, 0.076, 0.042, 0.302, 0.095, 0.317, 0.581, 0.143, 0.309, 0.189]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4863 | Steps: 4 | Val loss: 2.0537 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3451 | Steps: 4 | Val loss: 2.0867 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6049 | Steps: 4 | Val loss: 1.5598 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3822 | Steps: 4 | Val loss: 3.5793 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=81458)[0m top1: 0.22061567164179105
[2m[36m(func pid=81458)[0m top5: 0.7901119402985075
[2m[36m(func pid=81458)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=81458)[0m f1_macro: 0.10727897868897947
[2m[36m(func pid=81458)[0m f1_weighted: 0.22725922399255616
[2m[36m(func pid=81458)[0m f1_per_class: [0.089, 0.211, 0.0, 0.057, 0.054, 0.0, 0.57, 0.0, 0.093, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m top1: 0.2887126865671642
[2m[36m(func pid=75166)[0m top5: 0.7728544776119403
[2m[36m(func pid=75166)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=75166)[0m f1_macro: 0.26122643950700775
[2m[36m(func pid=75166)[0m f1_weighted: 0.2906678939354922
[2m[36m(func pid=75166)[0m f1_per_class: [0.201, 0.318, 0.393, 0.415, 0.059, 0.423, 0.138, 0.293, 0.115, 0.256]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.45242537313432835
[2m[36m(func pid=75223)[0m top5: 0.914179104477612
[2m[36m(func pid=75223)[0m f1_micro: 0.45242537313432835
[2m[36m(func pid=75223)[0m f1_macro: 0.39086999291097546
[2m[36m(func pid=75223)[0m f1_weighted: 0.48201642754398455
[2m[36m(func pid=75223)[0m f1_per_class: [0.491, 0.509, 0.26, 0.516, 0.12, 0.405, 0.501, 0.483, 0.332, 0.292]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:03:00 (running for 00:29:25.70)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.345 |      0.261 |                   57 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.605 |      0.391 |                   53 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.382 |      0.191 |                   58 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.486 |      0.107 |                   33 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.30223880597014924
[2m[36m(func pid=75300)[0m top5: 0.804570895522388
[2m[36m(func pid=75300)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=75300)[0m f1_macro: 0.1907616978218595
[2m[36m(func pid=75300)[0m f1_weighted: 0.32929191630708154
[2m[36m(func pid=75300)[0m f1_per_class: [0.0, 0.091, 0.078, 0.466, 0.065, 0.27, 0.47, 0.016, 0.25, 0.201]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2954 | Steps: 4 | Val loss: 2.1276 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3708 | Steps: 4 | Val loss: 2.0860 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7532 | Steps: 4 | Val loss: 1.5728 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1927 | Steps: 4 | Val loss: 3.6211 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=81458)[0m top1: 0.269589552238806
[2m[36m(func pid=81458)[0m top5: 0.746268656716418
[2m[36m(func pid=81458)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=81458)[0m f1_macro: 0.16769314230856022
[2m[36m(func pid=81458)[0m f1_weighted: 0.31474072252551305
[2m[36m(func pid=81458)[0m f1_per_class: [0.067, 0.284, 0.316, 0.434, 0.044, 0.008, 0.464, 0.0, 0.06, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m top1: 0.28591417910447764
[2m[36m(func pid=75166)[0m top5: 0.78125
[2m[36m(func pid=75166)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=75166)[0m f1_macro: 0.25648482372082426
[2m[36m(func pid=75166)[0m f1_weighted: 0.2871123161152664
[2m[36m(func pid=75166)[0m f1_per_class: [0.206, 0.313, 0.387, 0.416, 0.058, 0.419, 0.133, 0.281, 0.105, 0.246]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.24440298507462688
[2m[36m(func pid=75300)[0m top5: 0.8218283582089553
[2m[36m(func pid=75300)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=75300)[0m f1_macro: 0.17326964095848346
[2m[36m(func pid=75300)[0m f1_weighted: 0.2356416932362019
[2m[36m(func pid=75300)[0m f1_per_class: [0.151, 0.144, 0.118, 0.421, 0.072, 0.19, 0.191, 0.016, 0.21, 0.221]
[2m[36m(func pid=75300)[0m 
== Status ==
Current time: 2024-01-07 15:03:05 (running for 00:29:31.07)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.371 |      0.256 |                   58 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.605 |      0.391 |                   53 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.193 |      0.173 |                   59 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.295 |      0.168 |                   34 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.43236940298507465
[2m[36m(func pid=75223)[0m top5: 0.9174440298507462
[2m[36m(func pid=75223)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=75223)[0m f1_macro: 0.37714955729461164
[2m[36m(func pid=75223)[0m f1_weighted: 0.45476103208722385
[2m[36m(func pid=75223)[0m f1_per_class: [0.444, 0.499, 0.277, 0.417, 0.118, 0.424, 0.503, 0.492, 0.33, 0.267]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3322 | Steps: 4 | Val loss: 2.0807 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.0223 | Steps: 4 | Val loss: 2.2827 | Batch size: 32 | lr: 0.1 | Duration: 3.30s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7176 | Steps: 4 | Val loss: 4.2874 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6940 | Steps: 4 | Val loss: 1.5992 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
[2m[36m(func pid=75166)[0m top1: 0.292910447761194
[2m[36m(func pid=75166)[0m top5: 0.7770522388059702
[2m[36m(func pid=75166)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=75166)[0m f1_macro: 0.2650515515792272
[2m[36m(func pid=75166)[0m f1_weighted: 0.28835312909082994
[2m[36m(func pid=75166)[0m f1_per_class: [0.209, 0.338, 0.407, 0.42, 0.061, 0.408, 0.116, 0.3, 0.122, 0.269]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.24673507462686567
[2m[36m(func pid=81458)[0m top5: 0.7140858208955224
[2m[36m(func pid=81458)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=81458)[0m f1_macro: 0.19544481708681968
[2m[36m(func pid=81458)[0m f1_weighted: 0.310249040003744
[2m[36m(func pid=81458)[0m f1_per_class: [0.069, 0.227, 0.097, 0.414, 0.04, 0.099, 0.372, 0.477, 0.088, 0.071]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:03:10 (running for 00:29:36.47)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.332 |      0.265 |                   59 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.753 |      0.377 |                   54 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.718 |      0.191 |                   60 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.022 |      0.195 |                   35 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.20615671641791045
[2m[36m(func pid=75300)[0m top5: 0.8218283582089553
[2m[36m(func pid=75300)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=75300)[0m f1_macro: 0.19054018294117706
[2m[36m(func pid=75300)[0m f1_weighted: 0.19862062261460775
[2m[36m(func pid=75300)[0m f1_per_class: [0.264, 0.188, 0.146, 0.341, 0.131, 0.037, 0.123, 0.225, 0.217, 0.233]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4183768656716418
[2m[36m(func pid=75223)[0m top5: 0.9118470149253731
[2m[36m(func pid=75223)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=75223)[0m f1_macro: 0.3646799366785835
[2m[36m(func pid=75223)[0m f1_weighted: 0.43479835675325074
[2m[36m(func pid=75223)[0m f1_per_class: [0.371, 0.509, 0.257, 0.379, 0.117, 0.448, 0.462, 0.49, 0.326, 0.288]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3260 | Steps: 4 | Val loss: 2.0813 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2542 | Steps: 4 | Val loss: 2.4474 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8616 | Steps: 4 | Val loss: 4.2645 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9368 | Steps: 4 | Val loss: 1.6913 | Batch size: 32 | lr: 0.001 | Duration: 3.37s
[2m[36m(func pid=75166)[0m top1: 0.2868470149253731
[2m[36m(func pid=75166)[0m top5: 0.7817164179104478
[2m[36m(func pid=75166)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=75166)[0m f1_macro: 0.2587610998989637
[2m[36m(func pid=75166)[0m f1_weighted: 0.2837020556640526
[2m[36m(func pid=75166)[0m f1_per_class: [0.179, 0.334, 0.364, 0.396, 0.071, 0.427, 0.122, 0.306, 0.101, 0.288]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m top1: 0.13992537313432835
[2m[36m(func pid=81458)[0m top5: 0.7098880597014925
[2m[36m(func pid=81458)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=81458)[0m f1_macro: 0.1216996963732091
[2m[36m(func pid=81458)[0m f1_weighted: 0.14492691520098308
[2m[36m(func pid=81458)[0m f1_per_class: [0.092, 0.005, 0.086, 0.291, 0.0, 0.132, 0.054, 0.467, 0.051, 0.038]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:03:16 (running for 00:29:41.73)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.326 |      0.259 |                   60 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.694 |      0.365 |                   55 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.862 |      0.147 |                   61 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.254 |      0.122 |                   36 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.20755597014925373
[2m[36m(func pid=75300)[0m top5: 0.7378731343283582
[2m[36m(func pid=75300)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=75300)[0m f1_macro: 0.1465728489539805
[2m[36m(func pid=75300)[0m f1_weighted: 0.20914961358154271
[2m[36m(func pid=75300)[0m f1_per_class: [0.082, 0.183, 0.168, 0.412, 0.092, 0.027, 0.156, 0.069, 0.118, 0.159]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.36240671641791045
[2m[36m(func pid=75223)[0m top5: 0.9039179104477612
[2m[36m(func pid=75223)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=75223)[0m f1_macro: 0.32528767233053263
[2m[36m(func pid=75223)[0m f1_weighted: 0.36512415626694456
[2m[36m(func pid=75223)[0m f1_per_class: [0.348, 0.496, 0.22, 0.366, 0.13, 0.403, 0.289, 0.367, 0.341, 0.291]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3625 | Steps: 4 | Val loss: 2.0717 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2129 | Steps: 4 | Val loss: 2.7282 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5856 | Steps: 4 | Val loss: 3.5671 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=81458)[0m top1: 0.08069029850746269
[2m[36m(func pid=81458)[0m top5: 0.6520522388059702
[2m[36m(func pid=81458)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=81458)[0m f1_macro: 0.09669443441976316
[2m[36m(func pid=81458)[0m f1_weighted: 0.06255521064865696
[2m[36m(func pid=81458)[0m f1_per_class: [0.102, 0.0, 0.093, 0.052, 0.0, 0.071, 0.015, 0.509, 0.088, 0.037]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m top1: 0.2994402985074627
[2m[36m(func pid=75166)[0m top5: 0.784981343283582
[2m[36m(func pid=75166)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=75166)[0m f1_macro: 0.27202419718106075
[2m[36m(func pid=75166)[0m f1_weighted: 0.2934742484799944
[2m[36m(func pid=75166)[0m f1_per_class: [0.207, 0.362, 0.436, 0.408, 0.073, 0.434, 0.121, 0.307, 0.104, 0.268]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6617 | Steps: 4 | Val loss: 1.6816 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 15:03:21 (running for 00:29:47.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.363 |      0.272 |                   61 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.937 |      0.325 |                   56 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.586 |      0.244 |                   62 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.213 |      0.097 |                   37 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.2849813432835821
[2m[36m(func pid=75300)[0m top5: 0.7756529850746269
[2m[36m(func pid=75300)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=75300)[0m f1_macro: 0.24350262785653332
[2m[36m(func pid=75300)[0m f1_weighted: 0.3063325198005315
[2m[36m(func pid=75300)[0m f1_per_class: [0.253, 0.234, 0.393, 0.431, 0.239, 0.225, 0.331, 0.155, 0.056, 0.119]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m top1: 0.37406716417910446
[2m[36m(func pid=75223)[0m top5: 0.8978544776119403
[2m[36m(func pid=75223)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=75223)[0m f1_macro: 0.3320579095357065
[2m[36m(func pid=75223)[0m f1_weighted: 0.3775813245716975
[2m[36m(func pid=75223)[0m f1_per_class: [0.346, 0.51, 0.193, 0.43, 0.128, 0.414, 0.254, 0.374, 0.386, 0.286]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.9601 | Steps: 4 | Val loss: 2.2152 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.4230 | Steps: 4 | Val loss: 2.0776 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5777 | Steps: 4 | Val loss: 2.8545 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=81458)[0m top1: 0.14039179104477612
[2m[36m(func pid=81458)[0m top5: 0.7560634328358209
[2m[36m(func pid=81458)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=81458)[0m f1_macro: 0.1323145857171864
[2m[36m(func pid=81458)[0m f1_weighted: 0.1236598795502137
[2m[36m(func pid=81458)[0m f1_per_class: [0.092, 0.016, 0.143, 0.081, 0.053, 0.201, 0.153, 0.382, 0.105, 0.098]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m top1: 0.2873134328358209
[2m[36m(func pid=75166)[0m top5: 0.7728544776119403
[2m[36m(func pid=75166)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=75166)[0m f1_macro: 0.2608516479895544
[2m[36m(func pid=75166)[0m f1_weighted: 0.28052002521845776
[2m[36m(func pid=75166)[0m f1_per_class: [0.199, 0.344, 0.4, 0.421, 0.076, 0.413, 0.086, 0.297, 0.105, 0.267]
[2m[36m(func pid=75166)[0m 
== Status ==
Current time: 2024-01-07 15:03:26 (running for 00:29:52.41)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.423 |      0.261 |                   62 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.662 |      0.332 |                   57 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.578 |      0.283 |                   63 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.96  |      0.132 |                   38 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7380 | Steps: 4 | Val loss: 1.6262 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=75300)[0m top1: 0.34841417910447764
[2m[36m(func pid=75300)[0m top5: 0.8535447761194029
[2m[36m(func pid=75300)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=75300)[0m f1_macro: 0.28303416816938765
[2m[36m(func pid=75300)[0m f1_weighted: 0.36513179173670995
[2m[36m(func pid=75300)[0m f1_per_class: [0.187, 0.328, 0.312, 0.389, 0.161, 0.308, 0.459, 0.185, 0.209, 0.292]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1012 | Steps: 4 | Val loss: 2.0525 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3767 | Steps: 4 | Val loss: 2.0709 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=75223)[0m top1: 0.3927238805970149
[2m[36m(func pid=75223)[0m top5: 0.9034514925373134
[2m[36m(func pid=75223)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=75223)[0m f1_macro: 0.35064246984611663
[2m[36m(func pid=75223)[0m f1_weighted: 0.3960916151338439
[2m[36m(func pid=75223)[0m f1_per_class: [0.38, 0.513, 0.228, 0.464, 0.126, 0.411, 0.274, 0.388, 0.416, 0.307]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.0559 | Steps: 4 | Val loss: 2.8980 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=81458)[0m top1: 0.1828358208955224
[2m[36m(func pid=81458)[0m top5: 0.8143656716417911
[2m[36m(func pid=81458)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=81458)[0m f1_macro: 0.1955100699853894
[2m[36m(func pid=81458)[0m f1_weighted: 0.18842101641442455
[2m[36m(func pid=81458)[0m f1_per_class: [0.07, 0.095, 0.64, 0.268, 0.074, 0.083, 0.193, 0.331, 0.133, 0.067]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75166)[0m top1: 0.2943097014925373
[2m[36m(func pid=75166)[0m top5: 0.7826492537313433
[2m[36m(func pid=75166)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=75166)[0m f1_macro: 0.26347882324552374
[2m[36m(func pid=75166)[0m f1_weighted: 0.28718249039220267
[2m[36m(func pid=75166)[0m f1_per_class: [0.197, 0.373, 0.387, 0.418, 0.069, 0.413, 0.093, 0.303, 0.115, 0.266]
[2m[36m(func pid=75166)[0m 
== Status ==
Current time: 2024-01-07 15:03:32 (running for 00:29:57.87)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.377 |      0.263 |                   63 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.738 |      0.351 |                   58 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.056 |      0.276 |                   64 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.101 |      0.196 |                   39 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.33488805970149255
[2m[36m(func pid=75300)[0m top5: 0.8647388059701493
[2m[36m(func pid=75300)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=75300)[0m f1_macro: 0.2759899071906017
[2m[36m(func pid=75300)[0m f1_weighted: 0.37358637350167295
[2m[36m(func pid=75300)[0m f1_per_class: [0.12, 0.31, 0.17, 0.433, 0.081, 0.207, 0.468, 0.382, 0.152, 0.436]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5390 | Steps: 4 | Val loss: 1.6015 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3169 | Steps: 4 | Val loss: 2.1455 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3598 | Steps: 4 | Val loss: 2.0687 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=75223)[0m top1: 0.416044776119403
[2m[36m(func pid=75223)[0m top5: 0.9001865671641791
[2m[36m(func pid=75223)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=75223)[0m f1_macro: 0.3713487590794516
[2m[36m(func pid=75223)[0m f1_weighted: 0.4275808803887244
[2m[36m(func pid=75223)[0m f1_per_class: [0.384, 0.505, 0.306, 0.471, 0.123, 0.398, 0.379, 0.4, 0.395, 0.353]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5149 | Steps: 4 | Val loss: 5.8549 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=81458)[0m top1: 0.19916044776119404
[2m[36m(func pid=81458)[0m top5: 0.7980410447761194
[2m[36m(func pid=81458)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=81458)[0m f1_macro: 0.15289758417555296
[2m[36m(func pid=81458)[0m f1_weighted: 0.21005711817418998
[2m[36m(func pid=81458)[0m f1_per_class: [0.068, 0.101, 0.171, 0.257, 0.076, 0.125, 0.273, 0.359, 0.022, 0.077]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:03:37 (running for 00:30:02.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.36  |      0.247 |                   64 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.539 |      0.371 |                   59 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.056 |      0.276 |                   64 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.317 |      0.153 |                   40 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m top1: 0.2691231343283582
[2m[36m(func pid=75166)[0m top5: 0.7840485074626866
[2m[36m(func pid=75166)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=75166)[0m f1_macro: 0.24744837608312786
[2m[36m(func pid=75166)[0m f1_weighted: 0.2736095344369981
[2m[36m(func pid=75166)[0m f1_per_class: [0.199, 0.325, 0.273, 0.375, 0.056, 0.402, 0.12, 0.315, 0.098, 0.311]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.16371268656716417
[2m[36m(func pid=75300)[0m top5: 0.7700559701492538
[2m[36m(func pid=75300)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=75300)[0m f1_macro: 0.16156553013860325
[2m[36m(func pid=75300)[0m f1_weighted: 0.1965048173064744
[2m[36m(func pid=75300)[0m f1_per_class: [0.0, 0.289, 0.6, 0.394, 0.045, 0.043, 0.077, 0.015, 0.11, 0.043]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6755 | Steps: 4 | Val loss: 1.6073 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.0553 | Steps: 4 | Val loss: 3.1140 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3428 | Steps: 4 | Val loss: 2.0760 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6161 | Steps: 4 | Val loss: 5.4042 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=75223)[0m top1: 0.43470149253731344
[2m[36m(func pid=75223)[0m top5: 0.9001865671641791
[2m[36m(func pid=75223)[0m f1_micro: 0.43470149253731344
[2m[36m(func pid=75223)[0m f1_macro: 0.37434498647565484
[2m[36m(func pid=75223)[0m f1_weighted: 0.4568612520215857
[2m[36m(func pid=75223)[0m f1_per_class: [0.37, 0.504, 0.289, 0.447, 0.136, 0.379, 0.504, 0.456, 0.357, 0.302]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.08768656716417911
[2m[36m(func pid=81458)[0m top5: 0.6548507462686567
[2m[36m(func pid=81458)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=81458)[0m f1_macro: 0.10415858197805887
[2m[36m(func pid=81458)[0m f1_weighted: 0.10314317668887249
[2m[36m(func pid=81458)[0m f1_per_class: [0.072, 0.091, 0.138, 0.0, 0.036, 0.039, 0.192, 0.354, 0.071, 0.05]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:03:42 (running for 00:30:08.44)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.343 |      0.249 |                   65 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.675 |      0.374 |                   60 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.515 |      0.162 |                   65 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.055 |      0.104 |                   41 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m top1: 0.27798507462686567
[2m[36m(func pid=75166)[0m top5: 0.7784514925373134
[2m[36m(func pid=75166)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=75166)[0m f1_macro: 0.24898367554303968
[2m[36m(func pid=75166)[0m f1_weighted: 0.27809453514455607
[2m[36m(func pid=75166)[0m f1_per_class: [0.193, 0.371, 0.289, 0.377, 0.063, 0.404, 0.109, 0.312, 0.077, 0.293]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.177705223880597
[2m[36m(func pid=75300)[0m top5: 0.8255597014925373
[2m[36m(func pid=75300)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=75300)[0m f1_macro: 0.1876123641151022
[2m[36m(func pid=75300)[0m f1_weighted: 0.22758155973649727
[2m[36m(func pid=75300)[0m f1_per_class: [0.044, 0.338, 0.588, 0.29, 0.036, 0.11, 0.214, 0.05, 0.103, 0.103]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5155 | Steps: 4 | Val loss: 1.5627 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.9312 | Steps: 4 | Val loss: 2.4225 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.4038 | Steps: 4 | Val loss: 2.0601 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6097 | Steps: 4 | Val loss: 3.9713 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=75223)[0m top1: 0.44449626865671643
[2m[36m(func pid=75223)[0m top5: 0.9174440298507462
[2m[36m(func pid=75223)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=75223)[0m f1_macro: 0.38567445806524836
[2m[36m(func pid=75223)[0m f1_weighted: 0.46267537166492306
[2m[36m(func pid=75223)[0m f1_per_class: [0.395, 0.503, 0.371, 0.42, 0.126, 0.376, 0.551, 0.426, 0.365, 0.323]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.10914179104477612
[2m[36m(func pid=81458)[0m top5: 0.7653917910447762
[2m[36m(func pid=81458)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=81458)[0m f1_macro: 0.11705625396986089
[2m[36m(func pid=81458)[0m f1_weighted: 0.07138066792245117
[2m[36m(func pid=81458)[0m f1_per_class: [0.067, 0.041, 0.121, 0.0, 0.064, 0.232, 0.009, 0.502, 0.085, 0.049]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:03:48 (running for 00:30:13.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.404 |      0.265 |                   66 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.516 |      0.386 |                   61 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.616 |      0.188 |                   66 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.931 |      0.117 |                   42 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m top1: 0.29244402985074625
[2m[36m(func pid=75166)[0m top5: 0.7882462686567164
[2m[36m(func pid=75166)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=75166)[0m f1_macro: 0.2645116616539417
[2m[36m(func pid=75166)[0m f1_weighted: 0.29295348483771577
[2m[36m(func pid=75166)[0m f1_per_class: [0.2, 0.388, 0.329, 0.396, 0.074, 0.416, 0.124, 0.296, 0.119, 0.304]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.310634328358209
[2m[36m(func pid=75300)[0m top5: 0.8484141791044776
[2m[36m(func pid=75300)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=75300)[0m f1_macro: 0.22029374221663928
[2m[36m(func pid=75300)[0m f1_weighted: 0.34898813036084486
[2m[36m(func pid=75300)[0m f1_per_class: [0.087, 0.413, 0.108, 0.276, 0.043, 0.137, 0.568, 0.056, 0.192, 0.323]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6739 | Steps: 4 | Val loss: 1.5501 | Batch size: 32 | lr: 0.001 | Duration: 3.38s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9825 | Steps: 4 | Val loss: 2.2082 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3609 | Steps: 4 | Val loss: 2.0581 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2427 | Steps: 4 | Val loss: 3.9799 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=75223)[0m top1: 0.4449626865671642
[2m[36m(func pid=75223)[0m top5: 0.9216417910447762
[2m[36m(func pid=75223)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=75223)[0m f1_macro: 0.3712825799298664
[2m[36m(func pid=75223)[0m f1_weighted: 0.4642074547424587
[2m[36m(func pid=75223)[0m f1_per_class: [0.317, 0.508, 0.321, 0.416, 0.145, 0.367, 0.571, 0.423, 0.327, 0.318]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.15578358208955223
[2m[36m(func pid=81458)[0m top5: 0.7779850746268657
[2m[36m(func pid=81458)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=81458)[0m f1_macro: 0.15940050488329421
[2m[36m(func pid=81458)[0m f1_weighted: 0.09375014437546023
[2m[36m(func pid=81458)[0m f1_per_class: [0.097, 0.118, 0.305, 0.0, 0.081, 0.318, 0.0, 0.499, 0.084, 0.092]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:03:53 (running for 00:30:19.20)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.361 |      0.273 |                   67 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.674 |      0.371 |                   62 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.61  |      0.22  |                   67 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.983 |      0.159 |                   43 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75166)[0m top1: 0.292910447761194
[2m[36m(func pid=75166)[0m top5: 0.7980410447761194
[2m[36m(func pid=75166)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=75166)[0m f1_macro: 0.2726265385938281
[2m[36m(func pid=75166)[0m f1_weighted: 0.2879693935618909
[2m[36m(func pid=75166)[0m f1_per_class: [0.186, 0.404, 0.414, 0.371, 0.109, 0.416, 0.12, 0.299, 0.118, 0.29]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.3064365671641791
[2m[36m(func pid=75300)[0m top5: 0.832089552238806
[2m[36m(func pid=75300)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=75300)[0m f1_macro: 0.20700589723031432
[2m[36m(func pid=75300)[0m f1_weighted: 0.3421823622090791
[2m[36m(func pid=75300)[0m f1_per_class: [0.044, 0.536, 0.075, 0.162, 0.035, 0.08, 0.595, 0.14, 0.178, 0.224]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2412 | Steps: 4 | Val loss: 2.1747 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6588 | Steps: 4 | Val loss: 1.5072 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.3927 | Steps: 4 | Val loss: 2.0541 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3328 | Steps: 4 | Val loss: 4.2005 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:03:58 (running for 00:30:24.23)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.361 |      0.273 |                   67 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.674 |      0.371 |                   62 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.243 |      0.207 |                   68 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.241 |      0.167 |                   44 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.2140858208955224
[2m[36m(func pid=81458)[0m top5: 0.7803171641791045
[2m[36m(func pid=81458)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=81458)[0m f1_macro: 0.1668401000424973
[2m[36m(func pid=81458)[0m f1_weighted: 0.20279600025564118
[2m[36m(func pid=81458)[0m f1_per_class: [0.101, 0.317, 0.185, 0.017, 0.103, 0.146, 0.335, 0.343, 0.083, 0.038]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.44869402985074625
[2m[36m(func pid=75223)[0m top5: 0.9291044776119403
[2m[36m(func pid=75223)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=75223)[0m f1_macro: 0.39322192562295605
[2m[36m(func pid=75223)[0m f1_weighted: 0.46528831587792013
[2m[36m(func pid=75223)[0m f1_per_class: [0.329, 0.499, 0.419, 0.401, 0.124, 0.369, 0.569, 0.494, 0.388, 0.339]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m top1: 0.30736940298507465
[2m[36m(func pid=75166)[0m top5: 0.7980410447761194
[2m[36m(func pid=75166)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=75166)[0m f1_macro: 0.28343153561179224
[2m[36m(func pid=75166)[0m f1_weighted: 0.30353361818266245
[2m[36m(func pid=75166)[0m f1_per_class: [0.183, 0.401, 0.421, 0.405, 0.103, 0.438, 0.127, 0.316, 0.16, 0.281]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75300)[0m top1: 0.25
[2m[36m(func pid=75300)[0m top5: 0.7560634328358209
[2m[36m(func pid=75300)[0m f1_micro: 0.25
[2m[36m(func pid=75300)[0m f1_macro: 0.16405274250200094
[2m[36m(func pid=75300)[0m f1_weighted: 0.27454864968034476
[2m[36m(func pid=75300)[0m f1_per_class: [0.044, 0.565, 0.036, 0.026, 0.032, 0.024, 0.536, 0.027, 0.027, 0.323]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3566 | Steps: 4 | Val loss: 2.5908 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2831 | Steps: 4 | Val loss: 2.0457 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8623 | Steps: 4 | Val loss: 5.7915 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5492 | Steps: 4 | Val loss: 1.5225 | Batch size: 32 | lr: 0.001 | Duration: 3.35s
== Status ==
Current time: 2024-01-07 15:04:04 (running for 00:30:29.81)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.393 |      0.283 |                   68 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.659 |      0.393 |                   63 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.333 |      0.164 |                   69 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.357 |      0.132 |                   45 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.16791044776119404
[2m[36m(func pid=81458)[0m top5: 0.7980410447761194
[2m[36m(func pid=81458)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=81458)[0m f1_macro: 0.13211960005786322
[2m[36m(func pid=81458)[0m f1_weighted: 0.15665896890271447
[2m[36m(func pid=81458)[0m f1_per_class: [0.115, 0.355, 0.112, 0.204, 0.145, 0.0, 0.064, 0.22, 0.067, 0.039]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m top1: 0.1166044776119403
[2m[36m(func pid=75300)[0m top5: 0.6665111940298507
[2m[36m(func pid=75300)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=75300)[0m f1_macro: 0.12984442504345053
[2m[36m(func pid=75300)[0m f1_weighted: 0.12729178809074926
[2m[36m(func pid=75300)[0m f1_per_class: [0.175, 0.542, 0.031, 0.016, 0.026, 0.0, 0.055, 0.013, 0.15, 0.289]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.30923507462686567
[2m[36m(func pid=75166)[0m top5: 0.8017723880597015
[2m[36m(func pid=75166)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=75166)[0m f1_macro: 0.287655370626324
[2m[36m(func pid=75166)[0m f1_weighted: 0.307849874017534
[2m[36m(func pid=75166)[0m f1_per_class: [0.201, 0.391, 0.471, 0.404, 0.102, 0.429, 0.15, 0.325, 0.131, 0.273]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.45848880597014924
[2m[36m(func pid=75223)[0m top5: 0.9207089552238806
[2m[36m(func pid=75223)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=75223)[0m f1_macro: 0.3892611044275539
[2m[36m(func pid=75223)[0m f1_weighted: 0.4786977199156488
[2m[36m(func pid=75223)[0m f1_per_class: [0.315, 0.522, 0.283, 0.459, 0.124, 0.319, 0.57, 0.482, 0.362, 0.457]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3792 | Steps: 4 | Val loss: 2.2099 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.0333 | Steps: 4 | Val loss: 6.2100 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3162 | Steps: 4 | Val loss: 2.0336 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4106 | Steps: 4 | Val loss: 1.6275 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=81458)[0m top1: 0.271455223880597
[2m[36m(func pid=81458)[0m top5: 0.8507462686567164
[2m[36m(func pid=81458)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=81458)[0m f1_macro: 0.15242850667737248
[2m[36m(func pid=81458)[0m f1_weighted: 0.22848911556993645
[2m[36m(func pid=81458)[0m f1_per_class: [0.0, 0.185, 0.219, 0.554, 0.178, 0.0, 0.078, 0.246, 0.065, 0.0]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:04:09 (running for 00:30:35.28)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.283 |      0.288 |                   69 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.549 |      0.389 |                   64 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.862 |      0.13  |                   70 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.379 |      0.152 |                   46 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75300)[0m top1: 0.11847014925373134
[2m[36m(func pid=75300)[0m top5: 0.5004664179104478
[2m[36m(func pid=75300)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=75300)[0m f1_macro: 0.12257447419741632
[2m[36m(func pid=75300)[0m f1_weighted: 0.12021539527103844
[2m[36m(func pid=75300)[0m f1_per_class: [0.258, 0.512, 0.083, 0.046, 0.023, 0.0, 0.015, 0.062, 0.122, 0.105]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.31949626865671643
[2m[36m(func pid=75166)[0m top5: 0.8097014925373134
[2m[36m(func pid=75166)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=75166)[0m f1_macro: 0.2988337147688976
[2m[36m(func pid=75166)[0m f1_weighted: 0.31421659272446417
[2m[36m(func pid=75166)[0m f1_per_class: [0.243, 0.385, 0.522, 0.439, 0.089, 0.44, 0.134, 0.31, 0.157, 0.269]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.43843283582089554
[2m[36m(func pid=75223)[0m top5: 0.8973880597014925
[2m[36m(func pid=75223)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=75223)[0m f1_macro: 0.35532261720691716
[2m[36m(func pid=75223)[0m f1_weighted: 0.45998250736828833
[2m[36m(func pid=75223)[0m f1_per_class: [0.295, 0.528, 0.252, 0.487, 0.162, 0.257, 0.537, 0.376, 0.262, 0.396]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3590 | Steps: 4 | Val loss: 2.1129 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3416 | Steps: 4 | Val loss: 3.7441 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2739 | Steps: 4 | Val loss: 2.0343 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4362 | Steps: 4 | Val loss: 1.6781 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=81458)[0m top1: 0.31763059701492535
[2m[36m(func pid=81458)[0m top5: 0.832089552238806
[2m[36m(func pid=81458)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=81458)[0m f1_macro: 0.22835397475776403
[2m[36m(func pid=81458)[0m f1_weighted: 0.3340945031911892
[2m[36m(func pid=81458)[0m f1_per_class: [0.111, 0.474, 0.141, 0.468, 0.235, 0.0, 0.29, 0.484, 0.081, 0.0]
== Status ==
Current time: 2024-01-07 15:04:15 (running for 00:30:40.87)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.316 |      0.299 |                   70 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.411 |      0.355 |                   65 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  1.033 |      0.123 |                   71 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.359 |      0.228 |                   47 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=81458)[0m 

[2m[36m(func pid=75300)[0m top1: 0.27005597014925375
[2m[36m(func pid=75300)[0m top5: 0.7476679104477612
[2m[36m(func pid=75300)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=75300)[0m f1_macro: 0.22851843749912543
[2m[36m(func pid=75300)[0m f1_weighted: 0.27959495372885107
[2m[36m(func pid=75300)[0m f1_per_class: [0.221, 0.519, 0.414, 0.126, 0.056, 0.224, 0.381, 0.029, 0.15, 0.165]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.3185634328358209
[2m[36m(func pid=75166)[0m top5: 0.808768656716418
[2m[36m(func pid=75166)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=75166)[0m f1_macro: 0.29676443423175297
[2m[36m(func pid=75166)[0m f1_weighted: 0.32302638478145973
[2m[36m(func pid=75166)[0m f1_per_class: [0.259, 0.373, 0.48, 0.449, 0.073, 0.386, 0.179, 0.317, 0.169, 0.281]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4281716417910448
[2m[36m(func pid=75223)[0m top5: 0.8838619402985075
[2m[36m(func pid=75223)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=75223)[0m f1_macro: 0.33816401346959124
[2m[36m(func pid=75223)[0m f1_weighted: 0.45110830852262607
[2m[36m(func pid=75223)[0m f1_per_class: [0.346, 0.519, 0.243, 0.516, 0.149, 0.199, 0.523, 0.309, 0.232, 0.347]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3168 | Steps: 4 | Val loss: 2.0995 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4892 | Steps: 4 | Val loss: 3.9940 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3373 | Steps: 4 | Val loss: 2.0431 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3473 | Steps: 4 | Val loss: 1.5816 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 15:04:20 (running for 00:30:46.26)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.274 |      0.297 |                   71 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.436 |      0.338 |                   66 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.342 |      0.229 |                   72 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.317 |      0.188 |                   48 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.279384328358209
[2m[36m(func pid=81458)[0m top5: 0.7873134328358209
[2m[36m(func pid=81458)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=81458)[0m f1_macro: 0.18819430885475533
[2m[36m(func pid=81458)[0m f1_weighted: 0.2661724403169513
[2m[36m(func pid=81458)[0m f1_per_class: [0.117, 0.466, 0.132, 0.038, 0.058, 0.0, 0.467, 0.507, 0.097, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m top1: 0.28451492537313433
[2m[36m(func pid=75300)[0m top5: 0.7761194029850746
[2m[36m(func pid=75300)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=75300)[0m f1_macro: 0.2163721080223105
[2m[36m(func pid=75300)[0m f1_weighted: 0.2777842858804139
[2m[36m(func pid=75300)[0m f1_per_class: [0.176, 0.471, 0.169, 0.137, 0.109, 0.375, 0.338, 0.045, 0.137, 0.206]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.310634328358209
[2m[36m(func pid=75166)[0m top5: 0.7966417910447762
[2m[36m(func pid=75166)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=75166)[0m f1_macro: 0.28684360124633
[2m[36m(func pid=75166)[0m f1_weighted: 0.3128843909076637
[2m[36m(func pid=75166)[0m f1_per_class: [0.237, 0.366, 0.429, 0.433, 0.085, 0.392, 0.163, 0.335, 0.158, 0.271]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4612873134328358
[2m[36m(func pid=75223)[0m top5: 0.909981343283582
[2m[36m(func pid=75223)[0m f1_micro: 0.4612873134328358
[2m[36m(func pid=75223)[0m f1_macro: 0.37892567931361815
[2m[36m(func pid=75223)[0m f1_weighted: 0.48502299751858424
[2m[36m(func pid=75223)[0m f1_per_class: [0.425, 0.525, 0.333, 0.543, 0.156, 0.291, 0.547, 0.389, 0.262, 0.318]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.0672 | Steps: 4 | Val loss: 2.0643 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3937 | Steps: 4 | Val loss: 5.9736 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3172 | Steps: 4 | Val loss: 2.0441 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8025 | Steps: 4 | Val loss: 1.4859 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 15:04:26 (running for 00:30:51.64)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.337 |      0.287 |                   72 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.347 |      0.379 |                   67 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.489 |      0.216 |                   73 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.067 |      0.173 |                   49 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.2294776119402985
[2m[36m(func pid=81458)[0m top5: 0.7709888059701493
[2m[36m(func pid=81458)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=81458)[0m f1_macro: 0.1725239021025293
[2m[36m(func pid=81458)[0m f1_weighted: 0.2284326874886657
[2m[36m(func pid=81458)[0m f1_per_class: [0.123, 0.351, 0.13, 0.042, 0.061, 0.0, 0.401, 0.515, 0.104, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m top1: 0.16744402985074627
[2m[36m(func pid=75300)[0m top5: 0.6833022388059702
[2m[36m(func pid=75300)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=75300)[0m f1_macro: 0.17354810157753944
[2m[36m(func pid=75300)[0m f1_weighted: 0.17246870797482955
[2m[36m(func pid=75300)[0m f1_per_class: [0.115, 0.479, 0.017, 0.056, 0.165, 0.127, 0.098, 0.385, 0.062, 0.232]
[2m[36m(func pid=75300)[0m 
[2m[36m(func pid=75166)[0m top1: 0.30970149253731344
[2m[36m(func pid=75166)[0m top5: 0.7966417910447762
[2m[36m(func pid=75166)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=75166)[0m f1_macro: 0.2846075397073457
[2m[36m(func pid=75166)[0m f1_weighted: 0.3144296591388013
[2m[36m(func pid=75166)[0m f1_per_class: [0.232, 0.376, 0.393, 0.419, 0.078, 0.417, 0.166, 0.331, 0.169, 0.264]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=75223)[0m top1: 0.47201492537313433
[2m[36m(func pid=75223)[0m top5: 0.9286380597014925
[2m[36m(func pid=75223)[0m f1_micro: 0.47201492537313433
[2m[36m(func pid=75223)[0m f1_macro: 0.40001698102106475
[2m[36m(func pid=75223)[0m f1_weighted: 0.49835503555113725
[2m[36m(func pid=75223)[0m f1_per_class: [0.443, 0.488, 0.289, 0.547, 0.127, 0.381, 0.55, 0.452, 0.372, 0.35]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3383 | Steps: 4 | Val loss: 2.0284 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=75300)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4265 | Steps: 4 | Val loss: 5.6080 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1830 | Steps: 4 | Val loss: 2.0418 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 15:04:31 (running for 00:30:56.97)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.317 |      0.285 |                   73 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.803 |      0.4   |                   68 |
| train_5ae7f_00014 | RUNNING    | 192.168.7.53:75300 | 0.01   |       0.9  |         0.0001 |  0.394 |      0.174 |                   74 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.338 |      0.171 |                   50 |
| train_5ae7f_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.1921641791044776
[2m[36m(func pid=81458)[0m top5: 0.7649253731343284
[2m[36m(func pid=81458)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=81458)[0m f1_macro: 0.17127706697458148
[2m[36m(func pid=81458)[0m f1_weighted: 0.2026951998013725
[2m[36m(func pid=81458)[0m f1_per_class: [0.121, 0.289, 0.279, 0.104, 0.049, 0.0, 0.297, 0.478, 0.095, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75300)[0m top1: 0.16511194029850745
[2m[36m(func pid=75300)[0m top5: 0.675839552238806
[2m[36m(func pid=75300)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=75300)[0m f1_macro: 0.19284242622746323
[2m[36m(func pid=75300)[0m f1_weighted: 0.170274143563676
[2m[36m(func pid=75300)[0m f1_per_class: [0.251, 0.448, 0.039, 0.116, 0.25, 0.044, 0.059, 0.453, 0.065, 0.203]
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4265 | Steps: 4 | Val loss: 1.4767 | Batch size: 32 | lr: 0.001 | Duration: 3.29s
[2m[36m(func pid=75166)[0m top1: 0.3031716417910448
[2m[36m(func pid=75166)[0m top5: 0.7985074626865671
[2m[36m(func pid=75166)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=75166)[0m f1_macro: 0.28699232304346234
[2m[36m(func pid=75166)[0m f1_weighted: 0.3070215870639859
[2m[36m(func pid=75166)[0m f1_per_class: [0.236, 0.345, 0.462, 0.426, 0.071, 0.408, 0.154, 0.343, 0.162, 0.264]
[2m[36m(func pid=75166)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.9738 | Steps: 4 | Val loss: 2.1286 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=75223)[0m top1: 0.46455223880597013
[2m[36m(func pid=75223)[0m top5: 0.9328358208955224
[2m[36m(func pid=75223)[0m f1_micro: 0.46455223880597013
[2m[36m(func pid=75223)[0m f1_macro: 0.4129348725294958
[2m[36m(func pid=75223)[0m f1_weighted: 0.491820702677235
[2m[36m(func pid=75223)[0m f1_per_class: [0.52, 0.453, 0.333, 0.534, 0.118, 0.391, 0.537, 0.503, 0.414, 0.326]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=75166)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2510 | Steps: 4 | Val loss: 2.0410 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=81458)[0m top1: 0.10494402985074627
[2m[36m(func pid=81458)[0m top5: 0.7663246268656716
[2m[36m(func pid=81458)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=81458)[0m f1_macro: 0.11118343717064402
[2m[36m(func pid=81458)[0m f1_weighted: 0.09828388287641965
[2m[36m(func pid=81458)[0m f1_per_class: [0.068, 0.118, 0.057, 0.104, 0.05, 0.106, 0.006, 0.523, 0.08, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5956 | Steps: 4 | Val loss: 1.5217 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=75166)[0m top1: 0.30550373134328357
[2m[36m(func pid=75166)[0m top5: 0.7952425373134329
[2m[36m(func pid=75166)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=75166)[0m f1_macro: 0.28529428183724864
[2m[36m(func pid=75166)[0m f1_weighted: 0.3082595982448428
[2m[36m(func pid=75166)[0m f1_per_class: [0.253, 0.338, 0.414, 0.447, 0.065, 0.41, 0.141, 0.344, 0.158, 0.284]
== Status ==
Current time: 2024-01-07 15:04:36 (running for 00:31:02.47)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00012 | RUNNING    | 192.168.7.53:75166 | 0.0001 |       0.9  |         0.0001 |  2.183 |      0.287 |                   74 |
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.426 |      0.413 |                   69 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.974 |      0.111 |                   51 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93317)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=93317)[0m Configuration completed!
[2m[36m(func pid=93317)[0m New optimizer parameters:
[2m[36m(func pid=93317)[0m SGD (
[2m[36m(func pid=93317)[0m Parameter Group 0
[2m[36m(func pid=93317)[0m     dampening: 0
[2m[36m(func pid=93317)[0m     differentiable: False
[2m[36m(func pid=93317)[0m     foreach: None
[2m[36m(func pid=93317)[0m     lr: 0.0001
[2m[36m(func pid=93317)[0m     maximize: False
[2m[36m(func pid=93317)[0m     momentum: 0.99
[2m[36m(func pid=93317)[0m     nesterov: False
[2m[36m(func pid=93317)[0m     weight_decay: 1e-05
[2m[36m(func pid=93317)[0m )
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.8226 | Steps: 4 | Val loss: 2.3265 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=75223)[0m top1: 0.4552238805970149
[2m[36m(func pid=75223)[0m top5: 0.9291044776119403
[2m[36m(func pid=75223)[0m f1_micro: 0.4552238805970149
[2m[36m(func pid=75223)[0m f1_macro: 0.40345196903468744
[2m[36m(func pid=75223)[0m f1_weighted: 0.48256183686450815
[2m[36m(func pid=75223)[0m f1_per_class: [0.496, 0.454, 0.286, 0.506, 0.121, 0.425, 0.526, 0.478, 0.416, 0.328]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.14738805970149255
[2m[36m(func pid=81458)[0m top5: 0.7271455223880597
[2m[36m(func pid=81458)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=81458)[0m f1_macro: 0.1325007894183174
[2m[36m(func pid=81458)[0m f1_weighted: 0.16149355704969376
[2m[36m(func pid=81458)[0m f1_per_class: [0.097, 0.059, 0.089, 0.16, 0.04, 0.168, 0.183, 0.511, 0.019, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4273 | Steps: 4 | Val loss: 1.6057 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9965 | Steps: 4 | Val loss: 2.3400 | Batch size: 32 | lr: 0.0001 | Duration: 4.83s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4832 | Steps: 4 | Val loss: 2.1640 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=75223)[0m top1: 0.4458955223880597
[2m[36m(func pid=75223)[0m top5: 0.9104477611940298
[2m[36m(func pid=75223)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=75223)[0m f1_macro: 0.37348598137996103
[2m[36m(func pid=75223)[0m f1_weighted: 0.4761059308195333
[2m[36m(func pid=75223)[0m f1_per_class: [0.416, 0.472, 0.211, 0.479, 0.145, 0.379, 0.557, 0.464, 0.332, 0.28]
[2m[36m(func pid=93317)[0m top1: 0.17257462686567165
[2m[36m(func pid=93317)[0m top5: 0.5139925373134329
[2m[36m(func pid=93317)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=93317)[0m f1_macro: 0.11033395226110851
[2m[36m(func pid=93317)[0m f1_weighted: 0.12353945471268239
[2m[36m(func pid=93317)[0m f1_per_class: [0.255, 0.306, 0.0, 0.099, 0.011, 0.278, 0.009, 0.04, 0.0, 0.105]
[2m[36m(func pid=81458)[0m top1: 0.228544776119403
[2m[36m(func pid=81458)[0m top5: 0.7467350746268657
[2m[36m(func pid=81458)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=81458)[0m f1_macro: 0.1844548919675992
[2m[36m(func pid=81458)[0m f1_weighted: 0.24843705388208578
[2m[36m(func pid=81458)[0m f1_per_class: [0.113, 0.11, 0.278, 0.134, 0.046, 0.159, 0.467, 0.516, 0.024, 0.0]
== Status ==
Current time: 2024-01-07 15:04:42 (running for 00:31:08.20)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.33825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.596 |      0.403 |                   70 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.823 |      0.133 |                   52 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 15:04:48 (running for 00:31:14.57)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.33825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.596 |      0.403 |                   70 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.483 |      0.184 |                   53 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93934)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=93934)[0m Configuration completed!
[2m[36m(func pid=93934)[0m New optimizer parameters:
[2m[36m(func pid=93934)[0m SGD (
[2m[36m(func pid=93934)[0m Parameter Group 0
[2m[36m(func pid=93934)[0m     dampening: 0
[2m[36m(func pid=93934)[0m     differentiable: False
[2m[36m(func pid=93934)[0m     foreach: None
[2m[36m(func pid=93934)[0m     lr: 0.001
[2m[36m(func pid=93934)[0m     maximize: False
[2m[36m(func pid=93934)[0m     momentum: 0.99
[2m[36m(func pid=93934)[0m     nesterov: False
[2m[36m(func pid=93934)[0m     weight_decay: 1e-05
[2m[36m(func pid=93934)[0m )
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0239 | Steps: 4 | Val loss: 2.3729 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9940 | Steps: 4 | Val loss: 2.3926 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5266 | Steps: 4 | Val loss: 1.5638 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9698 | Steps: 4 | Val loss: 2.4012 | Batch size: 32 | lr: 0.001 | Duration: 4.95s
== Status ==
Current time: 2024-01-07 15:04:54 (running for 00:31:19.63)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.33825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.427 |      0.373 |                   71 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.483 |      0.184 |                   53 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.996 |      0.11  |                    1 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.2644589552238806
[2m[36m(func pid=81458)[0m top5: 0.753731343283582
[2m[36m(func pid=81458)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=81458)[0m f1_macro: 0.179569435052029
[2m[36m(func pid=81458)[0m f1_weighted: 0.26978785170276437
[2m[36m(func pid=81458)[0m f1_per_class: [0.091, 0.339, 0.0, 0.0, 0.06, 0.171, 0.525, 0.525, 0.085, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.13432835820895522
[2m[36m(func pid=93317)[0m top5: 0.46595149253731344
[2m[36m(func pid=93317)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=93317)[0m f1_macro: 0.0723887598484465
[2m[36m(func pid=93317)[0m f1_weighted: 0.09576153642748336
[2m[36m(func pid=93317)[0m f1_per_class: [0.084, 0.216, 0.0, 0.078, 0.0, 0.27, 0.006, 0.028, 0.012, 0.03]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m top1: 0.45988805970149255
[2m[36m(func pid=75223)[0m top5: 0.9249067164179104
[2m[36m(func pid=75223)[0m f1_micro: 0.45988805970149255
[2m[36m(func pid=75223)[0m f1_macro: 0.4085799212750813
[2m[36m(func pid=75223)[0m f1_weighted: 0.48191530981426645
[2m[36m(func pid=75223)[0m f1_per_class: [0.569, 0.498, 0.316, 0.489, 0.134, 0.389, 0.533, 0.454, 0.352, 0.353]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m top1: 0.12360074626865672
[2m[36m(func pid=93934)[0m top5: 0.4542910447761194
[2m[36m(func pid=93934)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=93934)[0m f1_macro: 0.07625648867804558
[2m[36m(func pid=93934)[0m f1_weighted: 0.08936969710222206
[2m[36m(func pid=93934)[0m f1_per_class: [0.136, 0.183, 0.0, 0.077, 0.0, 0.251, 0.006, 0.03, 0.01, 0.068]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3500 | Steps: 4 | Val loss: 2.7417 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9649 | Steps: 4 | Val loss: 2.3792 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3817 | Steps: 4 | Val loss: 1.5410 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9367 | Steps: 4 | Val loss: 2.3716 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:05:00 (running for 00:31:25.61)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.33825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.527 |      0.409 |                   72 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.35  |      0.115 |                   55 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.994 |      0.072 |                    2 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.97  |      0.076 |                    1 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.13526119402985073
[2m[36m(func pid=81458)[0m top5: 0.7686567164179104
[2m[36m(func pid=81458)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=81458)[0m f1_macro: 0.1148323760287026
[2m[36m(func pid=81458)[0m f1_weighted: 0.10515191886755985
[2m[36m(func pid=81458)[0m f1_per_class: [0.076, 0.179, 0.0, 0.0, 0.085, 0.256, 0.054, 0.418, 0.08, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.13386194029850745
[2m[36m(func pid=93317)[0m top5: 0.4906716417910448
[2m[36m(func pid=93317)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=93317)[0m f1_macro: 0.07187646391147401
[2m[36m(func pid=93317)[0m f1_weighted: 0.10645779581219626
[2m[36m(func pid=93317)[0m f1_per_class: [0.073, 0.177, 0.0, 0.095, 0.0, 0.262, 0.05, 0.041, 0.02, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m top1: 0.45755597014925375
[2m[36m(func pid=75223)[0m top5: 0.9356343283582089
[2m[36m(func pid=75223)[0m f1_micro: 0.45755597014925375
[2m[36m(func pid=75223)[0m f1_macro: 0.4107030617356571
[2m[36m(func pid=75223)[0m f1_weighted: 0.47898347000873914
[2m[36m(func pid=75223)[0m f1_per_class: [0.528, 0.488, 0.289, 0.477, 0.135, 0.406, 0.525, 0.498, 0.365, 0.396]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m top1: 0.12080223880597014
[2m[36m(func pid=93934)[0m top5: 0.49720149253731344
[2m[36m(func pid=93934)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=93934)[0m f1_macro: 0.09063270075945659
[2m[36m(func pid=93934)[0m f1_weighted: 0.110460653262005
[2m[36m(func pid=93934)[0m f1_per_class: [0.121, 0.151, 0.032, 0.138, 0.024, 0.223, 0.03, 0.129, 0.02, 0.037]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.0655 | Steps: 4 | Val loss: 3.1683 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9194 | Steps: 4 | Val loss: 2.3416 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4064 | Steps: 4 | Val loss: 1.5785 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8151 | Steps: 4 | Val loss: 2.2806 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=81458)[0m top1: 0.06389925373134328
[2m[36m(func pid=81458)[0m top5: 0.6492537313432836
[2m[36m(func pid=81458)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=81458)[0m f1_macro: 0.08584313661482527
[2m[36m(func pid=81458)[0m f1_weighted: 0.06283411786640666
[2m[36m(func pid=81458)[0m f1_per_class: [0.052, 0.162, 0.054, 0.003, 0.035, 0.037, 0.0, 0.45, 0.065, 0.0]
[2m[36m(func pid=81458)[0m 
== Status ==
Current time: 2024-01-07 15:05:05 (running for 00:31:30.99)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.33825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.382 |      0.411 |                   73 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.065 |      0.086 |                   56 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.965 |      0.072 |                    3 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.937 |      0.091 |                    2 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.15111940298507462
[2m[36m(func pid=93317)[0m top5: 0.5307835820895522
[2m[36m(func pid=93317)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=93317)[0m f1_macro: 0.0864972347652192
[2m[36m(func pid=93317)[0m f1_weighted: 0.1414130804848644
[2m[36m(func pid=93317)[0m f1_per_class: [0.061, 0.186, 0.0, 0.136, 0.013, 0.263, 0.118, 0.077, 0.01, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m top1: 0.42630597014925375
[2m[36m(func pid=75223)[0m top5: 0.9300373134328358
[2m[36m(func pid=75223)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=75223)[0m f1_macro: 0.3884575146489454
[2m[36m(func pid=75223)[0m f1_weighted: 0.4449045944371769
[2m[36m(func pid=75223)[0m f1_per_class: [0.505, 0.469, 0.211, 0.456, 0.131, 0.419, 0.443, 0.491, 0.344, 0.417]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m top1: 0.15391791044776118
[2m[36m(func pid=93934)[0m top5: 0.6021455223880597
[2m[36m(func pid=93934)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=93934)[0m f1_macro: 0.11157379728793068
[2m[36m(func pid=93934)[0m f1_weighted: 0.1857436881114262
[2m[36m(func pid=93934)[0m f1_per_class: [0.122, 0.183, 0.04, 0.202, 0.013, 0.134, 0.237, 0.152, 0.0, 0.034]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.8174 | Steps: 4 | Val loss: 2.7806 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9228 | Steps: 4 | Val loss: 2.3137 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6060 | Steps: 4 | Val loss: 1.5158 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6514 | Steps: 4 | Val loss: 2.2152 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 15:05:10 (running for 00:31:36.23)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.33825
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.406 |      0.388 |                   74 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.817 |      0.117 |                   57 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.919 |      0.086 |                    4 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.815 |      0.112 |                    3 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.1166044776119403
[2m[36m(func pid=81458)[0m top5: 0.7192164179104478
[2m[36m(func pid=81458)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=81458)[0m f1_macro: 0.11703815297761122
[2m[36m(func pid=81458)[0m f1_weighted: 0.10297572281640081
[2m[36m(func pid=81458)[0m f1_per_class: [0.089, 0.223, 0.063, 0.053, 0.041, 0.143, 0.0, 0.508, 0.05, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.15951492537313433
[2m[36m(func pid=93317)[0m top5: 0.5634328358208955
[2m[36m(func pid=93317)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=93317)[0m f1_macro: 0.09663046543279377
[2m[36m(func pid=93317)[0m f1_weighted: 0.1600779070601895
[2m[36m(func pid=93317)[0m f1_per_class: [0.056, 0.189, 0.025, 0.157, 0.011, 0.271, 0.154, 0.081, 0.022, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.19449626865671643
[2m[36m(func pid=93934)[0m top5: 0.6674440298507462
[2m[36m(func pid=93934)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=93934)[0m f1_macro: 0.12936476199444225
[2m[36m(func pid=93934)[0m f1_weighted: 0.2162225030643046
[2m[36m(func pid=93934)[0m f1_per_class: [0.141, 0.243, 0.064, 0.23, 0.031, 0.03, 0.308, 0.183, 0.0, 0.063]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.46222014925373134
[2m[36m(func pid=75223)[0m top5: 0.9286380597014925
[2m[36m(func pid=75223)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=75223)[0m f1_macro: 0.40303437521678137
[2m[36m(func pid=75223)[0m f1_weighted: 0.486475854623088
[2m[36m(func pid=75223)[0m f1_per_class: [0.509, 0.471, 0.22, 0.53, 0.161, 0.442, 0.508, 0.465, 0.342, 0.381]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3129 | Steps: 4 | Val loss: 2.2854 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9719 | Steps: 4 | Val loss: 2.2851 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6541 | Steps: 4 | Val loss: 2.1337 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:05:15 (running for 00:31:41.46)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.606 |      0.403 |                   75 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.313 |      0.168 |                   58 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.923 |      0.097 |                    5 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.651 |      0.129 |                    4 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.6009 | Steps: 4 | Val loss: 1.4879 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=81458)[0m top1: 0.21875
[2m[36m(func pid=81458)[0m top5: 0.7765858208955224
[2m[36m(func pid=81458)[0m f1_micro: 0.21875
[2m[36m(func pid=81458)[0m f1_macro: 0.16810048443761474
[2m[36m(func pid=81458)[0m f1_weighted: 0.20392294545089237
[2m[36m(func pid=81458)[0m f1_per_class: [0.161, 0.217, 0.087, 0.39, 0.056, 0.248, 0.0, 0.4, 0.026, 0.095]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.17117537313432835
[2m[36m(func pid=93317)[0m top5: 0.5816231343283582
[2m[36m(func pid=93317)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=93317)[0m f1_macro: 0.10844353418735793
[2m[36m(func pid=93317)[0m f1_weighted: 0.17796204867751475
[2m[36m(func pid=93317)[0m f1_per_class: [0.1, 0.234, 0.053, 0.187, 0.01, 0.278, 0.162, 0.047, 0.014, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.19916044776119404
[2m[36m(func pid=93934)[0m top5: 0.726679104477612
[2m[36m(func pid=93934)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=93934)[0m f1_macro: 0.1467769915396558
[2m[36m(func pid=93934)[0m f1_weighted: 0.20947785210746914
[2m[36m(func pid=93934)[0m f1_per_class: [0.2, 0.263, 0.073, 0.225, 0.085, 0.016, 0.275, 0.183, 0.0, 0.148]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.478544776119403
[2m[36m(func pid=75223)[0m top5: 0.925839552238806
[2m[36m(func pid=75223)[0m f1_micro: 0.478544776119403
[2m[36m(func pid=75223)[0m f1_macro: 0.4068590475192179
[2m[36m(func pid=75223)[0m f1_weighted: 0.5033310115720274
[2m[36m(func pid=75223)[0m f1_per_class: [0.491, 0.53, 0.22, 0.533, 0.145, 0.383, 0.55, 0.484, 0.325, 0.408]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6326 | Steps: 4 | Val loss: 2.1742 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8268 | Steps: 4 | Val loss: 2.2743 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.4420 | Steps: 4 | Val loss: 2.1122 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 15:05:21 (running for 00:31:47.12)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.601 |      0.407 |                   76 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.633 |      0.189 |                   59 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.972 |      0.108 |                    6 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.654 |      0.147 |                    5 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.23087686567164178
[2m[36m(func pid=81458)[0m top5: 0.7919776119402985
[2m[36m(func pid=81458)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=81458)[0m f1_macro: 0.18862927292659545
[2m[36m(func pid=81458)[0m f1_weighted: 0.26064483476856576
[2m[36m(func pid=81458)[0m f1_per_class: [0.133, 0.105, 0.055, 0.284, 0.083, 0.295, 0.318, 0.498, 0.047, 0.068]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3554 | Steps: 4 | Val loss: 1.5643 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
[2m[36m(func pid=93317)[0m top1: 0.1837686567164179
[2m[36m(func pid=93317)[0m top5: 0.6021455223880597
[2m[36m(func pid=93317)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=93317)[0m f1_macro: 0.12591481860447146
[2m[36m(func pid=93317)[0m f1_weighted: 0.19794849280750393
[2m[36m(func pid=93317)[0m f1_per_class: [0.115, 0.223, 0.12, 0.219, 0.028, 0.305, 0.191, 0.059, 0.0, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.17957089552238806
[2m[36m(func pid=93934)[0m top5: 0.7593283582089553
[2m[36m(func pid=93934)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=93934)[0m f1_macro: 0.14949581556827798
[2m[36m(func pid=93934)[0m f1_weighted: 0.16922926699017196
[2m[36m(func pid=93934)[0m f1_per_class: [0.301, 0.288, 0.057, 0.213, 0.145, 0.008, 0.125, 0.223, 0.0, 0.135]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4496268656716418
[2m[36m(func pid=75223)[0m top5: 0.9211753731343284
[2m[36m(func pid=75223)[0m f1_micro: 0.4496268656716418
[2m[36m(func pid=75223)[0m f1_macro: 0.38505589461109013
[2m[36m(func pid=75223)[0m f1_weighted: 0.4741370459436506
[2m[36m(func pid=75223)[0m f1_per_class: [0.405, 0.509, 0.226, 0.483, 0.15, 0.403, 0.506, 0.509, 0.332, 0.328]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.9561 | Steps: 4 | Val loss: 2.2677 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8214 | Steps: 4 | Val loss: 2.2682 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.2094 | Steps: 4 | Val loss: 2.0845 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:05:26 (running for 00:31:52.48)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.355 |      0.385 |                   77 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.956 |      0.14  |                   60 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.827 |      0.126 |                    7 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.442 |      0.149 |                    6 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.24207089552238806
[2m[36m(func pid=81458)[0m top5: 0.7994402985074627
[2m[36m(func pid=81458)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=81458)[0m f1_macro: 0.13974980029431025
[2m[36m(func pid=81458)[0m f1_weighted: 0.2551346058728982
[2m[36m(func pid=81458)[0m f1_per_class: [0.113, 0.115, 0.073, 0.148, 0.0, 0.311, 0.514, 0.0, 0.059, 0.065]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4894 | Steps: 4 | Val loss: 1.6266 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=93317)[0m top1: 0.18050373134328357
[2m[36m(func pid=93317)[0m top5: 0.625
[2m[36m(func pid=93317)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=93317)[0m f1_macro: 0.13409816192636764
[2m[36m(func pid=93317)[0m f1_weighted: 0.1971551581752615
[2m[36m(func pid=93317)[0m f1_per_class: [0.126, 0.221, 0.163, 0.219, 0.024, 0.323, 0.176, 0.074, 0.016, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.177705223880597
[2m[36m(func pid=93934)[0m top5: 0.7686567164179104
[2m[36m(func pid=93934)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=93934)[0m f1_macro: 0.14970736170555454
[2m[36m(func pid=93934)[0m f1_weighted: 0.14276630168226037
[2m[36m(func pid=93934)[0m f1_per_class: [0.392, 0.252, 0.077, 0.246, 0.126, 0.0, 0.015, 0.264, 0.0, 0.125]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.7115 | Steps: 4 | Val loss: 2.2327 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=75223)[0m top1: 0.41697761194029853
[2m[36m(func pid=75223)[0m top5: 0.9202425373134329
[2m[36m(func pid=75223)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=75223)[0m f1_macro: 0.3674238000226716
[2m[36m(func pid=75223)[0m f1_weighted: 0.43820344429281005
[2m[36m(func pid=75223)[0m f1_per_class: [0.416, 0.499, 0.283, 0.464, 0.119, 0.372, 0.436, 0.416, 0.355, 0.316]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7423 | Steps: 4 | Val loss: 2.2528 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.0500 | Steps: 4 | Val loss: 2.0560 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:05:32 (running for 00:31:58.06)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.489 |      0.367 |                   78 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.712 |      0.239 |                   61 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.821 |      0.134 |                    8 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.209 |      0.15  |                    7 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.31669776119402987
[2m[36m(func pid=81458)[0m top5: 0.7999067164179104
[2m[36m(func pid=81458)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=81458)[0m f1_macro: 0.2389217162750799
[2m[36m(func pid=81458)[0m f1_weighted: 0.3246926294692771
[2m[36m(func pid=81458)[0m f1_per_class: [0.105, 0.498, 0.667, 0.139, 0.049, 0.283, 0.529, 0.016, 0.067, 0.036]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4131 | Steps: 4 | Val loss: 1.7394 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=93317)[0m top1: 0.18190298507462688
[2m[36m(func pid=93317)[0m top5: 0.6450559701492538
[2m[36m(func pid=93317)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=93317)[0m f1_macro: 0.14339159384220754
[2m[36m(func pid=93317)[0m f1_weighted: 0.20066959280791163
[2m[36m(func pid=93317)[0m f1_per_class: [0.12, 0.187, 0.217, 0.235, 0.031, 0.331, 0.182, 0.098, 0.031, 0.0]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.1912313432835821
[2m[36m(func pid=93934)[0m top5: 0.7789179104477612
[2m[36m(func pid=93934)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=93934)[0m f1_macro: 0.16041172042075408
[2m[36m(func pid=93934)[0m f1_weighted: 0.15546545282590488
[2m[36m(func pid=93934)[0m f1_per_class: [0.41, 0.239, 0.096, 0.299, 0.115, 0.0, 0.009, 0.271, 0.028, 0.137]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1362 | Steps: 4 | Val loss: 2.7734 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=75223)[0m top1: 0.3927238805970149
[2m[36m(func pid=75223)[0m top5: 0.8903917910447762
[2m[36m(func pid=75223)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=75223)[0m f1_macro: 0.3379998966400686
[2m[36m(func pid=75223)[0m f1_weighted: 0.4107573010330235
[2m[36m(func pid=75223)[0m f1_per_class: [0.373, 0.516, 0.222, 0.484, 0.11, 0.348, 0.333, 0.444, 0.292, 0.259]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7730 | Steps: 4 | Val loss: 2.2344 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.0836 | Steps: 4 | Val loss: 2.0185 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:05:37 (running for 00:32:03.32)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.413 |      0.338 |                   79 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.136 |      0.186 |                   62 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.742 |      0.143 |                    9 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.05  |      0.16  |                    8 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.302705223880597
[2m[36m(func pid=81458)[0m top5: 0.7667910447761194
[2m[36m(func pid=81458)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=81458)[0m f1_macro: 0.18551878462596982
[2m[36m(func pid=81458)[0m f1_weighted: 0.3059417690883799
[2m[36m(func pid=81458)[0m f1_per_class: [0.113, 0.511, 0.0, 0.174, 0.015, 0.143, 0.426, 0.372, 0.043, 0.057]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.19309701492537312
[2m[36m(func pid=93317)[0m top5: 0.6571828358208955
[2m[36m(func pid=93317)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=93317)[0m f1_macro: 0.1542832195601269
[2m[36m(func pid=93317)[0m f1_weighted: 0.21824845749271746
[2m[36m(func pid=93317)[0m f1_per_class: [0.118, 0.167, 0.267, 0.279, 0.03, 0.316, 0.213, 0.128, 0.016, 0.009]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.6303 | Steps: 4 | Val loss: 1.8024 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=93934)[0m top1: 0.1982276119402985
[2m[36m(func pid=93934)[0m top5: 0.8055037313432836
[2m[36m(func pid=93934)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=93934)[0m f1_macro: 0.16277594860493688
[2m[36m(func pid=93934)[0m f1_weighted: 0.15799820103453321
[2m[36m(func pid=93934)[0m f1_per_class: [0.348, 0.296, 0.135, 0.261, 0.096, 0.016, 0.019, 0.265, 0.027, 0.164]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9160 | Steps: 4 | Val loss: 2.7799 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=75223)[0m top1: 0.3908582089552239
[2m[36m(func pid=75223)[0m top5: 0.8694029850746269
[2m[36m(func pid=75223)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=75223)[0m f1_macro: 0.3307297412462846
[2m[36m(func pid=75223)[0m f1_weighted: 0.40831203219474205
[2m[36m(func pid=75223)[0m f1_per_class: [0.419, 0.507, 0.238, 0.517, 0.119, 0.262, 0.337, 0.437, 0.225, 0.247]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7210 | Steps: 4 | Val loss: 2.2125 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9036 | Steps: 4 | Val loss: 1.9593 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:05:43 (running for 00:32:08.60)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.63  |      0.331 |                   80 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.916 |      0.187 |                   63 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.773 |      0.154 |                   10 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  2.084 |      0.163 |                    9 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.27052238805970147
[2m[36m(func pid=81458)[0m top5: 0.7924440298507462
[2m[36m(func pid=81458)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=81458)[0m f1_macro: 0.1874221084932925
[2m[36m(func pid=81458)[0m f1_weighted: 0.27532140289488666
[2m[36m(func pid=81458)[0m f1_per_class: [0.14, 0.403, 0.0, 0.179, 0.066, 0.191, 0.35, 0.411, 0.058, 0.075]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.21548507462686567
[2m[36m(func pid=93317)[0m top5: 0.6879664179104478
[2m[36m(func pid=93317)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=93317)[0m f1_macro: 0.16948992650431377
[2m[36m(func pid=93317)[0m f1_weighted: 0.24332167263501742
[2m[36m(func pid=93317)[0m f1_per_class: [0.123, 0.143, 0.282, 0.324, 0.034, 0.34, 0.253, 0.144, 0.034, 0.017]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.21641791044776118
[2m[36m(func pid=93934)[0m top5: 0.8297574626865671
[2m[36m(func pid=93934)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=93934)[0m f1_macro: 0.20144787339766462
[2m[36m(func pid=93934)[0m f1_weighted: 0.17515076311531816
[2m[36m(func pid=93934)[0m f1_per_class: [0.55, 0.351, 0.235, 0.272, 0.088, 0.024, 0.016, 0.247, 0.049, 0.182]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3667 | Steps: 4 | Val loss: 1.5386 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8673 | Steps: 4 | Val loss: 2.1858 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7134 | Steps: 4 | Val loss: 2.1965 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.9173 | Steps: 4 | Val loss: 1.9189 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=75223)[0m top1: 0.4780783582089552
[2m[36m(func pid=75223)[0m top5: 0.9263059701492538
[2m[36m(func pid=75223)[0m f1_micro: 0.4780783582089552
[2m[36m(func pid=75223)[0m f1_macro: 0.41187713210813265
[2m[36m(func pid=75223)[0m f1_weighted: 0.4988855857342356
[2m[36m(func pid=75223)[0m f1_per_class: [0.46, 0.532, 0.324, 0.533, 0.124, 0.378, 0.545, 0.421, 0.316, 0.484]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.24766791044776118
[2m[36m(func pid=81458)[0m top5: 0.7789179104477612
[2m[36m(func pid=81458)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=81458)[0m f1_macro: 0.2164091135481594== Status ==
Current time: 2024-01-07 15:05:48 (running for 00:32:13.77)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.367 |      0.412 |                   81 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.867 |      0.216 |                   64 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.721 |      0.169 |                   11 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.904 |      0.201 |                   10 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=81458)[0m f1_weighted: 0.27055240229001376
[2m[36m(func pid=81458)[0m f1_per_class: [0.211, 0.282, 0.389, 0.288, 0.056, 0.156, 0.311, 0.377, 0.057, 0.038]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m top1: 0.228544776119403
[2m[36m(func pid=93317)[0m top5: 0.6958955223880597
[2m[36m(func pid=93317)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=93317)[0m f1_macro: 0.1740197618596415
[2m[36m(func pid=93317)[0m f1_weighted: 0.2565654794179264
[2m[36m(func pid=93317)[0m f1_per_class: [0.122, 0.134, 0.227, 0.347, 0.033, 0.352, 0.269, 0.184, 0.034, 0.04]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.2392723880597015
[2m[36m(func pid=93934)[0m top5: 0.8334888059701493
[2m[36m(func pid=93934)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=93934)[0m f1_macro: 0.23069957810162398
[2m[36m(func pid=93934)[0m f1_weighted: 0.20600316601074187
[2m[36m(func pid=93934)[0m f1_per_class: [0.533, 0.388, 0.214, 0.274, 0.085, 0.164, 0.028, 0.269, 0.151, 0.2]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.7629 | Steps: 4 | Val loss: 1.4745 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6154 | Steps: 4 | Val loss: 2.1157 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6738 | Steps: 4 | Val loss: 2.1909 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8001 | Steps: 4 | Val loss: 1.8950 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:05:53 (running for 00:32:19.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.367 |      0.412 |                   81 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.615 |      0.231 |                   65 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.713 |      0.174 |                   12 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.917 |      0.231 |                   11 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.2453358208955224
[2m[36m(func pid=81458)[0m top5: 0.7821828358208955
[2m[36m(func pid=81458)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=81458)[0m f1_macro: 0.23114213782552864
[2m[36m(func pid=81458)[0m f1_weighted: 0.2686688974764661
[2m[36m(func pid=81458)[0m f1_per_class: [0.127, 0.137, 0.49, 0.258, 0.067, 0.276, 0.356, 0.438, 0.106, 0.058]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4925373134328358
[2m[36m(func pid=75223)[0m top5: 0.9347014925373134
[2m[36m(func pid=75223)[0m f1_micro: 0.4925373134328358
[2m[36m(func pid=75223)[0m f1_macro: 0.4236344554129053
[2m[36m(func pid=75223)[0m f1_weighted: 0.510753306307059
[2m[36m(func pid=75223)[0m f1_per_class: [0.488, 0.537, 0.338, 0.544, 0.14, 0.394, 0.562, 0.409, 0.351, 0.472]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m top1: 0.23507462686567165
[2m[36m(func pid=93317)[0m top5: 0.695429104477612
[2m[36m(func pid=93317)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=93317)[0m f1_macro: 0.17567411482218384
[2m[36m(func pid=93317)[0m f1_weighted: 0.2654531781071817
[2m[36m(func pid=93317)[0m f1_per_class: [0.122, 0.133, 0.191, 0.347, 0.037, 0.359, 0.296, 0.189, 0.034, 0.049]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.27705223880597013
[2m[36m(func pid=93934)[0m top5: 0.8334888059701493
[2m[36m(func pid=93934)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=93934)[0m f1_macro: 0.2741558201955938
[2m[36m(func pid=93934)[0m f1_weighted: 0.24917998953872927
[2m[36m(func pid=93934)[0m f1_per_class: [0.519, 0.411, 0.19, 0.281, 0.084, 0.378, 0.046, 0.336, 0.253, 0.244]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0716 | Steps: 4 | Val loss: 2.1049 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3026 | Steps: 4 | Val loss: 1.4960 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5901 | Steps: 4 | Val loss: 2.1717 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 15:05:58 (running for 00:32:24.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.763 |      0.424 |                   82 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.072 |      0.225 |                   66 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.674 |      0.176 |                   13 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.8   |      0.274 |                   12 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.2416044776119403
[2m[36m(func pid=81458)[0m top5: 0.7938432835820896
[2m[36m(func pid=81458)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=81458)[0m f1_macro: 0.22452912882887724
[2m[36m(func pid=81458)[0m f1_weighted: 0.2648768483984663
[2m[36m(func pid=81458)[0m f1_per_class: [0.124, 0.047, 0.426, 0.333, 0.025, 0.264, 0.328, 0.45, 0.083, 0.167]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.3308 | Steps: 4 | Val loss: 1.8355 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=75223)[0m top1: 0.47388059701492535
[2m[36m(func pid=75223)[0m top5: 0.9361007462686567
[2m[36m(func pid=75223)[0m f1_micro: 0.47388059701492535
[2m[36m(func pid=75223)[0m f1_macro: 0.4110372786669375
[2m[36m(func pid=75223)[0m f1_weighted: 0.49572180514440367
[2m[36m(func pid=75223)[0m f1_per_class: [0.483, 0.532, 0.248, 0.525, 0.133, 0.401, 0.525, 0.45, 0.346, 0.467]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m top1: 0.2392723880597015
[2m[36m(func pid=93317)[0m top5: 0.7173507462686567
[2m[36m(func pid=93317)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=93317)[0m f1_macro: 0.18462434578560075
[2m[36m(func pid=93317)[0m f1_weighted: 0.266758322174864
[2m[36m(func pid=93317)[0m f1_per_class: [0.135, 0.134, 0.193, 0.335, 0.061, 0.36, 0.306, 0.196, 0.036, 0.09]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.27705223880597013
[2m[36m(func pid=93934)[0m top5: 0.8591417910447762
[2m[36m(func pid=93934)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=93934)[0m f1_macro: 0.2705567736148974
[2m[36m(func pid=93934)[0m f1_weighted: 0.24935466963532912
[2m[36m(func pid=93934)[0m f1_per_class: [0.395, 0.399, 0.25, 0.245, 0.092, 0.432, 0.075, 0.351, 0.212, 0.255]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6647 | Steps: 4 | Val loss: 2.2804 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2968 | Steps: 4 | Val loss: 1.5278 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5996 | Steps: 4 | Val loss: 2.1556 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:06:04 (running for 00:32:29.74)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.303 |      0.411 |                   83 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.665 |      0.194 |                   67 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.59  |      0.185 |                   14 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.331 |      0.271 |                   13 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.24253731343283583
[2m[36m(func pid=81458)[0m top5: 0.7901119402985075
[2m[36m(func pid=81458)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=81458)[0m f1_macro: 0.1935217652942685
[2m[36m(func pid=81458)[0m f1_weighted: 0.27041264253527025
[2m[36m(func pid=81458)[0m f1_per_class: [0.116, 0.077, 0.085, 0.262, 0.0, 0.31, 0.378, 0.492, 0.102, 0.114]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.1862 | Steps: 4 | Val loss: 1.8040 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=75223)[0m top1: 0.46222014925373134
[2m[36m(func pid=75223)[0m top5: 0.9314365671641791
[2m[36m(func pid=75223)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=75223)[0m f1_macro: 0.4010206681783389
[2m[36m(func pid=75223)[0m f1_weighted: 0.4880945230493908
[2m[36m(func pid=75223)[0m f1_per_class: [0.471, 0.51, 0.239, 0.523, 0.11, 0.396, 0.513, 0.477, 0.365, 0.408]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m top1: 0.25046641791044777
[2m[36m(func pid=93317)[0m top5: 0.7229477611940298
[2m[36m(func pid=93317)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=93317)[0m f1_macro: 0.1854836730834335
[2m[36m(func pid=93317)[0m f1_weighted: 0.2742646138358661
[2m[36m(func pid=93317)[0m f1_per_class: [0.132, 0.088, 0.164, 0.378, 0.063, 0.366, 0.313, 0.205, 0.037, 0.107]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.8224 | Steps: 4 | Val loss: 2.0555 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=93934)[0m top1: 0.3003731343283582
[2m[36m(func pid=93934)[0m top5: 0.8838619402985075
[2m[36m(func pid=93934)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=93934)[0m f1_macro: 0.29557602254238147
[2m[36m(func pid=93934)[0m f1_weighted: 0.27959058138209575
[2m[36m(func pid=93934)[0m f1_per_class: [0.339, 0.423, 0.343, 0.249, 0.099, 0.424, 0.146, 0.443, 0.202, 0.288]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3528 | Steps: 4 | Val loss: 1.5250 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=81458)[0m top1: 0.33255597014925375
[2m[36m(func pid=81458)[0m top5: 0.8031716417910447
[2m[36m(func pid=81458)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=81458)[0m f1_macro: 0.24932087036173564
[2m[36m(func pid=81458)[0m f1_weighted: 0.37927736771177417
[2m[36m(func pid=81458)[0m f1_per_class: [0.221, 0.162, 0.179, 0.477, 0.0, 0.26, 0.5, 0.521, 0.101, 0.073]
== Status ==
Current time: 2024-01-07 15:06:09 (running for 00:32:35.00)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.297 |      0.401 |                   84 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.822 |      0.249 |                   68 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.6   |      0.185 |                   15 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.186 |      0.296 |                   14 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5410 | Steps: 4 | Val loss: 2.1288 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.3446 | Steps: 4 | Val loss: 1.7645 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=75223)[0m top1: 0.45755597014925375
[2m[36m(func pid=75223)[0m top5: 0.9347014925373134
[2m[36m(func pid=75223)[0m f1_micro: 0.45755597014925375
[2m[36m(func pid=75223)[0m f1_macro: 0.4044012985221239
[2m[36m(func pid=75223)[0m f1_weighted: 0.4801625571486044
[2m[36m(func pid=75223)[0m f1_per_class: [0.448, 0.496, 0.226, 0.549, 0.116, 0.408, 0.466, 0.431, 0.438, 0.467]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m top1: 0.2719216417910448
[2m[36m(func pid=93317)[0m top5: 0.7439365671641791
[2m[36m(func pid=93317)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=93317)[0m f1_macro: 0.20329160881188754
[2m[36m(func pid=93317)[0m f1_weighted: 0.29648035625204416
[2m[36m(func pid=93317)[0m f1_per_class: [0.134, 0.108, 0.15, 0.4, 0.08, 0.374, 0.343, 0.218, 0.084, 0.141]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.9012 | Steps: 4 | Val loss: 2.0504 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=93934)[0m top1: 0.3260261194029851
[2m[36m(func pid=93934)[0m top5: 0.8969216417910447
[2m[36m(func pid=93934)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=93934)[0m f1_macro: 0.33184561492981135
[2m[36m(func pid=93934)[0m f1_weighted: 0.33643511144639493
[2m[36m(func pid=93934)[0m f1_per_class: [0.408, 0.451, 0.414, 0.301, 0.124, 0.407, 0.27, 0.469, 0.159, 0.316]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.7794 | Steps: 4 | Val loss: 1.4929 | Batch size: 32 | lr: 0.001 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 15:06:14 (running for 00:32:40.32)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.353 |      0.404 |                   85 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.901 |      0.256 |                   69 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.541 |      0.203 |                   16 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.345 |      0.332 |                   15 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.39925373134328357
[2m[36m(func pid=81458)[0m top5: 0.8339552238805971
[2m[36m(func pid=81458)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=81458)[0m f1_macro: 0.2557957301237691
[2m[36m(func pid=81458)[0m f1_weighted: 0.4176904024036516
[2m[36m(func pid=81458)[0m f1_per_class: [0.166, 0.362, 0.0, 0.486, 0.0, 0.335, 0.485, 0.491, 0.116, 0.118]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5470 | Steps: 4 | Val loss: 2.0926 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.0937 | Steps: 4 | Val loss: 1.7551 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=75223)[0m top1: 0.4780783582089552
[2m[36m(func pid=75223)[0m top5: 0.933768656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.4780783582089552
[2m[36m(func pid=75223)[0m f1_macro: 0.40933980721526303
[2m[36m(func pid=75223)[0m f1_weighted: 0.503040632112666
[2m[36m(func pid=75223)[0m f1_per_class: [0.443, 0.514, 0.239, 0.554, 0.117, 0.388, 0.531, 0.475, 0.413, 0.42]
[2m[36m(func pid=93317)[0m top1: 0.29524253731343286
[2m[36m(func pid=93317)[0m top5: 0.7723880597014925
[2m[36m(func pid=93317)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=93317)[0m f1_macro: 0.21415304025315743
[2m[36m(func pid=93317)[0m f1_weighted: 0.3150482971006761
[2m[36m(func pid=93317)[0m f1_per_class: [0.172, 0.101, 0.164, 0.45, 0.091, 0.372, 0.363, 0.212, 0.065, 0.151]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.7350 | Steps: 4 | Val loss: 2.8786 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=93934)[0m top1: 0.3255597014925373
[2m[36m(func pid=93934)[0m top5: 0.9071828358208955
[2m[36m(func pid=93934)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=93934)[0m f1_macro: 0.3409954383416483
[2m[36m(func pid=93934)[0m f1_weighted: 0.3511930293779347
[2m[36m(func pid=93934)[0m f1_per_class: [0.491, 0.429, 0.429, 0.277, 0.135, 0.408, 0.35, 0.478, 0.149, 0.266]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:06:20 (running for 00:32:45.72)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.779 |      0.409 |                   86 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.735 |      0.205 |                   70 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.547 |      0.214 |                   17 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.094 |      0.341 |                   16 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.2737873134328358
[2m[36m(func pid=81458)[0m top5: 0.7630597014925373
[2m[36m(func pid=81458)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=81458)[0m f1_macro: 0.20545386503180058
[2m[36m(func pid=81458)[0m f1_weighted: 0.2506075124316251
[2m[36m(func pid=81458)[0m f1_per_class: [0.125, 0.413, 0.0, 0.06, 0.095, 0.399, 0.273, 0.496, 0.077, 0.116]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4647 | Steps: 4 | Val loss: 2.0801 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2748 | Steps: 4 | Val loss: 1.4982 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.0955 | Steps: 4 | Val loss: 1.6168 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=93317)[0m top1: 0.30830223880597013
[2m[36m(func pid=93317)[0m top5: 0.7770522388059702
[2m[36m(func pid=93317)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=93317)[0m f1_macro: 0.2225899963812751
[2m[36m(func pid=93317)[0m f1_weighted: 0.32571563522304536
[2m[36m(func pid=93317)[0m f1_per_class: [0.21, 0.091, 0.15, 0.479, 0.094, 0.361, 0.373, 0.231, 0.081, 0.155]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.4338 | Steps: 4 | Val loss: 3.1786 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=75223)[0m top1: 0.47947761194029853
[2m[36m(func pid=75223)[0m top5: 0.9379664179104478
[2m[36m(func pid=75223)[0m f1_micro: 0.47947761194029853
[2m[36m(func pid=75223)[0m f1_macro: 0.4156175475484375
[2m[36m(func pid=75223)[0m f1_weighted: 0.5015199711642064
[2m[36m(func pid=75223)[0m f1_per_class: [0.479, 0.493, 0.329, 0.504, 0.123, 0.386, 0.585, 0.438, 0.466, 0.353]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m top1: 0.3810634328358209
[2m[36m(func pid=93934)[0m top5: 0.9314365671641791
[2m[36m(func pid=93934)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=93934)[0m f1_macro: 0.3870579853158582
[2m[36m(func pid=93934)[0m f1_weighted: 0.41125291401560526
[2m[36m(func pid=93934)[0m f1_per_class: [0.622, 0.451, 0.49, 0.32, 0.128, 0.415, 0.479, 0.488, 0.174, 0.304]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:06:25 (running for 00:32:51.12)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.275 |      0.416 |                   87 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.434 |      0.176 |                   71 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.465 |      0.223 |                   18 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.096 |      0.387 |                   17 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.19682835820895522
[2m[36m(func pid=81458)[0m top5: 0.6949626865671642
[2m[36m(func pid=81458)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=81458)[0m f1_macro: 0.17553824688563063
[2m[36m(func pid=81458)[0m f1_weighted: 0.15973854389820547
[2m[36m(func pid=81458)[0m f1_per_class: [0.113, 0.394, 0.163, 0.01, 0.064, 0.306, 0.068, 0.459, 0.067, 0.112]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4221 | Steps: 4 | Val loss: 2.0569 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.9850 | Steps: 4 | Val loss: 1.5337 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.4387 | Steps: 4 | Val loss: 1.5499 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
[2m[36m(func pid=93317)[0m top1: 0.31949626865671643
[2m[36m(func pid=93317)[0m top5: 0.7901119402985075
[2m[36m(func pid=93317)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=93317)[0m f1_macro: 0.22968150190408498
[2m[36m(func pid=93317)[0m f1_weighted: 0.33445807405706574
[2m[36m(func pid=93317)[0m f1_per_class: [0.261, 0.122, 0.141, 0.511, 0.098, 0.345, 0.359, 0.238, 0.063, 0.159]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0061 | Steps: 4 | Val loss: 3.8456 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=93934)[0m top1: 0.4295708955223881
[2m[36m(func pid=93934)[0m top5: 0.941231343283582
[2m[36m(func pid=93934)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=93934)[0m f1_macro: 0.4052929587468109
[2m[36m(func pid=93934)[0m f1_weighted: 0.4565752846264546
[2m[36m(func pid=93934)[0m f1_per_class: [0.591, 0.499, 0.462, 0.371, 0.108, 0.424, 0.55, 0.481, 0.225, 0.343]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.46175373134328357
[2m[36m(func pid=75223)[0m top5: 0.9375
[2m[36m(func pid=75223)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=75223)[0m f1_macro: 0.39923076246856476
[2m[36m(func pid=75223)[0m f1_weighted: 0.4780654564196178
[2m[36m(func pid=75223)[0m f1_per_class: [0.504, 0.489, 0.321, 0.411, 0.112, 0.39, 0.609, 0.382, 0.417, 0.359]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:06:31 (running for 00:32:56.75)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.439 |      0.399 |                   88 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  2.006 |      0.077 |                   72 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.422 |      0.23  |                   19 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.985 |      0.405 |                   18 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.04477611940298507
[2m[36m(func pid=81458)[0m top5: 0.570429104477612
[2m[36m(func pid=81458)[0m f1_micro: 0.04477611940298508
[2m[36m(func pid=81458)[0m f1_macro: 0.07697505992217726
[2m[36m(func pid=81458)[0m f1_weighted: 0.035079521367430797
[2m[36m(func pid=81458)[0m f1_per_class: [0.079, 0.075, 0.107, 0.0, 0.029, 0.009, 0.0, 0.245, 0.085, 0.141]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4214 | Steps: 4 | Val loss: 2.0451 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7619 | Steps: 4 | Val loss: 1.6580 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3144 | Steps: 4 | Val loss: 1.5621 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=93317)[0m top1: 0.3087686567164179
[2m[36m(func pid=93317)[0m top5: 0.7975746268656716
[2m[36m(func pid=93317)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=93317)[0m f1_macro: 0.22405497114155729
[2m[36m(func pid=93317)[0m f1_weighted: 0.32881441596859234
[2m[36m(func pid=93317)[0m f1_per_class: [0.273, 0.146, 0.129, 0.496, 0.098, 0.368, 0.336, 0.235, 0.025, 0.135]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.5166 | Steps: 4 | Val loss: 3.9918 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=93934)[0m top1: 0.394589552238806
[2m[36m(func pid=93934)[0m top5: 0.9314365671641791
[2m[36m(func pid=93934)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=93934)[0m f1_macro: 0.38062307481311525
[2m[36m(func pid=93934)[0m f1_weighted: 0.4221570027929229
[2m[36m(func pid=93934)[0m f1_per_class: [0.568, 0.462, 0.381, 0.316, 0.083, 0.406, 0.513, 0.5, 0.241, 0.337]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4542910447761194
[2m[36m(func pid=75223)[0m top5: 0.9375
[2m[36m(func pid=75223)[0m f1_micro: 0.4542910447761194
[2m[36m(func pid=75223)[0m f1_macro: 0.40124065690274835
[2m[36m(func pid=75223)[0m f1_weighted: 0.45990750563425503
[2m[36m(func pid=75223)[0m f1_per_class: [0.472, 0.494, 0.382, 0.331, 0.135, 0.4, 0.61, 0.429, 0.389, 0.369]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:06:36 (running for 00:33:01.91)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.314 |      0.401 |                   89 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.517 |      0.046 |                   73 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.421 |      0.224 |                   20 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.762 |      0.381 |                   19 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.05317164179104478
[2m[36m(func pid=81458)[0m top5: 0.6408582089552238
[2m[36m(func pid=81458)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=81458)[0m f1_macro: 0.04557930819424424
[2m[36m(func pid=81458)[0m f1_weighted: 0.05101415150829188
[2m[36m(func pid=81458)[0m f1_per_class: [0.092, 0.148, 0.0, 0.048, 0.026, 0.015, 0.015, 0.0, 0.11, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3693 | Steps: 4 | Val loss: 2.0246 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8610 | Steps: 4 | Val loss: 1.7963 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3539 | Steps: 4 | Val loss: 1.5224 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.9013 | Steps: 4 | Val loss: 3.3164 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=93317)[0m top1: 0.30923507462686567
[2m[36m(func pid=93317)[0m top5: 0.8036380597014925
[2m[36m(func pid=93317)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=93317)[0m f1_macro: 0.2254372251414864
[2m[36m(func pid=93317)[0m f1_weighted: 0.3186154321625237
[2m[36m(func pid=93317)[0m f1_per_class: [0.257, 0.11, 0.13, 0.517, 0.142, 0.352, 0.303, 0.252, 0.049, 0.142]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.3414179104477612
[2m[36m(func pid=93934)[0m top5: 0.9207089552238806
[2m[36m(func pid=93934)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=93934)[0m f1_macro: 0.3306353988712109
[2m[36m(func pid=93934)[0m f1_weighted: 0.3609866749889315
[2m[36m(func pid=93934)[0m f1_per_class: [0.526, 0.452, 0.216, 0.258, 0.073, 0.417, 0.386, 0.405, 0.259, 0.313]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.46222014925373134
[2m[36m(func pid=75223)[0m top5: 0.9435634328358209
[2m[36m(func pid=75223)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=75223)[0m f1_macro: 0.42561789590002286
[2m[36m(func pid=75223)[0m f1_weighted: 0.46125722143589015
[2m[36m(func pid=75223)[0m f1_per_class: [0.489, 0.502, 0.456, 0.324, 0.162, 0.415, 0.588, 0.497, 0.44, 0.383]
[2m[36m(func pid=75223)[0m 
== Status ==
Current time: 2024-01-07 15:06:41 (running for 00:33:07.39)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.354 |      0.426 |                   90 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.901 |      0.167 |                   74 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.369 |      0.225 |                   21 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.861 |      0.331 |                   20 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=81458)[0m top1: 0.20662313432835822
[2m[36m(func pid=81458)[0m top5: 0.757929104477612
[2m[36m(func pid=81458)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=81458)[0m f1_macro: 0.16669897810583137
[2m[36m(func pid=81458)[0m f1_weighted: 0.24578189371512105
[2m[36m(func pid=81458)[0m f1_per_class: [0.098, 0.082, 0.0, 0.187, 0.052, 0.196, 0.405, 0.509, 0.139, 0.0]
[2m[36m(func pid=81458)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3621 | Steps: 4 | Val loss: 2.0267 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.0093 | Steps: 4 | Val loss: 1.9095 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3082 | Steps: 4 | Val loss: 1.4449 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=81458)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9320 | Steps: 4 | Val loss: 2.3016 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=93317)[0m top1: 0.29617537313432835
[2m[36m(func pid=93317)[0m top5: 0.7896455223880597
[2m[36m(func pid=93317)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=93317)[0m f1_macro: 0.2183539316317781
[2m[36m(func pid=93317)[0m f1_weighted: 0.30430510191903776
[2m[36m(func pid=93317)[0m f1_per_class: [0.25, 0.072, 0.105, 0.505, 0.134, 0.361, 0.283, 0.253, 0.069, 0.151]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.33348880597014924
[2m[36m(func pid=93934)[0m top5: 0.9076492537313433
[2m[36m(func pid=93934)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=93934)[0m f1_macro: 0.3116927656193081
[2m[36m(func pid=93934)[0m f1_weighted: 0.3494934115883093
[2m[36m(func pid=93934)[0m f1_per_class: [0.494, 0.473, 0.109, 0.234, 0.096, 0.416, 0.368, 0.371, 0.283, 0.271]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:06:47 (running for 00:33:12.72)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.362
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.308 |      0.43  |                   91 |
| train_5ae7f_00015 | RUNNING    | 192.168.7.53:81458 | 0.1    |       0.9  |         0.0001 |  1.901 |      0.167 |                   74 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.362 |      0.218 |                   22 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  1.009 |      0.312 |                   21 |
| train_5ae7f_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.4855410447761194
[2m[36m(func pid=75223)[0m top5: 0.9375
[2m[36m(func pid=75223)[0m f1_micro: 0.4855410447761194
[2m[36m(func pid=75223)[0m f1_macro: 0.4302137374737641
[2m[36m(func pid=75223)[0m f1_weighted: 0.49661177528685374
[2m[36m(func pid=75223)[0m f1_per_class: [0.464, 0.529, 0.388, 0.468, 0.17, 0.413, 0.571, 0.434, 0.442, 0.424]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=81458)[0m top1: 0.300839552238806
[2m[36m(func pid=81458)[0m top5: 0.7980410447761194
[2m[36m(func pid=81458)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=81458)[0m f1_macro: 0.29064106345602414
[2m[36m(func pid=81458)[0m f1_weighted: 0.338139013239451
[2m[36m(func pid=81458)[0m f1_per_class: [0.13, 0.093, 0.7, 0.4, 0.076, 0.254, 0.463, 0.511, 0.14, 0.139]
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7145 | Steps: 4 | Val loss: 1.9217 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1853 | Steps: 4 | Val loss: 2.0191 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3591 | Steps: 4 | Val loss: 1.5832 | Batch size: 32 | lr: 0.001 | Duration: 3.26s
[2m[36m(func pid=93317)[0m top1: 0.2994402985074627
[2m[36m(func pid=93317)[0m top5: 0.7957089552238806
[2m[36m(func pid=93317)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=93317)[0m f1_macro: 0.2242705367116135
[2m[36m(func pid=93317)[0m f1_weighted: 0.30793257791199746
[2m[36m(func pid=93317)[0m f1_per_class: [0.263, 0.127, 0.093, 0.517, 0.123, 0.355, 0.244, 0.305, 0.067, 0.149]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.345615671641791
[2m[36m(func pid=93934)[0m top5: 0.917910447761194
[2m[36m(func pid=93934)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=93934)[0m f1_macro: 0.31440542200345256
[2m[36m(func pid=93934)[0m f1_weighted: 0.3617853811293913
[2m[36m(func pid=93934)[0m f1_per_class: [0.423, 0.482, 0.087, 0.247, 0.146, 0.393, 0.399, 0.377, 0.329, 0.261]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.46361940298507465
[2m[36m(func pid=75223)[0m top5: 0.9132462686567164
[2m[36m(func pid=75223)[0m f1_micro: 0.46361940298507465
[2m[36m(func pid=75223)[0m f1_macro: 0.3888775451404719
[2m[36m(func pid=75223)[0m f1_weighted: 0.48405277654370454
[2m[36m(func pid=75223)[0m f1_per_class: [0.39, 0.517, 0.252, 0.52, 0.164, 0.376, 0.518, 0.436, 0.386, 0.328]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4240 | Steps: 4 | Val loss: 1.6501 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1104 | Steps: 4 | Val loss: 2.0053 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=93934)[0m top1: 0.42117537313432835
[2m[36m(func pid=93934)[0m top5: 0.9528917910447762
[2m[36m(func pid=93934)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=93934)[0m f1_macro: 0.35080451069725177
[2m[36m(func pid=93934)[0m f1_weighted: 0.4375466446808006
[2m[36m(func pid=93934)[0m f1_per_class: [0.395, 0.517, 0.161, 0.383, 0.12, 0.396, 0.499, 0.398, 0.347, 0.294]
== Status ==
Current time: 2024-01-07 15:06:53 (running for 00:33:18.71)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.359 |      0.389 |                   92 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.185 |      0.224 |                   23 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.714 |      0.314 |                   22 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=99348)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=99348)[0m Configuration completed!
[2m[36m(func pid=99348)[0m New optimizer parameters:
[2m[36m(func pid=99348)[0m SGD (
[2m[36m(func pid=99348)[0m Parameter Group 0
[2m[36m(func pid=99348)[0m     dampening: 0
[2m[36m(func pid=99348)[0m     differentiable: False
[2m[36m(func pid=99348)[0m     foreach: None
[2m[36m(func pid=99348)[0m     lr: 0.01
[2m[36m(func pid=99348)[0m     maximize: False
[2m[36m(func pid=99348)[0m     momentum: 0.99
[2m[36m(func pid=99348)[0m     nesterov: False
[2m[36m(func pid=99348)[0m     weight_decay: 1e-05
[2m[36m(func pid=99348)[0m )
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m top1: 0.300839552238806
[2m[36m(func pid=93317)[0m top5: 0.7980410447761194
[2m[36m(func pid=93317)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=93317)[0m f1_macro: 0.22503146384698464
[2m[36m(func pid=93317)[0m f1_weighted: 0.3056764428116151
[2m[36m(func pid=93317)[0m f1_per_class: [0.252, 0.123, 0.093, 0.514, 0.136, 0.335, 0.243, 0.344, 0.065, 0.145]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3454 | Steps: 4 | Val loss: 1.9796 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 15:06:59 (running for 00:33:24.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.345 |      0.305 |                   93 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.11  |      0.225 |                   24 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.424 |      0.351 |                   23 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.3498134328358209
[2m[36m(func pid=75223)[0m top5: 0.8558768656716418
[2m[36m(func pid=75223)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=75223)[0m f1_macro: 0.3053529305160455
[2m[36m(func pid=75223)[0m f1_weighted: 0.3497226952231596
[2m[36m(func pid=75223)[0m f1_per_class: [0.441, 0.479, 0.25, 0.42, 0.163, 0.332, 0.249, 0.286, 0.216, 0.218]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6445 | Steps: 4 | Val loss: 1.6248 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.2003 | Steps: 4 | Val loss: 2.0038 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9647 | Steps: 4 | Val loss: 2.4011 | Batch size: 32 | lr: 0.01 | Duration: 4.99s
[2m[36m(func pid=93934)[0m top1: 0.4435634328358209
[2m[36m(func pid=93934)[0m top5: 0.9505597014925373
[2m[36m(func pid=93934)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=93934)[0m f1_macro: 0.3679693366210895
[2m[36m(func pid=93934)[0m f1_weighted: 0.4535197476090925
[2m[36m(func pid=93934)[0m f1_per_class: [0.466, 0.525, 0.257, 0.392, 0.091, 0.407, 0.529, 0.433, 0.279, 0.3]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.2947761194029851
[2m[36m(func pid=93317)[0m top5: 0.7882462686567164
[2m[36m(func pid=93317)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=93317)[0m f1_macro: 0.22492782564267194
[2m[36m(func pid=93317)[0m f1_weighted: 0.30537981952087234
[2m[36m(func pid=93317)[0m f1_per_class: [0.241, 0.15, 0.083, 0.502, 0.132, 0.323, 0.241, 0.351, 0.068, 0.158]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.4352 | Steps: 4 | Val loss: 2.0388 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=99348)[0m top1: 0.06343283582089553
[2m[36m(func pid=99348)[0m top5: 0.40298507462686567
[2m[36m(func pid=99348)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=99348)[0m f1_macro: 0.06111496830382288
[2m[36m(func pid=99348)[0m f1_weighted: 0.049679715211223226
[2m[36m(func pid=99348)[0m f1_per_class: [0.097, 0.103, 0.035, 0.01, 0.025, 0.101, 0.021, 0.14, 0.0, 0.079]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5418 | Steps: 4 | Val loss: 1.7394 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0222 | Steps: 4 | Val loss: 2.0028 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 15:07:05 (running for 00:33:30.65)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.435 |      0.289 |                   94 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.2   |      0.225 |                   25 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.644 |      0.368 |                   24 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  2.965 |      0.061 |                    1 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.3208955223880597
[2m[36m(func pid=75223)[0m top5: 0.8568097014925373
[2m[36m(func pid=75223)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=75223)[0m f1_macro: 0.28892916939857016
[2m[36m(func pid=75223)[0m f1_weighted: 0.3229302644113403
[2m[36m(func pid=75223)[0m f1_per_class: [0.439, 0.444, 0.21, 0.349, 0.12, 0.333, 0.249, 0.254, 0.24, 0.25]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6253 | Steps: 4 | Val loss: 2.3788 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=93934)[0m top1: 0.4183768656716418
[2m[36m(func pid=93934)[0m top5: 0.9449626865671642
[2m[36m(func pid=93934)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=93934)[0m f1_macro: 0.36227596022779374
[2m[36m(func pid=93934)[0m f1_weighted: 0.4278411358624216
[2m[36m(func pid=93934)[0m f1_per_class: [0.465, 0.54, 0.25, 0.421, 0.105, 0.352, 0.427, 0.428, 0.291, 0.344]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.2943097014925373
[2m[36m(func pid=93317)[0m top5: 0.7859141791044776
[2m[36m(func pid=93317)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=93317)[0m f1_macro: 0.22528934502959888
[2m[36m(func pid=93317)[0m f1_weighted: 0.30098433895364735
[2m[36m(func pid=93317)[0m f1_per_class: [0.215, 0.152, 0.086, 0.496, 0.135, 0.344, 0.218, 0.391, 0.06, 0.157]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.5070 | Steps: 4 | Val loss: 1.8919 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=99348)[0m top1: 0.04757462686567164
[2m[36m(func pid=99348)[0m top5: 0.5825559701492538
[2m[36m(func pid=99348)[0m f1_micro: 0.04757462686567164
[2m[36m(func pid=99348)[0m f1_macro: 0.040562373966680394
[2m[36m(func pid=99348)[0m f1_weighted: 0.04310965373682927
[2m[36m(func pid=99348)[0m f1_per_class: [0.162, 0.0, 0.019, 0.136, 0.025, 0.0, 0.003, 0.0, 0.0, 0.061]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7119 | Steps: 4 | Val loss: 1.7724 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.2115 | Steps: 4 | Val loss: 1.9856 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 15:07:10 (running for 00:33:36.55)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.507 |      0.341 |                   95 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.022 |      0.225 |                   26 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.542 |      0.362 |                   25 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  2.625 |      0.041 |                    2 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.363339552238806
[2m[36m(func pid=75223)[0m top5: 0.9043843283582089
[2m[36m(func pid=75223)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=75223)[0m f1_macro: 0.3413199236088566
[2m[36m(func pid=75223)[0m f1_weighted: 0.3726447770139643
[2m[36m(func pid=75223)[0m f1_per_class: [0.467, 0.433, 0.333, 0.317, 0.115, 0.373, 0.396, 0.397, 0.316, 0.267]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.2573 | Steps: 4 | Val loss: 2.1309 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=93934)[0m top1: 0.4230410447761194
[2m[36m(func pid=93934)[0m top5: 0.9375
[2m[36m(func pid=93934)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=93934)[0m f1_macro: 0.3774652956254677
[2m[36m(func pid=93934)[0m f1_weighted: 0.43359911970179543
[2m[36m(func pid=93934)[0m f1_per_class: [0.521, 0.515, 0.312, 0.41, 0.117, 0.349, 0.466, 0.382, 0.373, 0.33]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.30550373134328357
[2m[36m(func pid=93317)[0m top5: 0.792910447761194
[2m[36m(func pid=93317)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=93317)[0m f1_macro: 0.2357393617317154
[2m[36m(func pid=93317)[0m f1_weighted: 0.3204504394632315
[2m[36m(func pid=93317)[0m f1_per_class: [0.194, 0.233, 0.101, 0.497, 0.132, 0.351, 0.231, 0.403, 0.064, 0.153]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4571 | Steps: 4 | Val loss: 1.7340 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=99348)[0m top1: 0.25699626865671643
[2m[36m(func pid=99348)[0m top5: 0.7164179104477612
[2m[36m(func pid=99348)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=99348)[0m f1_macro: 0.1936306228033541
[2m[36m(func pid=99348)[0m f1_weighted: 0.2027835357875878
[2m[36m(func pid=99348)[0m f1_per_class: [0.344, 0.0, 0.355, 0.59, 0.027, 0.177, 0.0, 0.072, 0.0, 0.37]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4770 | Steps: 4 | Val loss: 1.7699 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9316 | Steps: 4 | Val loss: 1.9628 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:07:16 (running for 00:33:42.43)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.457 |      0.378 |                   96 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.212 |      0.236 |                   27 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.712 |      0.377 |                   26 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  2.257 |      0.194 |                    3 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.3908582089552239
[2m[36m(func pid=75223)[0m top5: 0.9267723880597015
[2m[36m(func pid=75223)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=75223)[0m f1_macro: 0.3778804053276087
[2m[36m(func pid=75223)[0m f1_weighted: 0.39715259678933074
[2m[36m(func pid=75223)[0m f1_per_class: [0.486, 0.442, 0.371, 0.339, 0.122, 0.422, 0.412, 0.424, 0.422, 0.338]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.9106 | Steps: 4 | Val loss: 1.7319 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=93934)[0m top1: 0.43330223880597013
[2m[36m(func pid=93934)[0m top5: 0.9468283582089553
[2m[36m(func pid=93934)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=93934)[0m f1_macro: 0.3982279441617543
[2m[36m(func pid=93934)[0m f1_weighted: 0.4470623848531587
[2m[36m(func pid=93934)[0m f1_per_class: [0.605, 0.483, 0.375, 0.401, 0.142, 0.367, 0.518, 0.407, 0.4, 0.286]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.302705223880597
[2m[36m(func pid=93317)[0m top5: 0.8078358208955224
[2m[36m(func pid=93317)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=93317)[0m f1_macro: 0.23958120579389308
[2m[36m(func pid=93317)[0m f1_weighted: 0.311683645797534
[2m[36m(func pid=93317)[0m f1_per_class: [0.259, 0.268, 0.13, 0.483, 0.122, 0.32, 0.205, 0.378, 0.084, 0.148]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.36986940298507465
[2m[36m(func pid=99348)[0m top5: 0.8810634328358209
[2m[36m(func pid=99348)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=99348)[0m f1_macro: 0.31711376244982986
[2m[36m(func pid=99348)[0m f1_weighted: 0.33592399947025775
[2m[36m(func pid=99348)[0m f1_per_class: [0.354, 0.136, 0.6, 0.585, 0.151, 0.425, 0.197, 0.379, 0.184, 0.159]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3983 | Steps: 4 | Val loss: 1.6532 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6730 | Steps: 4 | Val loss: 1.9196 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0142 | Steps: 4 | Val loss: 1.9324 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.4409 | Steps: 4 | Val loss: 1.7298 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=75223)[0m top1: 0.4118470149253731== Status ==
Current time: 2024-01-07 15:07:22 (running for 00:33:48.49)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.401 |                   97 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  1.932 |      0.24  |                   28 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.477 |      0.398 |                   27 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  1.911 |      0.317 |                    4 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=75223)[0m top5: 0.9365671641791045
[2m[36m(func pid=75223)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=75223)[0m f1_macro: 0.40052595904217003
[2m[36m(func pid=75223)[0m f1_weighted: 0.4272030138993083
[2m[36m(func pid=75223)[0m f1_per_class: [0.52, 0.441, 0.456, 0.39, 0.112, 0.404, 0.462, 0.455, 0.432, 0.333]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m top1: 0.4281716417910448
[2m[36m(func pid=93934)[0m top5: 0.9486940298507462
[2m[36m(func pid=93934)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=93934)[0m f1_macro: 0.39188004226256945
[2m[36m(func pid=93934)[0m f1_weighted: 0.4395348366066813
[2m[36m(func pid=93934)[0m f1_per_class: [0.471, 0.457, 0.533, 0.345, 0.118, 0.326, 0.582, 0.413, 0.386, 0.288]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.30223880597014924
[2m[36m(func pid=93317)[0m top5: 0.820429104477612
[2m[36m(func pid=93317)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=93317)[0m f1_macro: 0.24565938046334682
[2m[36m(func pid=93317)[0m f1_weighted: 0.30954987987486715
[2m[36m(func pid=93317)[0m f1_per_class: [0.305, 0.281, 0.17, 0.483, 0.117, 0.333, 0.186, 0.339, 0.104, 0.138]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.33488805970149255
[2m[36m(func pid=99348)[0m top5: 0.9034514925373134
[2m[36m(func pid=99348)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=99348)[0m f1_macro: 0.29898977176960223
[2m[36m(func pid=99348)[0m f1_weighted: 0.362285934790274
[2m[36m(func pid=99348)[0m f1_per_class: [0.317, 0.342, 0.3, 0.483, 0.351, 0.258, 0.351, 0.275, 0.185, 0.127]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3131 | Steps: 4 | Val loss: 1.6264 | Batch size: 32 | lr: 0.001 | Duration: 3.36s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3421 | Steps: 4 | Val loss: 1.9830 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9515 | Steps: 4 | Val loss: 1.9022 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3051 | Steps: 4 | Val loss: 2.4538 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:07:28 (running for 00:33:54.51)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.313 |      0.41  |                   98 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.014 |      0.246 |                   29 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.673 |      0.392 |                   28 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  1.441 |      0.299 |                    5 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=75223)[0m top1: 0.44449626865671643
[2m[36m(func pid=75223)[0m top5: 0.9272388059701493
[2m[36m(func pid=75223)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=75223)[0m f1_macro: 0.4103823684603293
[2m[36m(func pid=75223)[0m f1_weighted: 0.46018907250628394
[2m[36m(func pid=75223)[0m f1_per_class: [0.467, 0.49, 0.537, 0.503, 0.144, 0.354, 0.465, 0.486, 0.317, 0.341]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93934)[0m top1: 0.41324626865671643
[2m[36m(func pid=93934)[0m top5: 0.945429104477612
[2m[36m(func pid=93934)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=93934)[0m f1_macro: 0.38820890643373235
[2m[36m(func pid=93934)[0m f1_weighted: 0.43268194760276796
[2m[36m(func pid=93934)[0m f1_per_class: [0.528, 0.422, 0.571, 0.345, 0.095, 0.298, 0.591, 0.407, 0.365, 0.259]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.32742537313432835
[2m[36m(func pid=93317)[0m top5: 0.8306902985074627
[2m[36m(func pid=93317)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=93317)[0m f1_macro: 0.25956016005462745
[2m[36m(func pid=93317)[0m f1_weighted: 0.33970799090394443
[2m[36m(func pid=93317)[0m f1_per_class: [0.281, 0.342, 0.164, 0.481, 0.122, 0.332, 0.249, 0.386, 0.083, 0.155]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.22201492537313433
[2m[36m(func pid=99348)[0m top5: 0.8386194029850746
[2m[36m(func pid=99348)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=99348)[0m f1_macro: 0.1775225554036403
[2m[36m(func pid=99348)[0m f1_weighted: 0.24589024000022525
[2m[36m(func pid=99348)[0m f1_per_class: [0.242, 0.128, 0.0, 0.359, 0.105, 0.177, 0.26, 0.265, 0.11, 0.13]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5255 | Steps: 4 | Val loss: 2.0732 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.6213 | Steps: 4 | Val loss: 1.7670 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0687 | Steps: 4 | Val loss: 1.8752 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4297 | Steps: 4 | Val loss: 3.0128 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:07:34 (running for 00:33:59.98)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.313 |      0.41  |                   98 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  1.951 |      0.26  |                   30 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.526 |      0.397 |                   30 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  1.305 |      0.178 |                    6 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.40904850746268656
[2m[36m(func pid=93934)[0m top5: 0.9347014925373134
[2m[36m(func pid=93934)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=93934)[0m f1_macro: 0.3972063150290521
[2m[36m(func pid=93934)[0m f1_weighted: 0.43602039518152197
[2m[36m(func pid=93934)[0m f1_per_class: [0.547, 0.412, 0.49, 0.403, 0.098, 0.307, 0.525, 0.482, 0.46, 0.247]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.4197761194029851
[2m[36m(func pid=75223)[0m top5: 0.9001865671641791
[2m[36m(func pid=75223)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=75223)[0m f1_macro: 0.37172642701366226
[2m[36m(func pid=75223)[0m f1_weighted: 0.4213770872009227
[2m[36m(func pid=75223)[0m f1_per_class: [0.449, 0.491, 0.49, 0.542, 0.166, 0.311, 0.368, 0.253, 0.242, 0.408]
[2m[36m(func pid=75223)[0m 
[2m[36m(func pid=93317)[0m top1: 0.3218283582089552
[2m[36m(func pid=93317)[0m top5: 0.840018656716418
[2m[36m(func pid=93317)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=93317)[0m f1_macro: 0.2597358446968909
[2m[36m(func pid=93317)[0m f1_weighted: 0.3365182682035258
[2m[36m(func pid=93317)[0m f1_per_class: [0.297, 0.33, 0.162, 0.46, 0.124, 0.311, 0.271, 0.372, 0.105, 0.164]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.1837686567164179
[2m[36m(func pid=99348)[0m top5: 0.7164179104477612
[2m[36m(func pid=99348)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=99348)[0m f1_macro: 0.1414586807884301
[2m[36m(func pid=99348)[0m f1_weighted: 0.20536241008405268
[2m[36m(func pid=99348)[0m f1_per_class: [0.151, 0.371, 0.14, 0.305, 0.071, 0.015, 0.147, 0.015, 0.15, 0.049]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3570 | Steps: 4 | Val loss: 2.0439 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=75223)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2850 | Steps: 4 | Val loss: 1.9922 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8418 | Steps: 4 | Val loss: 1.8780 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.2875 | Steps: 4 | Val loss: 3.8383 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 15:07:39 (running for 00:34:05.32)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00013 | RUNNING    | 192.168.7.53:75223 | 0.001  |       0.9  |         0.0001 |  0.621 |      0.372 |                   99 |
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317 | 0.0001 |       0.99 |         1e-05  |  2.069 |      0.26  |                   31 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934 | 0.001  |       0.99 |         1e-05  |  0.357 |      0.375 |                   31 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348 | 0.01   |       0.99 |         1e-05  |  1.43  |      0.141 |                    7 |
| train_5ae7f_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050 | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435 | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861 | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288 | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295 | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733 | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804 | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854 | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960 | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441 | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031 | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.42257462686567165
[2m[36m(func pid=93934)[0m top5: 0.9277052238805971
[2m[36m(func pid=93934)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=93934)[0m f1_macro: 0.3751260701079504
[2m[36m(func pid=93934)[0m f1_weighted: 0.4516058101638385
[2m[36m(func pid=93934)[0m f1_per_class: [0.448, 0.376, 0.353, 0.503, 0.109, 0.28, 0.533, 0.416, 0.513, 0.22]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=75223)[0m top1: 0.36100746268656714
[2m[36m(func pid=75223)[0m top5: 0.8852611940298507
[2m[36m(func pid=75223)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=75223)[0m f1_macro: 0.3189213423128957
[2m[36m(func pid=75223)[0m f1_weighted: 0.3676418059059472
[2m[36m(func pid=75223)[0m f1_per_class: [0.459, 0.461, 0.295, 0.517, 0.174, 0.276, 0.265, 0.193, 0.16, 0.389]
[2m[36m(func pid=93317)[0m top1: 0.30177238805970147
[2m[36m(func pid=93317)[0m top5: 0.835820895522388
[2m[36m(func pid=93317)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=93317)[0m f1_macro: 0.2463596861159189
[2m[36m(func pid=93317)[0m f1_weighted: 0.3171935191786896
[2m[36m(func pid=93317)[0m f1_per_class: [0.244, 0.322, 0.173, 0.433, 0.125, 0.272, 0.256, 0.362, 0.112, 0.165]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.16044776119402984
[2m[36m(func pid=99348)[0m top5: 0.6427238805970149
[2m[36m(func pid=99348)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=99348)[0m f1_macro: 0.09930402262598492
[2m[36m(func pid=99348)[0m f1_weighted: 0.15182205241925645
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.441, 0.051, 0.084, 0.107, 0.0, 0.158, 0.0, 0.115, 0.036]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3195 | Steps: 4 | Val loss: 2.1101 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9344 | Steps: 4 | Val loss: 1.8636 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.5630 | Steps: 4 | Val loss: 3.3651 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=93934)[0m top1: 0.4141791044776119
[2m[36m(func pid=93934)[0m top5: 0.9151119402985075
[2m[36m(func pid=93934)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=93934)[0m f1_macro: 0.3463173559140476
[2m[36m(func pid=93934)[0m f1_weighted: 0.4435622997649693
[2m[36m(func pid=93934)[0m f1_per_class: [0.289, 0.337, 0.263, 0.495, 0.114, 0.293, 0.551, 0.4, 0.459, 0.261]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.31576492537313433
[2m[36m(func pid=93317)[0m top5: 0.8344216417910447
[2m[36m(func pid=93317)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=93317)[0m f1_macro: 0.26022920936070126
[2m[36m(func pid=93317)[0m f1_weighted: 0.33640723038519216
[2m[36m(func pid=93317)[0m f1_per_class: [0.207, 0.372, 0.175, 0.416, 0.155, 0.299, 0.294, 0.376, 0.13, 0.179]
[2m[36m(func pid=99348)[0m top1: 0.292910447761194
[2m[36m(func pid=99348)[0m top5: 0.6916977611940298
[2m[36m(func pid=99348)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=99348)[0m f1_macro: 0.18166842973806122
[2m[36m(func pid=99348)[0m f1_weighted: 0.24797552192987266
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.379, 0.053, 0.0, 0.25, 0.0, 0.516, 0.346, 0.164, 0.109]
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3421 | Steps: 4 | Val loss: 2.2540 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:07:45 (running for 00:34:10.76)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.842 |      0.246 |                   32 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.319 |      0.346 |                   32 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.288 |      0.099 |                    8 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=102012)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=102012)[0m Configuration completed!
[2m[36m(func pid=102012)[0m New optimizer parameters:
[2m[36m(func pid=102012)[0m SGD (
[2m[36m(func pid=102012)[0m Parameter Group 0
[2m[36m(func pid=102012)[0m     dampening: 0
[2m[36m(func pid=102012)[0m     differentiable: False
[2m[36m(func pid=102012)[0m     foreach: None
[2m[36m(func pid=102012)[0m     lr: 0.1
[2m[36m(func pid=102012)[0m     maximize: False
[2m[36m(func pid=102012)[0m     momentum: 0.99
[2m[36m(func pid=102012)[0m     nesterov: False
[2m[36m(func pid=102012)[0m     weight_decay: 1e-05
[2m[36m(func pid=102012)[0m )
[2m[36m(func pid=102012)[0m 
== Status ==
Current time: 2024-01-07 15:07:50 (running for 00:34:16.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.934 |      0.26  |                   33 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.342 |      0.34  |                   33 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.563 |      0.182 |                    9 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.41138059701492535
[2m[36m(func pid=93934)[0m top5: 0.9071828358208955
[2m[36m(func pid=93934)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=93934)[0m f1_macro: 0.33995979026226075
[2m[36m(func pid=93934)[0m f1_weighted: 0.44172330695780665
[2m[36m(func pid=93934)[0m f1_per_class: [0.271, 0.361, 0.265, 0.43, 0.094, 0.276, 0.607, 0.388, 0.395, 0.311]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8225 | Steps: 4 | Val loss: 1.8464 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.5107 | Steps: 4 | Val loss: 11.8963 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.8476 | Steps: 4 | Val loss: 3.8112 | Batch size: 32 | lr: 0.1 | Duration: 4.75s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2409 | Steps: 4 | Val loss: 2.3343 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=99348)[0m top1: 0.11147388059701492
[2m[36m(func pid=99348)[0m top5: 0.4216417910447761
[2m[36m(func pid=99348)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=99348)[0m f1_macro: 0.08687544793434014
[2m[36m(func pid=99348)[0m f1_weighted: 0.07321868889919768
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.392, 0.0, 0.0, 0.308, 0.0, 0.0, 0.0, 0.072, 0.098]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m top1: 0.3306902985074627
[2m[36m(func pid=93317)[0m top5: 0.8414179104477612
[2m[36m(func pid=93317)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=93317)[0m f1_macro: 0.2682125645657468
[2m[36m(func pid=93317)[0m f1_weighted: 0.35518854511898623
[2m[36m(func pid=93317)[0m f1_per_class: [0.209, 0.409, 0.186, 0.414, 0.138, 0.279, 0.345, 0.372, 0.142, 0.19]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:07:56 (running for 00:34:21.80)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.822 |      0.268 |                   34 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.342 |      0.34  |                   33 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.511 |      0.087 |                   10 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.848 |      0.052 |                    1 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.4197761194029851
[2m[36m(func pid=93934)[0m top5: 0.9081156716417911
[2m[36m(func pid=93934)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=93934)[0m f1_macro: 0.3563648550740417
[2m[36m(func pid=93934)[0m f1_weighted: 0.4472599624864669
[2m[36m(func pid=93934)[0m f1_per_class: [0.319, 0.378, 0.312, 0.41, 0.089, 0.265, 0.626, 0.411, 0.422, 0.33]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m top1: 0.02378731343283582
[2m[36m(func pid=102012)[0m top5: 0.27005597014925375
[2m[36m(func pid=102012)[0m f1_micro: 0.02378731343283582
[2m[36m(func pid=102012)[0m f1_macro: 0.051882635392100626
[2m[36m(func pid=102012)[0m f1_weighted: 0.004962618388711678
[2m[36m(func pid=102012)[0m f1_per_class: [0.041, 0.0, 0.0, 0.0, 0.345, 0.0, 0.0, 0.0, 0.0, 0.133]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8861 | Steps: 4 | Val loss: 1.8332 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5330 | Steps: 4 | Val loss: 13.9429 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.0500 | Steps: 4 | Val loss: 2.4462 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.4530 | Steps: 4 | Val loss: 12.3265 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=93317)[0m top1: 0.355410447761194
[2m[36m(func pid=93317)[0m top5: 0.8432835820895522
[2m[36m(func pid=93317)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=93317)[0m f1_macro: 0.2887995566558747
[2m[36m(func pid=93317)[0m f1_weighted: 0.3824984381061055
[2m[36m(func pid=93317)[0m f1_per_class: [0.224, 0.422, 0.173, 0.461, 0.133, 0.327, 0.354, 0.402, 0.186, 0.206]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.08722014925373134
[2m[36m(func pid=99348)[0m top5: 0.48973880597014924
[2m[36m(func pid=99348)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=99348)[0m f1_macro: 0.06324636965575559
[2m[36m(func pid=99348)[0m f1_weighted: 0.0668489641933697
[2m[36m(func pid=99348)[0m f1_per_class: [0.153, 0.352, 0.05, 0.003, 0.0, 0.0, 0.0, 0.0, 0.051, 0.024]
[2m[36m(func pid=99348)[0m 
== Status ==
Current time: 2024-01-07 15:08:01 (running for 00:34:27.22)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.886 |      0.289 |                   35 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.241 |      0.356 |                   34 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.533 |      0.063 |                   11 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.453 |      0.011 |                    2 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102012)[0m top1: 0.013059701492537313
[2m[36m(func pid=102012)[0m top5: 0.24720149253731344
[2m[36m(func pid=102012)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=102012)[0m f1_macro: 0.011108569934982191
[2m[36m(func pid=102012)[0m f1_weighted: 0.0007664571903120395
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m top1: 0.4146455223880597
[2m[36m(func pid=93934)[0m top5: 0.9239738805970149
[2m[36m(func pid=93934)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=93934)[0m f1_macro: 0.3657710629779498
[2m[36m(func pid=93934)[0m f1_weighted: 0.41972393739692593
[2m[36m(func pid=93934)[0m f1_per_class: [0.432, 0.369, 0.522, 0.351, 0.087, 0.206, 0.637, 0.262, 0.37, 0.423]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.7764 | Steps: 4 | Val loss: 12.7100 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7181 | Steps: 4 | Val loss: 1.8038 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.6302 | Steps: 4 | Val loss: 5631.7930 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4979 | Steps: 4 | Val loss: 2.5139 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=99348)[0m top1: 0.10960820895522388
[2m[36m(func pid=99348)[0m top5: 0.7621268656716418
[2m[36m(func pid=99348)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=99348)[0m f1_macro: 0.05943934274166832
[2m[36m(func pid=99348)[0m f1_weighted: 0.060904667800927785
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.067, 0.2]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m top1: 0.37406716417910446
[2m[36m(func pid=93317)[0m top5: 0.8568097014925373
[2m[36m(func pid=93317)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=93317)[0m f1_macro: 0.3048053524969848
[2m[36m(func pid=93317)[0m f1_weighted: 0.4012530385444077
[2m[36m(func pid=93317)[0m f1_per_class: [0.237, 0.433, 0.175, 0.462, 0.122, 0.362, 0.389, 0.425, 0.19, 0.253]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.006063432835820896
[2m[36m(func pid=102012)[0m top5: 0.5093283582089553
[2m[36m(func pid=102012)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=102012)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=102012)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 15:08:07 (running for 00:34:32.60)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.718 |      0.305 |                   36 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.05  |      0.366 |                   35 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.776 |      0.059 |                   12 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.63  |      0.001 |                    3 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m top1: 0.38759328358208955
[2m[36m(func pid=93934)[0m top5: 0.9333022388059702
[2m[36m(func pid=93934)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=93934)[0m f1_macro: 0.34323770931159336
[2m[36m(func pid=93934)[0m f1_weighted: 0.3848314740249301
[2m[36m(func pid=93934)[0m f1_per_class: [0.514, 0.421, 0.5, 0.239, 0.07, 0.162, 0.618, 0.211, 0.403, 0.295]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.5949 | Steps: 4 | Val loss: 10.3016 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6660 | Steps: 4 | Val loss: 1.7914 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.5216 | Steps: 4 | Val loss: 82442.3047 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2387 | Steps: 4 | Val loss: 2.8829 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=99348)[0m top1: 0.09748134328358209
[2m[36m(func pid=99348)[0m top5: 0.6431902985074627
[2m[36m(func pid=99348)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=99348)[0m f1_macro: 0.05991142650582203
[2m[36m(func pid=99348)[0m f1_weighted: 0.0473476345487924
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.137, 0.0, 0.01, 0.0, 0.0, 0.003, 0.276, 0.1, 0.073]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m top1: 0.36847014925373134
[2m[36m(func pid=93317)[0m top5: 0.8591417910447762
[2m[36m(func pid=93317)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=93317)[0m f1_macro: 0.3035399703867231
[2m[36m(func pid=93317)[0m f1_weighted: 0.397263700224671
[2m[36m(func pid=93317)[0m f1_per_class: [0.217, 0.43, 0.195, 0.432, 0.133, 0.365, 0.404, 0.421, 0.215, 0.223]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:12 (running for 00:34:38.00)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.666 |      0.304 |                   37 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.498 |      0.343 |                   36 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.595 |      0.06  |                   13 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.522 |      0.004 |                    4 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.29757462686567165
[2m[36m(func pid=93934)[0m top5: 0.8502798507462687
[2m[36m(func pid=93934)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=93934)[0m f1_macro: 0.2623729777689636
[2m[36m(func pid=93934)[0m f1_weighted: 0.31725902172101716
[2m[36m(func pid=93934)[0m f1_per_class: [0.406, 0.42, 0.465, 0.233, 0.071, 0.098, 0.476, 0.101, 0.248, 0.107]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m top1: 0.020522388059701493
[2m[36m(func pid=102012)[0m top5: 0.5237873134328358
[2m[36m(func pid=102012)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=102012)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=102012)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=102012)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2402 | Steps: 4 | Val loss: 13.6660 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6260 | Steps: 4 | Val loss: 1.7732 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1600 | Steps: 4 | Val loss: 3.0930 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.9893 | Steps: 4 | Val loss: 435381.8125 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=99348)[0m top1: 0.13386194029850745
[2m[36m(func pid=99348)[0m top5: 0.6763059701492538
[2m[36m(func pid=99348)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=99348)[0m f1_macro: 0.07669110284382682
[2m[36m(func pid=99348)[0m f1_weighted: 0.1503236846282179
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.132, 0.0, 0.007, 0.111, 0.0, 0.41, 0.0, 0.072, 0.036]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m top1: 0.37546641791044777
[2m[36m(func pid=93317)[0m top5: 0.8666044776119403
[2m[36m(func pid=93317)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=93317)[0m f1_macro: 0.31063750151825986
[2m[36m(func pid=93317)[0m f1_weighted: 0.40496988226361047
[2m[36m(func pid=93317)[0m f1_per_class: [0.212, 0.442, 0.214, 0.41, 0.113, 0.376, 0.435, 0.432, 0.228, 0.244]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:17 (running for 00:34:43.47)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.626 |      0.311 |                   38 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.16  |      0.224 |                   38 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.24  |      0.077 |                   14 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.522 |      0.004 |                    4 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.27798507462686567
[2m[36m(func pid=93934)[0m top5: 0.8069029850746269
[2m[36m(func pid=93934)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=93934)[0m f1_macro: 0.22409614849926793
[2m[36m(func pid=93934)[0m f1_weighted: 0.2962261733023374
[2m[36m(func pid=93934)[0m f1_per_class: [0.233, 0.418, 0.383, 0.286, 0.08, 0.04, 0.399, 0.074, 0.241, 0.087]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m top1: 0.006063432835820896
[2m[36m(func pid=102012)[0m top5: 0.5093283582089553
[2m[36m(func pid=102012)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=102012)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=102012)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.5779 | Steps: 4 | Val loss: 13.8761 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.5571 | Steps: 4 | Val loss: 1.7548 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7765 | Steps: 4 | Val loss: 3.0851 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=99348)[0m top1: 0.13292910447761194
[2m[36m(func pid=99348)[0m top5: 0.7784514925373134
[2m[36m(func pid=99348)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=99348)[0m f1_macro: 0.07710567646827732
[2m[36m(func pid=99348)[0m f1_weighted: 0.12625624319543774
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.159, 0.0, 0.003, 0.0, 0.062, 0.258, 0.212, 0.032, 0.044]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 4.2340 | Steps: 4 | Val loss: 60744.7109 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=93317)[0m top1: 0.3694029850746269
[2m[36m(func pid=93317)[0m top5: 0.8726679104477612
[2m[36m(func pid=93317)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=93317)[0m f1_macro: 0.3061066867304696
[2m[36m(func pid=93317)[0m f1_weighted: 0.4001333043931616
[2m[36m(func pid=93317)[0m f1_per_class: [0.205, 0.427, 0.229, 0.392, 0.128, 0.375, 0.449, 0.424, 0.211, 0.221]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:23 (running for 00:34:48.69)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.557 |      0.306 |                   39 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.777 |      0.205 |                   39 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  3.578 |      0.077 |                   15 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.989 |      0.001 |                    5 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.300839552238806
[2m[36m(func pid=93934)[0m top5: 0.7644589552238806
[2m[36m(func pid=93934)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=93934)[0m f1_macro: 0.2050160179435588
[2m[36m(func pid=93934)[0m f1_weighted: 0.3076493482001275
[2m[36m(func pid=93934)[0m f1_per_class: [0.143, 0.436, 0.116, 0.335, 0.188, 0.022, 0.395, 0.096, 0.217, 0.102]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m top1: 0.03311567164179104
[2m[36m(func pid=102012)[0m top5: 0.5149253731343284
[2m[36m(func pid=102012)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=102012)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=102012)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.2294 | Steps: 4 | Val loss: 9.3057 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5770 | Steps: 4 | Val loss: 1.7579 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=99348)[0m top1: 0.3045708955223881
[2m[36m(func pid=99348)[0m top5: 0.8801305970149254
[2m[36m(func pid=99348)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=99348)[0m f1_macro: 0.17042844627983633
[2m[36m(func pid=99348)[0m f1_weighted: 0.29667745702701065
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.294, 0.0, 0.413, 0.16, 0.09, 0.341, 0.264, 0.053, 0.091]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5724 | Steps: 4 | Val loss: 2.4091 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.9952 | Steps: 4 | Val loss: 13203.0918 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=93317)[0m top1: 0.3689365671641791
[2m[36m(func pid=93317)[0m top5: 0.8722014925373134
[2m[36m(func pid=93317)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=93317)[0m f1_macro: 0.3108299459759615
[2m[36m(func pid=93317)[0m f1_weighted: 0.4030515663682633
[2m[36m(func pid=93317)[0m f1_per_class: [0.21, 0.412, 0.235, 0.397, 0.114, 0.379, 0.455, 0.427, 0.255, 0.224]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:28 (running for 00:34:54.17)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.577 |      0.311 |                   40 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.572 |      0.267 |                   40 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  3.229 |      0.17  |                   16 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  4.234 |      0.006 |                    6 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.3530783582089552
[2m[36m(func pid=93934)[0m top5: 0.8857276119402985
[2m[36m(func pid=93934)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=93934)[0m f1_macro: 0.26724054440407913
[2m[36m(func pid=93934)[0m f1_weighted: 0.361295558665962
[2m[36m(func pid=93934)[0m f1_per_class: [0.303, 0.397, 0.218, 0.349, 0.136, 0.048, 0.515, 0.328, 0.222, 0.157]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.6291 | Steps: 4 | Val loss: 9.0900 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=102012)[0m top1: 0.006063432835820896
[2m[36m(func pid=102012)[0m top5: 0.5097947761194029
[2m[36m(func pid=102012)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=102012)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=102012)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.6115 | Steps: 4 | Val loss: 1.7243 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=99348)[0m top1: 0.28031716417910446
[2m[36m(func pid=99348)[0m top5: 0.8992537313432836
[2m[36m(func pid=99348)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=99348)[0m f1_macro: 0.1553829704379831
[2m[36m(func pid=99348)[0m f1_weighted: 0.26927688381933346
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.126, 0.0, 0.514, 0.087, 0.231, 0.203, 0.233, 0.051, 0.108]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3655 | Steps: 4 | Val loss: 3.0728 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.5520 | Steps: 4 | Val loss: 1832.8461 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=93317)[0m top1: 0.3927238805970149
[2m[36m(func pid=93317)[0m top5: 0.878731343283582
[2m[36m(func pid=93317)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=93317)[0m f1_macro: 0.3277450389957726
[2m[36m(func pid=93317)[0m f1_weighted: 0.424761404043938
[2m[36m(func pid=93317)[0m f1_per_class: [0.232, 0.436, 0.282, 0.414, 0.13, 0.364, 0.497, 0.446, 0.26, 0.215]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.2994402985074627
[2m[36m(func pid=93934)[0m top5: 0.8479477611940298
[2m[36m(func pid=93934)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=93934)[0m f1_macro: 0.24807210505258004
[2m[36m(func pid=93934)[0m f1_weighted: 0.31843694989542154
[2m[36m(func pid=93934)[0m f1_per_class: [0.368, 0.34, 0.073, 0.364, 0.14, 0.035, 0.383, 0.352, 0.276, 0.15]
== Status ==
Current time: 2024-01-07 15:08:34 (running for 00:34:59.66)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.611 |      0.328 |                   41 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.366 |      0.248 |                   41 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  3.629 |      0.155 |                   17 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  4.995 |      0.001 |                    7 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7026 | Steps: 4 | Val loss: 14.3970 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=102012)[0m top1: 0.06669776119402986
[2m[36m(func pid=102012)[0m top5: 0.31529850746268656
[2m[36m(func pid=102012)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=102012)[0m f1_macro: 0.03975729668801112
[2m[36m(func pid=102012)[0m f1_weighted: 0.06608564487441201
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.383, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.4912 | Steps: 4 | Val loss: 1.7082 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=99348)[0m top1: 0.11986940298507463
[2m[36m(func pid=99348)[0m top5: 0.8152985074626866
[2m[36m(func pid=99348)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=99348)[0m f1_macro: 0.1125294020892413
[2m[36m(func pid=99348)[0m f1_weighted: 0.09381177365885074
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.04, 0.154, 0.016, 0.23, 0.162, 0.149, 0.233, 0.083, 0.059]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2815 | Steps: 4 | Val loss: 3.3645 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.2058 | Steps: 4 | Val loss: 450.2527 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=93317)[0m top1: 0.386660447761194
[2m[36m(func pid=93317)[0m top5: 0.8796641791044776
[2m[36m(func pid=93317)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=93317)[0m f1_macro: 0.3290104807705462
[2m[36m(func pid=93317)[0m f1_weighted: 0.41767683452330623
[2m[36m(func pid=93317)[0m f1_per_class: [0.242, 0.42, 0.308, 0.407, 0.136, 0.39, 0.479, 0.441, 0.271, 0.197]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:39 (running for 00:35:05.01)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.491 |      0.329 |                   42 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.281 |      0.266 |                   42 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.703 |      0.113 |                   18 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.552 |      0.04  |                    8 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.33488805970149255
[2m[36m(func pid=93934)[0m top5: 0.847481343283582
[2m[36m(func pid=93934)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=93934)[0m f1_macro: 0.2659120908473428
[2m[36m(func pid=93934)[0m f1_weighted: 0.3574083081408116
[2m[36m(func pid=93934)[0m f1_per_class: [0.31, 0.347, 0.093, 0.399, 0.117, 0.105, 0.441, 0.397, 0.333, 0.118]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5943 | Steps: 4 | Val loss: 19.9210 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=102012)[0m top1: 0.06389925373134328
[2m[36m(func pid=102012)[0m top5: 0.5848880597014925
[2m[36m(func pid=102012)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=102012)[0m f1_macro: 0.021663376540599945
[2m[36m(func pid=102012)[0m f1_weighted: 0.017650941885498848
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.043]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.3753 | Steps: 4 | Val loss: 1.6899 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=99348)[0m top1: 0.15764925373134328
[2m[36m(func pid=99348)[0m top5: 0.6408582089552238
[2m[36m(func pid=99348)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=99348)[0m f1_macro: 0.1564097004343098
[2m[36m(func pid=99348)[0m f1_weighted: 0.1324271048705284
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.011, 0.462, 0.0, 0.318, 0.016, 0.349, 0.288, 0.068, 0.053]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8016 | Steps: 4 | Val loss: 3.6867 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.0058 | Steps: 4 | Val loss: 183.6738 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=93317)[0m top1: 0.4039179104477612
[2m[36m(func pid=93317)[0m top5: 0.8894589552238806
[2m[36m(func pid=93317)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=93317)[0m f1_macro: 0.3447251035284996
[2m[36m(func pid=93317)[0m f1_weighted: 0.4348506962478573
[2m[36m(func pid=93317)[0m f1_per_class: [0.285, 0.425, 0.338, 0.45, 0.119, 0.39, 0.485, 0.456, 0.277, 0.222]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:44 (running for 00:35:10.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.375 |      0.345 |                   43 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.802 |      0.232 |                   43 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.594 |      0.156 |                   19 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  4.206 |      0.022 |                    9 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.34468283582089554
[2m[36m(func pid=93934)[0m top5: 0.8125
[2m[36m(func pid=93934)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=93934)[0m f1_macro: 0.23220669874856284
[2m[36m(func pid=93934)[0m f1_weighted: 0.3557378138411933
[2m[36m(func pid=93934)[0m f1_per_class: [0.212, 0.326, 0.051, 0.409, 0.113, 0.036, 0.514, 0.173, 0.32, 0.168]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2527 | Steps: 4 | Val loss: 35.3027 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=102012)[0m top1: 0.29197761194029853
[2m[36m(func pid=102012)[0m top5: 0.5326492537313433
[2m[36m(func pid=102012)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=102012)[0m f1_macro: 0.050442371759112016
[2m[36m(func pid=102012)[0m f1_weighted: 0.14559795246086793
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.485, 0.0, 0.0, 0.014]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.4508 | Steps: 4 | Val loss: 1.6804 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=99348)[0m top1: 0.09561567164179105
[2m[36m(func pid=99348)[0m top5: 0.6002798507462687
[2m[36m(func pid=99348)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=99348)[0m f1_macro: 0.06981730778923151
[2m[36m(func pid=99348)[0m f1_weighted: 0.05525725609930047
[2m[36m(func pid=99348)[0m f1_per_class: [0.04, 0.016, 0.0, 0.0, 0.133, 0.0, 0.111, 0.245, 0.079, 0.074]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8127 | Steps: 4 | Val loss: 3.4871 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.0408 | Steps: 4 | Val loss: 49.4563 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=93317)[0m top1: 0.3983208955223881
[2m[36m(func pid=93317)[0m top5: 0.8833955223880597
[2m[36m(func pid=93317)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=93317)[0m f1_macro: 0.34609016976856527
[2m[36m(func pid=93317)[0m f1_weighted: 0.42863781380152766
[2m[36m(func pid=93317)[0m f1_per_class: [0.311, 0.435, 0.375, 0.447, 0.099, 0.366, 0.466, 0.47, 0.275, 0.215]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:08:50 (running for 00:35:15.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.451 |      0.346 |                   44 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.813 |      0.251 |                   44 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.253 |      0.07  |                   20 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.006 |      0.05  |                   10 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.34888059701492535
[2m[36m(func pid=93934)[0m top5: 0.847481343283582
[2m[36m(func pid=93934)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=93934)[0m f1_macro: 0.2506047792821431
[2m[36m(func pid=93934)[0m f1_weighted: 0.3523137184792584
[2m[36m(func pid=93934)[0m f1_per_class: [0.395, 0.297, 0.154, 0.402, 0.11, 0.107, 0.515, 0.0, 0.354, 0.173]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4811 | Steps: 4 | Val loss: 66.6705 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=102012)[0m top1: 0.06996268656716417
[2m[36m(func pid=102012)[0m top5: 0.5424440298507462
[2m[36m(func pid=102012)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=102012)[0m f1_macro: 0.04201996896870299
[2m[36m(func pid=102012)[0m f1_weighted: 0.08169052698191405
[2m[36m(func pid=102012)[0m f1_per_class: [0.022, 0.077, 0.0, 0.0, 0.041, 0.0, 0.222, 0.0, 0.046, 0.013]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.4383 | Steps: 4 | Val loss: 1.6636 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5523 | Steps: 4 | Val loss: 3.1287 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=99348)[0m top1: 0.0625
[2m[36m(func pid=99348)[0m top5: 0.4841417910447761
[2m[36m(func pid=99348)[0m f1_micro: 0.0625
[2m[36m(func pid=99348)[0m f1_macro: 0.053264092051008925
[2m[36m(func pid=99348)[0m f1_weighted: 0.04490418578714006
[2m[36m(func pid=99348)[0m f1_per_class: [0.088, 0.0, 0.0, 0.023, 0.118, 0.0, 0.091, 0.101, 0.074, 0.039]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.4830 | Steps: 4 | Val loss: 90.1979 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 15:08:55 (running for 00:35:20.69)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.438 |      0.351 |                   45 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.813 |      0.251 |                   44 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.481 |      0.053 |                   21 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.041 |      0.042 |                   11 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.40298507462686567
[2m[36m(func pid=93317)[0m top5: 0.8922574626865671
[2m[36m(func pid=93317)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=93317)[0m f1_macro: 0.35121444760433196
[2m[36m(func pid=93317)[0m f1_weighted: 0.43588448636411825
[2m[36m(func pid=93317)[0m f1_per_class: [0.322, 0.438, 0.429, 0.461, 0.102, 0.385, 0.472, 0.468, 0.241, 0.194]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.35447761194029853
[2m[36m(func pid=93934)[0m top5: 0.8885261194029851
[2m[36m(func pid=93934)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=93934)[0m f1_macro: 0.272582214370651
[2m[36m(func pid=93934)[0m f1_weighted: 0.3531633085152899
[2m[36m(func pid=93934)[0m f1_per_class: [0.486, 0.308, 0.291, 0.311, 0.136, 0.155, 0.571, 0.0, 0.329, 0.137]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6614 | Steps: 4 | Val loss: 52.7277 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102012)[0m top1: 0.024720149253731342
[2m[36m(func pid=102012)[0m top5: 0.5111940298507462
[2m[36m(func pid=102012)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=102012)[0m f1_macro: 0.012309576984442941
[2m[36m(func pid=102012)[0m f1_weighted: 0.004897688377039883
[2m[36m(func pid=102012)[0m f1_per_class: [0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.0, 0.064, 0.019]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5006 | Steps: 4 | Val loss: 1.6693 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9823 | Steps: 4 | Val loss: 3.2842 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=99348)[0m top1: 0.09188432835820895
[2m[36m(func pid=99348)[0m top5: 0.6082089552238806
[2m[36m(func pid=99348)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=99348)[0m f1_macro: 0.05782083470796277
[2m[36m(func pid=99348)[0m f1_weighted: 0.06100177962235443
[2m[36m(func pid=99348)[0m f1_per_class: [0.08, 0.01, 0.0, 0.088, 0.0, 0.008, 0.052, 0.233, 0.094, 0.014]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7281 | Steps: 4 | Val loss: 121.4798 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 15:09:00 (running for 00:35:26.37)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.501 |      0.347 |                   46 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.552 |      0.273 |                   45 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.661 |      0.058 |                   22 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.483 |      0.012 |                   12 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.3917910447761194
[2m[36m(func pid=93317)[0m top5: 0.8927238805970149
[2m[36m(func pid=93317)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=93317)[0m f1_macro: 0.34665659481142364
[2m[36m(func pid=93317)[0m f1_weighted: 0.4280980939770474
[2m[36m(func pid=93317)[0m f1_per_class: [0.325, 0.399, 0.421, 0.451, 0.086, 0.396, 0.474, 0.458, 0.252, 0.204]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.34654850746268656
[2m[36m(func pid=93934)[0m top5: 0.8791977611940298
[2m[36m(func pid=93934)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=93934)[0m f1_macro: 0.254180858337163
[2m[36m(func pid=93934)[0m f1_weighted: 0.34694096137632946
[2m[36m(func pid=93934)[0m f1_per_class: [0.269, 0.331, 0.429, 0.293, 0.134, 0.145, 0.581, 0.0, 0.245, 0.115]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2276 | Steps: 4 | Val loss: 41.8127 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=102012)[0m top1: 0.05690298507462686
[2m[36m(func pid=102012)[0m top5: 0.5839552238805971
[2m[36m(func pid=102012)[0m f1_micro: 0.05690298507462686
[2m[36m(func pid=102012)[0m f1_macro: 0.03040942099195912
[2m[36m(func pid=102012)[0m f1_weighted: 0.036120971188535
[2m[36m(func pid=102012)[0m f1_per_class: [0.043, 0.189, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.045, 0.024]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.4381 | Steps: 4 | Val loss: 1.6648 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3799 | Steps: 4 | Val loss: 3.5120 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=99348)[0m top1: 0.14878731343283583
[2m[36m(func pid=99348)[0m top5: 0.6865671641791045
[2m[36m(func pid=99348)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=99348)[0m f1_macro: 0.11506268836565792
[2m[36m(func pid=99348)[0m f1_weighted: 0.13920647870382047
[2m[36m(func pid=99348)[0m f1_per_class: [0.015, 0.215, 0.232, 0.165, 0.0, 0.024, 0.109, 0.273, 0.098, 0.02]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6681 | Steps: 4 | Val loss: 120.5854 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:09:06 (running for 00:35:31.82)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.438 |      0.355 |                   47 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.982 |      0.254 |                   46 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.228 |      0.115 |                   23 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.728 |      0.03  |                   13 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.39972014925373134
[2m[36m(func pid=93317)[0m top5: 0.8917910447761194
[2m[36m(func pid=93317)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=93317)[0m f1_macro: 0.35501544962475223
[2m[36m(func pid=93317)[0m f1_weighted: 0.4343959221140341
[2m[36m(func pid=93317)[0m f1_per_class: [0.327, 0.426, 0.429, 0.465, 0.094, 0.383, 0.465, 0.474, 0.28, 0.208]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.3306902985074627
[2m[36m(func pid=93934)[0m top5: 0.8512126865671642
[2m[36m(func pid=93934)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=93934)[0m f1_macro: 0.26164081683228924
[2m[36m(func pid=93934)[0m f1_weighted: 0.34707608182293537
[2m[36m(func pid=93934)[0m f1_per_class: [0.267, 0.328, 0.417, 0.307, 0.121, 0.097, 0.556, 0.189, 0.208, 0.127]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5046 | Steps: 4 | Val loss: 27.2034 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102012)[0m top1: 0.055970149253731345
[2m[36m(func pid=102012)[0m top5: 0.6194029850746269
[2m[36m(func pid=102012)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=102012)[0m f1_macro: 0.023596337604564033
[2m[36m(func pid=102012)[0m f1_weighted: 0.022559829708007496
[2m[36m(func pid=102012)[0m f1_per_class: [0.068, 0.117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.032]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7853 | Steps: 4 | Val loss: 1.6226 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=99348)[0m top1: 0.23507462686567165
[2m[36m(func pid=99348)[0m top5: 0.7569962686567164
[2m[36m(func pid=99348)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=99348)[0m f1_macro: 0.13947796228890935
[2m[36m(func pid=99348)[0m f1_weighted: 0.24053092673341536
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.491, 0.0, 0.199, 0.0, 0.032, 0.26, 0.274, 0.083, 0.055]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5881 | Steps: 4 | Val loss: 3.8878 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.4411 | Steps: 4 | Val loss: 115.3626 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:09:11 (running for 00:35:37.30)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.785 |      0.378 |                   48 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.38  |      0.262 |                   47 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.505 |      0.139 |                   24 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.668 |      0.024 |                   14 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.3983208955223881
[2m[36m(func pid=93317)[0m top5: 0.9104477611940298
[2m[36m(func pid=93317)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=93317)[0m f1_macro: 0.37765132029414106
[2m[36m(func pid=93317)[0m f1_weighted: 0.4294541496579494
[2m[36m(func pid=93317)[0m f1_per_class: [0.492, 0.383, 0.571, 0.465, 0.085, 0.402, 0.458, 0.456, 0.252, 0.212]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.29384328358208955
[2m[36m(func pid=93934)[0m top5: 0.8255597014925373
[2m[36m(func pid=93934)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=93934)[0m f1_macro: 0.2303305287581356
[2m[36m(func pid=93934)[0m f1_weighted: 0.3204016734775114
[2m[36m(func pid=93934)[0m f1_per_class: [0.242, 0.357, 0.312, 0.318, 0.108, 0.062, 0.472, 0.143, 0.151, 0.139]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.9650 | Steps: 4 | Val loss: 16.3985 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=102012)[0m top1: 0.06949626865671642
[2m[36m(func pid=102012)[0m top5: 0.6105410447761194
[2m[36m(func pid=102012)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=102012)[0m f1_macro: 0.05273277622333241
[2m[36m(func pid=102012)[0m f1_weighted: 0.08012165103781912
[2m[36m(func pid=102012)[0m f1_per_class: [0.059, 0.257, 0.0, 0.095, 0.0, 0.0, 0.02, 0.0, 0.054, 0.042]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1838 | Steps: 4 | Val loss: 1.6123 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=99348)[0m top1: 0.25699626865671643
[2m[36m(func pid=99348)[0m top5: 0.7532649253731343
[2m[36m(func pid=99348)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=99348)[0m f1_macro: 0.14303736186246838
[2m[36m(func pid=99348)[0m f1_weighted: 0.26609886769426483
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.464, 0.0, 0.158, 0.0, 0.032, 0.4, 0.293, 0.066, 0.017]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9117 | Steps: 4 | Val loss: 3.8006 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8899 | Steps: 4 | Val loss: 83.0390 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:09:17 (running for 00:35:42.76)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.184 |      0.372 |                   49 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.588 |      0.23  |                   48 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.965 |      0.143 |                   25 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.441 |      0.053 |                   15 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.4099813432835821
[2m[36m(func pid=93317)[0m top5: 0.9071828358208955
[2m[36m(func pid=93317)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=93317)[0m f1_macro: 0.3722629906930287
[2m[36m(func pid=93317)[0m f1_weighted: 0.4416043965084508
[2m[36m(func pid=93317)[0m f1_per_class: [0.402, 0.405, 0.545, 0.474, 0.089, 0.418, 0.481, 0.461, 0.213, 0.234]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.3111007462686567
[2m[36m(func pid=93934)[0m top5: 0.8283582089552238
[2m[36m(func pid=93934)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=93934)[0m f1_macro: 0.2567851554663837
[2m[36m(func pid=93934)[0m f1_weighted: 0.33889552666551254
[2m[36m(func pid=93934)[0m f1_per_class: [0.288, 0.362, 0.348, 0.446, 0.084, 0.039, 0.393, 0.247, 0.17, 0.192]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1779 | Steps: 4 | Val loss: 19.9493 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=102012)[0m top1: 0.09514925373134328
[2m[36m(func pid=102012)[0m top5: 0.5965485074626866
[2m[36m(func pid=102012)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=102012)[0m f1_macro: 0.059234990899896846
[2m[36m(func pid=102012)[0m f1_weighted: 0.11478744085170972
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.38, 0.0, 0.142, 0.0, 0.0, 0.028, 0.0, 0.042, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2413 | Steps: 4 | Val loss: 1.6102 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=99348)[0m top1: 0.22527985074626866
[2m[36m(func pid=99348)[0m top5: 0.6585820895522388
[2m[36m(func pid=99348)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=99348)[0m f1_macro: 0.17300114002600347
[2m[36m(func pid=99348)[0m f1_weighted: 0.2715474614502459
[2m[36m(func pid=99348)[0m f1_per_class: [0.097, 0.471, 0.353, 0.265, 0.0, 0.008, 0.35, 0.076, 0.072, 0.038]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2864 | Steps: 4 | Val loss: 3.9321 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.2568 | Steps: 4 | Val loss: 67.3697 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=93317)[0m top1: 0.41091417910447764
[2m[36m(func pid=93317)[0m top5: 0.9048507462686567
[2m[36m(func pid=93317)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=93317)[0m f1_macro: 0.37676187323014165
[2m[36m(func pid=93317)[0m f1_weighted: 0.442425712564347
[2m[36m(func pid=93317)[0m f1_per_class: [0.41, 0.419, 0.533, 0.477, 0.086, 0.384, 0.476, 0.473, 0.272, 0.237]
[2m[36m(func pid=93317)[0m 
== Status ==
Current time: 2024-01-07 15:09:22 (running for 00:35:48.26)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.241 |      0.377 |                   50 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.912 |      0.257 |                   49 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.178 |      0.173 |                   26 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.89  |      0.059 |                   16 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.29617537313432835
[2m[36m(func pid=93934)[0m top5: 0.8083022388059702
[2m[36m(func pid=93934)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=93934)[0m f1_macro: 0.24925581147868128
[2m[36m(func pid=93934)[0m f1_weighted: 0.3118355530687004
[2m[36m(func pid=93934)[0m f1_per_class: [0.346, 0.329, 0.31, 0.411, 0.051, 0.039, 0.332, 0.325, 0.228, 0.122]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.2745 | Steps: 4 | Val loss: 19.3310 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=102012)[0m top1: 0.14319029850746268
[2m[36m(func pid=102012)[0m top5: 0.6828358208955224
[2m[36m(func pid=102012)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=102012)[0m f1_macro: 0.07204651064644563
[2m[36m(func pid=102012)[0m f1_weighted: 0.09165904398001588
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.425, 0.0, 0.02, 0.049, 0.0, 0.003, 0.174, 0.049, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.4052 | Steps: 4 | Val loss: 1.6141 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=99348)[0m top1: 0.18610074626865672
[2m[36m(func pid=99348)[0m top5: 0.7019589552238806
[2m[36m(func pid=99348)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=99348)[0m f1_macro: 0.16861886284174027
[2m[36m(func pid=99348)[0m f1_weighted: 0.18924467336689066
[2m[36m(func pid=99348)[0m f1_per_class: [0.056, 0.434, 0.512, 0.159, 0.0, 0.0, 0.166, 0.234, 0.074, 0.052]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4402 | Steps: 4 | Val loss: 4.2593 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9302 | Steps: 4 | Val loss: 51.0774 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:09:28 (running for 00:35:53.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.405 |      0.378 |                   51 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.286 |      0.249 |                   50 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.274 |      0.169 |                   27 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.257 |      0.072 |                   17 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.4146455223880597
[2m[36m(func pid=93317)[0m top5: 0.9043843283582089
[2m[36m(func pid=93317)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=93317)[0m f1_macro: 0.37819073916062107
[2m[36m(func pid=93317)[0m f1_weighted: 0.4452738104203867
[2m[36m(func pid=93317)[0m f1_per_class: [0.377, 0.409, 0.558, 0.487, 0.099, 0.419, 0.472, 0.464, 0.28, 0.216]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.2513992537313433
[2m[36m(func pid=93934)[0m top5: 0.8069029850746269
[2m[36m(func pid=93934)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=93934)[0m f1_macro: 0.22001660047530697
[2m[36m(func pid=93934)[0m f1_weighted: 0.26749389755887387
[2m[36m(func pid=93934)[0m f1_per_class: [0.256, 0.261, 0.269, 0.338, 0.065, 0.039, 0.288, 0.404, 0.188, 0.093]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.8256 | Steps: 4 | Val loss: 16.9887 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=102012)[0m top1: 0.10027985074626866
[2m[36m(func pid=102012)[0m top5: 0.5732276119402985
[2m[36m(func pid=102012)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=102012)[0m f1_macro: 0.05634759192355867
[2m[36m(func pid=102012)[0m f1_weighted: 0.06293345967854777
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.224, 0.0, 0.0, 0.033, 0.108, 0.003, 0.177, 0.018, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1285 | Steps: 4 | Val loss: 1.5832 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4739 | Steps: 4 | Val loss: 4.2134 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=99348)[0m top1: 0.17397388059701493
[2m[36m(func pid=99348)[0m top5: 0.7122201492537313
[2m[36m(func pid=99348)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=99348)[0m f1_macro: 0.1319803019805534
[2m[36m(func pid=99348)[0m f1_weighted: 0.13874220641216484
[2m[36m(func pid=99348)[0m f1_per_class: [0.034, 0.4, 0.176, 0.146, 0.077, 0.008, 0.009, 0.362, 0.053, 0.055]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7619 | Steps: 4 | Val loss: 34.1264 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:09:33 (running for 00:35:59.33)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.128 |      0.392 |                   52 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.44  |      0.22  |                   51 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.826 |      0.132 |                   28 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.93  |      0.056 |                   18 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.42723880597014924
[2m[36m(func pid=93317)[0m top5: 0.9118470149253731
[2m[36m(func pid=93317)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=93317)[0m f1_macro: 0.39208357997494875
[2m[36m(func pid=93317)[0m f1_weighted: 0.45914666645451235
[2m[36m(func pid=93317)[0m f1_per_class: [0.429, 0.411, 0.585, 0.499, 0.093, 0.404, 0.507, 0.451, 0.303, 0.239]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.259794776119403
[2m[36m(func pid=93934)[0m top5: 0.7639925373134329
[2m[36m(func pid=93934)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=93934)[0m f1_macro: 0.1922701708881212
[2m[36m(func pid=93934)[0m f1_weighted: 0.2862691309429612
[2m[36m(func pid=93934)[0m f1_per_class: [0.195, 0.299, 0.247, 0.249, 0.037, 0.054, 0.481, 0.061, 0.146, 0.155]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7101 | Steps: 4 | Val loss: 17.1140 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=102012)[0m top1: 0.12080223880597014
[2m[36m(func pid=102012)[0m top5: 0.40111940298507465
[2m[36m(func pid=102012)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=102012)[0m f1_macro: 0.061117335039202136
[2m[36m(func pid=102012)[0m f1_weighted: 0.07192849510059059
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.278, 0.0, 0.0, 0.0, 0.106, 0.003, 0.185, 0.0, 0.04]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.3492 | Steps: 4 | Val loss: 1.5734 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=99348)[0m top1: 0.19076492537313433
[2m[36m(func pid=99348)[0m top5: 0.7327425373134329
[2m[36m(func pid=99348)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=99348)[0m f1_macro: 0.13692623715457822
[2m[36m(func pid=99348)[0m f1_weighted: 0.13841036012426589
[2m[36m(func pid=99348)[0m f1_per_class: [0.034, 0.387, 0.256, 0.15, 0.057, 0.008, 0.009, 0.379, 0.037, 0.052]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4454 | Steps: 4 | Val loss: 6.2614 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9476 | Steps: 4 | Val loss: 26.5824 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 15:09:39 (running for 00:36:05.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.349 |      0.416 |                   53 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.474 |      0.192 |                   52 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.71  |      0.137 |                   29 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.762 |      0.061 |                   19 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.43656716417910446
[2m[36m(func pid=93317)[0m top5: 0.9132462686567164
[2m[36m(func pid=93317)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=93317)[0m f1_macro: 0.41622311291570097
[2m[36m(func pid=93317)[0m f1_weighted: 0.46932223394019995
[2m[36m(func pid=93317)[0m f1_per_class: [0.515, 0.43, 0.649, 0.528, 0.075, 0.397, 0.492, 0.467, 0.318, 0.292]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=93934)[0m top1: 0.12639925373134328
[2m[36m(func pid=93934)[0m top5: 0.6744402985074627
[2m[36m(func pid=93934)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=93934)[0m f1_macro: 0.12168378447099223
[2m[36m(func pid=93934)[0m f1_weighted: 0.12553131977441753
[2m[36m(func pid=93934)[0m f1_per_class: [0.109, 0.298, 0.233, 0.098, 0.08, 0.096, 0.091, 0.0, 0.098, 0.114]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.2994 | Steps: 4 | Val loss: 12.0497 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=102012)[0m top1: 0.15345149253731344
[2m[36m(func pid=102012)[0m top5: 0.40345149253731344
[2m[36m(func pid=102012)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=102012)[0m f1_macro: 0.04497958633123566
[2m[36m(func pid=102012)[0m f1_weighted: 0.06962490294700409
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.384, 0.0, 0.0, 0.0, 0.016, 0.0, 0.025, 0.0, 0.025]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4013 | Steps: 4 | Val loss: 6.9661 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=99348)[0m top1: 0.19962686567164178
[2m[36m(func pid=99348)[0m top5: 0.7831156716417911
[2m[36m(func pid=99348)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=99348)[0m f1_macro: 0.12931692822571117
[2m[36m(func pid=99348)[0m f1_weighted: 0.20075922423526532
[2m[36m(func pid=99348)[0m f1_per_class: [0.0, 0.394, 0.129, 0.246, 0.083, 0.0, 0.173, 0.139, 0.074, 0.054]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.1038 | Steps: 4 | Val loss: 1.5704 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8604 | Steps: 4 | Val loss: 26.7877 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:09:44 (running for 00:36:10.49)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.349 |      0.416 |                   53 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.401 |      0.137 |                   54 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.299 |      0.129 |                   30 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.948 |      0.045 |                   20 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.11940298507462686
[2m[36m(func pid=93934)[0m top5: 0.667910447761194
[2m[36m(func pid=93934)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=93934)[0m f1_macro: 0.13704228436025356
[2m[36m(func pid=93934)[0m f1_weighted: 0.11454285055421773
[2m[36m(func pid=93934)[0m f1_per_class: [0.111, 0.283, 0.345, 0.066, 0.096, 0.129, 0.076, 0.0, 0.082, 0.183]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.42490671641791045
[2m[36m(func pid=93317)[0m top5: 0.917910447761194
[2m[36m(func pid=93317)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=93317)[0m f1_macro: 0.41737327635899196
[2m[36m(func pid=93317)[0m f1_weighted: 0.4591404127668967
[2m[36m(func pid=93317)[0m f1_per_class: [0.531, 0.42, 0.727, 0.49, 0.075, 0.398, 0.5, 0.457, 0.304, 0.271]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2592 | Steps: 4 | Val loss: 9.4284 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102012)[0m top1: 0.022388059701492536
[2m[36m(func pid=102012)[0m top5: 0.4780783582089552
[2m[36m(func pid=102012)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=102012)[0m f1_macro: 0.022318108619180484
[2m[36m(func pid=102012)[0m f1_weighted: 0.01945939795702006
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.155, 0.0, 0.006, 0.036, 0.007]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.20615671641791045
[2m[36m(func pid=99348)[0m top5: 0.7877798507462687
[2m[36m(func pid=99348)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=99348)[0m f1_macro: 0.15806219230423516
[2m[36m(func pid=99348)[0m f1_weighted: 0.22559280663339512
[2m[36m(func pid=99348)[0m f1_per_class: [0.094, 0.386, 0.207, 0.323, 0.049, 0.0, 0.167, 0.206, 0.096, 0.053]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7315 | Steps: 4 | Val loss: 6.8714 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2055 | Steps: 4 | Val loss: 1.5848 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8357 | Steps: 4 | Val loss: 14.6391 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 15:09:50 (running for 00:36:16.02)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.104 |      0.417 |                   54 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.731 |      0.159 |                   55 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.259 |      0.158 |                   31 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.86  |      0.022 |                   21 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.13386194029850745
[2m[36m(func pid=93934)[0m top5: 0.707089552238806
[2m[36m(func pid=93934)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=93934)[0m f1_macro: 0.15905282963019593
[2m[36m(func pid=93934)[0m f1_weighted: 0.124613535270896
[2m[36m(func pid=93934)[0m f1_per_class: [0.114, 0.327, 0.27, 0.041, 0.092, 0.268, 0.039, 0.055, 0.101, 0.284]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.41744402985074625
[2m[36m(func pid=93317)[0m top5: 0.9137126865671642
[2m[36m(func pid=93317)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=93317)[0m f1_macro: 0.3993422944386877
[2m[36m(func pid=93317)[0m f1_weighted: 0.45343030774859383
[2m[36m(func pid=93317)[0m f1_per_class: [0.529, 0.416, 0.585, 0.484, 0.072, 0.388, 0.497, 0.445, 0.315, 0.261]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.8706 | Steps: 4 | Val loss: 7.1275 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=102012)[0m top1: 0.10634328358208955
[2m[36m(func pid=102012)[0m top5: 0.6697761194029851
[2m[36m(func pid=102012)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=102012)[0m f1_macro: 0.05161667430635093
[2m[36m(func pid=102012)[0m f1_weighted: 0.039435376829764564
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.0, 0.198, 0.0, 0.225, 0.088, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.0354 | Steps: 4 | Val loss: 5.9211 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=99348)[0m top1: 0.26399253731343286
[2m[36m(func pid=99348)[0m top5: 0.8148320895522388
[2m[36m(func pid=99348)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=99348)[0m f1_macro: 0.18376844976260065
[2m[36m(func pid=99348)[0m f1_weighted: 0.25699870962661214
[2m[36m(func pid=99348)[0m f1_per_class: [0.127, 0.479, 0.152, 0.43, 0.03, 0.052, 0.076, 0.297, 0.138, 0.056]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1733 | Steps: 4 | Val loss: 1.6193 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.0396 | Steps: 4 | Val loss: 6.5196 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=93934)[0m top1: 0.17817164179104478
[2m[36m(func pid=93934)[0m top5: 0.7490671641791045
[2m[36m(func pid=93934)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=93934)[0m f1_macro: 0.19458062017542616
[2m[36m(func pid=93934)[0m f1_weighted: 0.15547533332683205
[2m[36m(func pid=93934)[0m f1_per_class: [0.109, 0.377, 0.168, 0.051, 0.079, 0.348, 0.024, 0.27, 0.171, 0.349]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:09:55 (running for 00:36:21.49)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.205 |      0.399 |                   55 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.035 |      0.195 |                   56 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.871 |      0.184 |                   32 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.836 |      0.052 |                   22 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.4085820895522388
[2m[36m(func pid=93317)[0m top5: 0.9043843283582089
[2m[36m(func pid=93317)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=93317)[0m f1_macro: 0.3957718107397624
[2m[36m(func pid=93317)[0m f1_weighted: 0.44528357510920874
[2m[36m(func pid=93317)[0m f1_per_class: [0.512, 0.418, 0.571, 0.478, 0.069, 0.394, 0.471, 0.46, 0.319, 0.266]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2517 | Steps: 4 | Val loss: 4.2271 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=102012)[0m top1: 0.19962686567164178
[2m[36m(func pid=102012)[0m top5: 0.7523320895522388
[2m[36m(func pid=102012)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=102012)[0m f1_macro: 0.0983613166804616
[2m[36m(func pid=102012)[0m f1_weighted: 0.1617010969442638
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.007, 0.364, 0.0, 0.262, 0.044, 0.282, 0.025, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.1256 | Steps: 4 | Val loss: 4.6631 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=99348)[0m top1: 0.3218283582089552
[2m[36m(func pid=99348)[0m top5: 0.8763992537313433
[2m[36m(func pid=99348)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=99348)[0m f1_macro: 0.23685502229065727
[2m[36m(func pid=99348)[0m f1_weighted: 0.2764318036366821
[2m[36m(func pid=99348)[0m f1_per_class: [0.097, 0.499, 0.393, 0.441, 0.0, 0.302, 0.015, 0.313, 0.11, 0.197]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0527 | Steps: 4 | Val loss: 1.5842 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8464 | Steps: 4 | Val loss: 22.4718 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:10:01 (running for 00:36:26.97)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.173 |      0.396 |                   56 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.126 |      0.227 |                   57 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.252 |      0.237 |                   33 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.04  |      0.098 |                   23 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.21641791044776118
[2m[36m(func pid=93934)[0m top5: 0.8041044776119403
[2m[36m(func pid=93934)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=93934)[0m f1_macro: 0.22667022280001986
[2m[36m(func pid=93934)[0m f1_weighted: 0.19475234259219837
[2m[36m(func pid=93934)[0m f1_per_class: [0.12, 0.393, 0.255, 0.144, 0.058, 0.37, 0.039, 0.263, 0.293, 0.333]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.4146455223880597
[2m[36m(func pid=93317)[0m top5: 0.9118470149253731
[2m[36m(func pid=93317)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=93317)[0m f1_macro: 0.3973349977780437
[2m[36m(func pid=93317)[0m f1_weighted: 0.4470611817465892
[2m[36m(func pid=93317)[0m f1_per_class: [0.512, 0.42, 0.585, 0.479, 0.08, 0.396, 0.475, 0.446, 0.328, 0.251]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1252 | Steps: 4 | Val loss: 4.6264 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=102012)[0m top1: 0.21921641791044777
[2m[36m(func pid=102012)[0m top5: 0.5774253731343284
[2m[36m(func pid=102012)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=102012)[0m f1_macro: 0.10257440570159115
[2m[36m(func pid=102012)[0m f1_weighted: 0.21873500137229973
[2m[36m(func pid=102012)[0m f1_per_class: [0.104, 0.0, 0.028, 0.446, 0.0, 0.206, 0.227, 0.014, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.31902985074626866
[2m[36m(func pid=99348)[0m top5: 0.875
[2m[36m(func pid=99348)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=99348)[0m f1_macro: 0.20516893952163578
[2m[36m(func pid=99348)[0m f1_weighted: 0.2691982109018434
[2m[36m(func pid=99348)[0m f1_per_class: [0.085, 0.496, 0.0, 0.394, 0.059, 0.381, 0.018, 0.3, 0.072, 0.246]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4669 | Steps: 4 | Val loss: 3.2916 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2501 | Steps: 4 | Val loss: 1.6482 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.4598 | Steps: 4 | Val loss: 11.3312 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=93934)[0m top1: 0.3246268656716418
[2m[36m(func pid=93934)[0m top5: 0.8395522388059702
[2m[36m(func pid=93934)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=93934)[0m f1_macro: 0.2753581789161976
[2m[36m(func pid=93934)[0m f1_weighted: 0.35721509791993616
[2m[36m(func pid=93934)[0m f1_per_class: [0.18, 0.482, 0.5, 0.356, 0.081, 0.247, 0.415, 0.122, 0.206, 0.165]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:10:06 (running for 00:36:32.42)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.053 |      0.397 |                   57 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.467 |      0.275 |                   58 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.125 |      0.205 |                   34 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.846 |      0.103 |                   24 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.3824626865671642
[2m[36m(func pid=93317)[0m top5: 0.9029850746268657
[2m[36m(func pid=93317)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=93317)[0m f1_macro: 0.36281402185366035
[2m[36m(func pid=93317)[0m f1_weighted: 0.41680538274578344
[2m[36m(func pid=93317)[0m f1_per_class: [0.403, 0.424, 0.444, 0.417, 0.07, 0.399, 0.437, 0.465, 0.316, 0.253]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2762 | Steps: 4 | Val loss: 5.0081 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=102012)[0m top1: 0.24486940298507462
[2m[36m(func pid=102012)[0m top5: 0.644589552238806
[2m[36m(func pid=102012)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=102012)[0m f1_macro: 0.12139983644285016
[2m[36m(func pid=102012)[0m f1_weighted: 0.24753075656380247
[2m[36m(func pid=102012)[0m f1_per_class: [0.125, 0.0, 0.087, 0.411, 0.0, 0.188, 0.356, 0.021, 0.025, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.261660447761194
[2m[36m(func pid=99348)[0m top5: 0.8717350746268657
[2m[36m(func pid=99348)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=99348)[0m f1_macro: 0.20122347991858208
[2m[36m(func pid=99348)[0m f1_weighted: 0.2071312229243263
[2m[36m(func pid=99348)[0m f1_per_class: [0.2, 0.448, 0.0, 0.179, 0.094, 0.371, 0.027, 0.305, 0.128, 0.26]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9145 | Steps: 4 | Val loss: 3.6730 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.1128 | Steps: 4 | Val loss: 1.6282 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.9643 | Steps: 4 | Val loss: 7.3094 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 15:10:12 (running for 00:36:37.89)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.25  |      0.363 |                   58 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.914 |      0.248 |                   59 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.276 |      0.201 |                   35 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.46  |      0.121 |                   25 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.2933768656716418
[2m[36m(func pid=93934)[0m top5: 0.816231343283582
[2m[36m(func pid=93934)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=93934)[0m f1_macro: 0.24807700978980454
[2m[36m(func pid=93934)[0m f1_weighted: 0.32785745694514085
[2m[36m(func pid=93934)[0m f1_per_class: [0.181, 0.463, 0.429, 0.345, 0.056, 0.168, 0.381, 0.072, 0.171, 0.215]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.3880597014925373
[2m[36m(func pid=93317)[0m top5: 0.9043843283582089
[2m[36m(func pid=93317)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=93317)[0m f1_macro: 0.37773395426478823
[2m[36m(func pid=93317)[0m f1_weighted: 0.4203692038622552
[2m[36m(func pid=93317)[0m f1_per_class: [0.471, 0.425, 0.511, 0.441, 0.061, 0.402, 0.418, 0.465, 0.318, 0.265]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7435 | Steps: 4 | Val loss: 4.8881 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=102012)[0m top1: 0.12453358208955224
[2m[36m(func pid=102012)[0m top5: 0.7388059701492538
[2m[36m(func pid=102012)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=102012)[0m f1_macro: 0.0735595860346824
[2m[36m(func pid=102012)[0m f1_weighted: 0.1551409553081066
[2m[36m(func pid=102012)[0m f1_per_class: [0.062, 0.0, 0.0, 0.218, 0.0, 0.111, 0.254, 0.063, 0.026, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.1719 | Steps: 4 | Val loss: 4.1777 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=99348)[0m top1: 0.23600746268656717
[2m[36m(func pid=99348)[0m top5: 0.8698694029850746
[2m[36m(func pid=99348)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=99348)[0m f1_macro: 0.2721556999008151
[2m[36m(func pid=99348)[0m f1_weighted: 0.1988615927156609
[2m[36m(func pid=99348)[0m f1_per_class: [0.204, 0.378, 0.833, 0.144, 0.085, 0.369, 0.06, 0.288, 0.132, 0.229]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.1340 | Steps: 4 | Val loss: 1.6380 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.9862 | Steps: 4 | Val loss: 6.6045 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:10:17 (running for 00:36:43.53)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.113 |      0.378 |                   59 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.172 |      0.207 |                   60 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.743 |      0.272 |                   36 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.964 |      0.074 |                   26 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.269589552238806
[2m[36m(func pid=93934)[0m top5: 0.7798507462686567
[2m[36m(func pid=93934)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=93934)[0m f1_macro: 0.20711701070538843
[2m[36m(func pid=93934)[0m f1_weighted: 0.28474709295714185
[2m[36m(func pid=93934)[0m f1_per_class: [0.108, 0.448, 0.13, 0.145, 0.055, 0.12, 0.451, 0.121, 0.132, 0.36]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4044 | Steps: 4 | Val loss: 4.1903 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=93317)[0m top1: 0.3931902985074627
[2m[36m(func pid=93317)[0m top5: 0.8955223880597015
[2m[36m(func pid=93317)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=93317)[0m f1_macro: 0.37447879885512203
[2m[36m(func pid=93317)[0m f1_weighted: 0.42175191982996657
[2m[36m(func pid=93317)[0m f1_per_class: [0.416, 0.454, 0.462, 0.451, 0.065, 0.399, 0.392, 0.502, 0.344, 0.26]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.06110074626865672
[2m[36m(func pid=102012)[0m top5: 0.7985074626865671
[2m[36m(func pid=102012)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=102012)[0m f1_macro: 0.03634755320502968
[2m[36m(func pid=102012)[0m f1_weighted: 0.04861006053776559
[2m[36m(func pid=102012)[0m f1_per_class: [0.057, 0.0, 0.066, 0.073, 0.0, 0.0, 0.071, 0.097, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.23647388059701493
[2m[36m(func pid=99348)[0m top5: 0.8582089552238806
[2m[36m(func pid=99348)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=99348)[0m f1_macro: 0.2279450895887904
[2m[36m(func pid=99348)[0m f1_weighted: 0.22456850793416092
[2m[36m(func pid=99348)[0m f1_per_class: [0.121, 0.331, 0.453, 0.187, 0.068, 0.292, 0.17, 0.32, 0.149, 0.189]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4601 | Steps: 4 | Val loss: 4.3944 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.1338 | Steps: 4 | Val loss: 1.7350 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7810 | Steps: 4 | Val loss: 8.6189 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:10:23 (running for 00:36:49.06)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.134 |      0.374 |                   60 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.46  |      0.202 |                   61 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.404 |      0.228 |                   37 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.986 |      0.036 |                   27 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.2635261194029851
[2m[36m(func pid=93934)[0m top5: 0.7719216417910447
[2m[36m(func pid=93934)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=93934)[0m f1_macro: 0.20196024526791131
[2m[36m(func pid=93934)[0m f1_weighted: 0.2574324593314542
[2m[36m(func pid=93934)[0m f1_per_class: [0.143, 0.404, 0.131, 0.058, 0.042, 0.031, 0.476, 0.263, 0.071, 0.4]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=93317)[0m top1: 0.36473880597014924
[2m[36m(func pid=93317)[0m top5: 0.8791977611940298
[2m[36m(func pid=93317)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=93317)[0m f1_macro: 0.33635618269180706
[2m[36m(func pid=93317)[0m f1_weighted: 0.40311931007228224
[2m[36m(func pid=93317)[0m f1_per_class: [0.341, 0.443, 0.27, 0.414, 0.062, 0.369, 0.396, 0.493, 0.317, 0.257]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6782 | Steps: 4 | Val loss: 4.4281 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=102012)[0m top1: 0.03871268656716418
[2m[36m(func pid=102012)[0m top5: 0.753731343283582
[2m[36m(func pid=102012)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=102012)[0m f1_macro: 0.021481185866688533
[2m[36m(func pid=102012)[0m f1_weighted: 0.007640225109502485
[2m[36m(func pid=102012)[0m f1_per_class: [0.054, 0.0, 0.054, 0.0, 0.0, 0.0, 0.0, 0.107, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.2943097014925373
[2m[36m(func pid=99348)[0m top5: 0.8498134328358209
[2m[36m(func pid=99348)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=99348)[0m f1_macro: 0.2575151046338938
[2m[36m(func pid=99348)[0m f1_weighted: 0.2788841227941012
[2m[36m(func pid=99348)[0m f1_per_class: [0.179, 0.428, 0.571, 0.234, 0.088, 0.052, 0.324, 0.383, 0.18, 0.136]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6607 | Steps: 4 | Val loss: 4.5679 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.9418 | Steps: 4 | Val loss: 1.6757 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6102 | Steps: 4 | Val loss: 8.1223 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=93934)[0m top1: 0.27238805970149255
[2m[36m(func pid=93934)[0m top5: 0.7262126865671642
[2m[36m(func pid=93934)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=93934)[0m f1_macro: 0.2098683055768255
[2m[36m(func pid=93934)[0m f1_weighted: 0.24830868224296213
[2m[36m(func pid=93934)[0m f1_per_class: [0.151, 0.33, 0.104, 0.01, 0.076, 0.024, 0.52, 0.301, 0.114, 0.468]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:10:28 (running for 00:36:54.39)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.134 |      0.336 |                   61 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.661 |      0.21  |                   62 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.678 |      0.258 |                   38 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.781 |      0.021 |                   28 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.37220149253731344
[2m[36m(func pid=93317)[0m top5: 0.8936567164179104
[2m[36m(func pid=93317)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=93317)[0m f1_macro: 0.3370453759853027
[2m[36m(func pid=93317)[0m f1_weighted: 0.405299905157725
[2m[36m(func pid=93317)[0m f1_per_class: [0.361, 0.432, 0.267, 0.425, 0.066, 0.383, 0.4, 0.47, 0.283, 0.284]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6234 | Steps: 4 | Val loss: 5.1856 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=102012)[0m top1: 0.05923507462686567
[2m[36m(func pid=102012)[0m top5: 0.722481343283582
[2m[36m(func pid=102012)[0m f1_micro: 0.05923507462686567
[2m[36m(func pid=102012)[0m f1_macro: 0.047562025949147144
[2m[36m(func pid=102012)[0m f1_weighted: 0.03681464986621173
[2m[36m(func pid=102012)[0m f1_per_class: [0.097, 0.151, 0.053, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.035]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.29990671641791045
[2m[36m(func pid=99348)[0m top5: 0.8456156716417911
[2m[36m(func pid=99348)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=99348)[0m f1_macro: 0.24785210517726566
[2m[36m(func pid=99348)[0m f1_weighted: 0.2953638859724189
[2m[36m(func pid=99348)[0m f1_per_class: [0.111, 0.462, 0.595, 0.278, 0.041, 0.016, 0.339, 0.429, 0.109, 0.1]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.6938 | Steps: 4 | Val loss: 5.9569 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.9542 | Steps: 4 | Val loss: 1.6521 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.9547 | Steps: 4 | Val loss: 8.7895 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:10:34 (running for 00:36:59.81)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.942 |      0.337 |                   62 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.694 |      0.173 |                   63 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.623 |      0.248 |                   39 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.61  |      0.048 |                   29 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.28777985074626866
[2m[36m(func pid=93934)[0m top5: 0.6124067164179104
[2m[36m(func pid=93934)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=93934)[0m f1_macro: 0.17340985486644087
[2m[36m(func pid=93934)[0m f1_weighted: 0.24353147893636903
[2m[36m(func pid=93934)[0m f1_per_class: [0.141, 0.261, 0.05, 0.003, 0.127, 0.0, 0.61, 0.012, 0.219, 0.31]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5565 | Steps: 4 | Val loss: 3.8835 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=93317)[0m top1: 0.3805970149253731
[2m[36m(func pid=93317)[0m top5: 0.9011194029850746
[2m[36m(func pid=93317)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=93317)[0m f1_macro: 0.3574903903692878
[2m[36m(func pid=93317)[0m f1_weighted: 0.4125501137485376
[2m[36m(func pid=93317)[0m f1_per_class: [0.474, 0.43, 0.353, 0.437, 0.066, 0.385, 0.405, 0.467, 0.284, 0.273]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.05643656716417911
[2m[36m(func pid=102012)[0m top5: 0.691231343283582
[2m[36m(func pid=102012)[0m f1_micro: 0.05643656716417911
[2m[36m(func pid=102012)[0m f1_macro: 0.036855987038470706
[2m[36m(func pid=102012)[0m f1_weighted: 0.03430883194892049
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.156, 0.069, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.028]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.322294776119403
[2m[36m(func pid=99348)[0m top5: 0.8638059701492538
[2m[36m(func pid=99348)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=99348)[0m f1_macro: 0.2605223981790826
[2m[36m(func pid=99348)[0m f1_weighted: 0.3118776873154015
[2m[36m(func pid=99348)[0m f1_per_class: [0.231, 0.478, 0.5, 0.31, 0.0, 0.136, 0.311, 0.363, 0.142, 0.135]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5012 | Steps: 4 | Val loss: 4.4259 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9124 | Steps: 4 | Val loss: 1.6789 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.4651 | Steps: 4 | Val loss: 7.0040 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 15:10:39 (running for 00:37:05.19)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.954 |      0.357 |                   63 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.501 |      0.194 |                   64 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.557 |      0.261 |                   40 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.955 |      0.037 |                   30 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.35494402985074625
[2m[36m(func pid=93934)[0m top5: 0.7555970149253731
[2m[36m(func pid=93934)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=93934)[0m f1_macro: 0.19420158061062573
[2m[36m(func pid=93934)[0m f1_weighted: 0.3241359495385308
[2m[36m(func pid=93934)[0m f1_per_class: [0.204, 0.354, 0.057, 0.225, 0.15, 0.008, 0.627, 0.0, 0.152, 0.166]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4048 | Steps: 4 | Val loss: 4.1833 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=93317)[0m top1: 0.3787313432835821
[2m[36m(func pid=93317)[0m top5: 0.8917910447761194
[2m[36m(func pid=93317)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=93317)[0m f1_macro: 0.35398601575524935
[2m[36m(func pid=93317)[0m f1_weighted: 0.4098937391751272
[2m[36m(func pid=93317)[0m f1_per_class: [0.459, 0.439, 0.312, 0.443, 0.064, 0.353, 0.393, 0.477, 0.323, 0.277]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.1515858208955224
[2m[36m(func pid=102012)[0m top5: 0.6735074626865671
[2m[36m(func pid=102012)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=102012)[0m f1_macro: 0.06876811105654443
[2m[36m(func pid=102012)[0m f1_weighted: 0.09742670754457049
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.421, 0.065, 0.0, 0.0, 0.0, 0.058, 0.124, 0.0, 0.021]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.27705223880597013
[2m[36m(func pid=99348)[0m top5: 0.8596082089552238
[2m[36m(func pid=99348)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=99348)[0m f1_macro: 0.22701427520045567
[2m[36m(func pid=99348)[0m f1_weighted: 0.24583598359225928
[2m[36m(func pid=99348)[0m f1_per_class: [0.176, 0.471, 0.375, 0.258, 0.0, 0.141, 0.163, 0.264, 0.101, 0.321]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5482 | Steps: 4 | Val loss: 3.2607 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8669 | Steps: 4 | Val loss: 1.6606 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.0912 | Steps: 4 | Val loss: 5.6303 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=93934)[0m top1: 0.40111940298507465
[2m[36m(func pid=93934)[0m top5: 0.8549440298507462
[2m[36m(func pid=93934)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=93934)[0m f1_macro: 0.24738345855466695
[2m[36m(func pid=93934)[0m f1_weighted: 0.3731737637627263
[2m[36m(func pid=93934)[0m f1_per_class: [0.318, 0.362, 0.258, 0.383, 0.143, 0.04, 0.607, 0.067, 0.099, 0.197]
== Status ==
Current time: 2024-01-07 15:10:44 (running for 00:37:10.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.912 |      0.354 |                   64 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.548 |      0.247 |                   65 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.405 |      0.227 |                   41 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.465 |      0.069 |                   31 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8636 | Steps: 4 | Val loss: 4.3953 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=93317)[0m top1: 0.38992537313432835
[2m[36m(func pid=93317)[0m top5: 0.8913246268656716
[2m[36m(func pid=93317)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=93317)[0m f1_macro: 0.35322350942217595
[2m[36m(func pid=93317)[0m f1_weighted: 0.41788249822518
[2m[36m(func pid=93317)[0m f1_per_class: [0.427, 0.455, 0.308, 0.46, 0.07, 0.348, 0.4, 0.469, 0.325, 0.27]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.12919776119402984
[2m[36m(func pid=102012)[0m top5: 0.6138059701492538
[2m[36m(func pid=102012)[0m f1_micro: 0.12919776119402984
[2m[36m(func pid=102012)[0m f1_macro: 0.07127847535019112
[2m[36m(func pid=102012)[0m f1_weighted: 0.11776177597515856
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.308, 0.035, 0.016, 0.0, 0.0, 0.171, 0.152, 0.0, 0.031]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.24440298507462688
[2m[36m(func pid=99348)[0m top5: 0.8540111940298507
[2m[36m(func pid=99348)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=99348)[0m f1_macro: 0.2115209724123232
[2m[36m(func pid=99348)[0m f1_weighted: 0.20083023805148
[2m[36m(func pid=99348)[0m f1_per_class: [0.164, 0.457, 0.375, 0.228, 0.0, 0.099, 0.067, 0.245, 0.094, 0.386]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2913 | Steps: 4 | Val loss: 3.3267 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.0323 | Steps: 4 | Val loss: 1.6731 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8471 | Steps: 4 | Val loss: 3.5303 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=93934)[0m top1: 0.3414179104477612
[2m[36m(func pid=93934)[0m top5: 0.8861940298507462
[2m[36m(func pid=93934)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=93934)[0m f1_macro: 0.2733079945607119
[2m[36m(func pid=93934)[0m f1_weighted: 0.3399676904131391
[2m[36m(func pid=93934)[0m f1_per_class: [0.278, 0.377, 0.529, 0.33, 0.137, 0.062, 0.483, 0.306, 0.082, 0.149]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:10:50 (running for 00:37:15.95)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.867 |      0.353 |                   65 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.291 |      0.273 |                   66 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.864 |      0.212 |                   42 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.091 |      0.071 |                   32 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6732 | Steps: 4 | Val loss: 4.1670 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=93317)[0m top1: 0.4006529850746269
[2m[36m(func pid=93317)[0m top5: 0.8880597014925373
[2m[36m(func pid=93317)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=93317)[0m f1_macro: 0.37104658066296253
[2m[36m(func pid=93317)[0m f1_weighted: 0.4268264625000696
[2m[36m(func pid=93317)[0m f1_per_class: [0.444, 0.478, 0.393, 0.484, 0.069, 0.348, 0.386, 0.498, 0.32, 0.289]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.14598880597014927
[2m[36m(func pid=102012)[0m top5: 0.6361940298507462
[2m[36m(func pid=102012)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=102012)[0m f1_macro: 0.09096362021544056
[2m[36m(func pid=102012)[0m f1_weighted: 0.16120077065412697
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.158, 0.034, 0.003, 0.033, 0.0, 0.401, 0.186, 0.062, 0.032]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.23041044776119404
[2m[36m(func pid=99348)[0m top5: 0.8558768656716418
[2m[36m(func pid=99348)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=99348)[0m f1_macro: 0.20500680403523414
[2m[36m(func pid=99348)[0m f1_weighted: 0.17773844468887998
[2m[36m(func pid=99348)[0m f1_per_class: [0.142, 0.466, 0.333, 0.162, 0.08, 0.068, 0.061, 0.244, 0.046, 0.447]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.9186 | Steps: 4 | Val loss: 3.2073 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7775 | Steps: 4 | Val loss: 1.6792 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.8457 | Steps: 4 | Val loss: 2.8318 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:10:55 (running for 00:37:21.35)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  1.032 |      0.371 |                   66 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.919 |      0.221 |                   67 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.673 |      0.205 |                   43 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.847 |      0.091 |                   33 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.3530783582089552
[2m[36m(func pid=93934)[0m top5: 0.8903917910447762
[2m[36m(func pid=93934)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=93934)[0m f1_macro: 0.22052725565263526
[2m[36m(func pid=93934)[0m f1_weighted: 0.3443299199690443
[2m[36m(func pid=93934)[0m f1_per_class: [0.185, 0.36, 0.0, 0.249, 0.155, 0.091, 0.569, 0.404, 0.103, 0.089]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4884 | Steps: 4 | Val loss: 4.1012 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=93317)[0m top1: 0.404384328358209
[2m[36m(func pid=93317)[0m top5: 0.8843283582089553
[2m[36m(func pid=93317)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=93317)[0m f1_macro: 0.3879973080465463
[2m[36m(func pid=93317)[0m f1_weighted: 0.42815050835761487
[2m[36m(func pid=93317)[0m f1_per_class: [0.508, 0.497, 0.471, 0.489, 0.066, 0.34, 0.369, 0.508, 0.32, 0.312]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.22201492537313433
[2m[36m(func pid=102012)[0m top5: 0.6730410447761194
[2m[36m(func pid=102012)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=102012)[0m f1_macro: 0.1183277012087598
[2m[36m(func pid=102012)[0m f1_weighted: 0.222100615779321
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.134, 0.082, 0.0, 0.0, 0.354, 0.517, 0.031, 0.064, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.24067164179104478
[2m[36m(func pid=99348)[0m top5: 0.8600746268656716
[2m[36m(func pid=99348)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=99348)[0m f1_macro: 0.25361891026120303
[2m[36m(func pid=99348)[0m f1_weighted: 0.17654141664061404
[2m[36m(func pid=99348)[0m f1_per_class: [0.167, 0.472, 0.8, 0.148, 0.053, 0.041, 0.06, 0.257, 0.076, 0.462]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7222 | Steps: 4 | Val loss: 3.1811 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7675 | Steps: 4 | Val loss: 1.6579 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.8760 | Steps: 4 | Val loss: 2.5118 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:11:01 (running for 00:37:26.73)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.777 |      0.388 |                   67 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.722 |      0.181 |                   68 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.488 |      0.254 |                   44 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.846 |      0.118 |                   34 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.3694029850746269
[2m[36m(func pid=93934)[0m top5: 0.8736007462686567
[2m[36m(func pid=93934)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=93934)[0m f1_macro: 0.1812489789314561
[2m[36m(func pid=93934)[0m f1_weighted: 0.3488377350951126
[2m[36m(func pid=93934)[0m f1_per_class: [0.179, 0.37, 0.0, 0.329, 0.119, 0.0, 0.619, 0.0, 0.092, 0.104]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4156 | Steps: 4 | Val loss: 3.5931 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=93317)[0m top1: 0.40904850746268656
[2m[36m(func pid=93317)[0m top5: 0.886660447761194
[2m[36m(func pid=93317)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=93317)[0m f1_macro: 0.3797483674404913
[2m[36m(func pid=93317)[0m f1_weighted: 0.42963389265285784
[2m[36m(func pid=93317)[0m f1_per_class: [0.472, 0.493, 0.393, 0.483, 0.076, 0.366, 0.376, 0.502, 0.327, 0.309]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.19962686567164178
[2m[36m(func pid=102012)[0m top5: 0.84375
[2m[36m(func pid=102012)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=102012)[0m f1_macro: 0.12871388244190435
[2m[36m(func pid=102012)[0m f1_weighted: 0.18339516726563662
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.12, 0.124, 0.0, 0.0, 0.361, 0.345, 0.273, 0.064, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.25466417910447764
[2m[36m(func pid=99348)[0m top5: 0.8656716417910447
[2m[36m(func pid=99348)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=99348)[0m f1_macro: 0.2544684615918773
[2m[36m(func pid=99348)[0m f1_weighted: 0.20444592690194702
[2m[36m(func pid=99348)[0m f1_per_class: [0.136, 0.473, 0.7, 0.194, 0.06, 0.063, 0.102, 0.283, 0.052, 0.481]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4781 | Steps: 4 | Val loss: 3.1679 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.9311 | Steps: 4 | Val loss: 1.6856 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.8426 | Steps: 4 | Val loss: 3.1080 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:11:06 (running for 00:37:31.95)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.767 |      0.38  |                   68 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.478 |      0.223 |                   69 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.416 |      0.254 |                   45 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.876 |      0.129 |                   35 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.36613805970149255
[2m[36m(func pid=93934)[0m top5: 0.8661380597014925
[2m[36m(func pid=93934)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=93934)[0m f1_macro: 0.22262647793606463
[2m[36m(func pid=93934)[0m f1_weighted: 0.34659926168659894
[2m[36m(func pid=93934)[0m f1_per_class: [0.273, 0.38, 0.316, 0.295, 0.089, 0.007, 0.619, 0.0, 0.123, 0.125]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.8957 | Steps: 4 | Val loss: 3.1474 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=93317)[0m top1: 0.3941231343283582
[2m[36m(func pid=93317)[0m top5: 0.8810634328358209
[2m[36m(func pid=93317)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=93317)[0m f1_macro: 0.36478851400717754
[2m[36m(func pid=93317)[0m f1_weighted: 0.4195174151167786
[2m[36m(func pid=93317)[0m f1_per_class: [0.434, 0.476, 0.4, 0.466, 0.079, 0.34, 0.385, 0.508, 0.291, 0.269]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.21222014925373134
[2m[36m(func pid=102012)[0m top5: 0.7994402985074627
[2m[36m(func pid=102012)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=102012)[0m f1_macro: 0.13744111152528365
[2m[36m(func pid=102012)[0m f1_weighted: 0.15783934044413622
[2m[36m(func pid=102012)[0m f1_per_class: [0.065, 0.082, 0.117, 0.271, 0.0, 0.352, 0.0, 0.392, 0.095, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.25466417910447764
[2m[36m(func pid=99348)[0m top5: 0.8628731343283582
[2m[36m(func pid=99348)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=99348)[0m f1_macro: 0.19723996061379095
[2m[36m(func pid=99348)[0m f1_weighted: 0.23147967847750936
[2m[36m(func pid=99348)[0m f1_per_class: [0.127, 0.453, 0.0, 0.224, 0.073, 0.094, 0.174, 0.301, 0.082, 0.444]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8219 | Steps: 4 | Val loss: 3.0872 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8685 | Steps: 4 | Val loss: 1.7260 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.9147 | Steps: 4 | Val loss: 2.8894 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:11:11 (running for 00:37:37.31)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.931 |      0.365 |                   69 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.822 |      0.275 |                   70 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.896 |      0.197 |                   46 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.843 |      0.137 |                   36 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.38619402985074625
[2m[36m(func pid=93934)[0m top5: 0.8852611940298507
[2m[36m(func pid=93934)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=93934)[0m f1_macro: 0.27537011471387096
[2m[36m(func pid=93934)[0m f1_weighted: 0.37485387460709013
[2m[36m(func pid=93934)[0m f1_per_class: [0.271, 0.421, 0.588, 0.337, 0.069, 0.031, 0.618, 0.031, 0.22, 0.168]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.2649 | Steps: 4 | Val loss: 2.9250 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=93317)[0m top1: 0.37453358208955223
[2m[36m(func pid=93317)[0m top5: 0.8791977611940298
[2m[36m(func pid=93317)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=93317)[0m f1_macro: 0.34649515801936814
[2m[36m(func pid=93317)[0m f1_weighted: 0.40645536526797005
[2m[36m(func pid=93317)[0m f1_per_class: [0.386, 0.444, 0.32, 0.451, 0.066, 0.364, 0.372, 0.492, 0.292, 0.278]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.17444029850746268
[2m[36m(func pid=102012)[0m top5: 0.7472014925373134
[2m[36m(func pid=102012)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=102012)[0m f1_macro: 0.12630460844314761
[2m[36m(func pid=102012)[0m f1_weighted: 0.1057405197043611
[2m[36m(func pid=102012)[0m f1_per_class: [0.057, 0.128, 0.3, 0.073, 0.0, 0.334, 0.003, 0.368, 0.0, 0.0]
[2m[36m(func pid=99348)[0m top1: 0.2943097014925373
[2m[36m(func pid=99348)[0m top5: 0.8698694029850746
[2m[36m(func pid=99348)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=99348)[0m f1_macro: 0.21326867022795298
[2m[36m(func pid=99348)[0m f1_weighted: 0.282291147436208
[2m[36m(func pid=99348)[0m f1_per_class: [0.113, 0.458, 0.0, 0.348, 0.068, 0.107, 0.217, 0.334, 0.075, 0.412]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3493 | Steps: 4 | Val loss: 3.4434 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7846 | Steps: 4 | Val loss: 1.7028 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:11:17 (running for 00:37:42.78)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.868 |      0.346 |                   70 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.349 |      0.295 |                   71 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.265 |      0.213 |                   47 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.915 |      0.126 |                   37 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.3474813432835821
[2m[36m(func pid=93934)[0m top5: 0.8889925373134329
[2m[36m(func pid=93934)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=93934)[0m f1_macro: 0.29452250356897725
[2m[36m(func pid=93934)[0m f1_weighted: 0.3590984991351823
[2m[36m(func pid=93934)[0m f1_per_class: [0.182, 0.422, 0.524, 0.276, 0.044, 0.075, 0.537, 0.396, 0.236, 0.254]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5333 | Steps: 4 | Val loss: 2.6289 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4857 | Steps: 4 | Val loss: 2.7669 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=93317)[0m top1: 0.3824626865671642
[2m[36m(func pid=93317)[0m top5: 0.8815298507462687
[2m[36m(func pid=93317)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=93317)[0m f1_macro: 0.3421515682347323
[2m[36m(func pid=93317)[0m f1_weighted: 0.4156986371818046
[2m[36m(func pid=93317)[0m f1_per_class: [0.38, 0.433, 0.289, 0.467, 0.081, 0.373, 0.397, 0.481, 0.277, 0.243]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.31156716417910446
[2m[36m(func pid=99348)[0m top5: 0.8768656716417911
[2m[36m(func pid=99348)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=99348)[0m f1_macro: 0.2080863148872977
[2m[36m(func pid=99348)[0m f1_weighted: 0.29635583342747307
[2m[36m(func pid=99348)[0m f1_per_class: [0.137, 0.471, 0.0, 0.402, 0.074, 0.061, 0.23, 0.337, 0.026, 0.343]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.11847014925373134
[2m[36m(func pid=102012)[0m top5: 0.746268656716418
[2m[36m(func pid=102012)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=102012)[0m f1_macro: 0.10957511416184137
[2m[36m(func pid=102012)[0m f1_weighted: 0.07976808863881138
[2m[36m(func pid=102012)[0m f1_per_class: [0.052, 0.101, 0.176, 0.0, 0.0, 0.282, 0.0, 0.484, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.0106 | Steps: 4 | Val loss: 3.5967 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9184 | Steps: 4 | Val loss: 1.7001 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.9228 | Steps: 4 | Val loss: 2.2350 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=93934)[0m top1: 0.314365671641791
[2m[36m(func pid=93934)[0m top5: 0.8717350746268657
[2m[36m(func pid=93934)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=93934)[0m f1_macro: 0.27558818274922126
[2m[36m(func pid=93934)[0m f1_weighted: 0.33840365794071425
[2m[36m(func pid=93934)[0m f1_per_class: [0.066, 0.409, 0.367, 0.273, 0.041, 0.186, 0.439, 0.426, 0.222, 0.327]
[2m[36m(func pid=93934)[0m 
== Status ==
Current time: 2024-01-07 15:11:22 (running for 00:37:48.37)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.785 |      0.342 |                   71 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.011 |      0.276 |                   72 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.486 |      0.208 |                   48 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.533 |      0.11  |                   38 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4414 | Steps: 4 | Val loss: 2.6228 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=93317)[0m top1: 0.39365671641791045
[2m[36m(func pid=93317)[0m top5: 0.8791977611940298
[2m[36m(func pid=93317)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=93317)[0m f1_macro: 0.3446005736068863
[2m[36m(func pid=93317)[0m f1_weighted: 0.4270040471966604
[2m[36m(func pid=93317)[0m f1_per_class: [0.372, 0.465, 0.267, 0.472, 0.092, 0.376, 0.412, 0.483, 0.266, 0.24]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.37220149253731344
[2m[36m(func pid=99348)[0m top5: 0.8885261194029851
[2m[36m(func pid=99348)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=99348)[0m f1_macro: 0.2859550424676303
[2m[36m(func pid=99348)[0m f1_weighted: 0.36389275489106604
[2m[36m(func pid=99348)[0m f1_per_class: [0.144, 0.516, 0.5, 0.479, 0.071, 0.023, 0.35, 0.407, 0.0, 0.368]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.09514925373134328
[2m[36m(func pid=102012)[0m top5: 0.6585820895522388
[2m[36m(func pid=102012)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=102012)[0m f1_macro: 0.10763513139267682
[2m[36m(func pid=102012)[0m f1_weighted: 0.07687912521211226
[2m[36m(func pid=102012)[0m f1_per_class: [0.041, 0.129, 0.175, 0.0, 0.0, 0.186, 0.0, 0.545, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4350 | Steps: 4 | Val loss: 4.1346 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9266 | Steps: 4 | Val loss: 1.6687 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.0667 | Steps: 4 | Val loss: 2.1808 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:11:28 (running for 00:37:53.81)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.918 |      0.345 |                   72 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.435 |      0.225 |                   73 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.923 |      0.286 |                   49 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.441 |      0.108 |                   39 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.26725746268656714
[2m[36m(func pid=93934)[0m top5: 0.8460820895522388
[2m[36m(func pid=93934)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=93934)[0m f1_macro: 0.22478268288557457
[2m[36m(func pid=93934)[0m f1_weighted: 0.2848899257681429
[2m[36m(func pid=93934)[0m f1_per_class: [0.058, 0.387, 0.262, 0.24, 0.033, 0.243, 0.341, 0.137, 0.228, 0.32]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.9322 | Steps: 4 | Val loss: 2.5946 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=93317)[0m top1: 0.40904850746268656
[2m[36m(func pid=93317)[0m top5: 0.8861940298507462
[2m[36m(func pid=93317)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=93317)[0m f1_macro: 0.3591933411839519
[2m[36m(func pid=93317)[0m f1_weighted: 0.43995005725426556
[2m[36m(func pid=93317)[0m f1_per_class: [0.408, 0.482, 0.273, 0.487, 0.093, 0.379, 0.425, 0.474, 0.292, 0.278]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.3810634328358209
[2m[36m(func pid=99348)[0m top5: 0.8922574626865671
[2m[36m(func pid=99348)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=99348)[0m f1_macro: 0.2930561338204147
[2m[36m(func pid=99348)[0m f1_weighted: 0.3719636978235939
[2m[36m(func pid=99348)[0m f1_per_class: [0.106, 0.522, 0.385, 0.494, 0.065, 0.016, 0.357, 0.405, 0.051, 0.531]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.12686567164179105
[2m[36m(func pid=102012)[0m top5: 0.6875
[2m[36m(func pid=102012)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=102012)[0m f1_macro: 0.11802999650063231
[2m[36m(func pid=102012)[0m f1_weighted: 0.14040556670972193
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.045, 0.164, 0.0, 0.023, 0.189, 0.275, 0.485, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2846 | Steps: 4 | Val loss: 4.6459 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9268 | Steps: 4 | Val loss: 1.7428 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0310 | Steps: 4 | Val loss: 2.1135 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:11:33 (running for 00:37:59.55)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3545
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.927 |      0.359 |                   73 |
| train_5ae7f_00017 | RUNNING    | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  0.285 |      0.197 |                   74 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.067 |      0.293 |                   50 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.932 |      0.118 |                   40 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.23833955223880596
[2m[36m(func pid=93934)[0m top5: 0.7975746268656716
[2m[36m(func pid=93934)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=93934)[0m f1_macro: 0.19653516044415162
[2m[36m(func pid=93934)[0m f1_weighted: 0.23987253499072583
[2m[36m(func pid=93934)[0m f1_per_class: [0.089, 0.379, 0.16, 0.124, 0.029, 0.278, 0.298, 0.105, 0.202, 0.3]
[2m[36m(func pid=93934)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5896 | Steps: 4 | Val loss: 3.6584 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=93317)[0m top1: 0.38152985074626866
[2m[36m(func pid=93317)[0m top5: 0.8763992537313433
[2m[36m(func pid=93317)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=93317)[0m f1_macro: 0.3377019751928735
[2m[36m(func pid=93317)[0m f1_weighted: 0.42022056730469054
[2m[36m(func pid=93317)[0m f1_per_class: [0.361, 0.436, 0.231, 0.469, 0.083, 0.372, 0.416, 0.454, 0.268, 0.288]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.3805970149253731
[2m[36m(func pid=99348)[0m top5: 0.8880597014925373
[2m[36m(func pid=99348)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=99348)[0m f1_macro: 0.2834704319036513
[2m[36m(func pid=99348)[0m f1_weighted: 0.3740021655547736
[2m[36m(func pid=99348)[0m f1_per_class: [0.13, 0.462, 0.314, 0.518, 0.106, 0.032, 0.368, 0.408, 0.092, 0.405]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.08722014925373134
[2m[36m(func pid=102012)[0m top5: 0.5457089552238806
[2m[36m(func pid=102012)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=102012)[0m f1_macro: 0.08644195497417015
[2m[36m(func pid=102012)[0m f1_weighted: 0.07908849029790357
[2m[36m(func pid=102012)[0m f1_per_class: [0.034, 0.0, 0.057, 0.0, 0.023, 0.158, 0.109, 0.457, 0.025, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93934)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.4768 | Steps: 4 | Val loss: 4.4410 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7606 | Steps: 4 | Val loss: 1.7513 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3339 | Steps: 4 | Val loss: 2.2680 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3102 | Steps: 4 | Val loss: 3.8369 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:11:39 (running for 00:38:05.05)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.927 |      0.338 |                   74 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.031 |      0.283 |                   51 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.59  |      0.086 |                   41 |
| train_5ae7f_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93934)[0m top1: 0.23600746268656717
[2m[36m(func pid=93934)[0m top5: 0.7784514925373134
[2m[36m(func pid=93934)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=93934)[0m f1_macro: 0.21170265785860393
[2m[36m(func pid=93934)[0m f1_weighted: 0.23796536062824894
[2m[36m(func pid=93934)[0m f1_per_class: [0.142, 0.341, 0.122, 0.116, 0.061, 0.32, 0.282, 0.211, 0.181, 0.342]
[2m[36m(func pid=93317)[0m top1: 0.3833955223880597
[2m[36m(func pid=93317)[0m top5: 0.8833955223880597
[2m[36m(func pid=93317)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=93317)[0m f1_macro: 0.34778684867309545
[2m[36m(func pid=93317)[0m f1_weighted: 0.4221743851503219
[2m[36m(func pid=93317)[0m f1_per_class: [0.403, 0.438, 0.308, 0.45, 0.073, 0.357, 0.437, 0.472, 0.276, 0.264]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.36800373134328357
[2m[36m(func pid=99348)[0m top5: 0.8875932835820896
[2m[36m(func pid=99348)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=99348)[0m f1_macro: 0.25873750864366984
[2m[36m(func pid=99348)[0m f1_weighted: 0.36393155192239174
[2m[36m(func pid=99348)[0m f1_per_class: [0.11, 0.4, 0.235, 0.522, 0.109, 0.04, 0.369, 0.405, 0.104, 0.292]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.08348880597014925
[2m[36m(func pid=102012)[0m top5: 0.6506529850746269
[2m[36m(func pid=102012)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=102012)[0m f1_macro: 0.08023513985139377
[2m[36m(func pid=102012)[0m f1_weighted: 0.06891605279253175
[2m[36m(func pid=102012)[0m f1_per_class: [0.065, 0.0, 0.0, 0.0, 0.025, 0.153, 0.077, 0.449, 0.035, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.7937 | Steps: 4 | Val loss: 1.7114 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.6717 | Steps: 4 | Val loss: 2.7247 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7876 | Steps: 4 | Val loss: 3.3035 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=93317)[0m top1: 0.3927238805970149
[2m[36m(func pid=93317)[0m top5: 0.8955223880597015
[2m[36m(func pid=93317)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=93317)[0m f1_macro: 0.35565997170230995
[2m[36m(func pid=93317)[0m f1_weighted: 0.42815637894175507
[2m[36m(func pid=93317)[0m f1_per_class: [0.426, 0.422, 0.329, 0.471, 0.078, 0.378, 0.44, 0.447, 0.292, 0.275]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.33908582089552236
[2m[36m(func pid=99348)[0m top5: 0.878731343283582
[2m[36m(func pid=99348)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=99348)[0m f1_macro: 0.23640154450627882
[2m[36m(func pid=99348)[0m f1_weighted: 0.348775070416
[2m[36m(func pid=99348)[0m f1_per_class: [0.074, 0.361, 0.127, 0.438, 0.145, 0.055, 0.416, 0.423, 0.118, 0.206]
[2m[36m(func pid=102012)[0m top1: 0.12080223880597014
[2m[36m(func pid=102012)[0m top5: 0.7649253731343284
[2m[36m(func pid=102012)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=102012)[0m f1_macro: 0.10251031071271784
[2m[36m(func pid=102012)[0m f1_weighted: 0.11860529880433258
[2m[36m(func pid=102012)[0m f1_per_class: [0.039, 0.0, 0.074, 0.0, 0.031, 0.179, 0.24, 0.409, 0.037, 0.016]
== Status ==
Current time: 2024-01-07 15:11:45 (running for 00:38:11.17)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.794 |      0.356 |                   76 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.334 |      0.259 |                   52 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.31  |      0.08  |                   42 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=112174)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=112174)[0m Configuration completed!
[2m[36m(func pid=112174)[0m New optimizer parameters:
[2m[36m(func pid=112174)[0m SGD (
[2m[36m(func pid=112174)[0m Parameter Group 0
[2m[36m(func pid=112174)[0m     dampening: 0
[2m[36m(func pid=112174)[0m     differentiable: False
[2m[36m(func pid=112174)[0m     foreach: None
[2m[36m(func pid=112174)[0m     lr: 0.0001
[2m[36m(func pid=112174)[0m     maximize: False
[2m[36m(func pid=112174)[0m     momentum: 0.9
[2m[36m(func pid=112174)[0m     nesterov: False
[2m[36m(func pid=112174)[0m     weight_decay: 1e-05
[2m[36m(func pid=112174)[0m )
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.8261 | Steps: 4 | Val loss: 1.6775 | Batch size: 32 | lr: 0.0001 | Duration: 3.25s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.8582 | Steps: 4 | Val loss: 3.2650 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7767 | Steps: 4 | Val loss: 2.9550 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:11:51 (running for 00:38:16.98)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.826 |      0.37  |                   77 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.672 |      0.236 |                   53 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.788 |      0.103 |                   43 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.40298507462686567
[2m[36m(func pid=93317)[0m top5: 0.9034514925373134
[2m[36m(func pid=93317)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=93317)[0m f1_macro: 0.3697983596001251
[2m[36m(func pid=93317)[0m f1_weighted: 0.434711135746053
[2m[36m(func pid=93317)[0m f1_per_class: [0.441, 0.437, 0.369, 0.469, 0.077, 0.383, 0.446, 0.44, 0.335, 0.3]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9586 | Steps: 4 | Val loss: 2.3516 | Batch size: 32 | lr: 0.0001 | Duration: 4.94s
[2m[36m(func pid=102012)[0m top1: 0.15391791044776118
[2m[36m(func pid=102012)[0m top5: 0.7971082089552238
[2m[36m(func pid=102012)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=102012)[0m f1_macro: 0.1116947494671426
[2m[36m(func pid=102012)[0m f1_weighted: 0.15587995936775495
[2m[36m(func pid=102012)[0m f1_per_class: [0.041, 0.0, 0.116, 0.0, 0.025, 0.0, 0.421, 0.466, 0.048, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.314365671641791
[2m[36m(func pid=99348)[0m top5: 0.8656716417910447
[2m[36m(func pid=99348)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=99348)[0m f1_macro: 0.21788551779275306
[2m[36m(func pid=99348)[0m f1_weighted: 0.3366333401896527
[2m[36m(func pid=99348)[0m f1_per_class: [0.08, 0.303, 0.082, 0.327, 0.2, 0.007, 0.528, 0.479, 0.091, 0.084]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.8087 | Steps: 4 | Val loss: 1.6076 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=112174)[0m top1: 0.166044776119403
[2m[36m(func pid=112174)[0m top5: 0.4944029850746269
[2m[36m(func pid=112174)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=112174)[0m f1_macro: 0.10196860912582825
[2m[36m(func pid=112174)[0m f1_weighted: 0.11629733520574329
[2m[36m(func pid=112174)[0m f1_per_class: [0.189, 0.315, 0.0, 0.079, 0.01, 0.264, 0.009, 0.031, 0.0, 0.123]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6138 | Steps: 4 | Val loss: 3.3422 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.7459 | Steps: 4 | Val loss: 3.8212 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:11:56 (running for 00:38:22.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.809 |      0.39  |                   78 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.858 |      0.218 |                   54 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.777 |      0.112 |                   44 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.959 |      0.102 |                    1 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.42350746268656714
[2m[36m(func pid=93317)[0m top5: 0.9146455223880597
[2m[36m(func pid=93317)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=93317)[0m f1_macro: 0.3897408950518009
[2m[36m(func pid=93317)[0m f1_weighted: 0.45252296990114815
[2m[36m(func pid=93317)[0m f1_per_class: [0.478, 0.444, 0.429, 0.466, 0.091, 0.385, 0.495, 0.459, 0.344, 0.307]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9641 | Steps: 4 | Val loss: 2.3838 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=102012)[0m top1: 0.1525186567164179
[2m[36m(func pid=102012)[0m top5: 0.7206156716417911
[2m[36m(func pid=102012)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=102012)[0m f1_macro: 0.12426288430188362
[2m[36m(func pid=102012)[0m f1_weighted: 0.17415975912805792
[2m[36m(func pid=102012)[0m f1_per_class: [0.106, 0.0, 0.033, 0.0, 0.023, 0.0, 0.462, 0.548, 0.071, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.28591417910447764
[2m[36m(func pid=99348)[0m top5: 0.8423507462686567
[2m[36m(func pid=99348)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=99348)[0m f1_macro: 0.1972706483571351
[2m[36m(func pid=99348)[0m f1_weighted: 0.3155035373370206
[2m[36m(func pid=99348)[0m f1_per_class: [0.077, 0.395, 0.095, 0.229, 0.068, 0.0, 0.509, 0.436, 0.1, 0.064]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.9338 | Steps: 4 | Val loss: 1.5688 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=112174)[0m top1: 0.14505597014925373
[2m[36m(func pid=112174)[0m top5: 0.47574626865671643
[2m[36m(func pid=112174)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=112174)[0m f1_macro: 0.08517099795421511
[2m[36m(func pid=112174)[0m f1_weighted: 0.10627727641855385
[2m[36m(func pid=112174)[0m f1_per_class: [0.139, 0.256, 0.0, 0.085, 0.009, 0.279, 0.006, 0.023, 0.0, 0.056]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.1764 | Steps: 4 | Val loss: 3.7524 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6886 | Steps: 4 | Val loss: 3.0886 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=93317)[0m top1: 0.4375
[2m[36m(func pid=93317)[0m top5: 0.9169776119402985
[2m[36m(func pid=93317)[0m f1_micro: 0.4375
[2m[36m(func pid=93317)[0m f1_macro: 0.3912394818301067
[2m[36m(func pid=93317)[0m f1_weighted: 0.4632986285779634
[2m[36m(func pid=93317)[0m f1_per_class: [0.449, 0.484, 0.407, 0.435, 0.095, 0.382, 0.538, 0.474, 0.335, 0.313]
== Status ==
Current time: 2024-01-07 15:12:02 (running for 00:38:27.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.934 |      0.391 |                   79 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.746 |      0.197 |                   55 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.614 |      0.124 |                   45 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.964 |      0.085 |                    2 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.27658582089552236
[2m[36m(func pid=99348)[0m top5: 0.8325559701492538
[2m[36m(func pid=99348)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=99348)[0m f1_macro: 0.1781513428593494
[2m[36m(func pid=99348)[0m f1_weighted: 0.29251973763239114
[2m[36m(func pid=99348)[0m f1_per_class: [0.071, 0.409, 0.0, 0.155, 0.053, 0.0, 0.494, 0.453, 0.087, 0.06]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.14458955223880596
[2m[36m(func pid=102012)[0m top5: 0.7271455223880597
[2m[36m(func pid=102012)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=102012)[0m f1_macro: 0.12073657470387604
[2m[36m(func pid=102012)[0m f1_weighted: 0.1585193949252348
[2m[36m(func pid=102012)[0m f1_per_class: [0.107, 0.0, 0.051, 0.0, 0.033, 0.0, 0.41, 0.545, 0.061, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9825 | Steps: 4 | Val loss: 2.3697 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.7456 | Steps: 4 | Val loss: 1.6062 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=112174)[0m top1: 0.13805970149253732
[2m[36m(func pid=112174)[0m top5: 0.5013992537313433
[2m[36m(func pid=112174)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=112174)[0m f1_macro: 0.07333865467473873
[2m[36m(func pid=112174)[0m f1_weighted: 0.11199133962986832
[2m[36m(func pid=112174)[0m f1_per_class: [0.056, 0.205, 0.0, 0.106, 0.008, 0.272, 0.041, 0.045, 0.0, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3906 | Steps: 4 | Val loss: 3.4757 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.5738 | Steps: 4 | Val loss: 3.2846 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 15:12:07 (running for 00:38:33.43)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.746 |      0.372 |                   80 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  3.176 |      0.178 |                   56 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.689 |      0.121 |                   46 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.982 |      0.073 |                    3 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.42630597014925375
[2m[36m(func pid=93317)[0m top5: 0.9160447761194029
[2m[36m(func pid=93317)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=93317)[0m f1_macro: 0.3724769573305927
[2m[36m(func pid=93317)[0m f1_weighted: 0.45141504864679943
[2m[36m(func pid=93317)[0m f1_per_class: [0.44, 0.481, 0.289, 0.386, 0.087, 0.366, 0.556, 0.47, 0.342, 0.308]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m top1: 0.125
[2m[36m(func pid=102012)[0m top5: 0.6833022388059702
[2m[36m(func pid=102012)[0m f1_micro: 0.125
[2m[36m(func pid=102012)[0m f1_macro: 0.11375519908575957
[2m[36m(func pid=102012)[0m f1_weighted: 0.13671113027819082
[2m[36m(func pid=102012)[0m f1_per_class: [0.047, 0.0, 0.062, 0.0, 0.0, 0.098, 0.301, 0.556, 0.076, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=99348)[0m top1: 0.28451492537313433
[2m[36m(func pid=99348)[0m top5: 0.8288246268656716
[2m[36m(func pid=99348)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=99348)[0m f1_macro: 0.18358677231089285
[2m[36m(func pid=99348)[0m f1_weighted: 0.2689443143025979
[2m[36m(func pid=99348)[0m f1_per_class: [0.064, 0.467, 0.111, 0.121, 0.092, 0.031, 0.411, 0.391, 0.076, 0.071]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8816 | Steps: 4 | Val loss: 2.3539 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.5933 | Steps: 4 | Val loss: 1.5676 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=112174)[0m top1: 0.14505597014925373
[2m[36m(func pid=112174)[0m top5: 0.5093283582089553
[2m[36m(func pid=112174)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=112174)[0m f1_macro: 0.07753766941695485
[2m[36m(func pid=112174)[0m f1_weighted: 0.1235812275720671
[2m[36m(func pid=112174)[0m f1_per_class: [0.034, 0.2, 0.0, 0.127, 0.009, 0.273, 0.06, 0.062, 0.011, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.5977 | Steps: 4 | Val loss: 3.4730 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1602 | Steps: 4 | Val loss: 2.9782 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=93317)[0m top1: 0.43656716417910446
[2m[36m(func pid=93317)[0m top5: 0.9216417910447762
[2m[36m(func pid=93317)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=93317)[0m f1_macro: 0.3802002937559445
[2m[36m(func pid=93317)[0m f1_weighted: 0.4578912049261025
[2m[36m(func pid=93317)[0m f1_per_class: [0.476, 0.484, 0.264, 0.376, 0.094, 0.39, 0.571, 0.481, 0.346, 0.321]
== Status ==
Current time: 2024-01-07 15:12:13 (running for 00:38:38.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.593 |      0.38  |                   81 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.574 |      0.184 |                   57 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.391 |      0.114 |                   47 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.882 |      0.078 |                    4 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.24486940298507462
[2m[36m(func pid=99348)[0m top5: 0.8521455223880597
[2m[36m(func pid=99348)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=99348)[0m f1_macro: 0.21928897386122226
[2m[36m(func pid=99348)[0m f1_weighted: 0.21754949720486327
[2m[36m(func pid=99348)[0m f1_per_class: [0.141, 0.435, 0.516, 0.097, 0.11, 0.07, 0.264, 0.294, 0.09, 0.175]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9016 | Steps: 4 | Val loss: 2.3391 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=102012)[0m top1: 0.1501865671641791
[2m[36m(func pid=102012)[0m top5: 0.7346082089552238
[2m[36m(func pid=102012)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=102012)[0m f1_macro: 0.10808874789760918
[2m[36m(func pid=102012)[0m f1_weighted: 0.14693584753103256
[2m[36m(func pid=102012)[0m f1_per_class: [0.058, 0.0, 0.075, 0.0, 0.0, 0.0, 0.384, 0.489, 0.075, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.6711 | Steps: 4 | Val loss: 1.5686 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=112174)[0m top1: 0.14925373134328357
[2m[36m(func pid=112174)[0m top5: 0.5247201492537313
[2m[36m(func pid=112174)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=112174)[0m f1_macro: 0.08995768496897072
[2m[36m(func pid=112174)[0m f1_weighted: 0.1321165113732558
[2m[36m(func pid=112174)[0m f1_per_class: [0.013, 0.204, 0.1, 0.126, 0.015, 0.278, 0.082, 0.082, 0.0, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7484 | Steps: 4 | Val loss: 3.8622 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.3356 | Steps: 4 | Val loss: 2.7477 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=93317)[0m top1: 0.42863805970149255== Status ==
Current time: 2024-01-07 15:12:18 (running for 00:38:44.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.671 |      0.377 |                   82 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.598 |      0.219 |                   58 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.16  |      0.108 |                   48 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.902 |      0.09  |                    5 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=93317)[0m top5: 0.9249067164179104
[2m[36m(func pid=93317)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=93317)[0m f1_macro: 0.37703076987167733
[2m[36m(func pid=93317)[0m f1_weighted: 0.44980393193106427
[2m[36m(func pid=93317)[0m f1_per_class: [0.463, 0.467, 0.304, 0.371, 0.09, 0.406, 0.557, 0.464, 0.329, 0.32]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.2392723880597015
[2m[36m(func pid=99348)[0m top5: 0.886660447761194
[2m[36m(func pid=99348)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=99348)[0m f1_macro: 0.23063708622782655
[2m[36m(func pid=99348)[0m f1_weighted: 0.21463085848117375
[2m[36m(func pid=99348)[0m f1_per_class: [0.181, 0.44, 0.552, 0.118, 0.088, 0.113, 0.218, 0.262, 0.06, 0.275]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9413 | Steps: 4 | Val loss: 2.3285 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=102012)[0m top1: 0.22388059701492538
[2m[36m(func pid=102012)[0m top5: 0.7863805970149254
[2m[36m(func pid=102012)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=102012)[0m f1_macro: 0.1408472405591433
[2m[36m(func pid=102012)[0m f1_weighted: 0.22986979089625867
[2m[36m(func pid=102012)[0m f1_per_class: [0.055, 0.161, 0.145, 0.226, 0.0, 0.0, 0.379, 0.386, 0.057, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.8958 | Steps: 4 | Val loss: 1.6089 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.8721 | Steps: 4 | Val loss: 3.3183 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=112174)[0m top1: 0.14832089552238806
[2m[36m(func pid=112174)[0m top5: 0.5433768656716418
[2m[36m(func pid=112174)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=112174)[0m f1_macro: 0.09307113162397448
[2m[36m(func pid=112174)[0m f1_weighted: 0.13087543551406514
[2m[36m(func pid=112174)[0m f1_per_class: [0.024, 0.197, 0.133, 0.124, 0.015, 0.276, 0.083, 0.078, 0.0, 0.0]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:12:24 (running for 00:38:49.93)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.896 |      0.36  |                   83 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.748 |      0.231 |                   59 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.336 |      0.141 |                   49 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.941 |      0.093 |                    6 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.4146455223880597
[2m[36m(func pid=93317)[0m top5: 0.9118470149253731
[2m[36m(func pid=93317)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=93317)[0m f1_macro: 0.3601310915524072
[2m[36m(func pid=93317)[0m f1_weighted: 0.44346509979655524
[2m[36m(func pid=93317)[0m f1_per_class: [0.383, 0.467, 0.211, 0.393, 0.091, 0.394, 0.528, 0.447, 0.349, 0.339]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.2717 | Steps: 4 | Val loss: 2.5603 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=99348)[0m top1: 0.2523320895522388
[2m[36m(func pid=99348)[0m top5: 0.9067164179104478
[2m[36m(func pid=99348)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=99348)[0m f1_macro: 0.25291517935785235
[2m[36m(func pid=99348)[0m f1_weighted: 0.23931264439474703
[2m[36m(func pid=99348)[0m f1_per_class: [0.183, 0.439, 0.533, 0.176, 0.09, 0.201, 0.207, 0.277, 0.062, 0.361]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.37826492537313433
[2m[36m(func pid=102012)[0m top5: 0.8535447761194029
[2m[36m(func pid=102012)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=102012)[0m f1_macro: 0.1757990720255625
[2m[36m(func pid=102012)[0m f1_weighted: 0.33766614956587715
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.247, 0.333, 0.4, 0.0, 0.0, 0.576, 0.126, 0.075, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9253 | Steps: 4 | Val loss: 2.3245 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.8118 | Steps: 4 | Val loss: 1.6258 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.8432 | Steps: 4 | Val loss: 3.0154 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=112174)[0m top1: 0.1455223880597015
[2m[36m(func pid=112174)[0m top5: 0.5443097014925373
[2m[36m(func pid=112174)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=112174)[0m f1_macro: 0.09583335399081579
[2m[36m(func pid=112174)[0m f1_weighted: 0.12917459735265613
[2m[36m(func pid=112174)[0m f1_per_class: [0.043, 0.198, 0.154, 0.124, 0.022, 0.265, 0.081, 0.072, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 15:12:29 (running for 00:38:54.99)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.896 |      0.36  |                   83 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.872 |      0.253 |                   60 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.272 |      0.176 |                   50 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.925 |      0.096 |                    7 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m top1: 0.417910447761194
[2m[36m(func pid=93317)[0m top5: 0.9067164179104478
[2m[36m(func pid=93317)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=93317)[0m f1_macro: 0.36136920193113
[2m[36m(func pid=93317)[0m f1_weighted: 0.44880465431578037
[2m[36m(func pid=93317)[0m f1_per_class: [0.397, 0.464, 0.185, 0.396, 0.093, 0.398, 0.538, 0.474, 0.36, 0.31]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.9594 | Steps: 4 | Val loss: 2.4767 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=99348)[0m top1: 0.25419776119402987
[2m[36m(func pid=99348)[0m top5: 0.9118470149253731
[2m[36m(func pid=99348)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=99348)[0m f1_macro: 0.2062511106111878
[2m[36m(func pid=99348)[0m f1_weighted: 0.2549310155016002
[2m[36m(func pid=99348)[0m f1_per_class: [0.167, 0.404, 0.133, 0.253, 0.082, 0.172, 0.229, 0.291, 0.056, 0.275]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.38386194029850745
[2m[36m(func pid=102012)[0m top5: 0.8442164179104478
[2m[36m(func pid=102012)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=102012)[0m f1_macro: 0.12736916020084996
[2m[36m(func pid=102012)[0m f1_weighted: 0.32879669838951725
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.235, 0.0, 0.386, 0.0, 0.008, 0.599, 0.0, 0.047, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8919 | Steps: 4 | Val loss: 2.3220 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.5754 | Steps: 4 | Val loss: 1.6097 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3722 | Steps: 4 | Val loss: 2.7219 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:12:34 (running for 00:39:00.52)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.812 |      0.361 |                   84 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.843 |      0.206 |                   61 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.959 |      0.127 |                   51 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.892 |      0.095 |                    8 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.14085820895522388
[2m[36m(func pid=112174)[0m top5: 0.5480410447761194
[2m[36m(func pid=112174)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=112174)[0m f1_macro: 0.09467133098450298
[2m[36m(func pid=112174)[0m f1_weighted: 0.12514480930370186
[2m[36m(func pid=112174)[0m f1_per_class: [0.025, 0.2, 0.17, 0.114, 0.025, 0.267, 0.076, 0.07, 0.0, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m top1: 0.42490671641791045
[2m[36m(func pid=93317)[0m top5: 0.9174440298507462
[2m[36m(func pid=93317)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=93317)[0m f1_macro: 0.367329489836759
[2m[36m(func pid=93317)[0m f1_weighted: 0.45007877818093717
[2m[36m(func pid=93317)[0m f1_per_class: [0.419, 0.495, 0.186, 0.372, 0.093, 0.381, 0.549, 0.476, 0.364, 0.339]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.2578 | Steps: 4 | Val loss: 2.2224 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=99348)[0m top1: 0.3111007462686567
[2m[36m(func pid=99348)[0m top5: 0.8875932835820896
[2m[36m(func pid=99348)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=99348)[0m f1_macro: 0.23732278236691143
[2m[36m(func pid=99348)[0m f1_weighted: 0.326769412630602
[2m[36m(func pid=99348)[0m f1_per_class: [0.08, 0.445, 0.0, 0.308, 0.065, 0.301, 0.328, 0.384, 0.115, 0.348]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.2891791044776119
[2m[36m(func pid=102012)[0m top5: 0.8264925373134329
[2m[36m(func pid=102012)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=102012)[0m f1_macro: 0.11393774041625657
[2m[36m(func pid=102012)[0m f1_weighted: 0.2080967866058983
[2m[36m(func pid=102012)[0m f1_per_class: [0.071, 0.145, 0.0, 0.5, 0.0, 0.262, 0.015, 0.12, 0.026, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8903 | Steps: 4 | Val loss: 2.3007 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.5404 | Steps: 4 | Val loss: 1.5893 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.5268 | Steps: 4 | Val loss: 2.7081 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:12:40 (running for 00:39:06.12)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.575 |      0.367 |                   85 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.372 |      0.237 |                   62 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.258 |      0.114 |                   52 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.89  |      0.099 |                    9 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.14598880597014927
[2m[36m(func pid=112174)[0m top5: 0.574160447761194
[2m[36m(func pid=112174)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=112174)[0m f1_macro: 0.09944471384417064
[2m[36m(func pid=112174)[0m f1_weighted: 0.13686061082852038
[2m[36m(func pid=112174)[0m f1_per_class: [0.028, 0.193, 0.178, 0.166, 0.023, 0.264, 0.072, 0.071, 0.0, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m top1: 0.43050373134328357
[2m[36m(func pid=93317)[0m top5: 0.9193097014925373
[2m[36m(func pid=93317)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=93317)[0m f1_macro: 0.37549183539898184
[2m[36m(func pid=93317)[0m f1_weighted: 0.4507571137703364
[2m[36m(func pid=93317)[0m f1_per_class: [0.454, 0.513, 0.163, 0.342, 0.102, 0.397, 0.554, 0.502, 0.373, 0.356]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 3.2063 | Steps: 4 | Val loss: 2.3504 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=99348)[0m top1: 0.333955223880597
[2m[36m(func pid=99348)[0m top5: 0.8726679104477612
[2m[36m(func pid=99348)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=99348)[0m f1_macro: 0.23738691259833117
[2m[36m(func pid=99348)[0m f1_weighted: 0.3603628098555931
[2m[36m(func pid=99348)[0m f1_per_class: [0.1, 0.48, 0.0, 0.383, 0.068, 0.246, 0.391, 0.269, 0.143, 0.294]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.32882462686567165
[2m[36m(func pid=102012)[0m top5: 0.8157649253731343
[2m[36m(func pid=102012)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=102012)[0m f1_macro: 0.14622131619787232
[2m[36m(func pid=102012)[0m f1_weighted: 0.2531267945546736
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.1, 0.526, 0.0, 0.207, 0.199, 0.374, 0.024, 0.032]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9015 | Steps: 4 | Val loss: 2.2901 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.6976 | Steps: 4 | Val loss: 1.5356 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 3.4782 | Steps: 4 | Val loss: 2.7812 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=112174)[0m top1: 0.1478544776119403
[2m[36m(func pid=112174)[0m top5: 0.5844216417910447
[2m[36m(func pid=112174)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=112174)[0m f1_macro: 0.09996877039909416
[2m[36m(func pid=112174)[0m f1_weighted: 0.135445450093365
[2m[36m(func pid=112174)[0m f1_per_class: [0.031, 0.195, 0.157, 0.176, 0.023, 0.274, 0.048, 0.084, 0.012, 0.0]
== Status ==
Current time: 2024-01-07 15:12:46 (running for 00:39:11.66)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.54  |      0.375 |                   86 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.527 |      0.237 |                   63 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.206 |      0.146 |                   53 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.902 |      0.1   |                   10 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m top1: 0.43796641791044777
[2m[36m(func pid=93317)[0m top5: 0.9305037313432836
[2m[36m(func pid=93317)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=93317)[0m f1_macro: 0.38245483783756257
[2m[36m(func pid=93317)[0m f1_weighted: 0.4579608207901243
[2m[36m(func pid=93317)[0m f1_per_class: [0.488, 0.491, 0.255, 0.377, 0.103, 0.401, 0.565, 0.452, 0.36, 0.333]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4404 | Steps: 4 | Val loss: 2.3844 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=99348)[0m top1: 0.31156716417910446
[2m[36m(func pid=99348)[0m top5: 0.8614738805970149
[2m[36m(func pid=99348)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=99348)[0m f1_macro: 0.21672478033933826
[2m[36m(func pid=99348)[0m f1_weighted: 0.33109430721395366
[2m[36m(func pid=99348)[0m f1_per_class: [0.048, 0.461, 0.0, 0.444, 0.061, 0.201, 0.28, 0.182, 0.177, 0.312]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.09468283582089553
[2m[36m(func pid=102012)[0m top5: 0.7299440298507462
[2m[36m(func pid=102012)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=102012)[0m f1_macro: 0.09846099189234848
[2m[36m(func pid=102012)[0m f1_weighted: 0.08967207506048577
[2m[36m(func pid=102012)[0m f1_per_class: [0.083, 0.0, 0.24, 0.013, 0.0, 0.0, 0.195, 0.412, 0.019, 0.023]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8705 | Steps: 4 | Val loss: 2.2732 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.6953 | Steps: 4 | Val loss: 1.5333 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.1122 | Steps: 4 | Val loss: 2.7890 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 15:12:51 (running for 00:39:17.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.698 |      0.382 |                   87 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  3.478 |      0.217 |                   64 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.44  |      0.098 |                   54 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.87  |      0.111 |                   11 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.4412313432835821
[2m[36m(func pid=93317)[0m top5: 0.9328358208955224
[2m[36m(func pid=93317)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=93317)[0m f1_macro: 0.3932736198308861
[2m[36m(func pid=93317)[0m f1_weighted: 0.4606628432165791
[2m[36m(func pid=93317)[0m f1_per_class: [0.496, 0.51, 0.202, 0.378, 0.104, 0.404, 0.551, 0.464, 0.389, 0.435]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7779 | Steps: 4 | Val loss: 2.9066 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=112174)[0m top1: 0.1623134328358209
[2m[36m(func pid=112174)[0m top5: 0.6119402985074627
[2m[36m(func pid=112174)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=112174)[0m f1_macro: 0.11120819410825403
[2m[36m(func pid=112174)[0m f1_weighted: 0.15213315231145683
[2m[36m(func pid=112174)[0m f1_per_class: [0.031, 0.209, 0.189, 0.196, 0.023, 0.287, 0.07, 0.095, 0.013, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.2957089552238806
[2m[36m(func pid=99348)[0m top5: 0.8722014925373134
[2m[36m(func pid=99348)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=99348)[0m f1_macro: 0.21718357646070424
[2m[36m(func pid=99348)[0m f1_weighted: 0.315891872094093
[2m[36m(func pid=99348)[0m f1_per_class: [0.063, 0.401, 0.0, 0.459, 0.058, 0.243, 0.215, 0.307, 0.122, 0.304]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.07555970149253731
[2m[36m(func pid=102012)[0m top5: 0.6026119402985075
[2m[36m(func pid=102012)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=102012)[0m f1_macro: 0.11153694474644593
[2m[36m(func pid=102012)[0m f1_weighted: 0.07136784187996895
[2m[36m(func pid=102012)[0m f1_per_class: [0.079, 0.0, 0.333, 0.0, 0.0, 0.0, 0.12, 0.534, 0.029, 0.021]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.5850 | Steps: 4 | Val loss: 1.5743 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8457 | Steps: 4 | Val loss: 2.2688 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9246 | Steps: 4 | Val loss: 2.6574 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 15:12:57 (running for 00:39:23.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.695 |      0.393 |                   88 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.112 |      0.217 |                   65 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.778 |      0.112 |                   55 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.846 |      0.114 |                   12 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.43283582089552236
[2m[36m(func pid=93317)[0m top5: 0.9281716417910447
[2m[36m(func pid=93317)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=93317)[0m f1_macro: 0.37956006340621673
[2m[36m(func pid=93317)[0m f1_weighted: 0.4538152062257053
[2m[36m(func pid=93317)[0m f1_per_class: [0.449, 0.508, 0.203, 0.378, 0.101, 0.411, 0.537, 0.46, 0.347, 0.4]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.8143 | Steps: 4 | Val loss: 3.0951 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=112174)[0m top1: 0.166044776119403
[2m[36m(func pid=112174)[0m top5: 0.6175373134328358
[2m[36m(func pid=112174)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=112174)[0m f1_macro: 0.11430869940450657
[2m[36m(func pid=112174)[0m f1_weighted: 0.15845529685702892
[2m[36m(func pid=112174)[0m f1_per_class: [0.029, 0.203, 0.196, 0.209, 0.027, 0.28, 0.083, 0.104, 0.012, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.31576492537313433
[2m[36m(func pid=99348)[0m top5: 0.8782649253731343
[2m[36m(func pid=99348)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=99348)[0m f1_macro: 0.2296507867690712
[2m[36m(func pid=99348)[0m f1_weighted: 0.3236273125471524
[2m[36m(func pid=99348)[0m f1_per_class: [0.105, 0.346, 0.0, 0.504, 0.059, 0.27, 0.196, 0.43, 0.118, 0.27]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.09468283582089553
[2m[36m(func pid=102012)[0m top5: 0.6557835820895522
[2m[36m(func pid=102012)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=102012)[0m f1_macro: 0.09605987593295137
[2m[36m(func pid=102012)[0m f1_weighted: 0.11466971821705473
[2m[36m(func pid=102012)[0m f1_per_class: [0.105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.529, 0.032, 0.024]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.9621 | Steps: 4 | Val loss: 1.6180 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8455 | Steps: 4 | Val loss: 2.2690 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.9830 | Steps: 4 | Val loss: 2.4120 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 15:13:03 (running for 00:39:28.61)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.962 |      0.378 |                   90 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.925 |      0.23  |                   66 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.814 |      0.096 |                   56 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.846 |      0.114 |                   12 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.42630597014925375
[2m[36m(func pid=93317)[0m top5: 0.9221082089552238
[2m[36m(func pid=93317)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=93317)[0m f1_macro: 0.3778961000557308
[2m[36m(func pid=93317)[0m f1_weighted: 0.44621935297431725
[2m[36m(func pid=93317)[0m f1_per_class: [0.432, 0.517, 0.19, 0.371, 0.1, 0.418, 0.507, 0.47, 0.365, 0.408]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.9116 | Steps: 4 | Val loss: 3.0935 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=112174)[0m top1: 0.15904850746268656
[2m[36m(func pid=112174)[0m top5: 0.6203358208955224
[2m[36m(func pid=112174)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=112174)[0m f1_macro: 0.11225662888611762
[2m[36m(func pid=112174)[0m f1_weighted: 0.15579645510457804
[2m[36m(func pid=112174)[0m f1_per_class: [0.031, 0.192, 0.185, 0.228, 0.026, 0.265, 0.067, 0.109, 0.019, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.32975746268656714
[2m[36m(func pid=99348)[0m top5: 0.8894589552238806
[2m[36m(func pid=99348)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=99348)[0m f1_macro: 0.2438907639281748
[2m[36m(func pid=99348)[0m f1_weighted: 0.3217128854006804
[2m[36m(func pid=99348)[0m f1_per_class: [0.118, 0.312, 0.0, 0.527, 0.067, 0.336, 0.153, 0.445, 0.12, 0.36]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.14505597014925373
[2m[36m(func pid=102012)[0m top5: 0.7234141791044776
[2m[36m(func pid=102012)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=102012)[0m f1_macro: 0.13601132601521465
[2m[36m(func pid=102012)[0m f1_weighted: 0.15234943187103384
[2m[36m(func pid=102012)[0m f1_per_class: [0.105, 0.0, 0.308, 0.0, 0.0, 0.0, 0.398, 0.484, 0.04, 0.025]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.7423 | Steps: 4 | Val loss: 1.6199 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8272 | Steps: 4 | Val loss: 2.2547 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0140 | Steps: 4 | Val loss: 2.3598 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:13:08 (running for 00:39:34.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.742 |      0.383 |                   91 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.983 |      0.244 |                   67 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.912 |      0.136 |                   57 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.845 |      0.112 |                   13 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.4300373134328358
[2m[36m(func pid=93317)[0m top5: 0.9183768656716418
[2m[36m(func pid=93317)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=93317)[0m f1_macro: 0.3834384755542666
[2m[36m(func pid=93317)[0m f1_weighted: 0.45018469806092787
[2m[36m(func pid=93317)[0m f1_per_class: [0.422, 0.52, 0.2, 0.415, 0.094, 0.406, 0.482, 0.453, 0.395, 0.447]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.5339 | Steps: 4 | Val loss: 3.0700 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=112174)[0m top1: 0.16837686567164178
[2m[36m(func pid=112174)[0m top5: 0.6441231343283582
[2m[36m(func pid=112174)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=112174)[0m f1_macro: 0.12301415286325483
[2m[36m(func pid=112174)[0m f1_weighted: 0.16874272667751333
[2m[36m(func pid=112174)[0m f1_per_class: [0.045, 0.21, 0.213, 0.224, 0.026, 0.271, 0.097, 0.124, 0.02, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.3400186567164179
[2m[36m(func pid=99348)[0m top5: 0.8810634328358209
[2m[36m(func pid=99348)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=99348)[0m f1_macro: 0.2412840420349363
[2m[36m(func pid=99348)[0m f1_weighted: 0.31463976264661153
[2m[36m(func pid=99348)[0m f1_per_class: [0.106, 0.279, 0.0, 0.54, 0.093, 0.341, 0.136, 0.449, 0.105, 0.364]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.11800373134328358
[2m[36m(func pid=102012)[0m top5: 0.7094216417910447
[2m[36m(func pid=102012)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=102012)[0m f1_macro: 0.0958028419132729
[2m[36m(func pid=102012)[0m f1_weighted: 0.10159316662821768
[2m[36m(func pid=102012)[0m f1_per_class: [0.095, 0.0, 0.196, 0.0, 0.021, 0.0, 0.26, 0.324, 0.062, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.6395 | Steps: 4 | Val loss: 1.5979 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7732 | Steps: 4 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.8265 | Steps: 4 | Val loss: 2.1537 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:13:13 (running for 00:39:39.52)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.742 |      0.383 |                   91 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.014 |      0.241 |                   68 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.534 |      0.096 |                   58 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.773 |      0.138 |                   15 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.42723880597014924
[2m[36m(func pid=93317)[0m top5: 0.9244402985074627
[2m[36m(func pid=93317)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=93317)[0m f1_macro: 0.3856423786546476
[2m[36m(func pid=93317)[0m f1_weighted: 0.4489763764725239
[2m[36m(func pid=93317)[0m f1_per_class: [0.463, 0.521, 0.189, 0.431, 0.097, 0.396, 0.462, 0.456, 0.411, 0.432]
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6405 | Steps: 4 | Val loss: 2.9983 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=112174)[0m top1: 0.17583955223880596
[2m[36m(func pid=112174)[0m top5: 0.6497201492537313
[2m[36m(func pid=112174)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=112174)[0m f1_macro: 0.13759524461250278
[2m[36m(func pid=112174)[0m f1_weighted: 0.18103359335282207
[2m[36m(func pid=112174)[0m f1_per_class: [0.072, 0.212, 0.275, 0.233, 0.024, 0.291, 0.116, 0.132, 0.021, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.33348880597014924
[2m[36m(func pid=99348)[0m top5: 0.8824626865671642
[2m[36m(func pid=99348)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=99348)[0m f1_macro: 0.25494008648310296
[2m[36m(func pid=99348)[0m f1_weighted: 0.3101573819105101
[2m[36m(func pid=99348)[0m f1_per_class: [0.199, 0.277, 0.0, 0.531, 0.103, 0.351, 0.119, 0.432, 0.122, 0.416]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.24440298507462688
[2m[36m(func pid=102012)[0m top5: 0.7136194029850746
[2m[36m(func pid=102012)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=102012)[0m f1_macro: 0.12774019298020675
[2m[36m(func pid=102012)[0m f1_weighted: 0.15832179308048652
[2m[36m(func pid=102012)[0m f1_per_class: [0.107, 0.437, 0.175, 0.0, 0.057, 0.0, 0.21, 0.291, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.5477 | Steps: 4 | Val loss: 1.6310 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7824 | Steps: 4 | Val loss: 2.2320 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9519 | Steps: 4 | Val loss: 2.1369 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.5600 | Steps: 4 | Val loss: 2.8507 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 15:13:19 (running for 00:39:45.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.64  |      0.386 |                   92 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.827 |      0.255 |                   69 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.641 |      0.128 |                   59 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.782 |      0.145 |                   16 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.42117537313432835
[2m[36m(func pid=93317)[0m top5: 0.9155783582089553
[2m[36m(func pid=93317)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=93317)[0m f1_macro: 0.38020262913368474
[2m[36m(func pid=93317)[0m f1_weighted: 0.43966890526438435
[2m[36m(func pid=93317)[0m f1_per_class: [0.421, 0.533, 0.185, 0.429, 0.099, 0.401, 0.427, 0.464, 0.384, 0.46]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=112174)[0m top1: 0.17863805970149255
[2m[36m(func pid=112174)[0m top5: 0.6744402985074627
[2m[36m(func pid=112174)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=112174)[0m f1_macro: 0.14543254642761277
[2m[36m(func pid=112174)[0m f1_weighted: 0.18525936914029767
[2m[36m(func pid=112174)[0m f1_per_class: [0.068, 0.228, 0.333, 0.242, 0.023, 0.285, 0.113, 0.142, 0.022, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.3050373134328358
[2m[36m(func pid=99348)[0m top5: 0.8698694029850746
[2m[36m(func pid=99348)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=99348)[0m f1_macro: 0.24131012305885763
[2m[36m(func pid=99348)[0m f1_weighted: 0.2958295624960545
[2m[36m(func pid=99348)[0m f1_per_class: [0.15, 0.228, 0.0, 0.481, 0.133, 0.361, 0.143, 0.434, 0.17, 0.313]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.26492537313432835
[2m[36m(func pid=102012)[0m top5: 0.7397388059701493
[2m[36m(func pid=102012)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=102012)[0m f1_macro: 0.12408845666783012
[2m[36m(func pid=102012)[0m f1_weighted: 0.17281065591469869
[2m[36m(func pid=102012)[0m f1_per_class: [0.102, 0.427, 0.138, 0.0, 0.0, 0.0, 0.263, 0.311, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.8278 | Steps: 4 | Val loss: 1.6375 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8292 | Steps: 4 | Val loss: 2.2238 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3086 | Steps: 4 | Val loss: 2.2338 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:13:25 (running for 00:39:50.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.828 |      0.383 |                   94 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.952 |      0.241 |                   70 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.56  |      0.124 |                   60 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.782 |      0.145 |                   16 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.43050373134328357
[2m[36m(func pid=93317)[0m top5: 0.9113805970149254
[2m[36m(func pid=93317)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=93317)[0m f1_macro: 0.383135103671128
[2m[36m(func pid=93317)[0m f1_weighted: 0.4512049829220165
[2m[36m(func pid=93317)[0m f1_per_class: [0.434, 0.535, 0.183, 0.467, 0.101, 0.402, 0.428, 0.478, 0.354, 0.449]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.4857 | Steps: 4 | Val loss: 2.6030 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=112174)[0m top1: 0.18563432835820895
[2m[36m(func pid=112174)[0m top5: 0.6861007462686567
[2m[36m(func pid=112174)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=112174)[0m f1_macro: 0.155410548636903
[2m[36m(func pid=112174)[0m f1_weighted: 0.18841899643587212
[2m[36m(func pid=112174)[0m f1_per_class: [0.079, 0.226, 0.35, 0.257, 0.024, 0.29, 0.096, 0.183, 0.049, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.26492537313432835
[2m[36m(func pid=99348)[0m top5: 0.8572761194029851
[2m[36m(func pid=99348)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=99348)[0m f1_macro: 0.22161261460030154
[2m[36m(func pid=99348)[0m f1_weighted: 0.2669732210982755
[2m[36m(func pid=99348)[0m f1_per_class: [0.108, 0.14, 0.019, 0.421, 0.177, 0.381, 0.152, 0.439, 0.147, 0.232]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.31949626865671643
[2m[36m(func pid=102012)[0m top5: 0.7761194029850746
[2m[36m(func pid=102012)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=102012)[0m f1_macro: 0.14485262610925703
[2m[36m(func pid=102012)[0m f1_weighted: 0.22822350935738064
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.411, 0.182, 0.0, 0.0, 0.0, 0.445, 0.411, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.6965 | Steps: 4 | Val loss: 1.6328 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7432 | Steps: 4 | Val loss: 2.2268 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3145 | Steps: 4 | Val loss: 2.1809 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:13:30 (running for 00:39:56.06)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.828 |      0.383 |                   94 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.309 |      0.222 |                   71 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.486 |      0.145 |                   61 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.743 |      0.161 |                   18 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.18516791044776118
[2m[36m(func pid=112174)[0m top5: 0.679570895522388
[2m[36m(func pid=112174)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=112174)[0m f1_macro: 0.1611790168437151
[2m[36m(func pid=112174)[0m f1_weighted: 0.19236897253437926
[2m[36m(func pid=112174)[0m f1_per_class: [0.116, 0.224, 0.39, 0.274, 0.027, 0.292, 0.096, 0.163, 0.029, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m top1: 0.43283582089552236
[2m[36m(func pid=93317)[0m top5: 0.9132462686567164
[2m[36m(func pid=93317)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=93317)[0m f1_macro: 0.38815190789917126
[2m[36m(func pid=93317)[0m f1_weighted: 0.45243173018894123
[2m[36m(func pid=93317)[0m f1_per_class: [0.472, 0.52, 0.214, 0.473, 0.092, 0.392, 0.433, 0.486, 0.371, 0.429]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.1012 | Steps: 4 | Val loss: 3.0664 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=99348)[0m top1: 0.26119402985074625
[2m[36m(func pid=99348)[0m top5: 0.8610074626865671
[2m[36m(func pid=99348)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=99348)[0m f1_macro: 0.224339308311368
[2m[36m(func pid=99348)[0m f1_weighted: 0.2649291282083756
[2m[36m(func pid=99348)[0m f1_per_class: [0.107, 0.153, 0.0, 0.357, 0.186, 0.401, 0.191, 0.417, 0.17, 0.261]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.23134328358208955
[2m[36m(func pid=102012)[0m top5: 0.7504664179104478
[2m[36m(func pid=102012)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=102012)[0m f1_macro: 0.10173547983500479
[2m[36m(func pid=102012)[0m f1_weighted: 0.1391497309906686
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.428, 0.167, 0.0, 0.0, 0.0, 0.167, 0.256, 0.0, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4986 | Steps: 4 | Val loss: 1.5981 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7502 | Steps: 4 | Val loss: 2.2242 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3144 | Steps: 4 | Val loss: 2.2647 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 15:13:35 (running for 00:40:01.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.696 |      0.388 |                   95 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.315 |      0.224 |                   72 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.101 |      0.102 |                   62 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.75  |      0.163 |                   19 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=93317)[0m top1: 0.44216417910447764

[2m[36m(func pid=93317)[0m top5: 0.914179104477612
[2m[36m(func pid=93317)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=93317)[0m f1_macro: 0.38518028622386735
[2m[36m(func pid=93317)[0m f1_weighted: 0.4625328247647179
[2m[36m(func pid=93317)[0m f1_per_class: [0.414, 0.549, 0.198, 0.496, 0.111, 0.388, 0.438, 0.468, 0.368, 0.421]
[2m[36m(func pid=112174)[0m top1: 0.18936567164179105
[2m[36m(func pid=112174)[0m top5: 0.6823694029850746
[2m[36m(func pid=112174)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=112174)[0m f1_macro: 0.16297461793422846
[2m[36m(func pid=112174)[0m f1_weighted: 0.19377509481088934
[2m[36m(func pid=112174)[0m f1_per_class: [0.114, 0.251, 0.364, 0.251, 0.022, 0.296, 0.099, 0.19, 0.042, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.7491 | Steps: 4 | Val loss: 3.2066 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=99348)[0m top1: 0.2439365671641791
[2m[36m(func pid=99348)[0m top5: 0.8540111940298507
[2m[36m(func pid=99348)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=99348)[0m f1_macro: 0.20545942038527815
[2m[36m(func pid=99348)[0m f1_weighted: 0.2534651428614963
[2m[36m(func pid=99348)[0m f1_per_class: [0.111, 0.17, 0.0, 0.34, 0.181, 0.361, 0.182, 0.428, 0.104, 0.179]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.1958955223880597
[2m[36m(func pid=102012)[0m top5: 0.7751865671641791
[2m[36m(func pid=102012)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=102012)[0m f1_macro: 0.10531967364102923
[2m[36m(func pid=102012)[0m f1_weighted: 0.12797345653241457
[2m[36m(func pid=102012)[0m f1_per_class: [0.059, 0.413, 0.123, 0.0, 0.0, 0.103, 0.097, 0.233, 0.025, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7425 | Steps: 4 | Val loss: 2.2192 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.6227 | Steps: 4 | Val loss: 1.4940 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.9297 | Steps: 4 | Val loss: 2.3339 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:13:41 (running for 00:40:06.87)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.499 |      0.385 |                   96 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.314 |      0.205 |                   73 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.749 |      0.105 |                   63 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.743 |      0.163 |                   20 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.1912313432835821
[2m[36m(func pid=112174)[0m top5: 0.6879664179104478
[2m[36m(func pid=112174)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=112174)[0m f1_macro: 0.1634878704676363
[2m[36m(func pid=112174)[0m f1_weighted: 0.19679960268111066
[2m[36m(func pid=112174)[0m f1_per_class: [0.109, 0.254, 0.348, 0.256, 0.023, 0.308, 0.103, 0.175, 0.033, 0.028]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.4705 | Steps: 4 | Val loss: 2.9573 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=93317)[0m top1: 0.466884328358209
[2m[36m(func pid=93317)[0m top5: 0.9281716417910447
[2m[36m(func pid=93317)[0m f1_micro: 0.46688432835820903
[2m[36m(func pid=93317)[0m f1_macro: 0.4078800423430572
[2m[36m(func pid=93317)[0m f1_weighted: 0.48300806392220075
[2m[36m(func pid=93317)[0m f1_per_class: [0.46, 0.554, 0.242, 0.521, 0.136, 0.402, 0.47, 0.443, 0.4, 0.449]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=99348)[0m top1: 0.22154850746268656
[2m[36m(func pid=99348)[0m top5: 0.8390858208955224
[2m[36m(func pid=99348)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=99348)[0m f1_macro: 0.20518190975976675
[2m[36m(func pid=99348)[0m f1_weighted: 0.23508036993079348
[2m[36m(func pid=99348)[0m f1_per_class: [0.119, 0.204, 0.143, 0.296, 0.131, 0.288, 0.162, 0.47, 0.086, 0.151]
[2m[36m(func pid=99348)[0m 
[2m[36m(func pid=102012)[0m top1: 0.1259328358208955
[2m[36m(func pid=102012)[0m top5: 0.8264925373134329
[2m[36m(func pid=102012)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=102012)[0m f1_macro: 0.0868015635842748
[2m[36m(func pid=102012)[0m f1_weighted: 0.09595258880915207
[2m[36m(func pid=102012)[0m f1_per_class: [0.061, 0.135, 0.035, 0.0, 0.0, 0.245, 0.09, 0.261, 0.04, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7602 | Steps: 4 | Val loss: 2.2125 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.7490 | Steps: 4 | Val loss: 1.4973 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=99348)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1009 | Steps: 4 | Val loss: 2.5372 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:13:47 (running for 00:40:12.60)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.34775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.749 |      0.408 |                   98 |
| train_5ae7f_00018 | RUNNING    | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  1.93  |      0.205 |                   74 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.47  |      0.087 |                   64 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.743 |      0.163 |                   20 |
| train_5ae7f_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=93317)[0m top1: 0.46828358208955223
[2m[36m(func pid=93317)[0m top5: 0.9263059701492538
[2m[36m(func pid=93317)[0m f1_micro: 0.46828358208955223
[2m[36m(func pid=93317)[0m f1_macro: 0.4079794242813491
[2m[36m(func pid=93317)[0m f1_weighted: 0.4837188206367565
[2m[36m(func pid=93317)[0m f1_per_class: [0.475, 0.548, 0.253, 0.517, 0.136, 0.376, 0.485, 0.473, 0.402, 0.416]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 3.0212 | Steps: 4 | Val loss: 2.3835 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=112174)[0m top1: 0.20942164179104478
[2m[36m(func pid=112174)[0m top5: 0.6898320895522388
[2m[36m(func pid=112174)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=112174)[0m f1_macro: 0.17189961613156465
[2m[36m(func pid=112174)[0m f1_weighted: 0.2129086695161771
[2m[36m(func pid=112174)[0m f1_per_class: [0.12, 0.247, 0.372, 0.299, 0.025, 0.328, 0.11, 0.184, 0.034, 0.0]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=99348)[0m top1: 0.2150186567164179
[2m[36m(func pid=99348)[0m top5: 0.8274253731343284
[2m[36m(func pid=99348)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=99348)[0m f1_macro: 0.2015456924799385
[2m[36m(func pid=99348)[0m f1_weighted: 0.2347997483059463
[2m[36m(func pid=99348)[0m f1_per_class: [0.135, 0.215, 0.143, 0.303, 0.109, 0.246, 0.161, 0.49, 0.089, 0.124]
[2m[36m(func pid=102012)[0m top1: 0.251865671641791
[2m[36m(func pid=102012)[0m top5: 0.8302238805970149
[2m[36m(func pid=102012)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=102012)[0m f1_macro: 0.17171917387847563
[2m[36m(func pid=102012)[0m f1_weighted: 0.23636722594725026
[2m[36m(func pid=102012)[0m f1_per_class: [0.084, 0.005, 0.312, 0.306, 0.0, 0.227, 0.316, 0.442, 0.025, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.5548 | Steps: 4 | Val loss: 1.4908 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7653 | Steps: 4 | Val loss: 2.2173 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.8006 | Steps: 4 | Val loss: 2.3673 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=93317)[0m top1: 0.4743470149253731
[2m[36m(func pid=93317)[0m top5: 0.9249067164179104
[2m[36m(func pid=93317)[0m f1_micro: 0.4743470149253731
[2m[36m(func pid=93317)[0m f1_macro: 0.4233820543806213
[2m[36m(func pid=93317)[0m f1_weighted: 0.48716843375231433
[2m[36m(func pid=93317)[0m f1_per_class: [0.517, 0.553, 0.255, 0.538, 0.133, 0.392, 0.459, 0.477, 0.416, 0.494]
[2m[36m(func pid=93317)[0m 
[2m[36m(func pid=112174)[0m top1: 0.20475746268656717
[2m[36m(func pid=112174)[0m top5: 0.675839552238806
[2m[36m(func pid=112174)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=112174)[0m f1_macro: 0.17612397669854693
[2m[36m(func pid=112174)[0m f1_weighted: 0.20614560232324003
[2m[36m(func pid=112174)[0m f1_per_class: [0.119, 0.246, 0.4, 0.307, 0.029, 0.314, 0.081, 0.196, 0.034, 0.034]
[2m[36m(func pid=102012)[0m top1: 0.23274253731343283
[2m[36m(func pid=102012)[0m top5: 0.8143656716417911
[2m[36m(func pid=102012)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=102012)[0m f1_macro: 0.1223154274580652
[2m[36m(func pid=102012)[0m f1_weighted: 0.1937156562852141
[2m[36m(func pid=102012)[0m f1_per_class: [0.0, 0.0, 0.4, 0.4, 0.0, 0.252, 0.17, 0.0, 0.0, 0.0]
[2m[36m(func pid=93317)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3857 | Steps: 4 | Val loss: 1.4599 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 15:13:52 (running for 00:40:18.06)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00016 | RUNNING    | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.555 |      0.423 |                   99 |
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.021 |      0.172 |                   65 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.76  |      0.172 |                   21 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=117778)[0m Configuration completed!
[2m[36m(func pid=117778)[0m New optimizer parameters:
[2m[36m(func pid=117778)[0m SGD (
[2m[36m(func pid=117778)[0m Parameter Group 0
[2m[36m(func pid=117778)[0m     dampening: 0
[2m[36m(func pid=117778)[0m     differentiable: False
[2m[36m(func pid=117778)[0m     foreach: None
[2m[36m(func pid=117778)[0m     lr: 0.001
[2m[36m(func pid=117778)[0m     maximize: False
[2m[36m(func pid=117778)[0m     momentum: 0.9
[2m[36m(func pid=117778)[0m     nesterov: False
[2m[36m(func pid=117778)[0m     weight_decay: 1e-05
[2m[36m(func pid=117778)[0m )
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=93317)[0m top1: 0.4832089552238806
[2m[36m(func pid=93317)[0m top5: 0.9267723880597015
[2m[36m(func pid=93317)[0m f1_micro: 0.4832089552238806
[2m[36m(func pid=93317)[0m f1_macro: 0.42367768114079774
[2m[36m(func pid=93317)[0m f1_weighted: 0.49667914409219815
[2m[36m(func pid=93317)[0m f1_per_class: [0.517, 0.548, 0.264, 0.548, 0.149, 0.385, 0.493, 0.452, 0.414, 0.467]
== Status ==
Current time: 2024-01-07 15:13:58 (running for 00:40:23.82)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.801 |      0.122 |                   66 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.765 |      0.176 |                   22 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 3.0172 | Steps: 4 | Val loss: 2.3304 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7494 | Steps: 4 | Val loss: 2.2004 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9818 | Steps: 4 | Val loss: 2.4031 | Batch size: 32 | lr: 0.001 | Duration: 4.97s
[2m[36m(func pid=112174)[0m top1: 0.21222014925373134
[2m[36m(func pid=112174)[0m top5: 0.7038246268656716
[2m[36m(func pid=112174)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=112174)[0m f1_macro: 0.1817784025702374
[2m[36m(func pid=112174)[0m f1_weighted: 0.21520711324028013
[2m[36m(func pid=112174)[0m f1_per_class: [0.129, 0.25, 0.4, 0.304, 0.028, 0.341, 0.101, 0.198, 0.037, 0.03]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=102012)[0m top1: 0.35774253731343286
[2m[36m(func pid=102012)[0m top5: 0.8246268656716418
[2m[36m(func pid=102012)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=102012)[0m f1_macro: 0.20794542920877873
[2m[36m(func pid=102012)[0m f1_weighted: 0.34348804040598857
[2m[36m(func pid=102012)[0m f1_per_class: [0.072, 0.0, 0.294, 0.426, 0.0, 0.33, 0.539, 0.37, 0.048, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=117778)[0m top1: 0.12126865671641791
[2m[36m(func pid=117778)[0m top5: 0.4594216417910448
[2m[36m(func pid=117778)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=117778)[0m f1_macro: 0.07524074518156479
[2m[36m(func pid=117778)[0m f1_weighted: 0.08945132992167332
[2m[36m(func pid=117778)[0m f1_per_class: [0.133, 0.199, 0.0, 0.079, 0.0, 0.237, 0.003, 0.022, 0.011, 0.067]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5708 | Steps: 4 | Val loss: 3.4341 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6639 | Steps: 4 | Val loss: 2.1989 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9130 | Steps: 4 | Val loss: 2.3834 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:14:03 (running for 00:40:29.05)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.017 |      0.208 |                   67 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.749 |      0.182 |                   23 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.982 |      0.075 |                    1 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102012)[0m top1: 0.16557835820895522
[2m[36m(func pid=102012)[0m top5: 0.7887126865671642
[2m[36m(func pid=102012)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=102012)[0m f1_macro: 0.1065433203213002
[2m[36m(func pid=102012)[0m f1_weighted: 0.1376385387630976
[2m[36m(func pid=102012)[0m f1_per_class: [0.012, 0.0, 0.208, 0.003, 0.0, 0.008, 0.366, 0.391, 0.077, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=118433)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=118433)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=118433)[0m Configuration completed!
[2m[36m(func pid=118433)[0m New optimizer parameters:
[2m[36m(func pid=118433)[0m SGD (
[2m[36m(func pid=118433)[0m Parameter Group 0
[2m[36m(func pid=118433)[0m     dampening: 0
[2m[36m(func pid=118433)[0m     differentiable: False
[2m[36m(func pid=118433)[0m     foreach: None
[2m[36m(func pid=118433)[0m     lr: 0.01
[2m[36m(func pid=118433)[0m     maximize: False
[2m[36m(func pid=118433)[0m     momentum: 0.9
[2m[36m(func pid=118433)[0m     nesterov: False
[2m[36m(func pid=118433)[0m     weight_decay: 1e-05
[2m[36m(func pid=118433)[0m )
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=112174)[0m top1: 0.22061567164179105
[2m[36m(func pid=112174)[0m top5: 0.6996268656716418
[2m[36m(func pid=112174)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=112174)[0m f1_macro: 0.19845336660276627
[2m[36m(func pid=112174)[0m f1_weighted: 0.22321804841240392
[2m[36m(func pid=112174)[0m f1_per_class: [0.143, 0.259, 0.486, 0.329, 0.03, 0.327, 0.097, 0.214, 0.036, 0.062]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.13526119402985073
[2m[36m(func pid=117778)[0m top5: 0.4962686567164179
[2m[36m(func pid=117778)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=117778)[0m f1_macro: 0.0843982226712216
[2m[36m(func pid=117778)[0m f1_weighted: 0.1294993081297215
[2m[36m(func pid=117778)[0m f1_per_class: [0.107, 0.157, 0.0, 0.099, 0.018, 0.266, 0.132, 0.034, 0.017, 0.014]
[2m[36m(func pid=117778)[0m 
== Status ==
Current time: 2024-01-07 15:14:09 (running for 00:40:34.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.571 |      0.107 |                   68 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.664 |      0.198 |                   24 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.913 |      0.084 |                    2 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7199 | Steps: 4 | Val loss: 2.1925 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 3.3025 | Steps: 4 | Val loss: 3.7719 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9580 | Steps: 4 | Val loss: 2.3156 | Batch size: 32 | lr: 0.01 | Duration: 4.92s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8553 | Steps: 4 | Val loss: 2.3313 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=112174)[0m top1: 0.22014925373134328
[2m[36m(func pid=112174)[0m top5: 0.7140858208955224
[2m[36m(func pid=112174)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=112174)[0m f1_macro: 0.18663518128039133
[2m[36m(func pid=112174)[0m f1_weighted: 0.22805090491949903
[2m[36m(func pid=112174)[0m f1_per_class: [0.132, 0.266, 0.375, 0.317, 0.029, 0.335, 0.124, 0.202, 0.037, 0.049]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=102012)[0m top1: 0.2537313432835821
[2m[36m(func pid=102012)[0m top5: 0.761660447761194
[2m[36m(func pid=102012)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=102012)[0m f1_macro: 0.11261325012537907
[2m[36m(func pid=102012)[0m f1_weighted: 0.19246805800547556
[2m[36m(func pid=102012)[0m f1_per_class: [0.071, 0.0, 0.294, 0.0, 0.0, 0.118, 0.585, 0.0, 0.037, 0.022]
[2m[36m(func pid=102012)[0m 
== Status ==
Current time: 2024-01-07 15:14:14 (running for 00:40:40.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.303 |      0.113 |                   69 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.72  |      0.187 |                   25 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.913 |      0.084 |                    2 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  2.958 |      0.075 |                    1 |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.10261194029850747
[2m[36m(func pid=118433)[0m top5: 0.5736940298507462
[2m[36m(func pid=118433)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=118433)[0m f1_macro: 0.0747789705133473
[2m[36m(func pid=118433)[0m f1_weighted: 0.09635288988603319
[2m[36m(func pid=118433)[0m f1_per_class: [0.067, 0.322, 0.0, 0.028, 0.0, 0.0, 0.064, 0.213, 0.0, 0.055]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.16837686567164178
[2m[36m(func pid=117778)[0m top5: 0.5536380597014925
[2m[36m(func pid=117778)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=117778)[0m f1_macro: 0.12226087378103452
[2m[36m(func pid=117778)[0m f1_weighted: 0.18943773990987126
[2m[36m(func pid=117778)[0m f1_per_class: [0.098, 0.139, 0.152, 0.126, 0.051, 0.284, 0.307, 0.053, 0.0, 0.013]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6526 | Steps: 4 | Val loss: 2.7222 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6727 | Steps: 4 | Val loss: 2.1891 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6894 | Steps: 4 | Val loss: 2.2225 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7889 | Steps: 4 | Val loss: 2.2834 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=102012)[0m top1: 0.25419776119402987
[2m[36m(func pid=102012)[0m top5: 0.6585820895522388
[2m[36m(func pid=102012)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=102012)[0m f1_macro: 0.1323603270733423
[2m[36m(func pid=102012)[0m f1_weighted: 0.2032863463465346
[2m[36m(func pid=102012)[0m f1_per_class: [0.051, 0.0, 0.4, 0.0, 0.0, 0.182, 0.593, 0.0, 0.048, 0.05]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m top1: 0.22014925373134328
[2m[36m(func pid=112174)[0m top5: 0.7178171641791045
[2m[36m(func pid=112174)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=112174)[0m f1_macro: 0.18422849721317724
[2m[36m(func pid=112174)[0m f1_weighted: 0.23078133502304052
[2m[36m(func pid=112174)[0m f1_per_class: [0.131, 0.251, 0.34, 0.326, 0.026, 0.337, 0.13, 0.223, 0.026, 0.053]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:14:20 (running for 00:40:45.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.653 |      0.132 |                   70 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.673 |      0.184 |                   26 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.855 |      0.122 |                    3 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  2.689 |      0.136 |                    2 |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.1553171641791045
[2m[36m(func pid=118433)[0m top5: 0.6431902985074627
[2m[36m(func pid=118433)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=118433)[0m f1_macro: 0.1358096922086607
[2m[36m(func pid=118433)[0m f1_weighted: 0.129184011309123
[2m[36m(func pid=118433)[0m f1_per_class: [0.211, 0.265, 0.34, 0.016, 0.048, 0.0, 0.205, 0.168, 0.019, 0.087]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.19402985074626866
[2m[36m(func pid=117778)[0m top5: 0.6002798507462687
[2m[36m(func pid=117778)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=117778)[0m f1_macro: 0.14237902499321484
[2m[36m(func pid=117778)[0m f1_weighted: 0.212376095331457
[2m[36m(func pid=117778)[0m f1_per_class: [0.103, 0.124, 0.225, 0.131, 0.059, 0.316, 0.364, 0.101, 0.0, 0.0]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6141 | Steps: 4 | Val loss: 3.0398 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6894 | Steps: 4 | Val loss: 2.1796 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3974 | Steps: 4 | Val loss: 2.0439 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=102012)[0m top1: 0.14412313432835822
[2m[36m(func pid=102012)[0m top5: 0.5965485074626866
[2m[36m(func pid=102012)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=102012)[0m f1_macro: 0.09844812688094197
[2m[36m(func pid=102012)[0m f1_weighted: 0.08281585876145683
[2m[36m(func pid=102012)[0m f1_per_class: [0.146, 0.0, 0.385, 0.0, 0.0, 0.227, 0.167, 0.0, 0.047, 0.013]
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6857 | Steps: 4 | Val loss: 2.2301 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=112174)[0m top1: 0.2248134328358209
[2m[36m(func pid=112174)[0m top5: 0.7271455223880597
[2m[36m(func pid=112174)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=112174)[0m f1_macro: 0.18982447571021158
[2m[36m(func pid=112174)[0m f1_weighted: 0.2401327845463723
[2m[36m(func pid=112174)[0m f1_per_class: [0.129, 0.247, 0.4, 0.34, 0.028, 0.326, 0.157, 0.203, 0.04, 0.028]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:14:25 (running for 00:40:51.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.614 |      0.098 |                   71 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.689 |      0.19  |                   27 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.789 |      0.142 |                    4 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  2.397 |      0.17  |                    3 |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.22061567164179105
[2m[36m(func pid=118433)[0m top5: 0.8013059701492538
[2m[36m(func pid=118433)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=118433)[0m f1_macro: 0.1701469418162424
[2m[36m(func pid=118433)[0m f1_weighted: 0.21553991203703804
[2m[36m(func pid=118433)[0m f1_per_class: [0.273, 0.068, 0.053, 0.206, 0.081, 0.032, 0.374, 0.357, 0.04, 0.216]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.22807835820895522
[2m[36m(func pid=117778)[0m top5: 0.6539179104477612
[2m[36m(func pid=117778)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=117778)[0m f1_macro: 0.16777188049480582
[2m[36m(func pid=117778)[0m f1_weighted: 0.25331008752633394
[2m[36m(func pid=117778)[0m f1_per_class: [0.125, 0.116, 0.222, 0.236, 0.081, 0.341, 0.39, 0.112, 0.025, 0.028]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.0286 | Steps: 4 | Val loss: 3.0219 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6884 | Steps: 4 | Val loss: 2.1683 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.7605 | Steps: 4 | Val loss: 1.5945 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=102012)[0m top1: 0.06436567164179105
[2m[36m(func pid=102012)[0m top5: 0.5774253731343284
[2m[36m(func pid=102012)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=102012)[0m f1_macro: 0.1186361556071404
[2m[36m(func pid=102012)[0m f1_weighted: 0.052078379167614174
[2m[36m(func pid=102012)[0m f1_per_class: [0.078, 0.0, 0.4, 0.0, 0.0, 0.169, 0.0, 0.465, 0.048, 0.028]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5278 | Steps: 4 | Val loss: 2.1471 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=112174)[0m top1: 0.2392723880597015
[2m[36m(func pid=112174)[0m top5: 0.7336753731343284
[2m[36m(func pid=112174)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=112174)[0m f1_macro: 0.2089018383587003
[2m[36m(func pid=112174)[0m f1_weighted: 0.25025934181501414
[2m[36m(func pid=112174)[0m f1_per_class: [0.166, 0.252, 0.455, 0.367, 0.042, 0.342, 0.148, 0.228, 0.031, 0.058]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:14:31 (running for 00:40:57.07)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  3.029 |      0.119 |                   72 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.688 |      0.209 |                   28 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.686 |      0.168 |                    5 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.76  |      0.307 |                    4 |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.48600746268656714
[2m[36m(func pid=118433)[0m top5: 0.9365671641791045
[2m[36m(func pid=118433)[0m f1_micro: 0.48600746268656714
[2m[36m(func pid=118433)[0m f1_macro: 0.3066455977129783
[2m[36m(func pid=118433)[0m f1_weighted: 0.437811686936403
[2m[36m(func pid=118433)[0m f1_per_class: [0.429, 0.163, 0.4, 0.609, 0.129, 0.376, 0.599, 0.016, 0.051, 0.294]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.29384328358208955
[2m[36m(func pid=117778)[0m top5: 0.7075559701492538
[2m[36m(func pid=117778)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=117778)[0m f1_macro: 0.19410301304702893
[2m[36m(func pid=117778)[0m f1_weighted: 0.3128101666154217
[2m[36m(func pid=117778)[0m f1_per_class: [0.15, 0.092, 0.208, 0.366, 0.077, 0.352, 0.474, 0.108, 0.041, 0.072]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.9063 | Steps: 4 | Val loss: 2.9528 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6541 | Steps: 4 | Val loss: 2.1618 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.6368 | Steps: 4 | Val loss: 1.5965 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=102012)[0m top1: 0.07182835820895522
[2m[36m(func pid=102012)[0m top5: 0.5289179104477612
[2m[36m(func pid=102012)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=102012)[0m f1_macro: 0.1297676331787761
[2m[36m(func pid=102012)[0m f1_weighted: 0.05519861322126624
[2m[36m(func pid=102012)[0m f1_per_class: [0.077, 0.0, 0.4, 0.0, 0.02, 0.158, 0.0, 0.529, 0.048, 0.065]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5642 | Steps: 4 | Val loss: 2.0583 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=112174)[0m top1: 0.2490671641791045
[2m[36m(func pid=112174)[0m top5: 0.7355410447761194
[2m[36m(func pid=112174)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=112174)[0m f1_macro: 0.2058425233868495
[2m[36m(func pid=112174)[0m f1_weighted: 0.25901126923326745
[2m[36m(func pid=112174)[0m f1_per_class: [0.185, 0.271, 0.408, 0.405, 0.036, 0.318, 0.141, 0.216, 0.045, 0.032]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:14:36 (running for 00:41:02.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.906 |      0.13  |                   73 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.654 |      0.206 |                   29 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.528 |      0.194 |                    6 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.637 |      0.321 |                    5 |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.4594216417910448
[2m[36m(func pid=118433)[0m top5: 0.9039179104477612
[2m[36m(func pid=118433)[0m f1_micro: 0.4594216417910448
[2m[36m(func pid=118433)[0m f1_macro: 0.3207772058978186
[2m[36m(func pid=118433)[0m f1_weighted: 0.4473441998855467
[2m[36m(func pid=118433)[0m f1_per_class: [0.34, 0.264, 0.312, 0.595, 0.161, 0.362, 0.555, 0.156, 0.216, 0.247]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.36473880597014924
[2m[36m(func pid=117778)[0m top5: 0.769589552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=117778)[0m f1_macro: 0.22470941790458637
[2m[36m(func pid=117778)[0m f1_weighted: 0.3625651085158742
[2m[36m(func pid=117778)[0m f1_per_class: [0.189, 0.082, 0.202, 0.476, 0.092, 0.355, 0.534, 0.101, 0.091, 0.126]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6200 | Steps: 4 | Val loss: 3.2142 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6504 | Steps: 4 | Val loss: 2.1619 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=102012)[0m top1: 0.06296641791044776
[2m[36m(func pid=102012)[0m top5: 0.5032649253731343
[2m[36m(func pid=102012)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=102012)[0m f1_macro: 0.08508591726315776
[2m[36m(func pid=102012)[0m f1_weighted: 0.030209932310040533
[2m[36m(func pid=102012)[0m f1_per_class: [0.106, 0.0, 0.25, 0.0, 0.021, 0.008, 0.0, 0.406, 0.06, 0.0]
[2m[36m(func pid=102012)[0m 
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.1978 | Steps: 4 | Val loss: 1.7262 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=112174)[0m top1: 0.24720149253731344
[2m[36m(func pid=112174)[0m top5: 0.7369402985074627
[2m[36m(func pid=112174)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=112174)[0m f1_macro: 0.20155377688251278
[2m[36m(func pid=112174)[0m f1_weighted: 0.25811127825379926
[2m[36m(func pid=112174)[0m f1_per_class: [0.192, 0.264, 0.37, 0.404, 0.04, 0.321, 0.144, 0.219, 0.03, 0.032]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4797 | Steps: 4 | Val loss: 1.9956 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 15:14:42 (running for 00:41:08.18)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00019 | RUNNING    | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.62  |      0.085 |                   74 |
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.65  |      0.202 |                   30 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.564 |      0.225 |                    7 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.198 |      0.338 |                    6 |
| train_5ae7f_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.3833955223880597
[2m[36m(func pid=118433)[0m top5: 0.8815298507462687
[2m[36m(func pid=118433)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=118433)[0m f1_macro: 0.3377754132051795
[2m[36m(func pid=118433)[0m f1_weighted: 0.3878460365200046
[2m[36m(func pid=118433)[0m f1_per_class: [0.301, 0.32, 0.5, 0.58, 0.215, 0.392, 0.281, 0.408, 0.177, 0.205]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=102012)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6092 | Steps: 4 | Val loss: 2.9305 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=117778)[0m top1: 0.386660447761194
[2m[36m(func pid=117778)[0m top5: 0.7989738805970149
[2m[36m(func pid=117778)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=117778)[0m f1_macro: 0.24564296198403376
[2m[36m(func pid=117778)[0m f1_weighted: 0.37533675079749274
[2m[36m(func pid=117778)[0m f1_per_class: [0.223, 0.054, 0.222, 0.512, 0.08, 0.345, 0.557, 0.082, 0.098, 0.283]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6003 | Steps: 4 | Val loss: 2.1596 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.3336 | Steps: 4 | Val loss: 1.8102 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=102012)[0m top1: 0.0625
[2m[36m(func pid=102012)[0m top5: 0.5326492537313433
[2m[36m(func pid=102012)[0m f1_micro: 0.0625
[2m[36m(func pid=102012)[0m f1_macro: 0.07670350874211358
[2m[36m(func pid=102012)[0m f1_weighted: 0.0257050593828441
[2m[36m(func pid=102012)[0m f1_per_class: [0.071, 0.0, 0.273, 0.0, 0.022, 0.0, 0.0, 0.37, 0.031, 0.0]
[2m[36m(func pid=112174)[0m top1: 0.2555970149253731
[2m[36m(func pid=112174)[0m top5: 0.7504664179104478
[2m[36m(func pid=112174)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=112174)[0m f1_macro: 0.20632983032570343
[2m[36m(func pid=112174)[0m f1_weighted: 0.2698324638544257
[2m[36m(func pid=112174)[0m f1_per_class: [0.172, 0.266, 0.367, 0.41, 0.036, 0.343, 0.169, 0.214, 0.029, 0.057]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4200 | Steps: 4 | Val loss: 1.9782 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=118433)[0m top1: 0.28451492537313433
[2m[36m(func pid=118433)[0m top5: 0.8922574626865671
[2m[36m(func pid=118433)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=118433)[0m f1_macro: 0.3413408508429502
[2m[36m(func pid=118433)[0m f1_weighted: 0.2725485463805908
[2m[36m(func pid=118433)[0m f1_per_class: [0.429, 0.536, 0.774, 0.247, 0.272, 0.088, 0.197, 0.303, 0.136, 0.432]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.38619402985074625
[2m[36m(func pid=117778)[0m top5: 0.804570895522388
[2m[36m(func pid=117778)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=117778)[0m f1_macro: 0.2487832199157737
[2m[36m(func pid=117778)[0m f1_weighted: 0.3705644981961947
[2m[36m(func pid=117778)[0m f1_per_class: [0.235, 0.05, 0.279, 0.512, 0.08, 0.295, 0.566, 0.029, 0.13, 0.312]
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6473 | Steps: 4 | Val loss: 2.1601 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.3559 | Steps: 4 | Val loss: 2.1808 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=112174)[0m top1: 0.25513059701492535
[2m[36m(func pid=112174)[0m top5: 0.7444029850746269
[2m[36m(func pid=112174)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=112174)[0m f1_macro: 0.2060410369668558
[2m[36m(func pid=112174)[0m f1_weighted: 0.26787938735331757
[2m[36m(func pid=112174)[0m f1_per_class: [0.195, 0.277, 0.338, 0.407, 0.037, 0.347, 0.156, 0.222, 0.027, 0.056]
[2m[36m(func pid=118433)[0m top1: 0.23041044776119404
[2m[36m(func pid=118433)[0m top5: 0.8264925373134329
[2m[36m(func pid=118433)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=118433)[0m f1_macro: 0.21720877515340095
[2m[36m(func pid=118433)[0m f1_weighted: 0.26940047241348053
[2m[36m(func pid=118433)[0m f1_per_class: [0.44, 0.306, 0.056, 0.246, 0.25, 0.223, 0.341, 0.075, 0.112, 0.122]
== Status ==
Current time: 2024-01-07 15:14:48 (running for 00:41:13.71)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.6   |      0.206 |                   31 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.48  |      0.246 |                    8 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.334 |      0.341 |                    7 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m 
== Status ==
Current time: 2024-01-07 15:14:55 (running for 00:41:21.17)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.6   |      0.206 |                   31 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.42  |      0.249 |                    9 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.334 |      0.341 |                    7 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=120650)[0m Dataloader to compute accuracy: val

[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=120650)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=120650)[0m Configuration completed!
[2m[36m(func pid=120650)[0m New optimizer parameters:
[2m[36m(func pid=120650)[0m SGD (
[2m[36m(func pid=120650)[0m Parameter Group 0
[2m[36m(func pid=120650)[0m     dampening: 0
[2m[36m(func pid=120650)[0m     differentiable: False
[2m[36m(func pid=120650)[0m     foreach: None
[2m[36m(func pid=120650)[0m     lr: 0.1
[2m[36m(func pid=120650)[0m     maximize: False
[2m[36m(func pid=120650)[0m     momentum: 0.9
[2m[36m(func pid=120650)[0m     nesterov: False
[2m[36m(func pid=120650)[0m     weight_decay: 1e-05
[2m[36m(func pid=120650)[0m )
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6469 | Steps: 4 | Val loss: 2.1483 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3849 | Steps: 4 | Val loss: 1.9438 | Batch size: 32 | lr: 0.001 | Duration: 3.42s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.1589 | Steps: 4 | Val loss: 2.1317 | Batch size: 32 | lr: 0.01 | Duration: 3.35s
== Status ==
Current time: 2024-01-07 15:15:00 (running for 00:41:26.19)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.647 |      0.206 |                   32 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.42  |      0.249 |                    9 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.356 |      0.217 |                    8 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9178 | Steps: 4 | Val loss: 2.9216 | Batch size: 32 | lr: 0.1 | Duration: 5.32s
[2m[36m(func pid=117778)[0m top1: 0.40345149253731344
[2m[36m(func pid=117778)[0m top5: 0.8194962686567164
[2m[36m(func pid=117778)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=117778)[0m f1_macro: 0.28201846940161956
[2m[36m(func pid=117778)[0m f1_weighted: 0.39175234645859147
[2m[36m(func pid=117778)[0m f1_per_class: [0.309, 0.055, 0.329, 0.534, 0.08, 0.296, 0.575, 0.189, 0.144, 0.309]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m top1: 0.24813432835820895
[2m[36m(func pid=112174)[0m top5: 0.7560634328358209
[2m[36m(func pid=112174)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=112174)[0m f1_macro: 0.19838087199796764
[2m[36m(func pid=112174)[0m f1_weighted: 0.26510567141226454
[2m[36m(func pid=112174)[0m f1_per_class: [0.199, 0.256, 0.26, 0.369, 0.049, 0.361, 0.189, 0.209, 0.061, 0.031]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.3162313432835821
[2m[36m(func pid=118433)[0m top5: 0.8460820895522388
[2m[36m(func pid=118433)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=118433)[0m f1_macro: 0.2339954802518159
[2m[36m(func pid=118433)[0m f1_weighted: 0.28642197340905634
[2m[36m(func pid=118433)[0m f1_per_class: [0.255, 0.328, 0.187, 0.069, 0.222, 0.109, 0.551, 0.359, 0.1, 0.158]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.17210820895522388
[2m[36m(func pid=120650)[0m top5: 0.6879664179104478
[2m[36m(func pid=120650)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=120650)[0m f1_macro: 0.035280006889586546
[2m[36m(func pid=120650)[0m f1_weighted: 0.057119114302517586
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.291, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6188 | Steps: 4 | Val loss: 2.1496 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1661 | Steps: 4 | Val loss: 1.9481 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1462 | Steps: 4 | Val loss: 2.8577 | Batch size: 32 | lr: 0.01 | Duration: 3.33s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7446 | Steps: 4 | Val loss: 8.1750 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=112174)[0m top1: 0.24860074626865672
[2m[36m(func pid=112174)[0m top5: 0.7551305970149254
[2m[36m(func pid=112174)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=112174)[0m f1_macro: 0.20565664681855372
[2m[36m(func pid=112174)[0m f1_weighted: 0.2636399797336518
[2m[36m(func pid=112174)[0m f1_per_class: [0.203, 0.241, 0.314, 0.38, 0.043, 0.359, 0.18, 0.212, 0.063, 0.062]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:15:07 (running for 00:41:32.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.619 |      0.206 |                   34 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.385 |      0.282 |                   10 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.159 |      0.234 |                    9 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.918 |      0.035 |                    1 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.35401119402985076
[2m[36m(func pid=117778)[0m top5: 0.8152985074626866
[2m[36m(func pid=117778)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=117778)[0m f1_macro: 0.2825071212200481
[2m[36m(func pid=117778)[0m f1_weighted: 0.3579313668550473
[2m[36m(func pid=117778)[0m f1_per_class: [0.29, 0.063, 0.429, 0.506, 0.074, 0.217, 0.487, 0.315, 0.159, 0.286]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2080223880597015
[2m[36m(func pid=118433)[0m top5: 0.8022388059701493
[2m[36m(func pid=118433)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=118433)[0m f1_macro: 0.19412150101454836
[2m[36m(func pid=118433)[0m f1_weighted: 0.19931039225778627
[2m[36m(func pid=118433)[0m f1_per_class: [0.109, 0.335, 0.293, 0.108, 0.152, 0.008, 0.278, 0.279, 0.106, 0.275]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.02658582089552239
[2m[36m(func pid=120650)[0m top5: 0.5914179104477612
[2m[36m(func pid=120650)[0m f1_micro: 0.02658582089552239
[2m[36m(func pid=120650)[0m f1_macro: 0.013343501836429319
[2m[36m(func pid=120650)[0m f1_weighted: 0.028093337567084924
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.016, 0.0, 0.003, 0.0, 0.0, 0.079, 0.011, 0.0, 0.025]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1899 | Steps: 4 | Val loss: 1.9108 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6135 | Steps: 4 | Val loss: 2.1586 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.3039 | Steps: 4 | Val loss: 2.3488 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8394 | Steps: 4 | Val loss: 21.8625 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 15:15:12 (running for 00:41:38.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.619 |      0.206 |                   34 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.19  |      0.287 |                   12 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.146 |      0.194 |                   10 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.745 |      0.013 |                    2 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.3400186567164179
[2m[36m(func pid=117778)[0m top5: 0.824160447761194
[2m[36m(func pid=117778)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=117778)[0m f1_macro: 0.28730644079878653
[2m[36m(func pid=117778)[0m f1_weighted: 0.3217926631780686
[2m[36m(func pid=117778)[0m f1_per_class: [0.435, 0.077, 0.558, 0.564, 0.08, 0.201, 0.305, 0.322, 0.092, 0.239]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m top1: 0.24580223880597016
[2m[36m(func pid=112174)[0m top5: 0.7425373134328358
[2m[36m(func pid=112174)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=112174)[0m f1_macro: 0.2056930637212214
[2m[36m(func pid=112174)[0m f1_weighted: 0.2627929412616875
[2m[36m(func pid=112174)[0m f1_per_class: [0.196, 0.271, 0.324, 0.361, 0.046, 0.364, 0.177, 0.213, 0.056, 0.049]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.22294776119402984
[2m[36m(func pid=118433)[0m top5: 0.8619402985074627
[2m[36m(func pid=118433)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=118433)[0m f1_macro: 0.2799367948335512
[2m[36m(func pid=118433)[0m f1_weighted: 0.24408381403450613
[2m[36m(func pid=118433)[0m f1_per_class: [0.27, 0.093, 0.727, 0.237, 0.037, 0.301, 0.288, 0.391, 0.172, 0.284]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.0065298507462686565
[2m[36m(func pid=120650)[0m top5: 0.5825559701492538
[2m[36m(func pid=120650)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=120650)[0m f1_macro: 0.0017620298988918425
[2m[36m(func pid=120650)[0m f1_weighted: 0.0009971600148519285
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0620 | Steps: 4 | Val loss: 1.9100 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6511 | Steps: 4 | Val loss: 2.1519 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.9249 | Steps: 4 | Val loss: 2.7122 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.5082 | Steps: 4 | Val loss: 160.8390 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 15:15:18 (running for 00:41:43.91)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.613 |      0.206 |                   35 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  2.062 |      0.269 |                   13 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.304 |      0.28  |                   11 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.839 |      0.002 |                    3 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.32276119402985076
[2m[36m(func pid=117778)[0m top5: 0.8208955223880597
[2m[36m(func pid=117778)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=117778)[0m f1_macro: 0.2692946595309482
[2m[36m(func pid=117778)[0m f1_weighted: 0.2931202305205511
[2m[36m(func pid=117778)[0m f1_per_class: [0.4, 0.086, 0.462, 0.577, 0.078, 0.193, 0.194, 0.323, 0.123, 0.257]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m top1: 0.2630597014925373
[2m[36m(func pid=112174)[0m top5: 0.7509328358208955
[2m[36m(func pid=112174)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=112174)[0m f1_macro: 0.21623880227082198
[2m[36m(func pid=112174)[0m f1_weighted: 0.28023724204950134
[2m[36m(func pid=112174)[0m f1_per_class: [0.215, 0.302, 0.319, 0.384, 0.044, 0.36, 0.199, 0.201, 0.047, 0.093]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.16277985074626866
[2m[36m(func pid=118433)[0m top5: 0.8334888059701493
[2m[36m(func pid=118433)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=118433)[0m f1_macro: 0.2185089179300847
[2m[36m(func pid=118433)[0m f1_weighted: 0.1704204666415751
[2m[36m(func pid=118433)[0m f1_per_class: [0.151, 0.168, 0.412, 0.215, 0.043, 0.208, 0.06, 0.45, 0.112, 0.367]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.16930970149253732
[2m[36m(func pid=120650)[0m top5: 0.2653917910447761
[2m[36m(func pid=120650)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=120650)[0m f1_macro: 0.029274193548387097
[2m[36m(func pid=120650)[0m f1_weighted: 0.05038329020221473
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9345 | Steps: 4 | Val loss: 1.9184 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6101 | Steps: 4 | Val loss: 2.1543 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.0457 | Steps: 4 | Val loss: 3.9136 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7710 | Steps: 4 | Val loss: 111.6866 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 15:15:24 (running for 00:41:49.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.651 |      0.216 |                   36 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.934 |      0.279 |                   14 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.925 |      0.219 |                   12 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  3.508 |      0.029 |                    4 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.3204291044776119
[2m[36m(func pid=117778)[0m top5: 0.8208955223880597
[2m[36m(func pid=117778)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=117778)[0m f1_macro: 0.27859059701295646
[2m[36m(func pid=117778)[0m f1_weighted: 0.28083113998151826
[2m[36m(func pid=117778)[0m f1_per_class: [0.446, 0.124, 0.436, 0.571, 0.073, 0.192, 0.121, 0.334, 0.226, 0.262]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m top1: 0.2555970149253731
[2m[36m(func pid=112174)[0m top5: 0.746268656716418
[2m[36m(func pid=112174)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=112174)[0m f1_macro: 0.21519567814581753
[2m[36m(func pid=112174)[0m f1_weighted: 0.2710505814276582
[2m[36m(func pid=112174)[0m f1_per_class: [0.202, 0.31, 0.333, 0.361, 0.048, 0.365, 0.182, 0.213, 0.036, 0.102]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.11893656716417911
[2m[36m(func pid=118433)[0m top5: 0.7061567164179104
[2m[36m(func pid=118433)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=118433)[0m f1_macro: 0.1279128765948462
[2m[36m(func pid=118433)[0m f1_weighted: 0.10592869677563167
[2m[36m(func pid=118433)[0m f1_per_class: [0.182, 0.258, 0.176, 0.097, 0.097, 0.0, 0.075, 0.0, 0.089, 0.304]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.13526119402985073
[2m[36m(func pid=120650)[0m top5: 0.3302238805970149
[2m[36m(func pid=120650)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=120650)[0m f1_macro: 0.032630956049380024
[2m[36m(func pid=120650)[0m f1_weighted: 0.049057034187569085
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.284, 0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6189 | Steps: 4 | Val loss: 2.1414 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8963 | Steps: 4 | Val loss: 1.8917 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8598 | Steps: 4 | Val loss: 4.0165 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6888 | Steps: 4 | Val loss: 159.2017 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=112174)[0m top1: 0.2667910447761194
[2m[36m(func pid=112174)[0m top5: 0.7555970149253731
[2m[36m(func pid=112174)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=112174)[0m f1_macro: 0.2237607934400064
[2m[36m(func pid=112174)[0m f1_weighted: 0.2813088460710867
[2m[36m(func pid=112174)[0m f1_per_class: [0.213, 0.3, 0.355, 0.376, 0.043, 0.387, 0.197, 0.209, 0.048, 0.109]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:15:29 (running for 00:41:55.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.619 |      0.224 |                   38 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.934 |      0.279 |                   14 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.046 |      0.128 |                   13 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.771 |      0.033 |                    5 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.3199626865671642
[2m[36m(func pid=117778)[0m top5: 0.8302238805970149
[2m[36m(func pid=117778)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=117778)[0m f1_macro: 0.29379078297923894
[2m[36m(func pid=117778)[0m f1_weighted: 0.2820195702125363
[2m[36m(func pid=117778)[0m f1_per_class: [0.525, 0.197, 0.511, 0.566, 0.079, 0.142, 0.101, 0.315, 0.239, 0.262]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.14272388059701493
[2m[36m(func pid=118433)[0m top5: 0.6655783582089553
[2m[36m(func pid=118433)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=118433)[0m f1_macro: 0.21637443008010634
[2m[36m(func pid=118433)[0m f1_weighted: 0.14272425422146753
[2m[36m(func pid=118433)[0m f1_per_class: [0.361, 0.295, 0.706, 0.226, 0.092, 0.037, 0.003, 0.098, 0.082, 0.264]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.018656716417910446
[2m[36m(func pid=120650)[0m top5: 0.5643656716417911
[2m[36m(func pid=120650)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=120650)[0m f1_macro: 0.008798716781549401
[2m[36m(func pid=120650)[0m f1_weighted: 0.020398818755746565
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.0, 0.0, 0.073, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5718 | Steps: 4 | Val loss: 2.1529 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.8224 | Steps: 4 | Val loss: 1.8447 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7376 | Steps: 4 | Val loss: 2.7957 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9386 | Steps: 4 | Val loss: 148.3086 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=112174)[0m top1: 0.25466417910447764
[2m[36m(func pid=112174)[0m top5: 0.7341417910447762
[2m[36m(func pid=112174)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=112174)[0m f1_macro: 0.21592813648694845
[2m[36m(func pid=112174)[0m f1_weighted: 0.2668431822187477
[2m[36m(func pid=112174)[0m f1_per_class: [0.218, 0.344, 0.306, 0.328, 0.042, 0.383, 0.167, 0.228, 0.055, 0.089]
== Status ==
Current time: 2024-01-07 15:15:35 (running for 00:42:00.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.572 |      0.216 |                   39 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.896 |      0.294 |                   15 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.86  |      0.216 |                   14 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.689 |      0.009 |                    6 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.3362873134328358
[2m[36m(func pid=117778)[0m top5: 0.8628731343283582
[2m[36m(func pid=117778)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=117778)[0m f1_macro: 0.30460719245709084
[2m[36m(func pid=117778)[0m f1_weighted: 0.32740626599648087
[2m[36m(func pid=117778)[0m f1_per_class: [0.509, 0.251, 0.49, 0.571, 0.081, 0.146, 0.219, 0.323, 0.226, 0.23]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.25699626865671643
[2m[36m(func pid=118433)[0m top5: 0.8013059701492538
[2m[36m(func pid=118433)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=118433)[0m f1_macro: 0.2768378601582235
[2m[36m(func pid=118433)[0m f1_weighted: 0.24444331942683467
[2m[36m(func pid=118433)[0m f1_per_class: [0.443, 0.365, 0.364, 0.396, 0.081, 0.214, 0.022, 0.332, 0.131, 0.421]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.017257462686567165
[2m[36m(func pid=120650)[0m top5: 0.3400186567164179
[2m[36m(func pid=120650)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=120650)[0m f1_macro: 0.009710623936368587
[2m[36m(func pid=120650)[0m f1_weighted: 0.018618348858223926
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.042, 0.015, 0.041, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5057 | Steps: 4 | Val loss: 2.1387 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9354 | Steps: 4 | Val loss: 1.8282 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.9119 | Steps: 4 | Val loss: 2.4693 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3227 | Steps: 4 | Val loss: 102.3528 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 15:15:40 (running for 00:42:06.28)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.506 |      0.223 |                   40 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.822 |      0.305 |                   16 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.738 |      0.277 |                   15 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.939 |      0.01  |                    7 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.25886194029850745
[2m[36m(func pid=112174)[0m top5: 0.7551305970149254
[2m[36m(func pid=112174)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=112174)[0m f1_macro: 0.22318855342050567
[2m[36m(func pid=112174)[0m f1_weighted: 0.27900430266635395
[2m[36m(func pid=112174)[0m f1_per_class: [0.211, 0.318, 0.324, 0.326, 0.047, 0.37, 0.231, 0.213, 0.053, 0.14]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.35447761194029853
[2m[36m(func pid=117778)[0m top5: 0.8675373134328358
[2m[36m(func pid=117778)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=117778)[0m f1_macro: 0.30266967455925087
[2m[36m(func pid=117778)[0m f1_weighted: 0.35893306961539134
[2m[36m(func pid=117778)[0m f1_per_class: [0.52, 0.239, 0.407, 0.583, 0.079, 0.141, 0.327, 0.327, 0.208, 0.196]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.30830223880597013
[2m[36m(func pid=118433)[0m top5: 0.8479477611940298
[2m[36m(func pid=118433)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=118433)[0m f1_macro: 0.31747964471979184
[2m[36m(func pid=118433)[0m f1_weighted: 0.33933476797690476
[2m[36m(func pid=118433)[0m f1_per_class: [0.505, 0.411, 0.168, 0.334, 0.179, 0.236, 0.348, 0.384, 0.133, 0.478]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.20009328358208955
[2m[36m(func pid=120650)[0m top5: 0.6464552238805971
[2m[36m(func pid=120650)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=120650)[0m f1_macro: 0.07316424898436127
[2m[36m(func pid=120650)[0m f1_weighted: 0.08933492016654246
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.386, 0.0, 0.007, 0.0, 0.053, 0.0, 0.223, 0.063, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5214 | Steps: 4 | Val loss: 2.1413 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8043 | Steps: 4 | Val loss: 1.8080 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6497 | Steps: 4 | Val loss: 3.0029 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9409 | Steps: 4 | Val loss: 29.2722 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 15:15:46 (running for 00:42:11.89)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.521 |      0.224 |                   41 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.935 |      0.303 |                   17 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.912 |      0.317 |                   16 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.323 |      0.073 |                    8 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.261660447761194
[2m[36m(func pid=112174)[0m top5: 0.75
[2m[36m(func pid=112174)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=112174)[0m f1_macro: 0.22360425077857315
[2m[36m(func pid=112174)[0m f1_weighted: 0.28375117188460663
[2m[36m(func pid=112174)[0m f1_per_class: [0.196, 0.301, 0.282, 0.343, 0.044, 0.393, 0.227, 0.241, 0.066, 0.144]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.3591417910447761
[2m[36m(func pid=117778)[0m top5: 0.8759328358208955
[2m[36m(func pid=117778)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=117778)[0m f1_macro: 0.304531163433014
[2m[36m(func pid=117778)[0m f1_weighted: 0.3694970508767799
[2m[36m(func pid=117778)[0m f1_per_class: [0.448, 0.231, 0.436, 0.558, 0.085, 0.17, 0.378, 0.352, 0.222, 0.165]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.20988805970149255
[2m[36m(func pid=118433)[0m top5: 0.7901119402985075
[2m[36m(func pid=118433)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=118433)[0m f1_macro: 0.16244309394776654
[2m[36m(func pid=118433)[0m f1_weighted: 0.22560061222744357
[2m[36m(func pid=118433)[0m f1_per_class: [0.219, 0.381, 0.0, 0.081, 0.068, 0.11, 0.351, 0.16, 0.141, 0.114]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.09514925373134328
[2m[36m(func pid=120650)[0m top5: 0.5914179104477612
[2m[36m(func pid=120650)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=120650)[0m f1_macro: 0.06366424872522322
[2m[36m(func pid=120650)[0m f1_weighted: 0.046851338622043874
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.147, 0.0, 0.013, 0.169, 0.016, 0.0, 0.208, 0.083, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5152 | Steps: 4 | Val loss: 2.1306 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5672 | Steps: 4 | Val loss: 1.8040 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8672 | Steps: 4 | Val loss: 3.6972 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4993 | Steps: 4 | Val loss: 11.0716 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=112174)[0m top1: 0.2728544776119403
[2m[36m(func pid=112174)[0m top5: 0.7644589552238806
[2m[36m(func pid=112174)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=112174)[0m f1_macro: 0.23220999988186808
[2m[36m(func pid=112174)[0m f1_weighted: 0.2982806590074283
[2m[36m(func pid=112174)[0m f1_per_class: [0.211, 0.281, 0.293, 0.376, 0.048, 0.396, 0.253, 0.244, 0.06, 0.159]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:15:51 (running for 00:42:17.39)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.515 |      0.232 |                   42 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.804 |      0.305 |                   18 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.65  |      0.162 |                   17 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.941 |      0.064 |                    9 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.37779850746268656
[2m[36m(func pid=117778)[0m top5: 0.8689365671641791
[2m[36m(func pid=117778)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=117778)[0m f1_macro: 0.3369452560031848
[2m[36m(func pid=117778)[0m f1_weighted: 0.4065259589607804
[2m[36m(func pid=117778)[0m f1_per_class: [0.415, 0.358, 0.462, 0.522, 0.08, 0.331, 0.391, 0.393, 0.248, 0.169]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.1417910447761194
[2m[36m(func pid=118433)[0m top5: 0.6040111940298507
[2m[36m(func pid=118433)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=118433)[0m f1_macro: 0.18835427837881777
[2m[36m(func pid=118433)[0m f1_weighted: 0.13423036908969033
[2m[36m(func pid=118433)[0m f1_per_class: [0.136, 0.259, 0.5, 0.0, 0.042, 0.074, 0.153, 0.325, 0.286, 0.109]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.10121268656716417
[2m[36m(func pid=120650)[0m top5: 0.5816231343283582
[2m[36m(func pid=120650)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=120650)[0m f1_macro: 0.07297108589605851
[2m[36m(func pid=120650)[0m f1_weighted: 0.06151557452989962
[2m[36m(func pid=120650)[0m f1_per_class: [0.015, 0.191, 0.0, 0.003, 0.137, 0.106, 0.0, 0.203, 0.074, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5214 | Steps: 4 | Val loss: 2.1301 | Batch size: 32 | lr: 0.0001 | Duration: 3.25s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8775 | Steps: 4 | Val loss: 5.1633 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7308 | Steps: 4 | Val loss: 1.7902 | Batch size: 32 | lr: 0.001 | Duration: 3.33s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8116 | Steps: 4 | Val loss: 5.6578 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 15:15:57 (running for 00:42:23.15)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.521 |      0.235 |                   43 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.567 |      0.337 |                   19 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.867 |      0.188 |                   18 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.499 |      0.073 |                   10 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.27238805970149255
[2m[36m(func pid=112174)[0m top5: 0.7593283582089553
[2m[36m(func pid=112174)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=112174)[0m f1_macro: 0.2350061300744238
[2m[36m(func pid=112174)[0m f1_weighted: 0.30177331066924706
[2m[36m(func pid=112174)[0m f1_per_class: [0.231, 0.25, 0.328, 0.397, 0.042, 0.358, 0.274, 0.254, 0.055, 0.161]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.09001865671641791
[2m[36m(func pid=118433)[0m top5: 0.3810634328358209
[2m[36m(func pid=118433)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=118433)[0m f1_macro: 0.09345866012520083
[2m[36m(func pid=118433)[0m f1_weighted: 0.07832989242588735
[2m[36m(func pid=118433)[0m f1_per_class: [0.082, 0.404, 0.224, 0.0, 0.059, 0.0, 0.003, 0.009, 0.093, 0.059]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.37173507462686567
[2m[36m(func pid=117778)[0m top5: 0.8722014925373134
[2m[36m(func pid=117778)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=117778)[0m f1_macro: 0.33469338814647476
[2m[36m(func pid=117778)[0m f1_weighted: 0.40200592106362687
[2m[36m(func pid=117778)[0m f1_per_class: [0.335, 0.389, 0.471, 0.451, 0.089, 0.394, 0.405, 0.397, 0.246, 0.17]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.2126865671641791
[2m[36m(func pid=120650)[0m top5: 0.617070895522388
[2m[36m(func pid=120650)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=120650)[0m f1_macro: 0.09845407644506979
[2m[36m(func pid=120650)[0m f1_weighted: 0.1988253308275672
[2m[36m(func pid=120650)[0m f1_per_class: [0.032, 0.0, 0.0, 0.0, 0.126, 0.159, 0.59, 0.03, 0.048, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5295 | Steps: 4 | Val loss: 2.1302 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9472 | Steps: 4 | Val loss: 3.0972 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4605 | Steps: 4 | Val loss: 1.7571 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6431 | Steps: 4 | Val loss: 2.4629 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 15:16:03 (running for 00:42:28.73)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.529 |      0.228 |                   44 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.731 |      0.335 |                   20 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.878 |      0.093 |                   19 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.812 |      0.098 |                   11 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.27098880597014924
[2m[36m(func pid=112174)[0m top5: 0.757929104477612
[2m[36m(func pid=112174)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=112174)[0m f1_macro: 0.22846122610358183
[2m[36m(func pid=112174)[0m f1_weighted: 0.30142599939628156
[2m[36m(func pid=112174)[0m f1_per_class: [0.21, 0.255, 0.278, 0.389, 0.043, 0.37, 0.277, 0.248, 0.053, 0.162]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.18889925373134328
[2m[36m(func pid=118433)[0m top5: 0.7322761194029851
[2m[36m(func pid=118433)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=118433)[0m f1_macro: 0.1557587067603906
[2m[36m(func pid=118433)[0m f1_weighted: 0.21852472806891504
[2m[36m(func pid=118433)[0m f1_per_class: [0.144, 0.273, 0.066, 0.141, 0.133, 0.0, 0.374, 0.172, 0.166, 0.088]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4141791044776119
[2m[36m(func pid=117778)[0m top5: 0.8801305970149254
[2m[36m(func pid=117778)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=117778)[0m f1_macro: 0.36398698677866054
[2m[36m(func pid=117778)[0m f1_weighted: 0.4487313376938926
[2m[36m(func pid=117778)[0m f1_per_class: [0.333, 0.425, 0.571, 0.488, 0.108, 0.377, 0.507, 0.415, 0.241, 0.173]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.27845149253731344
[2m[36m(func pid=120650)[0m top5: 0.7439365671641791
[2m[36m(func pid=120650)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=120650)[0m f1_macro: 0.10999005991518832
[2m[36m(func pid=120650)[0m f1_weighted: 0.25351403248146864
[2m[36m(func pid=120650)[0m f1_per_class: [0.04, 0.0, 0.0, 0.222, 0.108, 0.108, 0.59, 0.032, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.5048 | Steps: 4 | Val loss: 2.1300 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6587 | Steps: 4 | Val loss: 4.5263 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5990 | Steps: 4 | Val loss: 1.7506 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.0394 | Steps: 4 | Val loss: 2.2725 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:16:08 (running for 00:42:34.34)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.505 |      0.232 |                   45 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.461 |      0.364 |                   21 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.947 |      0.156 |                   20 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.643 |      0.11  |                   12 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.2677238805970149
[2m[36m(func pid=112174)[0m top5: 0.7593283582089553
[2m[36m(func pid=112174)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=112174)[0m f1_macro: 0.2317706905815505
[2m[36m(func pid=112174)[0m f1_weighted: 0.2970144773965677
[2m[36m(func pid=112174)[0m f1_per_class: [0.224, 0.241, 0.262, 0.392, 0.045, 0.368, 0.262, 0.261, 0.063, 0.2]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.08488805970149253
[2m[36m(func pid=118433)[0m top5: 0.7691231343283582
[2m[36m(func pid=118433)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=118433)[0m f1_macro: 0.07174396703517563
[2m[36m(func pid=118433)[0m f1_weighted: 0.05150679994283805
[2m[36m(func pid=118433)[0m f1_per_class: [0.098, 0.06, 0.0, 0.032, 0.182, 0.0, 0.045, 0.245, 0.019, 0.035]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4048507462686567
[2m[36m(func pid=117778)[0m top5: 0.8740671641791045
[2m[36m(func pid=117778)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=117778)[0m f1_macro: 0.3530071532532792
[2m[36m(func pid=117778)[0m f1_weighted: 0.4376226589330689
[2m[36m(func pid=117778)[0m f1_per_class: [0.286, 0.414, 0.533, 0.48, 0.13, 0.354, 0.497, 0.397, 0.258, 0.179]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.3204291044776119
[2m[36m(func pid=120650)[0m top5: 0.7798507462686567
[2m[36m(func pid=120650)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=120650)[0m f1_macro: 0.14112752168573245
[2m[36m(func pid=120650)[0m f1_weighted: 0.31622412126041627
[2m[36m(func pid=120650)[0m f1_per_class: [0.034, 0.0, 0.0, 0.344, 0.073, 0.303, 0.609, 0.048, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4299 | Steps: 4 | Val loss: 2.1229 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.4812 | Steps: 4 | Val loss: 1.7343 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.9589 | Steps: 4 | Val loss: 4.4016 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.1251 | Steps: 4 | Val loss: 2.4434 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=112174)[0m top1: 0.2691231343283582
[2m[36m(func pid=112174)[0m top5: 0.7639925373134329
[2m[36m(func pid=112174)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=112174)[0m f1_macro: 0.24378371374693267
[2m[36m(func pid=112174)[0m f1_weighted: 0.29344541505802635
[2m[36m(func pid=112174)[0m f1_per_class: [0.244, 0.273, 0.338, 0.382, 0.04, 0.384, 0.228, 0.283, 0.056, 0.21]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:16:14 (running for 00:42:39.88)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.43  |      0.244 |                   46 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.599 |      0.353 |                   22 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.659 |      0.072 |                   21 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  3.039 |      0.141 |                   13 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.40531716417910446
[2m[36m(func pid=117778)[0m top5: 0.875
[2m[36m(func pid=117778)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=117778)[0m f1_macro: 0.3625727589772271
[2m[36m(func pid=117778)[0m f1_weighted: 0.4378518300023225
[2m[36m(func pid=117778)[0m f1_per_class: [0.265, 0.418, 0.571, 0.481, 0.126, 0.398, 0.471, 0.43, 0.27, 0.195]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.10494402985074627
[2m[36m(func pid=118433)[0m top5: 0.7098880597014925
[2m[36m(func pid=118433)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=118433)[0m f1_macro: 0.09733209813268215
[2m[36m(func pid=118433)[0m f1_weighted: 0.07323938173948734
[2m[36m(func pid=118433)[0m f1_per_class: [0.168, 0.179, 0.118, 0.041, 0.108, 0.0, 0.035, 0.235, 0.044, 0.045]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.21641791044776118
[2m[36m(func pid=120650)[0m top5: 0.7709888059701493
[2m[36m(func pid=120650)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=120650)[0m f1_macro: 0.10043806412784054
[2m[36m(func pid=120650)[0m f1_weighted: 0.1173114757889277
[2m[36m(func pid=120650)[0m f1_per_class: [0.093, 0.454, 0.0, 0.0, 0.078, 0.287, 0.0, 0.031, 0.061, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4393 | Steps: 4 | Val loss: 2.1211 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7601 | Steps: 4 | Val loss: 4.2178 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4461 | Steps: 4 | Val loss: 1.7233 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.9241 | Steps: 4 | Val loss: 2.4201 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:16:19 (running for 00:42:45.37)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.439 |      0.239 |                   47 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.481 |      0.363 |                   23 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.959 |      0.097 |                   22 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  3.125 |      0.1   |                   14 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.271455223880597
[2m[36m(func pid=112174)[0m top5: 0.7653917910447762
[2m[36m(func pid=112174)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=112174)[0m f1_macro: 0.23857484392005984
[2m[36m(func pid=112174)[0m f1_weighted: 0.2944136757851334
[2m[36m(func pid=112174)[0m f1_per_class: [0.228, 0.287, 0.275, 0.383, 0.047, 0.384, 0.222, 0.285, 0.07, 0.204]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.17583955223880596
[2m[36m(func pid=118433)[0m top5: 0.6371268656716418
[2m[36m(func pid=118433)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=118433)[0m f1_macro: 0.1445958652133393
[2m[36m(func pid=118433)[0m f1_weighted: 0.13761980363939036
[2m[36m(func pid=118433)[0m f1_per_class: [0.169, 0.372, 0.145, 0.069, 0.073, 0.024, 0.085, 0.257, 0.156, 0.095]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.394589552238806
[2m[36m(func pid=117778)[0m top5: 0.8768656716417911
[2m[36m(func pid=117778)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=117778)[0m f1_macro: 0.367038731795495
[2m[36m(func pid=117778)[0m f1_weighted: 0.4227460147136532
[2m[36m(func pid=117778)[0m f1_per_class: [0.239, 0.447, 0.6, 0.427, 0.122, 0.422, 0.439, 0.444, 0.287, 0.241]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.19916044776119404
[2m[36m(func pid=120650)[0m top5: 0.675839552238806
[2m[36m(func pid=120650)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=120650)[0m f1_macro: 0.14935835678947634
[2m[36m(func pid=120650)[0m f1_weighted: 0.1353524605817608
[2m[36m(func pid=120650)[0m f1_per_class: [0.117, 0.422, 0.107, 0.0, 0.039, 0.244, 0.0, 0.52, 0.045, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4367 | Steps: 4 | Val loss: 2.1133 | Batch size: 32 | lr: 0.0001 | Duration: 3.31s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.5294 | Steps: 4 | Val loss: 1.7064 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5544 | Steps: 4 | Val loss: 3.7459 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4928 | Steps: 4 | Val loss: 2.1386 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 15:16:25 (running for 00:42:51.20)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.437 |      0.249 |                   48 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.446 |      0.367 |                   24 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.76  |      0.145 |                   23 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.924 |      0.149 |                   15 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.27798507462686567
[2m[36m(func pid=112174)[0m top5: 0.7709888059701493
[2m[36m(func pid=112174)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=112174)[0m f1_macro: 0.248886718317965
[2m[36m(func pid=112174)[0m f1_weighted: 0.2959888417995894
[2m[36m(func pid=112174)[0m f1_per_class: [0.242, 0.28, 0.361, 0.413, 0.041, 0.396, 0.197, 0.29, 0.065, 0.206]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.38619402985074625
[2m[36m(func pid=117778)[0m top5: 0.8773320895522388
[2m[36m(func pid=117778)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=117778)[0m f1_macro: 0.3765290047663369
[2m[36m(func pid=117778)[0m f1_weighted: 0.3993232883913878
[2m[36m(func pid=117778)[0m f1_per_class: [0.266, 0.479, 0.686, 0.346, 0.094, 0.436, 0.408, 0.444, 0.282, 0.326]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.15858208955223882
[2m[36m(func pid=118433)[0m top5: 0.6861007462686567
[2m[36m(func pid=118433)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=118433)[0m f1_macro: 0.14893616268122545
[2m[36m(func pid=118433)[0m f1_weighted: 0.14008382645136827
[2m[36m(func pid=118433)[0m f1_per_class: [0.352, 0.203, 0.078, 0.007, 0.098, 0.024, 0.245, 0.221, 0.154, 0.108]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.2080223880597015
[2m[36m(func pid=120650)[0m top5: 0.7602611940298507
[2m[36m(func pid=120650)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=120650)[0m f1_macro: 0.16289378161131435
[2m[36m(func pid=120650)[0m f1_weighted: 0.18152047146097222
[2m[36m(func pid=120650)[0m f1_per_class: [0.122, 0.338, 0.232, 0.249, 0.055, 0.254, 0.0, 0.351, 0.0, 0.027]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4969 | Steps: 4 | Val loss: 2.0963 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8375 | Steps: 4 | Val loss: 2.6970 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3737 | Steps: 4 | Val loss: 1.7184 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4648 | Steps: 4 | Val loss: 1.9205 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:16:31 (running for 00:42:56.66)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.497 |      0.257 |                   49 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.529 |      0.377 |                   25 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.554 |      0.149 |                   24 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.493 |      0.163 |                   16 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.302705223880597
[2m[36m(func pid=112174)[0m top5: 0.7807835820895522
[2m[36m(func pid=112174)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=112174)[0m f1_macro: 0.25708672272393174
[2m[36m(func pid=112174)[0m f1_weighted: 0.323074617579912
[2m[36m(func pid=112174)[0m f1_per_class: [0.264, 0.294, 0.306, 0.458, 0.045, 0.383, 0.243, 0.273, 0.069, 0.235]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2896455223880597
[2m[36m(func pid=118433)[0m top5: 0.8386194029850746
[2m[36m(func pid=118433)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=118433)[0m f1_macro: 0.22143835819731753
[2m[36m(func pid=118433)[0m f1_weighted: 0.30815031749245336
[2m[36m(func pid=118433)[0m f1_per_class: [0.27, 0.214, 0.17, 0.474, 0.095, 0.0, 0.367, 0.221, 0.237, 0.167]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.37453358208955223
[2m[36m(func pid=117778)[0m top5: 0.8726679104477612
[2m[36m(func pid=117778)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=117778)[0m f1_macro: 0.3752890459823637
[2m[36m(func pid=117778)[0m f1_weighted: 0.38355526964547565
[2m[36m(func pid=117778)[0m f1_per_class: [0.275, 0.47, 0.632, 0.306, 0.087, 0.421, 0.39, 0.473, 0.35, 0.35]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.31763059701492535
[2m[36m(func pid=120650)[0m top5: 0.7952425373134329
[2m[36m(func pid=120650)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=120650)[0m f1_macro: 0.1996747178164085
[2m[36m(func pid=120650)[0m f1_weighted: 0.2846100820882796
[2m[36m(func pid=120650)[0m f1_per_class: [0.054, 0.0, 0.56, 0.508, 0.051, 0.0, 0.383, 0.392, 0.028, 0.021]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4987 | Steps: 4 | Val loss: 2.0957 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7429 | Steps: 4 | Val loss: 2.6880 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.4271 | Steps: 4 | Val loss: 1.7103 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4587 | Steps: 4 | Val loss: 1.8984 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:16:36 (running for 00:43:02.45)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.499 |      0.236 |                   50 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.374 |      0.375 |                   26 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.838 |      0.221 |                   25 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.465 |      0.2   |                   17 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.28031716417910446
[2m[36m(func pid=112174)[0m top5: 0.7789179104477612
[2m[36m(func pid=112174)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=112174)[0m f1_macro: 0.23625201214001268
[2m[36m(func pid=112174)[0m f1_weighted: 0.31004270352155117
[2m[36m(func pid=112174)[0m f1_per_class: [0.229, 0.251, 0.22, 0.424, 0.046, 0.357, 0.272, 0.26, 0.087, 0.217]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.35401119402985076
[2m[36m(func pid=118433)[0m top5: 0.8390858208955224
[2m[36m(func pid=118433)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=118433)[0m f1_macro: 0.25417974947398747
[2m[36m(func pid=118433)[0m f1_weighted: 0.3745063681531691
[2m[36m(func pid=118433)[0m f1_per_class: [0.292, 0.205, 0.537, 0.559, 0.044, 0.0, 0.542, 0.077, 0.213, 0.074]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.38386194029850745
[2m[36m(func pid=117778)[0m top5: 0.8838619402985075
[2m[36m(func pid=117778)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=117778)[0m f1_macro: 0.3801448045703512
[2m[36m(func pid=117778)[0m f1_weighted: 0.3887366800388395
[2m[36m(func pid=117778)[0m f1_per_class: [0.386, 0.484, 0.686, 0.298, 0.079, 0.379, 0.43, 0.403, 0.326, 0.331]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.3763992537313433
[2m[36m(func pid=120650)[0m top5: 0.7709888059701493
[2m[36m(func pid=120650)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=120650)[0m f1_macro: 0.16852464284130902
[2m[36m(func pid=120650)[0m f1_weighted: 0.3531269590553547
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.0, 0.0, 0.503, 0.049, 0.0, 0.621, 0.436, 0.049, 0.026]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4297 | Steps: 4 | Val loss: 2.0931 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.1829 | Steps: 4 | Val loss: 3.9135 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.3328 | Steps: 4 | Val loss: 1.6783 | Batch size: 32 | lr: 0.001 | Duration: 3.33s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3170 | Steps: 4 | Val loss: 2.0390 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:16:42 (running for 00:43:07.99)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.43  |      0.251 |                   51 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.427 |      0.38  |                   27 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.743 |      0.254 |                   26 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.459 |      0.169 |                   18 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.2905783582089552
[2m[36m(func pid=112174)[0m top5: 0.7803171641791045
[2m[36m(func pid=112174)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=112174)[0m f1_macro: 0.25076675683628424
[2m[36m(func pid=112174)[0m f1_weighted: 0.3120258601717182
[2m[36m(func pid=112174)[0m f1_per_class: [0.251, 0.245, 0.297, 0.435, 0.048, 0.387, 0.253, 0.28, 0.08, 0.231]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.1571828358208955
[2m[36m(func pid=118433)[0m top5: 0.7262126865671642
[2m[36m(func pid=118433)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=118433)[0m f1_macro: 0.15885059158407994
[2m[36m(func pid=118433)[0m f1_weighted: 0.17701480365636962
[2m[36m(func pid=118433)[0m f1_per_class: [0.136, 0.318, 0.076, 0.106, 0.05, 0.083, 0.235, 0.0, 0.127, 0.457]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4025186567164179
[2m[36m(func pid=117778)[0m top5: 0.8852611940298507
[2m[36m(func pid=117778)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=117778)[0m f1_macro: 0.37213908216739283
[2m[36m(func pid=117778)[0m f1_weighted: 0.40887657854006776
[2m[36m(func pid=117778)[0m f1_per_class: [0.395, 0.499, 0.522, 0.328, 0.083, 0.36, 0.467, 0.44, 0.309, 0.319]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.22901119402985073
[2m[36m(func pid=120650)[0m top5: 0.7178171641791045
[2m[36m(func pid=120650)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=120650)[0m f1_macro: 0.10871649877322656
[2m[36m(func pid=120650)[0m f1_weighted: 0.24132155968707722
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.0, 0.0, 0.497, 0.033, 0.122, 0.271, 0.09, 0.075, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4871 | Steps: 4 | Val loss: 2.0878 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6031 | Steps: 4 | Val loss: 3.0825 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3472 | Steps: 4 | Val loss: 1.6661 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.5097 | Steps: 4 | Val loss: 2.0255 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:16:47 (running for 00:43:13.47)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.487 |      0.256 |                   52 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.333 |      0.372 |                   28 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.183 |      0.159 |                   27 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.317 |      0.109 |                   19 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.2989738805970149
[2m[36m(func pid=112174)[0m top5: 0.7840485074626866
[2m[36m(func pid=112174)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=112174)[0m f1_macro: 0.2561685023230591
[2m[36m(func pid=112174)[0m f1_weighted: 0.31778256500221497
[2m[36m(func pid=112174)[0m f1_per_class: [0.228, 0.289, 0.297, 0.429, 0.058, 0.402, 0.248, 0.275, 0.089, 0.248]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.17583955223880596
[2m[36m(func pid=118433)[0m top5: 0.7915111940298507
[2m[36m(func pid=118433)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=118433)[0m f1_macro: 0.19766225414708624
[2m[36m(func pid=118433)[0m f1_weighted: 0.1955004454835904
[2m[36m(func pid=118433)[0m f1_per_class: [0.243, 0.31, 0.169, 0.111, 0.049, 0.197, 0.214, 0.176, 0.107, 0.4]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.416044776119403
[2m[36m(func pid=117778)[0m top5: 0.8903917910447762
[2m[36m(func pid=117778)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=117778)[0m f1_macro: 0.369165438925528
[2m[36m(func pid=117778)[0m f1_weighted: 0.43210378528703425
[2m[36m(func pid=117778)[0m f1_per_class: [0.409, 0.497, 0.364, 0.365, 0.076, 0.349, 0.51, 0.466, 0.331, 0.325]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.2439365671641791
[2m[36m(func pid=120650)[0m top5: 0.7168843283582089
[2m[36m(func pid=120650)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=120650)[0m f1_macro: 0.17937012377526954
[2m[36m(func pid=120650)[0m f1_weighted: 0.2698389351771227
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.016, 0.346, 0.481, 0.031, 0.14, 0.292, 0.459, 0.027, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4138 | Steps: 4 | Val loss: 2.0904 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9023 | Steps: 4 | Val loss: 2.9518 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8385 | Steps: 4 | Val loss: 2.2324 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.1131 | Steps: 4 | Val loss: 1.6577 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=112174)[0m top1: 0.2905783582089552
[2m[36m(func pid=112174)[0m top5: 0.7789179104477612
[2m[36m(func pid=112174)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=112174)[0m f1_macro: 0.2501335488601758
[2m[36m(func pid=112174)[0m f1_weighted: 0.3164925353817146
[2m[36m(func pid=112174)[0m f1_per_class: [0.22, 0.261, 0.297, 0.414, 0.05, 0.38, 0.281, 0.296, 0.073, 0.229]
[2m[36m(func pid=112174)[0m == Status ==
Current time: 2024-01-07 15:16:53 (running for 00:43:19.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.414 |      0.25  |                   53 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.347 |      0.369 |                   29 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.603 |      0.198 |                   28 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.51  |      0.179 |                   20 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=118433)[0m top1: 0.2248134328358209
[2m[36m(func pid=118433)[0m top5: 0.8236940298507462
[2m[36m(func pid=118433)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=118433)[0m f1_macro: 0.19341319513575064
[2m[36m(func pid=118433)[0m f1_weighted: 0.2530361578401438
[2m[36m(func pid=118433)[0m f1_per_class: [0.259, 0.233, 0.071, 0.167, 0.088, 0.147, 0.431, 0.108, 0.157, 0.274]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.42677238805970147
[2m[36m(func pid=117778)[0m top5: 0.8950559701492538
[2m[36m(func pid=117778)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=117778)[0m f1_macro: 0.3813372660894134
[2m[36m(func pid=117778)[0m f1_weighted: 0.4496541554973318
[2m[36m(func pid=117778)[0m f1_per_class: [0.465, 0.506, 0.353, 0.43, 0.081, 0.385, 0.49, 0.454, 0.309, 0.341]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.08722014925373134
[2m[36m(func pid=120650)[0m top5: 0.7112873134328358
[2m[36m(func pid=120650)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=120650)[0m f1_macro: 0.1305966809947794
[2m[36m(func pid=120650)[0m f1_weighted: 0.07100015548866048
[2m[36m(func pid=120650)[0m f1_per_class: [0.103, 0.016, 0.483, 0.0, 0.027, 0.081, 0.08, 0.515, 0.0, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4646 | Steps: 4 | Val loss: 2.0932 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4107 | Steps: 4 | Val loss: 3.8061 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6958 | Steps: 4 | Val loss: 2.3448 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2699 | Steps: 4 | Val loss: 1.6588 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:16:58 (running for 00:43:24.40)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.465 |      0.229 |                   54 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.113 |      0.381 |                   30 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.902 |      0.193 |                   29 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.839 |      0.131 |                   21 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.27052238805970147
[2m[36m(func pid=112174)[0m top5: 0.7728544776119403
[2m[36m(func pid=112174)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=112174)[0m f1_macro: 0.22948860870080207
[2m[36m(func pid=112174)[0m f1_weighted: 0.30527666192661407
[2m[36m(func pid=112174)[0m f1_per_class: [0.211, 0.224, 0.191, 0.381, 0.048, 0.367, 0.31, 0.256, 0.086, 0.22]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.16744402985074627
[2m[36m(func pid=118433)[0m top5: 0.7392723880597015
[2m[36m(func pid=118433)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=118433)[0m f1_macro: 0.1599190534190036
[2m[36m(func pid=118433)[0m f1_weighted: 0.18540929791478714
[2m[36m(func pid=118433)[0m f1_per_class: [0.032, 0.331, 0.0, 0.125, 0.078, 0.257, 0.156, 0.145, 0.123, 0.353]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.42350746268656714
[2m[36m(func pid=117778)[0m top5: 0.8894589552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=117778)[0m f1_macro: 0.37773110138456734
[2m[36m(func pid=117778)[0m f1_weighted: 0.45052415649371913
[2m[36m(func pid=117778)[0m f1_per_class: [0.431, 0.501, 0.324, 0.47, 0.079, 0.392, 0.456, 0.45, 0.325, 0.348]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.05457089552238806
[2m[36m(func pid=120650)[0m top5: 0.59375
[2m[36m(func pid=120650)[0m f1_micro: 0.05457089552238806
[2m[36m(func pid=120650)[0m f1_macro: 0.07723391063860657
[2m[36m(func pid=120650)[0m f1_weighted: 0.05371961807092143
[2m[36m(func pid=120650)[0m f1_per_class: [0.096, 0.005, 0.143, 0.0, 0.023, 0.0, 0.089, 0.388, 0.028, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3954 | Steps: 4 | Val loss: 2.0754 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4300 | Steps: 4 | Val loss: 2.6568 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.3209 | Steps: 4 | Val loss: 1.6432 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5471 | Steps: 4 | Val loss: 2.1642 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 15:17:04 (running for 00:43:29.97)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.395 |      0.234 |                   55 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.27  |      0.378 |                   31 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.411 |      0.16  |                   30 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.696 |      0.077 |                   22 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.28078358208955223
[2m[36m(func pid=112174)[0m top5: 0.7873134328358209
[2m[36m(func pid=112174)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=112174)[0m f1_macro: 0.23350646177936105
[2m[36m(func pid=112174)[0m f1_weighted: 0.3108454176693406
[2m[36m(func pid=112174)[0m f1_per_class: [0.216, 0.237, 0.22, 0.38, 0.057, 0.391, 0.32, 0.217, 0.085, 0.212]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.26725746268656714
[2m[36m(func pid=118433)[0m top5: 0.840018656716418
[2m[36m(func pid=118433)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=118433)[0m f1_macro: 0.20905457754546758
[2m[36m(func pid=118433)[0m f1_weighted: 0.27607427806465745
[2m[36m(func pid=118433)[0m f1_per_class: [0.261, 0.518, 0.0, 0.298, 0.136, 0.297, 0.183, 0.034, 0.126, 0.238]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4146455223880597
[2m[36m(func pid=117778)[0m top5: 0.8899253731343284
[2m[36m(func pid=117778)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=117778)[0m f1_macro: 0.37756917662265815
[2m[36m(func pid=117778)[0m f1_weighted: 0.43543138740509996
[2m[36m(func pid=117778)[0m f1_per_class: [0.462, 0.503, 0.32, 0.484, 0.08, 0.42, 0.383, 0.438, 0.294, 0.393]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.23180970149253732
[2m[36m(func pid=120650)[0m top5: 0.6012126865671642
[2m[36m(func pid=120650)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=120650)[0m f1_macro: 0.15244918583838524
[2m[36m(func pid=120650)[0m f1_weighted: 0.2376438994099635
[2m[36m(func pid=120650)[0m f1_per_class: [0.146, 0.072, 0.147, 0.495, 0.03, 0.0, 0.196, 0.413, 0.025, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4081 | Steps: 4 | Val loss: 2.0675 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.1869 | Steps: 4 | Val loss: 3.0942 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.1754 | Steps: 4 | Val loss: 1.6153 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5415 | Steps: 4 | Val loss: 2.0514 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=112174)[0m top1: 0.2835820895522388
[2m[36m(func pid=112174)[0m top5: 0.7947761194029851
[2m[36m(func pid=112174)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=112174)[0m f1_macro: 0.24395277019111586
[2m[36m(func pid=112174)[0m f1_weighted: 0.31077483406683976
[2m[36m(func pid=112174)[0m f1_per_class: [0.244, 0.229, 0.256, 0.379, 0.056, 0.408, 0.314, 0.229, 0.081, 0.245]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:17:09 (running for 00:43:35.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.408 |      0.244 |                   56 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.321 |      0.378 |                   32 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.43  |      0.209 |                   31 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.547 |      0.152 |                   23 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.2896455223880597
[2m[36m(func pid=118433)[0m top5: 0.8288246268656716
[2m[36m(func pid=118433)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=118433)[0m f1_macro: 0.2816086737372744
[2m[36m(func pid=118433)[0m f1_weighted: 0.2624637901365741
[2m[36m(func pid=118433)[0m f1_per_class: [0.44, 0.473, 0.774, 0.326, 0.143, 0.3, 0.118, 0.0, 0.14, 0.102]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.3148320895522388
[2m[36m(func pid=120650)[0m top5: 0.7509328358208955
[2m[36m(func pid=120650)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=120650)[0m f1_macro: 0.16341478455041242
[2m[36m(func pid=120650)[0m f1_weighted: 0.3032907465424411
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.0, 0.0, 0.534, 0.039, 0.0, 0.403, 0.539, 0.056, 0.062]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4361007462686567
[2m[36m(func pid=117778)[0m top5: 0.8885261194029851
[2m[36m(func pid=117778)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=117778)[0m f1_macro: 0.38897123294131875
[2m[36m(func pid=117778)[0m f1_weighted: 0.4575817906683967
[2m[36m(func pid=117778)[0m f1_per_class: [0.473, 0.514, 0.343, 0.488, 0.089, 0.372, 0.457, 0.468, 0.317, 0.368]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4200 | Steps: 4 | Val loss: 2.0677 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7064 | Steps: 4 | Val loss: 2.6363 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3876 | Steps: 4 | Val loss: 1.9777 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.0751 | Steps: 4 | Val loss: 1.6124 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:17:15 (running for 00:43:41.31)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.42  |      0.246 |                   57 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.175 |      0.389 |                   33 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.187 |      0.282 |                   32 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.541 |      0.163 |                   24 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.28638059701492535
[2m[36m(func pid=112174)[0m top5: 0.7989738805970149
[2m[36m(func pid=112174)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=112174)[0m f1_macro: 0.24606513114514933
[2m[36m(func pid=112174)[0m f1_weighted: 0.316125160964304
[2m[36m(func pid=112174)[0m f1_per_class: [0.23, 0.235, 0.244, 0.389, 0.052, 0.386, 0.327, 0.227, 0.076, 0.294]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.37546641791044777
[2m[36m(func pid=118433)[0m top5: 0.8395522388059702
[2m[36m(func pid=118433)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=118433)[0m f1_macro: 0.3123489036331182
[2m[36m(func pid=118433)[0m f1_weighted: 0.36137956570952573
[2m[36m(func pid=118433)[0m f1_per_class: [0.125, 0.526, 0.579, 0.42, 0.125, 0.339, 0.256, 0.426, 0.15, 0.177]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.43843283582089554
[2m[36m(func pid=117778)[0m top5: 0.8964552238805971
[2m[36m(func pid=117778)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=117778)[0m f1_macro: 0.3800937284203337
[2m[36m(func pid=117778)[0m f1_weighted: 0.4636890912369553
[2m[36m(func pid=117778)[0m f1_per_class: [0.456, 0.505, 0.289, 0.479, 0.093, 0.386, 0.488, 0.474, 0.327, 0.304]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m top1: 0.29990671641791045
[2m[36m(func pid=120650)[0m top5: 0.8176305970149254
[2m[36m(func pid=120650)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=120650)[0m f1_macro: 0.18492733105002512
[2m[36m(func pid=120650)[0m f1_weighted: 0.30631754877161277
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.0, 0.296, 0.506, 0.0, 0.0, 0.439, 0.524, 0.064, 0.021]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.4039 | Steps: 4 | Val loss: 2.0665 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.1304 | Steps: 4 | Val loss: 2.2761 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.1501 | Steps: 4 | Val loss: 1.5699 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.2993 | Steps: 4 | Val loss: 2.0507 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:17:21 (running for 00:43:46.95)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.404 |      0.247 |                   58 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.075 |      0.38  |                   34 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.706 |      0.312 |                   33 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.388 |      0.185 |                   25 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.29011194029850745
[2m[36m(func pid=112174)[0m top5: 0.8055037313432836
[2m[36m(func pid=112174)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=112174)[0m f1_macro: 0.24734508969994312
[2m[36m(func pid=112174)[0m f1_weighted: 0.3213181944901981
[2m[36m(func pid=112174)[0m f1_per_class: [0.21, 0.229, 0.253, 0.408, 0.054, 0.39, 0.328, 0.23, 0.092, 0.28]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.457089552238806
[2m[36m(func pid=118433)[0m top5: 0.8763992537313433
[2m[36m(func pid=118433)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=118433)[0m f1_macro: 0.3710232798630072
[2m[36m(func pid=118433)[0m f1_weighted: 0.44794131600198933
[2m[36m(func pid=118433)[0m f1_per_class: [0.218, 0.545, 0.667, 0.575, 0.149, 0.426, 0.347, 0.414, 0.179, 0.19]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.13992537313432835
[2m[36m(func pid=120650)[0m top5: 0.784981343283582
[2m[36m(func pid=120650)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=120650)[0m f1_macro: 0.14329810046889227
[2m[36m(func pid=120650)[0m f1_weighted: 0.15761598823146772
[2m[36m(func pid=120650)[0m f1_per_class: [0.077, 0.0, 0.164, 0.029, 0.0, 0.228, 0.292, 0.533, 0.082, 0.029]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4496268656716418
[2m[36m(func pid=117778)[0m top5: 0.9053171641791045
[2m[36m(func pid=117778)[0m f1_micro: 0.4496268656716418
[2m[36m(func pid=117778)[0m f1_macro: 0.3856487408188035
[2m[36m(func pid=117778)[0m f1_weighted: 0.47315190390214595
[2m[36m(func pid=117778)[0m f1_per_class: [0.472, 0.528, 0.343, 0.491, 0.113, 0.394, 0.498, 0.446, 0.309, 0.261]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.4032 | Steps: 4 | Val loss: 2.0673 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8429 | Steps: 4 | Val loss: 2.5432 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4507 | Steps: 4 | Val loss: 2.2366 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9054 | Steps: 4 | Val loss: 1.5474 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 15:17:26 (running for 00:43:52.42)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.403 |      0.256 |                   59 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.15  |      0.386 |                   35 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.13  |      0.371 |                   34 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.299 |      0.143 |                   26 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.2947761194029851
[2m[36m(func pid=112174)[0m top5: 0.8022388059701493
[2m[36m(func pid=112174)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=112174)[0m f1_macro: 0.2563463844723374
[2m[36m(func pid=112174)[0m f1_weighted: 0.32287910471623515
[2m[36m(func pid=112174)[0m f1_per_class: [0.21, 0.265, 0.27, 0.407, 0.056, 0.403, 0.306, 0.23, 0.095, 0.321]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.37593283582089554
[2m[36m(func pid=118433)[0m top5: 0.7957089552238806
[2m[36m(func pid=118433)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=118433)[0m f1_macro: 0.27026354265030517
[2m[36m(func pid=118433)[0m f1_weighted: 0.3657691207562206
[2m[36m(func pid=118433)[0m f1_per_class: [0.324, 0.502, 0.264, 0.382, 0.163, 0.408, 0.369, 0.0, 0.162, 0.129]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.12733208955223882
[2m[36m(func pid=120650)[0m top5: 0.7336753731343284
[2m[36m(func pid=120650)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=120650)[0m f1_macro: 0.13768967008597116
[2m[36m(func pid=120650)[0m f1_weighted: 0.0735252846629974
[2m[36m(func pid=120650)[0m f1_per_class: [0.099, 0.0, 0.333, 0.0, 0.0, 0.315, 0.0, 0.535, 0.064, 0.03]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4626865671641791
[2m[36m(func pid=117778)[0m top5: 0.9137126865671642
[2m[36m(func pid=117778)[0m f1_micro: 0.4626865671641791
[2m[36m(func pid=117778)[0m f1_macro: 0.4133146991065078
[2m[36m(func pid=117778)[0m f1_weighted: 0.4843263829132093
[2m[36m(func pid=117778)[0m f1_per_class: [0.544, 0.536, 0.49, 0.518, 0.125, 0.401, 0.493, 0.453, 0.323, 0.251]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3791 | Steps: 4 | Val loss: 2.0613 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4908 | Steps: 4 | Val loss: 3.2024 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6242 | Steps: 4 | Val loss: 2.2448 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.2076 | Steps: 4 | Val loss: 1.5526 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 15:17:32 (running for 00:43:58.03)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.379 |      0.268 |                   60 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.905 |      0.413 |                   36 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.843 |      0.27  |                   35 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.451 |      0.138 |                   27 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.30363805970149255
[2m[36m(func pid=112174)[0m top5: 0.8017723880597015
[2m[36m(func pid=112174)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=112174)[0m f1_macro: 0.268073675428175
[2m[36m(func pid=112174)[0m f1_weighted: 0.3298930681500401
[2m[36m(func pid=112174)[0m f1_per_class: [0.226, 0.302, 0.333, 0.398, 0.062, 0.406, 0.31, 0.244, 0.096, 0.303]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.3138992537313433
[2m[36m(func pid=118433)[0m top5: 0.6963619402985075
[2m[36m(func pid=118433)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=118433)[0m f1_macro: 0.23967795871375036
[2m[36m(func pid=118433)[0m f1_weighted: 0.2824153301576182
[2m[36m(func pid=118433)[0m f1_per_class: [0.292, 0.406, 0.261, 0.079, 0.146, 0.429, 0.417, 0.0, 0.196, 0.17]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.12826492537313433
[2m[36m(func pid=120650)[0m top5: 0.7094216417910447
[2m[36m(func pid=120650)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=120650)[0m f1_macro: 0.12679522250137082
[2m[36m(func pid=120650)[0m f1_weighted: 0.07117992653111095
[2m[36m(func pid=120650)[0m f1_per_class: [0.096, 0.0, 0.25, 0.0, 0.0, 0.309, 0.0, 0.511, 0.077, 0.025]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.45382462686567165
[2m[36m(func pid=117778)[0m top5: 0.9123134328358209
[2m[36m(func pid=117778)[0m f1_micro: 0.45382462686567165
[2m[36m(func pid=117778)[0m f1_macro: 0.4050050527364822
[2m[36m(func pid=117778)[0m f1_weighted: 0.47378589811915023
[2m[36m(func pid=117778)[0m f1_per_class: [0.5, 0.516, 0.48, 0.532, 0.131, 0.408, 0.457, 0.444, 0.324, 0.258]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3201 | Steps: 4 | Val loss: 2.0667 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.1245 | Steps: 4 | Val loss: 3.4764 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.1382 | Steps: 4 | Val loss: 2.2732 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0086 | Steps: 4 | Val loss: 1.5658 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 15:17:38 (running for 00:44:03.67)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.32  |      0.258 |                   61 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.208 |      0.405 |                   37 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.491 |      0.24  |                   36 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.624 |      0.127 |                   28 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.28777985074626866
[2m[36m(func pid=112174)[0m top5: 0.8013059701492538
[2m[36m(func pid=112174)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=112174)[0m f1_macro: 0.2578320725892061
[2m[36m(func pid=112174)[0m f1_weighted: 0.3124379302803676
[2m[36m(func pid=112174)[0m f1_per_class: [0.194, 0.296, 0.324, 0.366, 0.071, 0.405, 0.284, 0.265, 0.099, 0.273]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2728544776119403
[2m[36m(func pid=118433)[0m top5: 0.6693097014925373
[2m[36m(func pid=118433)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=118433)[0m f1_macro: 0.24281409309553706
[2m[36m(func pid=118433)[0m f1_weighted: 0.2501028414871115
[2m[36m(func pid=118433)[0m f1_per_class: [0.15, 0.406, 0.323, 0.01, 0.114, 0.412, 0.346, 0.195, 0.214, 0.258]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.13852611940298507
[2m[36m(func pid=120650)[0m top5: 0.7010261194029851
[2m[36m(func pid=120650)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=120650)[0m f1_macro: 0.1475738908133187
[2m[36m(func pid=120650)[0m f1_weighted: 0.15211985109796497
[2m[36m(func pid=120650)[0m f1_per_class: [0.084, 0.0, 0.25, 0.0, 0.043, 0.109, 0.345, 0.522, 0.077, 0.045]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.44776119402985076
[2m[36m(func pid=117778)[0m top5: 0.9113805970149254
[2m[36m(func pid=117778)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=117778)[0m f1_macro: 0.40719448306401895
[2m[36m(func pid=117778)[0m f1_weighted: 0.4621942276439401
[2m[36m(func pid=117778)[0m f1_per_class: [0.5, 0.538, 0.5, 0.524, 0.132, 0.414, 0.413, 0.423, 0.327, 0.302]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3497 | Steps: 4 | Val loss: 2.0635 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1231 | Steps: 4 | Val loss: 2.9723 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3671 | Steps: 4 | Val loss: 2.1304 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2351 | Steps: 4 | Val loss: 1.5748 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 15:17:43 (running for 00:44:09.35)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.35  |      0.265 |                   62 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.009 |      0.407 |                   38 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.125 |      0.243 |                   37 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.138 |      0.148 |                   29 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.30177238805970147
[2m[36m(func pid=112174)[0m top5: 0.7961753731343284
[2m[36m(func pid=112174)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=112174)[0m f1_macro: 0.26490617137648226
[2m[36m(func pid=112174)[0m f1_weighted: 0.3247481590227843
[2m[36m(func pid=112174)[0m f1_per_class: [0.224, 0.282, 0.304, 0.424, 0.06, 0.389, 0.278, 0.296, 0.103, 0.291]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.3087686567164179
[2m[36m(func pid=118433)[0m top5: 0.8199626865671642
[2m[36m(func pid=118433)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=118433)[0m f1_macro: 0.3062914346787054
[2m[36m(func pid=118433)[0m f1_weighted: 0.31488895334071876
[2m[36m(func pid=118433)[0m f1_per_class: [0.167, 0.417, 0.581, 0.26, 0.151, 0.408, 0.282, 0.38, 0.238, 0.179]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.17117537313432835
[2m[36m(func pid=120650)[0m top5: 0.7318097014925373
[2m[36m(func pid=120650)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=120650)[0m f1_macro: 0.16928260075335308
[2m[36m(func pid=120650)[0m f1_weighted: 0.20116364471803388
[2m[36m(func pid=120650)[0m f1_per_class: [0.077, 0.164, 0.369, 0.0, 0.032, 0.008, 0.456, 0.507, 0.079, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.44869402985074625
[2m[36m(func pid=117778)[0m top5: 0.9085820895522388
[2m[36m(func pid=117778)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=117778)[0m f1_macro: 0.4097210054089889
[2m[36m(func pid=117778)[0m f1_weighted: 0.4493756047236701
[2m[36m(func pid=117778)[0m f1_per_class: [0.477, 0.534, 0.511, 0.563, 0.173, 0.427, 0.332, 0.405, 0.337, 0.339]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3573 | Steps: 4 | Val loss: 2.0560 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2524 | Steps: 4 | Val loss: 2.7179 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3370 | Steps: 4 | Val loss: 2.0314 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9418 | Steps: 4 | Val loss: 1.5408 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 15:17:49 (running for 00:44:15.22)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.35  |      0.265 |                   62 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.235 |      0.41  |                   39 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.252 |      0.327 |                   39 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.367 |      0.169 |                   30 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.2980410447761194
[2m[36m(func pid=112174)[0m top5: 0.7994402985074627
[2m[36m(func pid=112174)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=112174)[0m f1_macro: 0.2633368397735234
[2m[36m(func pid=112174)[0m f1_weighted: 0.3261140441419045
[2m[36m(func pid=112174)[0m f1_per_class: [0.23, 0.273, 0.258, 0.416, 0.059, 0.382, 0.297, 0.298, 0.087, 0.333]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.36986940298507465
[2m[36m(func pid=118433)[0m top5: 0.8708022388059702
[2m[36m(func pid=118433)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=118433)[0m f1_macro: 0.3271257982384307
[2m[36m(func pid=118433)[0m f1_weighted: 0.37696425815255274
[2m[36m(func pid=118433)[0m f1_per_class: [0.262, 0.524, 0.667, 0.452, 0.078, 0.396, 0.271, 0.28, 0.209, 0.133]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.20988805970149255
[2m[36m(func pid=120650)[0m top5: 0.773320895522388
[2m[36m(func pid=120650)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=120650)[0m f1_macro: 0.16700753784978797
[2m[36m(func pid=120650)[0m f1_weighted: 0.23197164941761764
[2m[36m(func pid=120650)[0m f1_per_class: [0.054, 0.372, 0.128, 0.063, 0.038, 0.142, 0.348, 0.444, 0.082, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.46408582089552236
[2m[36m(func pid=117778)[0m top5: 0.9057835820895522
[2m[36m(func pid=117778)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=117778)[0m f1_macro: 0.4111184020368894
[2m[36m(func pid=117778)[0m f1_weighted: 0.47457655182982084
[2m[36m(func pid=117778)[0m f1_per_class: [0.463, 0.548, 0.453, 0.538, 0.177, 0.431, 0.429, 0.456, 0.278, 0.339]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2672 | Steps: 4 | Val loss: 2.0423 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6210 | Steps: 4 | Val loss: 3.7067 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.1322 | Steps: 4 | Val loss: 2.0398 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8206 | Steps: 4 | Val loss: 1.5276 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 15:17:55 (running for 00:44:20.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.357 |      0.263 |                   63 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.942 |      0.411 |                   40 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.252 |      0.327 |                   39 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.132 |      0.15  |                   32 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.23507462686567165
[2m[36m(func pid=120650)[0m top5: 0.7709888059701493
[2m[36m(func pid=120650)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=120650)[0m f1_macro: 0.15017656717549197
[2m[36m(func pid=120650)[0m f1_weighted: 0.2760113120171744
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.334, 0.0, 0.181, 0.033, 0.109, 0.45, 0.325, 0.07, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m top1: 0.30177238805970147
[2m[36m(func pid=112174)[0m top5: 0.800839552238806
[2m[36m(func pid=112174)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=112174)[0m f1_macro: 0.26685930275385783
[2m[36m(func pid=112174)[0m f1_weighted: 0.32294056437790153
[2m[36m(func pid=112174)[0m f1_per_class: [0.227, 0.276, 0.324, 0.436, 0.061, 0.383, 0.269, 0.277, 0.088, 0.327]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.28777985074626866
[2m[36m(func pid=118433)[0m top5: 0.8432835820895522
[2m[36m(func pid=118433)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=118433)[0m f1_macro: 0.26152675350055626
[2m[36m(func pid=118433)[0m f1_weighted: 0.2587100824171907
[2m[36m(func pid=118433)[0m f1_per_class: [0.491, 0.499, 0.686, 0.315, 0.0, 0.318, 0.098, 0.0, 0.132, 0.077]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4748134328358209
[2m[36m(func pid=117778)[0m top5: 0.9090485074626866
[2m[36m(func pid=117778)[0m f1_micro: 0.4748134328358209
[2m[36m(func pid=117778)[0m f1_macro: 0.4247575566152503
[2m[36m(func pid=117778)[0m f1_weighted: 0.4876749497182713
[2m[36m(func pid=117778)[0m f1_per_class: [0.55, 0.552, 0.49, 0.54, 0.172, 0.425, 0.466, 0.443, 0.297, 0.313]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3561 | Steps: 4 | Val loss: 2.0467 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2387 | Steps: 4 | Val loss: 2.1322 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5702 | Steps: 4 | Val loss: 3.6969 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.0851 | Steps: 4 | Val loss: 1.5397 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 15:18:00 (running for 00:44:26.32)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.267 |      0.267 |                   64 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.821 |      0.425 |                   41 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.621 |      0.262 |                   40 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.239 |      0.135 |                   33 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.17863805970149255
[2m[36m(func pid=120650)[0m top5: 0.7761194029850746
[2m[36m(func pid=120650)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=120650)[0m f1_macro: 0.13485529548823966
[2m[36m(func pid=120650)[0m f1_weighted: 0.2030024119051795
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.168, 0.0, 0.428, 0.029, 0.105, 0.036, 0.504, 0.078, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m top1: 0.29990671641791045
[2m[36m(func pid=112174)[0m top5: 0.808768656716418
[2m[36m(func pid=112174)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=112174)[0m f1_macro: 0.2654647603431189
[2m[36m(func pid=112174)[0m f1_weighted: 0.3197106981276825
[2m[36m(func pid=112174)[0m f1_per_class: [0.191, 0.292, 0.308, 0.423, 0.065, 0.384, 0.256, 0.302, 0.103, 0.33]
[2m[36m(func pid=118433)[0m top1: 0.2537313432835821
[2m[36m(func pid=118433)[0m top5: 0.7686567164179104
[2m[36m(func pid=118433)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=118433)[0m f1_macro: 0.24876270430142636
[2m[36m(func pid=118433)[0m f1_weighted: 0.2427548918167106
[2m[36m(func pid=118433)[0m f1_per_class: [0.258, 0.455, 0.462, 0.138, 0.37, 0.339, 0.235, 0.0, 0.164, 0.066]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.46968283582089554
[2m[36m(func pid=117778)[0m top5: 0.9127798507462687
[2m[36m(func pid=117778)[0m f1_micro: 0.46968283582089554
[2m[36m(func pid=117778)[0m f1_macro: 0.43142509534618617
[2m[36m(func pid=117778)[0m f1_weighted: 0.480378539914797
[2m[36m(func pid=117778)[0m f1_per_class: [0.628, 0.547, 0.48, 0.496, 0.147, 0.444, 0.467, 0.444, 0.346, 0.314]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7476 | Steps: 4 | Val loss: 2.1164 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.1926 | Steps: 4 | Val loss: 3.2740 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2408 | Steps: 4 | Val loss: 2.0442 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8398 | Steps: 4 | Val loss: 1.6348 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=120650)[0m top1: 0.22434701492537312
[2m[36m(func pid=120650)[0m top5: 0.8041044776119403
[2m[36m(func pid=120650)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=120650)[0m f1_macro: 0.12887338611029717
[2m[36m(func pid=120650)[0m f1_weighted: 0.18858741733957413
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.071, 0.133, 0.492, 0.039, 0.138, 0.0, 0.341, 0.074, 0.0]
[2m[36m(func pid=120650)[0m 
== Status ==
Current time: 2024-01-07 15:18:06 (running for 00:44:32.01)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.356 |      0.265 |                   65 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  1.085 |      0.431 |                   42 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.57  |      0.249 |                   41 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.748 |      0.129 |                   34 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.25326492537313433
[2m[36m(func pid=118433)[0m top5: 0.7182835820895522
[2m[36m(func pid=118433)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=118433)[0m f1_macro: 0.19414766643999432
[2m[36m(func pid=118433)[0m f1_weighted: 0.23983569846698113
[2m[36m(func pid=118433)[0m f1_per_class: [0.131, 0.396, 0.18, 0.056, 0.136, 0.337, 0.349, 0.033, 0.143, 0.18]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=112174)[0m top1: 0.2933768656716418
[2m[36m(func pid=112174)[0m top5: 0.8041044776119403
[2m[36m(func pid=112174)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=112174)[0m f1_macro: 0.260332309873366
[2m[36m(func pid=112174)[0m f1_weighted: 0.3066250372135672
[2m[36m(func pid=112174)[0m f1_per_class: [0.202, 0.296, 0.289, 0.417, 0.069, 0.381, 0.218, 0.302, 0.086, 0.342]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4430970149253731
[2m[36m(func pid=117778)[0m top5: 0.8871268656716418
[2m[36m(func pid=117778)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=117778)[0m f1_macro: 0.37144949452921755
[2m[36m(func pid=117778)[0m f1_weighted: 0.45566450325240443
[2m[36m(func pid=117778)[0m f1_per_class: [0.494, 0.53, 0.293, 0.486, 0.154, 0.343, 0.483, 0.343, 0.267, 0.321]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2218 | Steps: 4 | Val loss: 2.5263 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3208 | Steps: 4 | Val loss: 3.5111 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2476 | Steps: 4 | Val loss: 2.0358 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8575 | Steps: 4 | Val loss: 1.5925 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 15:18:12 (running for 00:44:37.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.248 |      0.272 |                   67 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.84  |      0.371 |                   43 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.193 |      0.194 |                   42 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.748 |      0.129 |                   34 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.3111007462686567
[2m[36m(func pid=112174)[0m top5: 0.8069029850746269
[2m[36m(func pid=112174)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=112174)[0m f1_macro: 0.2717285945296245
[2m[36m(func pid=112174)[0m f1_weighted: 0.3245796722120181
[2m[36m(func pid=112174)[0m f1_per_class: [0.251, 0.286, 0.324, 0.454, 0.068, 0.383, 0.245, 0.31, 0.089, 0.308]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=120650)[0m top1: 0.1296641791044776
[2m[36m(func pid=120650)[0m top5: 0.8078358208955224
[2m[36m(func pid=120650)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=120650)[0m f1_macro: 0.11326856510937648
[2m[36m(func pid=120650)[0m f1_weighted: 0.10453939028624967
[2m[36m(func pid=120650)[0m f1_per_class: [0.141, 0.0, 0.216, 0.237, 0.036, 0.075, 0.027, 0.242, 0.076, 0.083]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.27098880597014924
[2m[36m(func pid=118433)[0m top5: 0.7346082089552238
[2m[36m(func pid=118433)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=118433)[0m f1_macro: 0.2372413839630973
[2m[36m(func pid=118433)[0m f1_weighted: 0.2618523020044416
[2m[36m(func pid=118433)[0m f1_per_class: [0.262, 0.407, 0.147, 0.038, 0.123, 0.414, 0.35, 0.251, 0.153, 0.227]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4510261194029851
[2m[36m(func pid=117778)[0m top5: 0.8861940298507462
[2m[36m(func pid=117778)[0m f1_micro: 0.4510261194029851
[2m[36m(func pid=117778)[0m f1_macro: 0.3773812929674846
[2m[36m(func pid=117778)[0m f1_weighted: 0.46792764682263926
[2m[36m(func pid=117778)[0m f1_per_class: [0.505, 0.537, 0.286, 0.5, 0.146, 0.375, 0.496, 0.336, 0.267, 0.326]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2757 | Steps: 4 | Val loss: 1.9616 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2875 | Steps: 4 | Val loss: 2.0256 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3654 | Steps: 4 | Val loss: 3.1191 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.9492 | Steps: 4 | Val loss: 1.6095 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:18:17 (running for 00:44:43.09)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.248 |      0.272 |                   67 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.857 |      0.377 |                   44 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.321 |      0.237 |                   43 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.276 |      0.201 |                   36 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.3255597014925373
[2m[36m(func pid=120650)[0m top5: 0.8194962686567164
[2m[36m(func pid=120650)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=120650)[0m f1_macro: 0.20099194706733017
[2m[36m(func pid=120650)[0m f1_weighted: 0.3135892000138593
[2m[36m(func pid=120650)[0m f1_per_class: [0.128, 0.0, 0.275, 0.422, 0.05, 0.081, 0.509, 0.493, 0.051, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m top1: 0.30923507462686567
[2m[36m(func pid=112174)[0m top5: 0.816231343283582
[2m[36m(func pid=112174)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=112174)[0m f1_macro: 0.27075810496603087
[2m[36m(func pid=112174)[0m f1_weighted: 0.3171696947715899
[2m[36m(func pid=112174)[0m f1_per_class: [0.245, 0.25, 0.348, 0.473, 0.066, 0.395, 0.215, 0.313, 0.116, 0.286]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.28824626865671643
[2m[36m(func pid=118433)[0m top5: 0.7947761194029851
[2m[36m(func pid=118433)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=118433)[0m f1_macro: 0.22881870774917062
[2m[36m(func pid=118433)[0m f1_weighted: 0.2925373316660705
[2m[36m(func pid=118433)[0m f1_per_class: [0.063, 0.374, 0.092, 0.11, 0.124, 0.4, 0.411, 0.328, 0.146, 0.24]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4519589552238806
[2m[36m(func pid=117778)[0m top5: 0.8861940298507462
[2m[36m(func pid=117778)[0m f1_micro: 0.4519589552238806
[2m[36m(func pid=117778)[0m f1_macro: 0.38648032086711953
[2m[36m(func pid=117778)[0m f1_weighted: 0.46422178377937046
[2m[36m(func pid=117778)[0m f1_per_class: [0.549, 0.551, 0.4, 0.511, 0.153, 0.351, 0.478, 0.305, 0.233, 0.333]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.3261 | Steps: 4 | Val loss: 2.0307 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4510 | Steps: 4 | Val loss: 2.0137 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2393 | Steps: 4 | Val loss: 2.6081 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9225 | Steps: 4 | Val loss: 1.5647 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:18:22 (running for 00:44:48.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.288 |      0.271 |                   68 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.949 |      0.386 |                   45 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.365 |      0.229 |                   44 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.451 |      0.158 |                   37 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.29990671641791045
[2m[36m(func pid=112174)[0m top5: 0.8129664179104478
[2m[36m(func pid=112174)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=112174)[0m f1_macro: 0.26708984460223867
[2m[36m(func pid=112174)[0m f1_weighted: 0.3098720661514656
[2m[36m(func pid=112174)[0m f1_per_class: [0.212, 0.247, 0.364, 0.427, 0.086, 0.396, 0.236, 0.315, 0.125, 0.264]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=120650)[0m top1: 0.2957089552238806
[2m[36m(func pid=120650)[0m top5: 0.7882462686567164
[2m[36m(func pid=120650)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=120650)[0m f1_macro: 0.1578553756114584
[2m[36m(func pid=120650)[0m f1_weighted: 0.22633223019370666
[2m[36m(func pid=120650)[0m f1_per_class: [0.076, 0.0, 0.224, 0.53, 0.0, 0.227, 0.089, 0.38, 0.028, 0.025]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.33955223880597013
[2m[36m(func pid=118433)[0m top5: 0.8540111940298507
[2m[36m(func pid=118433)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=118433)[0m f1_macro: 0.25797216915382426
[2m[36m(func pid=118433)[0m f1_weighted: 0.35604639047831327
[2m[36m(func pid=118433)[0m f1_per_class: [0.069, 0.384, 0.085, 0.252, 0.114, 0.359, 0.5, 0.284, 0.181, 0.35]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.43796641791044777
[2m[36m(func pid=117778)[0m top5: 0.9001865671641791
[2m[36m(func pid=117778)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=117778)[0m f1_macro: 0.3981020688482332
[2m[36m(func pid=117778)[0m f1_weighted: 0.44754126079015316
[2m[36m(func pid=117778)[0m f1_per_class: [0.479, 0.544, 0.333, 0.482, 0.117, 0.426, 0.385, 0.475, 0.348, 0.393]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2580 | Steps: 4 | Val loss: 2.0257 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.1578 | Steps: 4 | Val loss: 2.3238 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5402 | Steps: 4 | Val loss: 2.9903 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.9719 | Steps: 4 | Val loss: 1.5811 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 15:18:28 (running for 00:44:53.93)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.258 |      0.274 |                   70 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.922 |      0.398 |                   46 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.239 |      0.258 |                   45 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.451 |      0.158 |                   37 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.3101679104477612
[2m[36m(func pid=112174)[0m top5: 0.8166977611940298
[2m[36m(func pid=112174)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=112174)[0m f1_macro: 0.2740504981640711
[2m[36m(func pid=112174)[0m f1_weighted: 0.317700392507533
[2m[36m(func pid=112174)[0m f1_per_class: [0.221, 0.217, 0.358, 0.47, 0.076, 0.402, 0.232, 0.321, 0.145, 0.297]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=120650)[0m top1: 0.22014925373134328
[2m[36m(func pid=120650)[0m top5: 0.6823694029850746
[2m[36m(func pid=120650)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=120650)[0m f1_macro: 0.14990416778648225
[2m[36m(func pid=120650)[0m f1_weighted: 0.19055573808873216
[2m[36m(func pid=120650)[0m f1_per_class: [0.148, 0.032, 0.186, 0.486, 0.0, 0.074, 0.022, 0.509, 0.022, 0.019]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.34281716417910446
[2m[36m(func pid=118433)[0m top5: 0.8773320895522388
[2m[36m(func pid=118433)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=118433)[0m f1_macro: 0.31803543537982104
[2m[36m(func pid=118433)[0m f1_weighted: 0.3503954048086699
[2m[36m(func pid=118433)[0m f1_per_class: [0.135, 0.42, 0.511, 0.456, 0.109, 0.243, 0.28, 0.338, 0.252, 0.436]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.42677238805970147
[2m[36m(func pid=117778)[0m top5: 0.902518656716418
[2m[36m(func pid=117778)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=117778)[0m f1_macro: 0.3891422912382839
[2m[36m(func pid=117778)[0m f1_weighted: 0.4308212103994962
[2m[36m(func pid=117778)[0m f1_per_class: [0.429, 0.529, 0.377, 0.499, 0.135, 0.454, 0.323, 0.444, 0.296, 0.404]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3089 | Steps: 4 | Val loss: 2.0189 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4167 | Steps: 4 | Val loss: 2.2574 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5928 | Steps: 4 | Val loss: 3.1241 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.8053 | Steps: 4 | Val loss: 1.6016 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 15:18:34 (running for 00:44:59.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.258 |      0.274 |                   70 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.972 |      0.389 |                   47 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.54  |      0.318 |                   46 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.417 |      0.138 |                   39 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.31902985074626866
[2m[36m(func pid=112174)[0m top5: 0.8227611940298507
[2m[36m(func pid=112174)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=112174)[0m f1_macro: 0.2777231385346404
[2m[36m(func pid=112174)[0m f1_weighted: 0.332260950887749
[2m[36m(func pid=112174)[0m f1_per_class: [0.221, 0.242, 0.324, 0.454, 0.08, 0.424, 0.274, 0.333, 0.118, 0.306]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=120650)[0m top1: 0.11940298507462686
[2m[36m(func pid=120650)[0m top5: 0.7178171641791045
[2m[36m(func pid=120650)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=120650)[0m f1_macro: 0.13763444929220536
[2m[36m(func pid=120650)[0m f1_weighted: 0.11883457420122792
[2m[36m(func pid=120650)[0m f1_per_class: [0.082, 0.277, 0.234, 0.003, 0.0, 0.115, 0.075, 0.516, 0.052, 0.023]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.3344216417910448
[2m[36m(func pid=118433)[0m top5: 0.8432835820895522
[2m[36m(func pid=118433)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=118433)[0m f1_macro: 0.3419253578011078
[2m[36m(func pid=118433)[0m f1_weighted: 0.3356935419443448
[2m[36m(func pid=118433)[0m f1_per_class: [0.302, 0.452, 0.615, 0.373, 0.084, 0.265, 0.254, 0.388, 0.323, 0.364]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.416044776119403
[2m[36m(func pid=117778)[0m top5: 0.9001865671641791
[2m[36m(func pid=117778)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=117778)[0m f1_macro: 0.385543364185061
[2m[36m(func pid=117778)[0m f1_weighted: 0.42557391138523576
[2m[36m(func pid=117778)[0m f1_per_class: [0.443, 0.525, 0.333, 0.453, 0.108, 0.429, 0.35, 0.476, 0.35, 0.388]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1173 | Steps: 4 | Val loss: 2.1341 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3018 | Steps: 4 | Val loss: 2.0036 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4917 | Steps: 4 | Val loss: 3.6813 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7037 | Steps: 4 | Val loss: 1.5661 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=112174)[0m top1: 0.34468283582089554
[2m[36m(func pid=112174)[0m top5: 0.8255597014925373
[2m[36m(func pid=112174)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=112174)[0m f1_macro: 0.30639277599271075
[2m[36m(func pid=112174)[0m f1_weighted: 0.35306327116139635
[2m[36m(func pid=112174)[0m f1_per_class: [0.268, 0.289, 0.471, 0.491, 0.086, 0.425, 0.276, 0.317, 0.13, 0.31]
[2m[36m(func pid=112174)[0m 
== Status ==
Current time: 2024-01-07 15:18:39 (running for 00:45:05.09)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.302 |      0.306 |                   72 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.805 |      0.386 |                   48 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.593 |      0.342 |                   47 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.417 |      0.138 |                   39 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.16324626865671643
[2m[36m(func pid=120650)[0m top5: 0.7518656716417911
[2m[36m(func pid=120650)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=120650)[0m f1_macro: 0.17011648000740653
[2m[36m(func pid=120650)[0m f1_weighted: 0.18873857715789322
[2m[36m(func pid=120650)[0m f1_per_class: [0.084, 0.172, 0.351, 0.0, 0.0, 0.133, 0.37, 0.456, 0.085, 0.051]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.29384328358208955
[2m[36m(func pid=118433)[0m top5: 0.7569962686567164
[2m[36m(func pid=118433)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=118433)[0m f1_macro: 0.33620151079583377
[2m[36m(func pid=118433)[0m f1_weighted: 0.2863105918284061
[2m[36m(func pid=118433)[0m f1_per_class: [0.338, 0.4, 0.632, 0.149, 0.083, 0.288, 0.304, 0.444, 0.325, 0.4]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.44822761194029853
[2m[36m(func pid=117778)[0m top5: 0.902518656716418
[2m[36m(func pid=117778)[0m f1_micro: 0.44822761194029853
[2m[36m(func pid=117778)[0m f1_macro: 0.3956057505682796
[2m[36m(func pid=117778)[0m f1_weighted: 0.4705306470630033
[2m[36m(func pid=117778)[0m f1_per_class: [0.432, 0.534, 0.324, 0.459, 0.12, 0.414, 0.496, 0.505, 0.315, 0.356]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3108 | Steps: 4 | Val loss: 2.0030 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.8897 | Steps: 4 | Val loss: 3.8569 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.2759 | Steps: 4 | Val loss: 2.0699 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7846 | Steps: 4 | Val loss: 1.6127 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 15:18:45 (running for 00:45:10.83)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.302 |      0.306 |                   72 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.704 |      0.396 |                   49 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.492 |      0.336 |                   48 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.276 |      0.159 |                   41 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=112174)[0m top1: 0.33302238805970147
[2m[36m(func pid=112174)[0m top5: 0.8274253731343284
[2m[36m(func pid=112174)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=112174)[0m f1_macro: 0.2856256625169691
[2m[36m(func pid=112174)[0m f1_weighted: 0.3461760085956963
[2m[36m(func pid=112174)[0m f1_per_class: [0.24, 0.235, 0.4, 0.474, 0.082, 0.403, 0.314, 0.337, 0.1, 0.271]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2798507462686567
[2m[36m(func pid=118433)[0m top5: 0.7994402985074627
[2m[36m(func pid=118433)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=118433)[0m f1_macro: 0.2981974615690883
[2m[36m(func pid=118433)[0m f1_weighted: 0.2728407714674087
[2m[36m(func pid=118433)[0m f1_per_class: [0.287, 0.362, 0.524, 0.204, 0.08, 0.307, 0.261, 0.316, 0.256, 0.386]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.19636194029850745
[2m[36m(func pid=120650)[0m top5: 0.8092350746268657
[2m[36m(func pid=120650)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=120650)[0m f1_macro: 0.15908948616046886
[2m[36m(func pid=120650)[0m f1_weighted: 0.22217859627025818
[2m[36m(func pid=120650)[0m f1_per_class: [0.108, 0.311, 0.299, 0.05, 0.0, 0.0, 0.447, 0.238, 0.088, 0.05]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.43236940298507465
[2m[36m(func pid=117778)[0m top5: 0.9039179104477612
[2m[36m(func pid=117778)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=117778)[0m f1_macro: 0.3885597954936992
[2m[36m(func pid=117778)[0m f1_weighted: 0.4545021267551264
[2m[36m(func pid=117778)[0m f1_per_class: [0.467, 0.512, 0.407, 0.42, 0.102, 0.365, 0.511, 0.5, 0.297, 0.304]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.1975 | Steps: 4 | Val loss: 1.9586 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1886 | Steps: 4 | Val loss: 1.9946 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.9901 | Steps: 4 | Val loss: 3.6658 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.8858 | Steps: 4 | Val loss: 1.6698 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:18:50 (running for 00:45:16.31)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.311 |      0.286 |                   73 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.785 |      0.389 |                   50 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.89  |      0.298 |                   49 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.197 |      0.28  |                   42 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.355410447761194
[2m[36m(func pid=120650)[0m top5: 0.820429104477612
[2m[36m(func pid=120650)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=120650)[0m f1_macro: 0.27996843303264485
[2m[36m(func pid=120650)[0m f1_weighted: 0.3883116410702168
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.399, 0.8, 0.525, 0.0, 0.313, 0.386, 0.238, 0.099, 0.04]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m top1: 0.3474813432835821
[2m[36m(func pid=112174)[0m top5: 0.8283582089552238
[2m[36m(func pid=112174)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=112174)[0m f1_macro: 0.3005217872180458
[2m[36m(func pid=112174)[0m f1_weighted: 0.36447564518067166
[2m[36m(func pid=112174)[0m f1_per_class: [0.296, 0.285, 0.4, 0.498, 0.084, 0.405, 0.321, 0.313, 0.126, 0.277]
[2m[36m(func pid=112174)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2583955223880597
[2m[36m(func pid=118433)[0m top5: 0.8227611940298507
[2m[36m(func pid=118433)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=118433)[0m f1_macro: 0.2657418503100898
[2m[36m(func pid=118433)[0m f1_weighted: 0.2416293349586471
[2m[36m(func pid=118433)[0m f1_per_class: [0.281, 0.429, 0.381, 0.254, 0.116, 0.148, 0.116, 0.429, 0.268, 0.235]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4118470149253731
[2m[36m(func pid=117778)[0m top5: 0.8927238805970149
[2m[36m(func pid=117778)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=117778)[0m f1_macro: 0.3627748093560154
[2m[36m(func pid=117778)[0m f1_weighted: 0.43296567226099825
[2m[36m(func pid=117778)[0m f1_per_class: [0.388, 0.513, 0.296, 0.41, 0.103, 0.36, 0.459, 0.494, 0.283, 0.321]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2918 | Steps: 4 | Val loss: 2.3283 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=112174)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2638 | Steps: 4 | Val loss: 2.0069 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5484 | Steps: 4 | Val loss: 4.5255 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7725 | Steps: 4 | Val loss: 1.6972 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 15:18:56 (running for 00:45:21.72)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00020 | RUNNING    | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.189 |      0.301 |                   74 |
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.886 |      0.363 |                   51 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.99  |      0.266 |                   50 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.292 |      0.153 |                   43 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.251865671641791
[2m[36m(func pid=120650)[0m top5: 0.7378731343283582
[2m[36m(func pid=120650)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=120650)[0m f1_macro: 0.1532044838661894
[2m[36m(func pid=120650)[0m f1_weighted: 0.26790347672081755
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.483, 0.0, 0.433, 0.0, 0.096, 0.107, 0.325, 0.066, 0.023]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=112174)[0m top1: 0.3255597014925373
[2m[36m(func pid=112174)[0m top5: 0.8185634328358209
[2m[36m(func pid=112174)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=112174)[0m f1_macro: 0.28087832207186686
[2m[36m(func pid=112174)[0m f1_weighted: 0.3450040799852181
[2m[36m(func pid=112174)[0m f1_per_class: [0.28, 0.296, 0.329, 0.471, 0.074, 0.394, 0.284, 0.303, 0.12, 0.257]
[2m[36m(func pid=118433)[0m top1: 0.20942164179104478
[2m[36m(func pid=118433)[0m top5: 0.7220149253731343
[2m[36m(func pid=118433)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=118433)[0m f1_macro: 0.1847292610158237
[2m[36m(func pid=118433)[0m f1_weighted: 0.2335620168608397
[2m[36m(func pid=118433)[0m f1_per_class: [0.103, 0.312, 0.0, 0.116, 0.109, 0.043, 0.35, 0.44, 0.262, 0.111]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.39972014925373134
[2m[36m(func pid=117778)[0m top5: 0.8838619402985075
[2m[36m(func pid=117778)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=117778)[0m f1_macro: 0.3538269218493987
[2m[36m(func pid=117778)[0m f1_weighted: 0.41351787307597077
[2m[36m(func pid=117778)[0m f1_per_class: [0.337, 0.511, 0.253, 0.409, 0.12, 0.415, 0.384, 0.466, 0.283, 0.362]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3678 | Steps: 4 | Val loss: 2.0301 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8891 | Steps: 4 | Val loss: 3.5679 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6963 | Steps: 4 | Val loss: 1.7602 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:19:01 (running for 00:45:27.55)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.773 |      0.354 |                   52 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.548 |      0.185 |                   51 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.368 |      0.157 |                   44 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.2140858208955224
[2m[36m(func pid=120650)[0m top5: 0.8022388059701493
[2m[36m(func pid=120650)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=120650)[0m f1_macro: 0.15714331566224354
[2m[36m(func pid=120650)[0m f1_weighted: 0.20958267873606035
[2m[36m(func pid=120650)[0m f1_per_class: [0.098, 0.421, 0.0, 0.143, 0.103, 0.24, 0.155, 0.314, 0.072, 0.025]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2653917910447761
[2m[36m(func pid=118433)[0m top5: 0.773320895522388
[2m[36m(func pid=118433)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=118433)[0m f1_macro: 0.20419465307146253
[2m[36m(func pid=118433)[0m f1_weighted: 0.28808985346803206
[2m[36m(func pid=118433)[0m f1_per_class: [0.174, 0.373, 0.0, 0.264, 0.091, 0.026, 0.372, 0.374, 0.28, 0.086]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.38199626865671643
[2m[36m(func pid=117778)[0m top5: 0.8694029850746269
[2m[36m(func pid=117778)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=117778)[0m f1_macro: 0.3331801819191432
[2m[36m(func pid=117778)[0m f1_weighted: 0.4057227574468431
[2m[36m(func pid=117778)[0m f1_per_class: [0.234, 0.497, 0.205, 0.397, 0.108, 0.409, 0.393, 0.435, 0.287, 0.368]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.1989 | Steps: 4 | Val loss: 2.0360 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2521 | Steps: 4 | Val loss: 4.5188 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8827 | Steps: 4 | Val loss: 1.7442 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:19:07 (running for 00:45:33.22)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.696 |      0.333 |                   53 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.889 |      0.204 |                   52 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.199 |      0.23  |                   45 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.23274253731343283
[2m[36m(func pid=118433)[0m top5: 0.7168843283582089
[2m[36m(func pid=118433)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=118433)[0m f1_macro: 0.18223644350877427
[2m[36m(func pid=118433)[0m f1_weighted: 0.19205244660512058
[2m[36m(func pid=118433)[0m f1_per_class: [0.152, 0.478, 0.112, 0.082, 0.068, 0.016, 0.175, 0.298, 0.294, 0.148]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m top1: 0.24860074626865672
[2m[36m(func pid=120650)[0m top5: 0.7593283582089553
[2m[36m(func pid=120650)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=120650)[0m f1_macro: 0.22979973869192594
[2m[36m(func pid=120650)[0m f1_weighted: 0.23540081212908925
[2m[36m(func pid=120650)[0m f1_per_class: [0.128, 0.381, 0.6, 0.0, 0.091, 0.176, 0.388, 0.408, 0.093, 0.033]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.38899253731343286
[2m[36m(func pid=117778)[0m top5: 0.8694029850746269
[2m[36m(func pid=117778)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=117778)[0m f1_macro: 0.3296674649170898
[2m[36m(func pid=117778)[0m f1_weighted: 0.4223232801396666
[2m[36m(func pid=117778)[0m f1_per_class: [0.237, 0.483, 0.157, 0.39, 0.099, 0.38, 0.478, 0.41, 0.307, 0.356]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.1250 | Steps: 4 | Val loss: 2.1064 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6197 | Steps: 4 | Val loss: 4.4479 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6709 | Steps: 4 | Val loss: 1.6573 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:19:13 (running for 00:45:38.94)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.883 |      0.33  |                   54 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.252 |      0.182 |                   53 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.125 |      0.2   |                   46 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.23041044776119404
[2m[36m(func pid=120650)[0m top5: 0.71875
[2m[36m(func pid=120650)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=120650)[0m f1_macro: 0.19983252122867984
[2m[36m(func pid=120650)[0m f1_weighted: 0.2425854950618809
[2m[36m(func pid=120650)[0m f1_per_class: [0.197, 0.236, 0.5, 0.0, 0.057, 0.235, 0.537, 0.047, 0.117, 0.072]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.23694029850746268
[2m[36m(func pid=118433)[0m top5: 0.7285447761194029
[2m[36m(func pid=118433)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=118433)[0m f1_macro: 0.1875212233073242
[2m[36m(func pid=118433)[0m f1_weighted: 0.20375188413206236
[2m[36m(func pid=118433)[0m f1_per_class: [0.146, 0.49, 0.083, 0.01, 0.042, 0.032, 0.265, 0.308, 0.305, 0.194]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4244402985074627
[2m[36m(func pid=117778)[0m top5: 0.8833955223880597
[2m[36m(func pid=117778)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=117778)[0m f1_macro: 0.36020352978577747
[2m[36m(func pid=117778)[0m f1_weighted: 0.4561253523651971
[2m[36m(func pid=117778)[0m f1_per_class: [0.302, 0.486, 0.245, 0.472, 0.093, 0.374, 0.504, 0.416, 0.328, 0.382]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.9291 | Steps: 4 | Val loss: 2.1330 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.1502 | Steps: 4 | Val loss: 3.4806 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7611 | Steps: 4 | Val loss: 1.5957 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 15:19:19 (running for 00:45:44.71)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.671 |      0.36  |                   55 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.62  |      0.188 |                   54 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.929 |      0.161 |                   47 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.19076492537313433
[2m[36m(func pid=120650)[0m top5: 0.7625932835820896
[2m[36m(func pid=120650)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=120650)[0m f1_macro: 0.16131987947465048
[2m[36m(func pid=120650)[0m f1_weighted: 0.21936796814323667
[2m[36m(func pid=120650)[0m f1_per_class: [0.196, 0.206, 0.0, 0.0, 0.053, 0.083, 0.472, 0.438, 0.093, 0.072]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.3530783582089552
[2m[36m(func pid=118433)[0m top5: 0.7705223880597015
[2m[36m(func pid=118433)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=118433)[0m f1_macro: 0.2657549292656983
[2m[36m(func pid=118433)[0m f1_weighted: 0.32081301283800423
[2m[36m(func pid=118433)[0m f1_per_class: [0.192, 0.507, 0.087, 0.007, 0.112, 0.141, 0.58, 0.429, 0.255, 0.348]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.42024253731343286
[2m[36m(func pid=117778)[0m top5: 0.8997201492537313
[2m[36m(func pid=117778)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=117778)[0m f1_macro: 0.3693002341082239
[2m[36m(func pid=117778)[0m f1_weighted: 0.4464857191971336
[2m[36m(func pid=117778)[0m f1_per_class: [0.369, 0.488, 0.273, 0.483, 0.1, 0.393, 0.443, 0.443, 0.326, 0.375]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1748 | Steps: 4 | Val loss: 2.2652 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4923 | Steps: 4 | Val loss: 2.8202 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6899 | Steps: 4 | Val loss: 1.5823 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:19:24 (running for 00:45:50.26)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.761 |      0.369 |                   56 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  1.15  |      0.266 |                   55 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.175 |      0.139 |                   48 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.18516791044776118
[2m[36m(func pid=120650)[0m top5: 0.757929104477612
[2m[36m(func pid=120650)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=120650)[0m f1_macro: 0.13912935666412501
[2m[36m(func pid=120650)[0m f1_weighted: 0.22691965523092997
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.441, 0.0, 0.071, 0.047, 0.049, 0.347, 0.319, 0.091, 0.025]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.34701492537313433
[2m[36m(func pid=118433)[0m top5: 0.847481343283582
[2m[36m(func pid=118433)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=118433)[0m f1_macro: 0.3161754202236503
[2m[36m(func pid=118433)[0m f1_weighted: 0.3306529528060531
[2m[36m(func pid=118433)[0m f1_per_class: [0.442, 0.408, 0.195, 0.12, 0.126, 0.217, 0.516, 0.353, 0.349, 0.435]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4281716417910448
[2m[36m(func pid=117778)[0m top5: 0.9113805970149254
[2m[36m(func pid=117778)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=117778)[0m f1_macro: 0.385313138815915
[2m[36m(func pid=117778)[0m f1_weighted: 0.4488264218862403
[2m[36m(func pid=117778)[0m f1_per_class: [0.422, 0.507, 0.255, 0.465, 0.091, 0.423, 0.428, 0.497, 0.345, 0.42]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.9802 | Steps: 4 | Val loss: 2.7669 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7390 | Steps: 4 | Val loss: 2.8572 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7841 | Steps: 4 | Val loss: 1.5882 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 15:19:30 (running for 00:45:55.90)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.69  |      0.385 |                   57 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.492 |      0.316 |                   56 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.98  |      0.113 |                   49 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.18703358208955223
[2m[36m(func pid=120650)[0m top5: 0.6786380597014925
[2m[36m(func pid=120650)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=120650)[0m f1_macro: 0.11274653165637467
[2m[36m(func pid=120650)[0m f1_weighted: 0.2097084059476037
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.507, 0.0, 0.291, 0.045, 0.014, 0.115, 0.015, 0.116, 0.024]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.291044776119403
[2m[36m(func pid=118433)[0m top5: 0.8614738805970149
[2m[36m(func pid=118433)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=118433)[0m f1_macro: 0.25467653859620754
[2m[36m(func pid=118433)[0m f1_weighted: 0.3222549873514689
[2m[36m(func pid=118433)[0m f1_per_class: [0.328, 0.194, 0.133, 0.322, 0.085, 0.142, 0.488, 0.265, 0.328, 0.262]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4458955223880597
[2m[36m(func pid=117778)[0m top5: 0.9146455223880597
[2m[36m(func pid=117778)[0m f1_micro: 0.44589552238805963
[2m[36m(func pid=117778)[0m f1_macro: 0.3892497301060736
[2m[36m(func pid=117778)[0m f1_weighted: 0.47316020224220295
[2m[36m(func pid=117778)[0m f1_per_class: [0.42, 0.505, 0.264, 0.449, 0.094, 0.397, 0.535, 0.502, 0.365, 0.362]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2559 | Steps: 4 | Val loss: 2.1370 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3735 | Steps: 4 | Val loss: 3.6330 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5738 | Steps: 4 | Val loss: 1.5910 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=120650)[0m top1: 0.324160447761194
[2m[36m(func pid=120650)[0m top5: 0.8013059701492538
[2m[36m(func pid=120650)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=120650)[0m f1_macro: 0.18903762763332027
[2m[36m(func pid=120650)[0m f1_weighted: 0.33554561866652177
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.494, 0.0, 0.403, 0.055, 0.303, 0.282, 0.291, 0.062, 0.0]
[2m[36m(func pid=120650)[0m 
== Status ==
Current time: 2024-01-07 15:19:36 (running for 00:46:01.67)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.784 |      0.389 |                   58 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.739 |      0.255 |                   57 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.256 |      0.189 |                   50 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=118433)[0m top1: 0.29151119402985076
[2m[36m(func pid=118433)[0m top5: 0.8423507462686567
[2m[36m(func pid=118433)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=118433)[0m f1_macro: 0.2333362353217975
[2m[36m(func pid=118433)[0m f1_weighted: 0.3192661241719576
[2m[36m(func pid=118433)[0m f1_per_class: [0.277, 0.087, 0.214, 0.496, 0.081, 0.109, 0.396, 0.33, 0.235, 0.109]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4519589552238806
[2m[36m(func pid=117778)[0m top5: 0.9113805970149254
[2m[36m(func pid=117778)[0m f1_micro: 0.4519589552238806
[2m[36m(func pid=117778)[0m f1_macro: 0.37543237354956976
[2m[36m(func pid=117778)[0m f1_weighted: 0.4809003961712492
[2m[36m(func pid=117778)[0m f1_per_class: [0.383, 0.507, 0.267, 0.469, 0.115, 0.362, 0.57, 0.471, 0.317, 0.294]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0379 | Steps: 4 | Val loss: 2.0944 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4743 | Steps: 4 | Val loss: 2.8605 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7015 | Steps: 4 | Val loss: 1.4879 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 15:19:41 (running for 00:46:07.54)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.574 |      0.375 |                   59 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.373 |      0.233 |                   58 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.038 |      0.176 |                   51 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.3246268656716418
[2m[36m(func pid=120650)[0m top5: 0.8278917910447762
[2m[36m(func pid=120650)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=120650)[0m f1_macro: 0.17614562479579274
[2m[36m(func pid=120650)[0m f1_weighted: 0.28202338974558283
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.342, 0.0, 0.565, 0.086, 0.295, 0.018, 0.434, 0.021, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.35494402985074625
[2m[36m(func pid=118433)[0m top5: 0.8913246268656716
[2m[36m(func pid=118433)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=118433)[0m f1_macro: 0.2872512366115677
[2m[36m(func pid=118433)[0m f1_weighted: 0.37967494656111006
[2m[36m(func pid=118433)[0m f1_per_class: [0.273, 0.192, 0.358, 0.512, 0.068, 0.173, 0.477, 0.422, 0.221, 0.177]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4841417910447761
[2m[36m(func pid=117778)[0m top5: 0.9235074626865671
[2m[36m(func pid=117778)[0m f1_micro: 0.4841417910447761
[2m[36m(func pid=117778)[0m f1_macro: 0.4090584014240136
[2m[36m(func pid=117778)[0m f1_weighted: 0.505238074834715
[2m[36m(func pid=117778)[0m f1_per_class: [0.403, 0.512, 0.338, 0.501, 0.137, 0.392, 0.591, 0.498, 0.369, 0.35]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3901 | Steps: 4 | Val loss: 2.0761 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7600 | Steps: 4 | Val loss: 3.0340 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7080 | Steps: 4 | Val loss: 1.4884 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=120650)[0m top1: 0.3101679104477612
[2m[36m(func pid=120650)[0m top5: 0.8423507462686567
[2m[36m(func pid=120650)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=120650)[0m f1_macro: 0.21129718310147014
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=120650)[0m f1_weighted: 0.25857217294493673
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.248, 0.632, 0.576, 0.081, 0.307, 0.0, 0.269, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 15:19:47 (running for 00:46:13.19)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.701 |      0.409 |                   60 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.474 |      0.287 |                   59 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.39  |      0.211 |                   52 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.30223880597014924
[2m[36m(func pid=118433)[0m top5: 0.8512126865671642
[2m[36m(func pid=118433)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=118433)[0m f1_macro: 0.3100764535133541
[2m[36m(func pid=118433)[0m f1_weighted: 0.32010619492342957
[2m[36m(func pid=118433)[0m f1_per_class: [0.391, 0.337, 0.727, 0.263, 0.063, 0.056, 0.463, 0.408, 0.155, 0.238]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.47527985074626866
[2m[36m(func pid=117778)[0m top5: 0.9183768656716418
[2m[36m(func pid=117778)[0m f1_micro: 0.47527985074626866
[2m[36m(func pid=117778)[0m f1_macro: 0.4011434208365869
[2m[36m(func pid=117778)[0m f1_weighted: 0.4934951764107816
[2m[36m(func pid=117778)[0m f1_per_class: [0.386, 0.508, 0.414, 0.478, 0.157, 0.363, 0.596, 0.477, 0.332, 0.302]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.9048 | Steps: 4 | Val loss: 2.1278 | Batch size: 32 | lr: 0.1 | Duration: 3.26s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3233 | Steps: 4 | Val loss: 3.6852 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5748 | Steps: 4 | Val loss: 1.5121 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 15:19:53 (running for 00:46:18.89)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.708 |      0.401 |                   61 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.76  |      0.31  |                   60 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.39  |      0.211 |                   52 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.21082089552238806
[2m[36m(func pid=120650)[0m top5: 0.8306902985074627
[2m[36m(func pid=120650)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=120650)[0m f1_macro: 0.18818630714108792
[2m[36m(func pid=120650)[0m f1_weighted: 0.18507111539995044
[2m[36m(func pid=120650)[0m f1_per_class: [0.101, 0.382, 0.632, 0.317, 0.069, 0.046, 0.006, 0.292, 0.0, 0.037]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.26399253731343286
[2m[36m(func pid=118433)[0m top5: 0.820429104477612
[2m[36m(func pid=118433)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=118433)[0m f1_macro: 0.27853715158383696
[2m[36m(func pid=118433)[0m f1_weighted: 0.28386464064392836
[2m[36m(func pid=118433)[0m f1_per_class: [0.233, 0.403, 0.815, 0.084, 0.051, 0.124, 0.487, 0.241, 0.172, 0.176]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.46828358208955223
[2m[36m(func pid=117778)[0m top5: 0.9155783582089553
[2m[36m(func pid=117778)[0m f1_micro: 0.46828358208955223
[2m[36m(func pid=117778)[0m f1_macro: 0.39594252244823425
[2m[36m(func pid=117778)[0m f1_weighted: 0.4908659833809348
[2m[36m(func pid=117778)[0m f1_per_class: [0.392, 0.509, 0.387, 0.474, 0.152, 0.379, 0.583, 0.508, 0.295, 0.28]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1077 | Steps: 4 | Val loss: 2.4567 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4639 | Steps: 4 | Val loss: 3.7564 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7302 | Steps: 4 | Val loss: 1.5159 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 15:19:58 (running for 00:46:24.57)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.575 |      0.396 |                   62 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.323 |      0.279 |                   61 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.108 |      0.078 |                   54 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.16371268656716417
[2m[36m(func pid=120650)[0m top5: 0.7924440298507462
[2m[36m(func pid=120650)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=120650)[0m f1_macro: 0.07759773216470704
[2m[36m(func pid=120650)[0m f1_weighted: 0.09005353087899448
[2m[36m(func pid=120650)[0m f1_per_class: [0.0, 0.358, 0.0, 0.023, 0.059, 0.0, 0.015, 0.288, 0.0, 0.033]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2453358208955224
[2m[36m(func pid=118433)[0m top5: 0.7868470149253731
[2m[36m(func pid=118433)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=118433)[0m f1_macro: 0.23440419087983733
[2m[36m(func pid=118433)[0m f1_weighted: 0.2699874742385183
[2m[36m(func pid=118433)[0m f1_per_class: [0.201, 0.412, 0.533, 0.092, 0.043, 0.086, 0.464, 0.153, 0.215, 0.145]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.46175373134328357
[2m[36m(func pid=117778)[0m top5: 0.9267723880597015
[2m[36m(func pid=117778)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=117778)[0m f1_macro: 0.40059054139011324
[2m[36m(func pid=117778)[0m f1_weighted: 0.4839325788408895
[2m[36m(func pid=117778)[0m f1_per_class: [0.462, 0.498, 0.393, 0.482, 0.153, 0.408, 0.546, 0.489, 0.305, 0.271]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8966 | Steps: 4 | Val loss: 2.2361 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2203 | Steps: 4 | Val loss: 3.5907 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5921 | Steps: 4 | Val loss: 1.5236 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:20:04 (running for 00:46:30.18)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.73  |      0.401 |                   63 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.464 |      0.234 |                   62 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.108 |      0.078 |                   54 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.21875
[2m[36m(func pid=120650)[0m top5: 0.7961753731343284
[2m[36m(func pid=120650)[0m f1_micro: 0.21875
[2m[36m(func pid=120650)[0m f1_macro: 0.1092821397548698
[2m[36m(func pid=120650)[0m f1_weighted: 0.13836781272874682
[2m[36m(func pid=120650)[0m f1_per_class: [0.037, 0.425, 0.0, 0.013, 0.073, 0.031, 0.12, 0.315, 0.077, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.30363805970149255
[2m[36m(func pid=118433)[0m top5: 0.7630597014925373
[2m[36m(func pid=118433)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=118433)[0m f1_macro: 0.24120624897395238
[2m[36m(func pid=118433)[0m f1_weighted: 0.30382784362786663
[2m[36m(func pid=118433)[0m f1_per_class: [0.215, 0.463, 0.162, 0.045, 0.04, 0.091, 0.557, 0.282, 0.323, 0.235]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4631529850746269
[2m[36m(func pid=117778)[0m top5: 0.9216417910447762
[2m[36m(func pid=117778)[0m f1_micro: 0.4631529850746269
[2m[36m(func pid=117778)[0m f1_macro: 0.41397912241385415
[2m[36m(func pid=117778)[0m f1_weighted: 0.4834051975047197
[2m[36m(func pid=117778)[0m f1_per_class: [0.448, 0.486, 0.545, 0.48, 0.166, 0.386, 0.558, 0.472, 0.354, 0.246]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.0531 | Steps: 4 | Val loss: 2.1512 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6534 | Steps: 4 | Val loss: 4.1506 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6501 | Steps: 4 | Val loss: 1.6481 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:20:10 (running for 00:46:35.74)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.592 |      0.414 |                   64 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.22  |      0.241 |                   63 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.897 |      0.109 |                   55 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.28031716417910446
[2m[36m(func pid=120650)[0m top5: 0.777518656716418
[2m[36m(func pid=120650)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=120650)[0m f1_macro: 0.15001760678155604
[2m[36m(func pid=120650)[0m f1_weighted: 0.21976095704923482
[2m[36m(func pid=120650)[0m f1_per_class: [0.055, 0.452, 0.0, 0.029, 0.022, 0.031, 0.329, 0.48, 0.104, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.23134328358208955
[2m[36m(func pid=118433)[0m top5: 0.7667910447761194
[2m[36m(func pid=118433)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=118433)[0m f1_macro: 0.20151974212046686
[2m[36m(func pid=118433)[0m f1_weighted: 0.2497813368059013
[2m[36m(func pid=118433)[0m f1_per_class: [0.228, 0.442, 0.186, 0.041, 0.029, 0.095, 0.392, 0.425, 0.079, 0.097]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.41511194029850745
[2m[36m(func pid=117778)[0m top5: 0.914179104477612
[2m[36m(func pid=117778)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=117778)[0m f1_macro: 0.3707676317237022
[2m[36m(func pid=117778)[0m f1_weighted: 0.4385095417053552
[2m[36m(func pid=117778)[0m f1_per_class: [0.423, 0.489, 0.312, 0.431, 0.108, 0.369, 0.46, 0.491, 0.354, 0.269]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.8582 | Steps: 4 | Val loss: 2.0417 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7188 | Steps: 4 | Val loss: 1.6663 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4874 | Steps: 4 | Val loss: 5.8387 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:20:15 (running for 00:46:41.37)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.65  |      0.371 |                   65 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.653 |      0.202 |                   64 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.053 |      0.15  |                   56 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.2490671641791045
[2m[36m(func pid=120650)[0m top5: 0.7947761194029851
[2m[36m(func pid=120650)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=120650)[0m f1_macro: 0.15911901323432723
[2m[36m(func pid=120650)[0m f1_weighted: 0.20568652695211975
[2m[36m(func pid=120650)[0m f1_per_class: [0.119, 0.514, 0.0, 0.099, 0.026, 0.132, 0.142, 0.451, 0.108, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4085820895522388
[2m[36m(func pid=117778)[0m top5: 0.9127798507462687
[2m[36m(func pid=117778)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=117778)[0m f1_macro: 0.3764389770812464
[2m[36m(func pid=117778)[0m f1_weighted: 0.42977734288055613
[2m[36m(func pid=117778)[0m f1_per_class: [0.434, 0.481, 0.364, 0.441, 0.093, 0.369, 0.424, 0.486, 0.358, 0.316]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.1599813432835821
[2m[36m(func pid=118433)[0m top5: 0.8059701492537313
[2m[36m(func pid=118433)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=118433)[0m f1_macro: 0.15516915094272213
[2m[36m(func pid=118433)[0m f1_weighted: 0.19520444195864795
[2m[36m(func pid=118433)[0m f1_per_class: [0.176, 0.183, 0.0, 0.061, 0.063, 0.124, 0.332, 0.416, 0.15, 0.048]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.9543 | Steps: 4 | Val loss: 1.8008 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8594 | Steps: 4 | Val loss: 7.5532 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4138 | Steps: 4 | Val loss: 1.6803 | Batch size: 32 | lr: 0.001 | Duration: 3.34s
== Status ==
Current time: 2024-01-07 15:20:21 (running for 00:46:46.97)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.719 |      0.376 |                   66 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.487 |      0.155 |                   65 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.858 |      0.159 |                   57 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.39925373134328357
[2m[36m(func pid=120650)[0m top5: 0.8591417910447762
[2m[36m(func pid=120650)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=120650)[0m f1_macro: 0.27037142220827437
[2m[36m(func pid=120650)[0m f1_weighted: 0.3982906335988709
[2m[36m(func pid=120650)[0m f1_per_class: [0.183, 0.545, 0.209, 0.331, 0.082, 0.332, 0.466, 0.455, 0.101, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.40625
[2m[36m(func pid=117778)[0m top5: 0.9057835820895522
[2m[36m(func pid=117778)[0m f1_micro: 0.40625
[2m[36m(func pid=117778)[0m f1_macro: 0.3792010740282944
[2m[36m(func pid=117778)[0m f1_weighted: 0.4240837773236541
[2m[36m(func pid=117778)[0m f1_per_class: [0.431, 0.479, 0.381, 0.446, 0.1, 0.394, 0.395, 0.477, 0.32, 0.368]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.10541044776119403
[2m[36m(func pid=118433)[0m top5: 0.7821828358208955
[2m[36m(func pid=118433)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=118433)[0m f1_macro: 0.0783299527918864
[2m[36m(func pid=118433)[0m f1_weighted: 0.13100983424328386
[2m[36m(func pid=118433)[0m f1_per_class: [0.083, 0.091, 0.0, 0.117, 0.116, 0.008, 0.254, 0.031, 0.043, 0.041]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.7494 | Steps: 4 | Val loss: 1.8758 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4340 | Steps: 4 | Val loss: 4.3502 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7950 | Steps: 4 | Val loss: 1.6468 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=120650)[0m top1: 0.4388992537313433
[2m[36m(func pid=120650)[0m top5: 0.8498134328358209
[2m[36m(func pid=120650)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=120650)[0m f1_macro: 0.23731039670536055
[2m[36m(func pid=120650)[0m f1_weighted: 0.4177876968751329
[2m[36m(func pid=120650)[0m f1_per_class: [0.201, 0.515, 0.202, 0.344, 0.078, 0.401, 0.606, 0.0, 0.026, 0.0]
== Status ==
Current time: 2024-01-07 15:20:27 (running for 00:46:52.69)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.414 |      0.379 |                   67 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.859 |      0.078 |                   66 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.749 |      0.237 |                   59 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.2140858208955224
[2m[36m(func pid=118433)[0m top5: 0.7961753731343284
[2m[36m(func pid=118433)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=118433)[0m f1_macro: 0.14320825005569582
[2m[36m(func pid=118433)[0m f1_weighted: 0.2160883547639083
[2m[36m(func pid=118433)[0m f1_per_class: [0.192, 0.292, 0.0, 0.159, 0.124, 0.014, 0.356, 0.045, 0.15, 0.098]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.43796641791044777
[2m[36m(func pid=117778)[0m top5: 0.8894589552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=117778)[0m f1_macro: 0.39481156017010255
[2m[36m(func pid=117778)[0m f1_weighted: 0.4604320378629774
[2m[36m(func pid=117778)[0m f1_per_class: [0.389, 0.51, 0.429, 0.491, 0.119, 0.401, 0.458, 0.486, 0.288, 0.378]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.7595 | Steps: 4 | Val loss: 2.1737 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2389 | Steps: 4 | Val loss: 3.6313 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5022 | Steps: 4 | Val loss: 1.6470 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:20:32 (running for 00:46:58.35)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.795 |      0.395 |                   68 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.434 |      0.143 |                   67 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.76  |      0.244 |                   60 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.3414179104477612
[2m[36m(func pid=120650)[0m top5: 0.7831156716417911
[2m[36m(func pid=120650)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=120650)[0m f1_macro: 0.2444658816439301
[2m[36m(func pid=120650)[0m f1_weighted: 0.30275607581772374
[2m[36m(func pid=120650)[0m f1_per_class: [0.06, 0.521, 0.571, 0.466, 0.06, 0.315, 0.08, 0.236, 0.136, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.27472014925373134
[2m[36m(func pid=118433)[0m top5: 0.8526119402985075
[2m[36m(func pid=118433)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=118433)[0m f1_macro: 0.22590193435244318
[2m[36m(func pid=118433)[0m f1_weighted: 0.28583544378513176
[2m[36m(func pid=118433)[0m f1_per_class: [0.087, 0.426, 0.143, 0.113, 0.107, 0.058, 0.451, 0.509, 0.138, 0.229]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.43050373134328357
[2m[36m(func pid=117778)[0m top5: 0.8941231343283582
[2m[36m(func pid=117778)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=117778)[0m f1_macro: 0.3693256746250596
[2m[36m(func pid=117778)[0m f1_weighted: 0.4558994791032938
[2m[36m(func pid=117778)[0m f1_per_class: [0.466, 0.459, 0.48, 0.483, 0.136, 0.289, 0.557, 0.328, 0.23, 0.265]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.9004 | Steps: 4 | Val loss: 2.6839 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7424 | Steps: 4 | Val loss: 4.4980 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5276 | Steps: 4 | Val loss: 1.7383 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 15:20:38 (running for 00:47:04.03)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.502 |      0.369 |                   69 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.239 |      0.226 |                   68 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.9   |      0.145 |                   61 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.3292910447761194
[2m[36m(func pid=120650)[0m top5: 0.6944962686567164
[2m[36m(func pid=120650)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=120650)[0m f1_macro: 0.1451464880075997
[2m[36m(func pid=120650)[0m f1_weighted: 0.267615564437562
[2m[36m(func pid=120650)[0m f1_per_class: [0.036, 0.416, 0.0, 0.574, 0.033, 0.27, 0.0, 0.0, 0.123, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.22761194029850745
[2m[36m(func pid=118433)[0m top5: 0.855410447761194
[2m[36m(func pid=118433)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=118433)[0m f1_macro: 0.262051943369513
[2m[36m(func pid=118433)[0m f1_weighted: 0.2460336658781126
[2m[36m(func pid=118433)[0m f1_per_class: [0.087, 0.451, 0.606, 0.099, 0.137, 0.162, 0.277, 0.472, 0.106, 0.224]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.3987873134328358
[2m[36m(func pid=117778)[0m top5: 0.8894589552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=117778)[0m f1_macro: 0.3324519899394758
[2m[36m(func pid=117778)[0m f1_weighted: 0.4195864826453925
[2m[36m(func pid=117778)[0m f1_per_class: [0.471, 0.474, 0.49, 0.478, 0.127, 0.172, 0.51, 0.169, 0.213, 0.221]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.0370 | Steps: 4 | Val loss: 2.3689 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4890 | Steps: 4 | Val loss: 1.7003 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1694 | Steps: 4 | Val loss: 4.9919 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 15:20:44 (running for 00:47:09.65)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.528 |      0.332 |                   70 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.742 |      0.262 |                   69 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.037 |      0.192 |                   62 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.2789179104477612
[2m[36m(func pid=120650)[0m top5: 0.7602611940298507
[2m[36m(func pid=120650)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=120650)[0m f1_macro: 0.19203239604033528
[2m[36m(func pid=120650)[0m f1_weighted: 0.2337333720567055
[2m[36m(func pid=120650)[0m f1_per_class: [0.154, 0.101, 0.0, 0.504, 0.089, 0.32, 0.0, 0.506, 0.152, 0.094]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4146455223880597
[2m[36m(func pid=117778)[0m top5: 0.894589552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=117778)[0m f1_macro: 0.3387696094635564
[2m[36m(func pid=117778)[0m f1_weighted: 0.42985509577257497
[2m[36m(func pid=117778)[0m f1_per_class: [0.466, 0.492, 0.4, 0.454, 0.116, 0.137, 0.546, 0.275, 0.26, 0.242]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.21082089552238806
[2m[36m(func pid=118433)[0m top5: 0.8488805970149254
[2m[36m(func pid=118433)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=118433)[0m f1_macro: 0.2582719697250807
[2m[36m(func pid=118433)[0m f1_weighted: 0.23598808621945344
[2m[36m(func pid=118433)[0m f1_per_class: [0.355, 0.397, 0.308, 0.35, 0.172, 0.163, 0.033, 0.43, 0.089, 0.286]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9633 | Steps: 4 | Val loss: 2.3161 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6181 | Steps: 4 | Val loss: 3.5177 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5601 | Steps: 4 | Val loss: 1.5841 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
[2m[36m(func pid=120650)[0m top1: 0.3148320895522388
[2m[36m(func pid=120650)[0m top5: 0.820429104477612
[2m[36m(func pid=120650)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=120650)[0m f1_macro: 0.24023492604023816
[2m[36m(func pid=120650)[0m f1_weighted: 0.25384711616785716
[2m[36m(func pid=120650)[0m f1_per_class: [0.259, 0.111, 0.471, 0.563, 0.082, 0.396, 0.0, 0.348, 0.074, 0.099]
== Status ==
Current time: 2024-01-07 15:20:49 (running for 00:47:15.42)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.489 |      0.339 |                   71 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.169 |      0.258 |                   70 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.963 |      0.24  |                   63 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.30830223880597013
[2m[36m(func pid=118433)[0m top5: 0.863339552238806
[2m[36m(func pid=118433)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=118433)[0m f1_macro: 0.3066292054351673
[2m[36m(func pid=118433)[0m f1_weighted: 0.3394280572473562
[2m[36m(func pid=118433)[0m f1_per_class: [0.447, 0.384, 0.178, 0.453, 0.198, 0.256, 0.231, 0.514, 0.123, 0.281]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=117778)[0m top1: 0.45242537313432835
[2m[36m(func pid=117778)[0m top5: 0.9272388059701493
[2m[36m(func pid=117778)[0m f1_micro: 0.45242537313432835
[2m[36m(func pid=117778)[0m f1_macro: 0.3983033851204718
[2m[36m(func pid=117778)[0m f1_weighted: 0.4711905058035111
[2m[36m(func pid=117778)[0m f1_per_class: [0.486, 0.504, 0.248, 0.387, 0.112, 0.375, 0.587, 0.494, 0.402, 0.389]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.4008 | Steps: 4 | Val loss: 1.9838 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3900 | Steps: 4 | Val loss: 1.6534 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3655 | Steps: 4 | Val loss: 2.8964 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 15:20:55 (running for 00:47:21.08)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.56  |      0.398 |                   72 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.618 |      0.307 |                   71 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.401 |      0.255 |                   64 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.291044776119403
[2m[36m(func pid=120650)[0m top5: 0.8652052238805971
[2m[36m(func pid=120650)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=120650)[0m f1_macro: 0.2545213963261214
[2m[36m(func pid=120650)[0m f1_weighted: 0.2646322226910053
[2m[36m(func pid=120650)[0m f1_per_class: [0.254, 0.299, 0.667, 0.504, 0.054, 0.371, 0.0, 0.315, 0.071, 0.012]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.416044776119403
[2m[36m(func pid=117778)[0m top5: 0.929570895522388
[2m[36m(func pid=117778)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=117778)[0m f1_macro: 0.4045850529394187
[2m[36m(func pid=117778)[0m f1_weighted: 0.43752007589949926
[2m[36m(func pid=117778)[0m f1_per_class: [0.52, 0.488, 0.325, 0.372, 0.091, 0.359, 0.485, 0.535, 0.46, 0.412]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.35027985074626866
[2m[36m(func pid=118433)[0m top5: 0.8250932835820896
[2m[36m(func pid=118433)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=118433)[0m f1_macro: 0.29052152001384945
[2m[36m(func pid=118433)[0m f1_weighted: 0.3805671350046409
[2m[36m(func pid=118433)[0m f1_per_class: [0.323, 0.437, 0.131, 0.399, 0.197, 0.217, 0.428, 0.453, 0.131, 0.19]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.8048 | Steps: 4 | Val loss: 1.9843 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5406 | Steps: 4 | Val loss: 1.7361 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1571 | Steps: 4 | Val loss: 3.1981 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 15:21:00 (running for 00:47:26.56)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.39  |      0.405 |                   73 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.366 |      0.291 |                   72 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.805 |      0.226 |                   65 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.33908582089552236
[2m[36m(func pid=120650)[0m top5: 0.8577425373134329
[2m[36m(func pid=120650)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=120650)[0m f1_macro: 0.22618233410054045
[2m[36m(func pid=120650)[0m f1_weighted: 0.34537584700980956
[2m[36m(func pid=120650)[0m f1_per_class: [0.202, 0.449, 0.0, 0.303, 0.047, 0.069, 0.453, 0.514, 0.181, 0.044]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.3931902985074627
[2m[36m(func pid=117778)[0m top5: 0.9253731343283582
[2m[36m(func pid=117778)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=117778)[0m f1_macro: 0.3786354973000341
[2m[36m(func pid=117778)[0m f1_weighted: 0.41639436692548226
[2m[36m(func pid=117778)[0m f1_per_class: [0.495, 0.473, 0.252, 0.358, 0.084, 0.383, 0.444, 0.483, 0.433, 0.381]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.30783582089552236
[2m[36m(func pid=118433)[0m top5: 0.7756529850746269
[2m[36m(func pid=118433)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=118433)[0m f1_macro: 0.24994843054233326
[2m[36m(func pid=118433)[0m f1_weighted: 0.3163288129904558
[2m[36m(func pid=118433)[0m f1_per_class: [0.227, 0.452, 0.2, 0.318, 0.143, 0.198, 0.304, 0.422, 0.118, 0.118]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0564 | Steps: 4 | Val loss: 1.8701 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5628 | Steps: 4 | Val loss: 1.6842 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3021 | Steps: 4 | Val loss: 3.2659 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:21:06 (running for 00:47:32.11)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.347
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.541 |      0.379 |                   74 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.157 |      0.25  |                   73 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.056 |      0.21  |                   66 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.3460820895522388
[2m[36m(func pid=120650)[0m top5: 0.851679104477612
[2m[36m(func pid=120650)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=120650)[0m f1_macro: 0.20990105093274014
[2m[36m(func pid=120650)[0m f1_weighted: 0.3491991325425924
[2m[36m(func pid=120650)[0m f1_per_class: [0.145, 0.456, 0.32, 0.28, 0.081, 0.0, 0.61, 0.0, 0.126, 0.08]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.42024253731343286
[2m[36m(func pid=117778)[0m top5: 0.9197761194029851
[2m[36m(func pid=117778)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=117778)[0m f1_macro: 0.3857501737747917
[2m[36m(func pid=117778)[0m f1_weighted: 0.4447122305909602
[2m[36m(func pid=117778)[0m f1_per_class: [0.449, 0.515, 0.23, 0.382, 0.082, 0.386, 0.487, 0.521, 0.437, 0.368]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=118433)[0m top1: 0.302705223880597
[2m[36m(func pid=118433)[0m top5: 0.7597947761194029
[2m[36m(func pid=118433)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=118433)[0m f1_macro: 0.24913584528199548
[2m[36m(func pid=118433)[0m f1_weighted: 0.32312741283644664
[2m[36m(func pid=118433)[0m f1_per_class: [0.173, 0.421, 0.118, 0.284, 0.123, 0.212, 0.355, 0.498, 0.177, 0.13]
[2m[36m(func pid=118433)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.0421 | Steps: 4 | Val loss: 2.5394 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=118433)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2089 | Steps: 4 | Val loss: 3.2022 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.5223 | Steps: 4 | Val loss: 1.7094 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 15:21:12 (running for 00:47:37.86)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.34775
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.563 |      0.386 |                   75 |
| train_5ae7f_00022 | RUNNING    | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.302 |      0.249 |                   74 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.042 |      0.095 |                   67 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.12639925373134328
[2m[36m(func pid=120650)[0m top5: 0.7486007462686567
[2m[36m(func pid=120650)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=120650)[0m f1_macro: 0.09465083187887371
[2m[36m(func pid=120650)[0m f1_weighted: 0.15170812979118656
[2m[36m(func pid=120650)[0m f1_per_class: [0.047, 0.128, 0.113, 0.065, 0.1, 0.0, 0.353, 0.0, 0.108, 0.032]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=118433)[0m top1: 0.3218283582089552
[2m[36m(func pid=118433)[0m top5: 0.8180970149253731
[2m[36m(func pid=118433)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=118433)[0m f1_macro: 0.2697112503402216
[2m[36m(func pid=118433)[0m f1_weighted: 0.3584729870737341
[2m[36m(func pid=118433)[0m f1_per_class: [0.226, 0.369, 0.087, 0.349, 0.124, 0.269, 0.421, 0.432, 0.247, 0.172]
[2m[36m(func pid=117778)[0m top1: 0.42863805970149255
[2m[36m(func pid=117778)[0m top5: 0.9109141791044776
[2m[36m(func pid=117778)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=117778)[0m f1_macro: 0.36423172613720406
[2m[36m(func pid=117778)[0m f1_weighted: 0.44836282815885825
[2m[36m(func pid=117778)[0m f1_per_class: [0.406, 0.528, 0.277, 0.37, 0.094, 0.297, 0.568, 0.427, 0.356, 0.318]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5277 | Steps: 4 | Val loss: 2.6045 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3816 | Steps: 4 | Val loss: 1.7139 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 15:21:17 (running for 00:47:43.24)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.522 |      0.364 |                   76 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.528 |      0.117 |                   68 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.10820895522388059
[2m[36m(func pid=120650)[0m top5: 0.7140858208955224
[2m[36m(func pid=120650)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=120650)[0m f1_macro: 0.11748774140771687
[2m[36m(func pid=120650)[0m f1_weighted: 0.12400965293312226
[2m[36m(func pid=120650)[0m f1_per_class: [0.068, 0.011, 0.124, 0.202, 0.088, 0.007, 0.116, 0.402, 0.122, 0.036]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4337686567164179
[2m[36m(func pid=117778)[0m top5: 0.9137126865671642
[2m[36m(func pid=117778)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=117778)[0m f1_macro: 0.3741020311829587
[2m[36m(func pid=117778)[0m f1_weighted: 0.4351863086913086
[2m[36m(func pid=117778)[0m f1_per_class: [0.509, 0.509, 0.426, 0.313, 0.114, 0.282, 0.605, 0.327, 0.337, 0.318]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0234 | Steps: 4 | Val loss: 2.3671 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3691 | Steps: 4 | Val loss: 1.6048 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 15:21:23 (running for 00:47:48.79)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.382 |      0.374 |                   77 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.023 |      0.178 |                   69 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.13852611940298507
[2m[36m(func pid=120650)[0m top5: 0.6725746268656716
[2m[36m(func pid=120650)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=120650)[0m f1_macro: 0.17777610310281747
[2m[36m(func pid=120650)[0m f1_weighted: 0.15086037406668298
[2m[36m(func pid=120650)[0m f1_per_class: [0.087, 0.206, 0.444, 0.189, 0.045, 0.2, 0.009, 0.508, 0.09, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.46222014925373134
[2m[36m(func pid=117778)[0m top5: 0.929570895522388
[2m[36m(func pid=117778)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=117778)[0m f1_macro: 0.43066448412236813
[2m[36m(func pid=117778)[0m f1_weighted: 0.4598724969457775
[2m[36m(func pid=117778)[0m f1_per_class: [0.582, 0.522, 0.464, 0.3, 0.126, 0.38, 0.604, 0.475, 0.45, 0.404]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9639 | Steps: 4 | Val loss: 2.3636 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4369 | Steps: 4 | Val loss: 1.5308 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:21:28 (running for 00:47:54.51)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.369 |      0.431 |                   78 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.964 |      0.192 |                   70 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.15858208955223882
[2m[36m(func pid=120650)[0m top5: 0.7360074626865671
[2m[36m(func pid=120650)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=120650)[0m f1_macro: 0.19167168695877607
[2m[36m(func pid=120650)[0m f1_weighted: 0.18081040945371385
[2m[36m(func pid=120650)[0m f1_per_class: [0.096, 0.176, 0.4, 0.198, 0.041, 0.306, 0.076, 0.493, 0.13, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.46828358208955223
[2m[36m(func pid=117778)[0m top5: 0.9347014925373134
[2m[36m(func pid=117778)[0m f1_micro: 0.46828358208955223
[2m[36m(func pid=117778)[0m f1_macro: 0.42637937710691476
[2m[36m(func pid=117778)[0m f1_weighted: 0.47354350955911223
[2m[36m(func pid=117778)[0m f1_per_class: [0.55, 0.532, 0.382, 0.346, 0.127, 0.399, 0.591, 0.5, 0.476, 0.359]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.9101 | Steps: 4 | Val loss: 2.2143 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4154 | Steps: 4 | Val loss: 1.4452 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:21:34 (running for 00:48:00.18)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.437 |      0.426 |                   79 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.91  |      0.229 |                   71 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.23460820895522388
[2m[36m(func pid=120650)[0m top5: 0.8232276119402985
[2m[36m(func pid=120650)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=120650)[0m f1_macro: 0.22946121939706562
[2m[36m(func pid=120650)[0m f1_weighted: 0.2697220295028762
[2m[36m(func pid=120650)[0m f1_per_class: [0.105, 0.158, 0.349, 0.168, 0.064, 0.413, 0.366, 0.51, 0.162, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.49533582089552236
[2m[36m(func pid=117778)[0m top5: 0.9402985074626866
[2m[36m(func pid=117778)[0m f1_micro: 0.49533582089552236
[2m[36m(func pid=117778)[0m f1_macro: 0.444499531294861
[2m[36m(func pid=117778)[0m f1_weighted: 0.5079766089812285
[2m[36m(func pid=117778)[0m f1_per_class: [0.483, 0.557, 0.49, 0.521, 0.163, 0.394, 0.539, 0.482, 0.444, 0.372]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0080 | Steps: 4 | Val loss: 2.2105 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4776 | Steps: 4 | Val loss: 1.4944 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
== Status ==
Current time: 2024-01-07 15:21:40 (running for 00:48:05.88)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.415 |      0.444 |                   80 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.91  |      0.229 |                   71 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.2789179104477612
[2m[36m(func pid=120650)[0m top5: 0.7919776119402985
[2m[36m(func pid=120650)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=120650)[0m f1_macro: 0.2504919390290451
[2m[36m(func pid=120650)[0m f1_weighted: 0.29518540207735516
[2m[36m(func pid=120650)[0m f1_per_class: [0.113, 0.386, 0.75, 0.392, 0.117, 0.386, 0.21, 0.0, 0.151, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.4780783582089552
[2m[36m(func pid=117778)[0m top5: 0.9249067164179104
[2m[36m(func pid=117778)[0m f1_micro: 0.4780783582089552
[2m[36m(func pid=117778)[0m f1_macro: 0.41526218911702373
[2m[36m(func pid=117778)[0m f1_weighted: 0.4921697234772969
[2m[36m(func pid=117778)[0m f1_per_class: [0.441, 0.549, 0.375, 0.521, 0.173, 0.406, 0.502, 0.482, 0.366, 0.339]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.7523 | Steps: 4 | Val loss: 1.9531 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2798 | Steps: 4 | Val loss: 1.5018 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 15:21:46 (running for 00:48:11.69)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.478 |      0.415 |                   81 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  2.008 |      0.25  |                   72 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.38619402985074625
[2m[36m(func pid=120650)[0m top5: 0.8213619402985075
[2m[36m(func pid=120650)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=120650)[0m f1_macro: 0.3210953643033596
[2m[36m(func pid=120650)[0m f1_weighted: 0.3925145482778456
[2m[36m(func pid=120650)[0m f1_per_class: [0.213, 0.545, 0.538, 0.444, 0.111, 0.403, 0.287, 0.504, 0.165, 0.0]
[2m[36m(func pid=120650)[0m 
[2m[36m(func pid=117778)[0m top1: 0.49300373134328357
[2m[36m(func pid=117778)[0m top5: 0.914179104477612
[2m[36m(func pid=117778)[0m f1_micro: 0.49300373134328357
[2m[36m(func pid=117778)[0m f1_macro: 0.40872844100986533
[2m[36m(func pid=117778)[0m f1_weighted: 0.5082114382825548
[2m[36m(func pid=117778)[0m f1_per_class: [0.441, 0.54, 0.347, 0.526, 0.216, 0.364, 0.588, 0.427, 0.326, 0.313]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.7217 | Steps: 4 | Val loss: 2.5471 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3922 | Steps: 4 | Val loss: 1.6366 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=120650)[0m top1: 0.22807835820895522
[2m[36m(func pid=120650)[0m top5: 0.7845149253731343
[2m[36m(func pid=120650)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=120650)[0m f1_macro: 0.2329468288632599
[2m[36m(func pid=120650)[0m f1_weighted: 0.226197587716644
[2m[36m(func pid=120650)[0m f1_per_class: [0.135, 0.411, 0.512, 0.315, 0.095, 0.235, 0.015, 0.425, 0.132, 0.054]
[2m[36m(func pid=120650)[0m 
== Status ==
Current time: 2024-01-07 15:21:51 (running for 00:48:17.41)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3475
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.28  |      0.409 |                   82 |
| train_5ae7f_00023 | RUNNING    | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.722 |      0.233 |                   74 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.4612873134328358
[2m[36m(func pid=117778)[0m top5: 0.8894589552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.4612873134328358
[2m[36m(func pid=117778)[0m f1_macro: 0.37023624493410245
[2m[36m(func pid=117778)[0m f1_weighted: 0.47481754577376445
[2m[36m(func pid=117778)[0m f1_per_class: [0.422, 0.53, 0.329, 0.525, 0.258, 0.26, 0.571, 0.225, 0.233, 0.35]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=120650)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8864 | Steps: 4 | Val loss: 3.1239 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4476 | Steps: 4 | Val loss: 1.4862 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 15:21:57 (running for 00:48:22.99)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.392 |      0.37  |                   83 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120650)[0m top1: 0.11986940298507463
[2m[36m(func pid=120650)[0m top5: 0.7448694029850746
[2m[36m(func pid=120650)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=120650)[0m f1_macro: 0.14604817126980268
[2m[36m(func pid=120650)[0m f1_weighted: 0.12005205126676542
[2m[36m(func pid=120650)[0m f1_per_class: [0.087, 0.067, 0.113, 0.217, 0.211, 0.095, 0.0, 0.486, 0.136, 0.05]
[2m[36m(func pid=117778)[0m top1: 0.5135261194029851
[2m[36m(func pid=117778)[0m top5: 0.917910447761194
[2m[36m(func pid=117778)[0m f1_micro: 0.5135261194029851
[2m[36m(func pid=117778)[0m f1_macro: 0.4471469496941808
[2m[36m(func pid=117778)[0m f1_weighted: 0.5231654835979245
[2m[36m(func pid=117778)[0m f1_per_class: [0.489, 0.562, 0.571, 0.564, 0.231, 0.338, 0.596, 0.383, 0.321, 0.416]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4157 | Steps: 4 | Val loss: 1.4584 | Batch size: 32 | lr: 0.001 | Duration: 3.41s
== Status ==
Current time: 2024-01-07 15:22:03 (running for 00:48:28.83)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.448 |      0.447 |                   84 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.5144589552238806
[2m[36m(func pid=117778)[0m top5: 0.9263059701492538
[2m[36m(func pid=117778)[0m f1_micro: 0.5144589552238806
[2m[36m(func pid=117778)[0m f1_macro: 0.45025273771328844
[2m[36m(func pid=117778)[0m f1_weighted: 0.5272489305385707
[2m[36m(func pid=117778)[0m f1_per_class: [0.54, 0.552, 0.558, 0.584, 0.159, 0.338, 0.582, 0.422, 0.381, 0.385]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3731 | Steps: 4 | Val loss: 1.5108 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 15:22:09 (running for 00:48:34.84)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.416 |      0.45  |                   85 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.4864738805970149
[2m[36m(func pid=117778)[0m top5: 0.9342350746268657
[2m[36m(func pid=117778)[0m f1_micro: 0.4864738805970149
[2m[36m(func pid=117778)[0m f1_macro: 0.4410955821634766
[2m[36m(func pid=117778)[0m f1_weighted: 0.5008545332811051
[2m[36m(func pid=117778)[0m f1_per_class: [0.577, 0.543, 0.462, 0.506, 0.137, 0.375, 0.543, 0.485, 0.416, 0.368]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2637 | Steps: 4 | Val loss: 1.5329 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 15:22:15 (running for 00:48:40.72)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.373 |      0.441 |                   86 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.4832089552238806
[2m[36m(func pid=117778)[0m top5: 0.9393656716417911
[2m[36m(func pid=117778)[0m f1_micro: 0.4832089552238806
[2m[36m(func pid=117778)[0m f1_macro: 0.4369222999343079
[2m[36m(func pid=117778)[0m f1_weighted: 0.4951157346949399
[2m[36m(func pid=117778)[0m f1_per_class: [0.583, 0.544, 0.456, 0.461, 0.149, 0.372, 0.572, 0.445, 0.433, 0.355]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3248 | Steps: 4 | Val loss: 1.6230 | Batch size: 32 | lr: 0.001 | Duration: 3.40s
== Status ==
Current time: 2024-01-07 15:22:20 (running for 00:48:46.44)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.264 |      0.437 |                   87 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.46408582089552236
[2m[36m(func pid=117778)[0m top5: 0.9221082089552238
[2m[36m(func pid=117778)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=117778)[0m f1_macro: 0.4102537708214201
[2m[36m(func pid=117778)[0m f1_weighted: 0.4766618057835856
[2m[36m(func pid=117778)[0m f1_per_class: [0.544, 0.544, 0.441, 0.456, 0.136, 0.365, 0.542, 0.376, 0.365, 0.333]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3831 | Steps: 4 | Val loss: 1.5852 | Batch size: 32 | lr: 0.001 | Duration: 3.34s
== Status ==
Current time: 2024-01-07 15:22:27 (running for 00:48:52.59)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.325 |      0.41  |                   88 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.4822761194029851
[2m[36m(func pid=117778)[0m top5: 0.9207089552238806
[2m[36m(func pid=117778)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=117778)[0m f1_macro: 0.4165109743620781
[2m[36m(func pid=117778)[0m f1_weighted: 0.5014378049527344
[2m[36m(func pid=117778)[0m f1_per_class: [0.512, 0.554, 0.338, 0.502, 0.123, 0.376, 0.562, 0.443, 0.363, 0.393]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2591 | Steps: 4 | Val loss: 1.5362 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 15:22:32 (running for 00:48:58.46)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.383 |      0.417 |                   89 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.480410447761194
[2m[36m(func pid=117778)[0m top5: 0.9244402985074627
[2m[36m(func pid=117778)[0m f1_micro: 0.480410447761194
[2m[36m(func pid=117778)[0m f1_macro: 0.4208109294365735
[2m[36m(func pid=117778)[0m f1_weighted: 0.49686464264708496
[2m[36m(func pid=117778)[0m f1_per_class: [0.492, 0.549, 0.321, 0.537, 0.136, 0.381, 0.502, 0.494, 0.405, 0.393]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3768 | Steps: 4 | Val loss: 1.5869 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 15:22:38 (running for 00:49:04.34)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.259 |      0.421 |                   90 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.46408582089552236
[2m[36m(func pid=117778)[0m top5: 0.9249067164179104
[2m[36m(func pid=117778)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=117778)[0m f1_macro: 0.42193765569583663
[2m[36m(func pid=117778)[0m f1_weighted: 0.4742558822039582
[2m[36m(func pid=117778)[0m f1_per_class: [0.56, 0.542, 0.358, 0.507, 0.146, 0.385, 0.458, 0.454, 0.404, 0.404]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.4226 | Steps: 4 | Val loss: 1.6117 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 15:22:44 (running for 00:49:10.10)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.377 |      0.422 |                   91 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.4594216417910448
[2m[36m(func pid=117778)[0m top5: 0.9197761194029851
[2m[36m(func pid=117778)[0m f1_micro: 0.4594216417910448
[2m[36m(func pid=117778)[0m f1_macro: 0.41573683801787065
[2m[36m(func pid=117778)[0m f1_weighted: 0.4693052722822753
[2m[36m(func pid=117778)[0m f1_per_class: [0.473, 0.541, 0.366, 0.492, 0.142, 0.358, 0.465, 0.487, 0.404, 0.429]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.5213 | Steps: 4 | Val loss: 1.6051 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 15:22:50 (running for 00:49:16.00)
Memory usage on this node: 17.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.423 |      0.416 |                   92 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.46548507462686567
[2m[36m(func pid=117778)[0m top5: 0.9118470149253731
[2m[36m(func pid=117778)[0m f1_micro: 0.4654850746268657
[2m[36m(func pid=117778)[0m f1_macro: 0.4064899505439935
[2m[36m(func pid=117778)[0m f1_weighted: 0.4794694113352639
[2m[36m(func pid=117778)[0m f1_per_class: [0.418, 0.541, 0.289, 0.502, 0.14, 0.34, 0.504, 0.464, 0.424, 0.444]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.4570 | Steps: 4 | Val loss: 1.5763 | Batch size: 32 | lr: 0.001 | Duration: 3.36s
== Status ==
Current time: 2024-01-07 15:22:56 (running for 00:49:22.30)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.521 |      0.406 |                   93 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.47154850746268656
[2m[36m(func pid=117778)[0m top5: 0.9183768656716418
[2m[36m(func pid=117778)[0m f1_micro: 0.47154850746268656
[2m[36m(func pid=117778)[0m f1_macro: 0.4098226760118327
[2m[36m(func pid=117778)[0m f1_weighted: 0.49298991857480934
[2m[36m(func pid=117778)[0m f1_per_class: [0.4, 0.558, 0.2, 0.482, 0.124, 0.393, 0.528, 0.511, 0.447, 0.454]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2849 | Steps: 4 | Val loss: 1.6350 | Batch size: 32 | lr: 0.001 | Duration: 3.41s
== Status ==
Current time: 2024-01-07 15:23:02 (running for 00:49:28.19)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.457 |      0.41  |                   94 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.45149253731343286
[2m[36m(func pid=117778)[0m top5: 0.9277052238805971
[2m[36m(func pid=117778)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=117778)[0m f1_macro: 0.38648255256389674
[2m[36m(func pid=117778)[0m f1_weighted: 0.47952902065432396
[2m[36m(func pid=117778)[0m f1_per_class: [0.465, 0.553, 0.127, 0.415, 0.132, 0.395, 0.57, 0.435, 0.384, 0.39]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3514 | Steps: 4 | Val loss: 1.6226 | Batch size: 32 | lr: 0.001 | Duration: 3.35s
== Status ==
Current time: 2024-01-07 15:23:08 (running for 00:49:34.29)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.285 |      0.386 |                   95 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.4594216417910448
[2m[36m(func pid=117778)[0m top5: 0.9263059701492538
[2m[36m(func pid=117778)[0m f1_micro: 0.4594216417910448
[2m[36m(func pid=117778)[0m f1_macro: 0.39419276651721014
[2m[36m(func pid=117778)[0m f1_weighted: 0.49233832657677845
[2m[36m(func pid=117778)[0m f1_per_class: [0.46, 0.529, 0.141, 0.484, 0.13, 0.402, 0.554, 0.471, 0.365, 0.407]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.5009 | Steps: 4 | Val loss: 1.6715 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 15:23:14 (running for 00:49:40.27)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.351 |      0.394 |                   96 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.42630597014925375
[2m[36m(func pid=117778)[0m top5: 0.9211753731343284
[2m[36m(func pid=117778)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=117778)[0m f1_macro: 0.3822940946485888
[2m[36m(func pid=117778)[0m f1_weighted: 0.45383997202710386
[2m[36m(func pid=117778)[0m f1_per_class: [0.431, 0.477, 0.142, 0.486, 0.131, 0.401, 0.445, 0.512, 0.387, 0.412]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2541 | Steps: 4 | Val loss: 1.7211 | Batch size: 32 | lr: 0.001 | Duration: 3.35s
== Status ==
Current time: 2024-01-07 15:23:20 (running for 00:49:46.10)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.501 |      0.382 |                   97 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.43097014925373134
[2m[36m(func pid=117778)[0m top5: 0.9155783582089553
[2m[36m(func pid=117778)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=117778)[0m f1_macro: 0.37595028012058734
[2m[36m(func pid=117778)[0m f1_weighted: 0.46471563220441336
[2m[36m(func pid=117778)[0m f1_per_class: [0.322, 0.531, 0.126, 0.46, 0.151, 0.382, 0.487, 0.523, 0.387, 0.389]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3884 | Steps: 4 | Val loss: 1.6546 | Batch size: 32 | lr: 0.001 | Duration: 3.39s
== Status ==
Current time: 2024-01-07 15:23:26 (running for 00:49:52.04)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.254 |      0.376 |                   98 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=117778)[0m top1: 0.46361940298507465
[2m[36m(func pid=117778)[0m top5: 0.9151119402985075
[2m[36m(func pid=117778)[0m f1_micro: 0.46361940298507465
[2m[36m(func pid=117778)[0m f1_macro: 0.3874663262047447
[2m[36m(func pid=117778)[0m f1_weighted: 0.49256620205909624
[2m[36m(func pid=117778)[0m f1_per_class: [0.388, 0.548, 0.208, 0.503, 0.139, 0.365, 0.548, 0.461, 0.359, 0.356]
[2m[36m(func pid=117778)[0m 
[2m[36m(func pid=117778)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3236 | Steps: 4 | Val loss: 2.0400 | Batch size: 32 | lr: 0.001 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 15:23:32 (running for 00:49:58.03)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00021 | RUNNING    | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.388 |      0.387 |                   99 |
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 15:23:33 (running for 00:49:59.07)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.34724999999999995
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_5ae7f_00000 | TERMINATED | 192.168.7.53:13050  | 0.0001 |       0.99 |         0      |  0.433 |      0.362 |                  100 |
| train_5ae7f_00001 | TERMINATED | 192.168.7.53:13435  | 0.001  |       0.99 |         0      |  0.715 |      0.235 |                  100 |
| train_5ae7f_00002 | TERMINATED | 192.168.7.53:13861  | 0.01   |       0.99 |         0      |  1.327 |      0.277 |                  100 |
| train_5ae7f_00003 | TERMINATED | 192.168.7.53:14288  | 0.1    |       0.99 |         0      |  2.931 |      0.095 |                   75 |
| train_5ae7f_00004 | TERMINATED | 192.168.7.53:32295  | 0.0001 |       0.9  |         0      |  2.393 |      0.273 |                   75 |
| train_5ae7f_00005 | TERMINATED | 192.168.7.53:37733  | 0.001  |       0.9  |         0      |  0.401 |      0.395 |                  100 |
| train_5ae7f_00006 | TERMINATED | 192.168.7.53:37804  | 0.01   |       0.9  |         0      |  0.292 |      0.312 |                   75 |
| train_5ae7f_00007 | TERMINATED | 192.168.7.53:37854  | 0.1    |       0.9  |         0      |  1.894 |      0.298 |                   75 |
| train_5ae7f_00008 | TERMINATED | 192.168.7.53:50960  | 0.0001 |       0.99 |         0.0001 |  0.598 |      0.396 |                  100 |
| train_5ae7f_00009 | TERMINATED | 192.168.7.53:56441  | 0.001  |       0.99 |         0.0001 |  0.88  |      0.251 |                   75 |
| train_5ae7f_00010 | TERMINATED | 192.168.7.53:57031  | 0.01   |       0.99 |         0.0001 |  2.176 |      0.347 |                   75 |
| train_5ae7f_00011 | TERMINATED | 192.168.7.53:62931  | 0.1    |       0.99 |         0.0001 |  2.918 |      0.099 |                   75 |
| train_5ae7f_00012 | TERMINATED | 192.168.7.53:75166  | 0.0001 |       0.9  |         0.0001 |  2.251 |      0.285 |                   75 |
| train_5ae7f_00013 | TERMINATED | 192.168.7.53:75223  | 0.001  |       0.9  |         0.0001 |  0.285 |      0.319 |                  100 |
| train_5ae7f_00014 | TERMINATED | 192.168.7.53:75300  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.193 |                   75 |
| train_5ae7f_00015 | TERMINATED | 192.168.7.53:81458  | 0.1    |       0.9  |         0.0001 |  1.932 |      0.291 |                   75 |
| train_5ae7f_00016 | TERMINATED | 192.168.7.53:93317  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.424 |                  100 |
| train_5ae7f_00017 | TERMINATED | 192.168.7.53:93934  | 0.001  |       0.99 |         1e-05  |  1.477 |      0.212 |                   75 |
| train_5ae7f_00018 | TERMINATED | 192.168.7.53:99348  | 0.01   |       0.99 |         1e-05  |  2.101 |      0.202 |                   75 |
| train_5ae7f_00019 | TERMINATED | 192.168.7.53:102012 | 0.1    |       0.99 |         1e-05  |  2.609 |      0.077 |                   75 |
| train_5ae7f_00020 | TERMINATED | 192.168.7.53:112174 | 0.0001 |       0.9  |         1e-05  |  2.264 |      0.281 |                   75 |
| train_5ae7f_00021 | TERMINATED | 192.168.7.53:117778 | 0.001  |       0.9  |         1e-05  |  0.324 |      0.282 |                  100 |
| train_5ae7f_00022 | TERMINATED | 192.168.7.53:118433 | 0.01   |       0.9  |         1e-05  |  0.209 |      0.27  |                   75 |
| train_5ae7f_00023 | TERMINATED | 192.168.7.53:120650 | 0.1    |       0.9  |         1e-05  |  1.886 |      0.146 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 15:23:33,499	INFO tune.py:798 -- Total run time: 3000.18 seconds (2999.05 seconds for the tuning loop).
[2m[36m(func pid=117778)[0m top1: 0.345615671641791
[2m[36m(func pid=117778)[0m top5: 0.8619402985074627
[2m[36m(func pid=117778)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=117778)[0m f1_macro: 0.28197599715691757
[2m[36m(func pid=117778)[0m f1_weighted: 0.35610807718914916
[2m[36m(func pid=117778)[0m f1_per_class: [0.413, 0.512, 0.321, 0.397, 0.12, 0.229, 0.353, 0.101, 0.205, 0.169]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341350.1 ON aap04 CANCELLED AT 2024-01-07T15:23:41 ***
srun: error: aap04: task 0: Exited with exit code 1

IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 03:18:39,311	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 03:18:39,311	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 03:18:41,852	SUCC scripts.py:747 -- --------------------
2024-01-07 03:18:41,852	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 03:18:41,852	SUCC scripts.py:749 -- --------------------
2024-01-07 03:18:41,852	INFO scripts.py:751 -- Next steps
2024-01-07 03:18:41,853	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 03:18:41,853	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 03:18:41,853	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 03:18:41,853	INFO scripts.py:773 -- import ray
2024-01-07 03:18:41,853	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 03:18:41,853	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 03:18:41,853	INFO scripts.py:791 --   ray status
2024-01-07 03:18:41,853	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 03:18:41,853	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 03:18:41,853	INFO scripts.py:810 --   ray stop
2024-01-07 03:18:41,854	INFO scripts.py:891 -- --block
2024-01-07 03:18:41,854	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 03:18:41,854	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              10296336882946318977
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f047a8f7160>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         imagenet
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [5 5 5 5 5 5 5 5 5 5]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.84
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [5 5 5 5 5 5 5 5 5 5]
Done!
Using ImageNet weights

Supervised model resnet18 with imagenet weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 03:19:24,061	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 03:19:24,072	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 03:19:47,776	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 03:19:47 (running for 00:00:22.76)
Memory usage on this node: 13.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |
| train_2d480_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_2d480_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_2d480_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157245)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=157245)[0m Configuration completed!
[2m[36m(func pid=157245)[0m New optimizer parameters:
[2m[36m(func pid=157245)[0m SGD (
[2m[36m(func pid=157245)[0m Parameter Group 0
[2m[36m(func pid=157245)[0m     dampening: 0
[2m[36m(func pid=157245)[0m     differentiable: False
[2m[36m(func pid=157245)[0m     foreach: None
[2m[36m(func pid=157245)[0m     lr: 0.0001
[2m[36m(func pid=157245)[0m     maximize: False
[2m[36m(func pid=157245)[0m     momentum: 0.99
[2m[36m(func pid=157245)[0m     nesterov: False
[2m[36m(func pid=157245)[0m     weight_decay: 0
[2m[36m(func pid=157245)[0m )
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1136 | Steps: 2 | Val loss: 2.5142 | Batch size: 32 | lr: 0.0001 | Duration: 5.13s
[2m[36m(func pid=157245)[0m top1: 0.06716417910447761
[2m[36m(func pid=157245)[0m top5: 0.48507462686567165
[2m[36m(func pid=157245)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=157245)[0m f1_macro: 0.04033914958544452
[2m[36m(func pid=157245)[0m f1_weighted: 0.03826545049230524
[2m[36m(func pid=157245)[0m f1_per_class: [0.124, 0.01, 0.0, 0.088, 0.0, 0.019, 0.0, 0.105, 0.022, 0.035]
== Status ==
Current time: 2024-01-07 03:19:57 (running for 00:00:32.54)
Memory usage on this node: 15.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |
| train_2d480_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_2d480_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157619)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=157619)[0m Configuration completed!
[2m[36m(func pid=157619)[0m New optimizer parameters:
[2m[36m(func pid=157619)[0m SGD (
[2m[36m(func pid=157619)[0m Parameter Group 0
[2m[36m(func pid=157619)[0m     dampening: 0
[2m[36m(func pid=157619)[0m     differentiable: False
[2m[36m(func pid=157619)[0m     foreach: None
[2m[36m(func pid=157619)[0m     lr: 0.001
[2m[36m(func pid=157619)[0m     maximize: False
[2m[36m(func pid=157619)[0m     momentum: 0.99
[2m[36m(func pid=157619)[0m     nesterov: False
[2m[36m(func pid=157619)[0m     weight_decay: 0
[2m[36m(func pid=157619)[0m )
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0948 | Steps: 2 | Val loss: 2.4825 | Batch size: 32 | lr: 0.001 | Duration: 5.14s
[2m[36m(func pid=157619)[0m top1: 0.06902985074626866
[2m[36m(func pid=157619)[0m top5: 0.48740671641791045
[2m[36m(func pid=157619)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=157619)[0m f1_macro: 0.04454694792431653
[2m[36m(func pid=157619)[0m f1_weighted: 0.0437836563238177
[2m[36m(func pid=157619)[0m f1_per_class: [0.154, 0.01, 0.0, 0.107, 0.0, 0.017, 0.0, 0.103, 0.021, 0.033]
== Status ==
Current time: 2024-01-07 03:20:06 (running for 00:00:41.08)
Memory usage on this node: 17.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |
| train_2d480_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158041)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=158041)[0m Configuration completed!
[2m[36m(func pid=158041)[0m New optimizer parameters:
[2m[36m(func pid=158041)[0m SGD (
[2m[36m(func pid=158041)[0m Parameter Group 0
[2m[36m(func pid=158041)[0m     dampening: 0
[2m[36m(func pid=158041)[0m     differentiable: False
[2m[36m(func pid=158041)[0m     foreach: None
[2m[36m(func pid=158041)[0m     lr: 0.01
[2m[36m(func pid=158041)[0m     maximize: False
[2m[36m(func pid=158041)[0m     momentum: 0.99
[2m[36m(func pid=158041)[0m     nesterov: False
[2m[36m(func pid=158041)[0m     weight_decay: 0
[2m[36m(func pid=158041)[0m )
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0425 | Steps: 2 | Val loss: 2.3395 | Batch size: 32 | lr: 0.01 | Duration: 4.59s
[2m[36m(func pid=158041)[0m top1: 0.08208955223880597
[2m[36m(func pid=158041)[0m top5: 0.5289179104477612
[2m[36m(func pid=158041)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=158041)[0m f1_macro: 0.08100738947460556
[2m[36m(func pid=158041)[0m f1_weighted: 0.08336076038521915
[2m[36m(func pid=158041)[0m f1_per_class: [0.088, 0.11, 0.11, 0.151, 0.023, 0.03, 0.018, 0.111, 0.12, 0.05]
== Status ==
Current time: 2024-01-07 03:20:14 (running for 00:00:49.74)
Memory usage on this node: 20.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 03:20:22 (running for 00:00:57.12)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  3.095 |      0.045 |                    1 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |        |            |                      |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |        |            |                      |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158460)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=158460)[0m Configuration completed!
[2m[36m(func pid=158460)[0m New optimizer parameters:
[2m[36m(func pid=158460)[0m SGD (
[2m[36m(func pid=158460)[0m Parameter Group 0
[2m[36m(func pid=158460)[0m     dampening: 0
[2m[36m(func pid=158460)[0m     differentiable: False
[2m[36m(func pid=158460)[0m     foreach: None
[2m[36m(func pid=158460)[0m     lr: 0.1
[2m[36m(func pid=158460)[0m     maximize: False
[2m[36m(func pid=158460)[0m     momentum: 0.99
[2m[36m(func pid=158460)[0m     nesterov: False
[2m[36m(func pid=158460)[0m     weight_decay: 0
[2m[36m(func pid=158460)[0m )
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9361 | Steps: 2 | Val loss: 2.4624 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0833 | Steps: 2 | Val loss: 2.5374 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9377 | Steps: 2 | Val loss: 2.1891 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.8265 | Steps: 2 | Val loss: 6.9655 | Batch size: 32 | lr: 0.1 | Duration: 4.49s
== Status ==
Current time: 2024-01-07 03:20:27 (running for 00:01:02.15)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  3.114 |      0.04  |                    1 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  3.095 |      0.045 |                    1 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  3.043 |      0.081 |                    1 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |        |            |                      |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.0732276119402985
[2m[36m(func pid=157619)[0m top5: 0.4780783582089552
[2m[36m(func pid=157619)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=157619)[0m f1_macro: 0.054742390283868916
[2m[36m(func pid=157619)[0m f1_weighted: 0.053391580975150975
[2m[36m(func pid=157619)[0m f1_per_class: [0.141, 0.052, 0.0, 0.107, 0.0, 0.026, 0.0, 0.105, 0.057, 0.059]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.19449626865671643
[2m[36m(func pid=158041)[0m top5: 0.7014925373134329
[2m[36m(func pid=158041)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=158041)[0m f1_macro: 0.13628848619022466
[2m[36m(func pid=158041)[0m f1_weighted: 0.21136484930657776
[2m[36m(func pid=158041)[0m f1_per_class: [0.135, 0.121, 0.101, 0.171, 0.046, 0.06, 0.424, 0.0, 0.115, 0.19]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.06343283582089553
[2m[36m(func pid=157245)[0m top5: 0.48367537313432835
[2m[36m(func pid=157245)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=157245)[0m f1_macro: 0.034277441304661475
[2m[36m(func pid=157245)[0m f1_weighted: 0.034578836944070816
[2m[36m(func pid=157245)[0m f1_per_class: [0.076, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.103, 0.023, 0.033]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.010727611940298507
[2m[36m(func pid=158460)[0m top5: 0.3255597014925373
[2m[36m(func pid=158460)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=158460)[0m f1_macro: 0.005298244664781924
[2m[36m(func pid=158460)[0m f1_weighted: 0.0008597640811727206
[2m[36m(func pid=158460)[0m f1_per_class: [0.037, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6087 | Steps: 2 | Val loss: 2.4302 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8688 | Steps: 2 | Val loss: 2.1099 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0465 | Steps: 2 | Val loss: 2.5544 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.2575 | Steps: 2 | Val loss: 26.6475 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
== Status ==
Current time: 2024-01-07 03:20:32 (running for 00:01:07.35)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  3.083 |      0.034 |                    2 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  2.609 |      0.07  |                    3 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.938 |      0.136 |                    2 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.827 |      0.005 |                    1 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.07602611940298508
[2m[36m(func pid=157619)[0m top5: 0.46875
[2m[36m(func pid=157619)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=157619)[0m f1_macro: 0.07047276245832435
[2m[36m(func pid=157619)[0m f1_weighted: 0.0689356493287684
[2m[36m(func pid=157619)[0m f1_per_class: [0.12, 0.086, 0.074, 0.123, 0.0, 0.026, 0.012, 0.108, 0.103, 0.052]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.24813432835820895
[2m[36m(func pid=158041)[0m top5: 0.7588619402985075
[2m[36m(func pid=158041)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=158041)[0m f1_macro: 0.17316325131824267
[2m[36m(func pid=158041)[0m f1_weighted: 0.23835720852844136
[2m[36m(func pid=158041)[0m f1_per_class: [0.251, 0.128, 0.14, 0.396, 0.115, 0.036, 0.294, 0.0, 0.133, 0.238]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.06343283582089553
[2m[36m(func pid=157245)[0m top5: 0.4748134328358209
[2m[36m(func pid=157245)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=157245)[0m f1_macro: 0.03148894264569985
[2m[36m(func pid=157245)[0m f1_weighted: 0.03486776564811902
[2m[36m(func pid=157245)[0m f1_per_class: [0.055, 0.015, 0.0, 0.081, 0.0, 0.02, 0.0, 0.103, 0.0, 0.042]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.011194029850746268
[2m[36m(func pid=158460)[0m top5: 0.5363805970149254
[2m[36m(func pid=158460)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=158460)[0m f1_macro: 0.0022242817423540314
[2m[36m(func pid=158460)[0m f1_weighted: 0.0002593612106289682
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.2810 | Steps: 2 | Val loss: 2.3891 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.3105 | Steps: 2 | Val loss: 2.1199 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0633 | Steps: 2 | Val loss: 2.5569 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9117 | Steps: 2 | Val loss: 31543.8164 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=158041)[0m top1: 0.25326492537313433
[2m[36m(func pid=158041)[0m top5: 0.7658582089552238
[2m[36m(func pid=158041)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=158041)[0m f1_macro: 0.20837521669303144
[2m[36m(func pid=158041)[0m f1_weighted: 0.19151367814271558
[2m[36m(func pid=158041)[0m f1_per_class: [0.329, 0.105, 0.183, 0.407, 0.169, 0.052, 0.061, 0.32, 0.129, 0.328]
== Status ==
Current time: 2024-01-07 03:20:37 (running for 00:01:12.44)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  3.047 |      0.031 |                    3 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  2.609 |      0.07  |                    3 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.311 |      0.208 |                    4 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.257 |      0.002 |                    2 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.07742537313432836
[2m[36m(func pid=157619)[0m top5: 0.4869402985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=157619)[0m f1_macro: 0.09213364992861131
[2m[36m(func pid=157619)[0m f1_weighted: 0.07694439204425971
[2m[36m(func pid=157619)[0m f1_per_class: [0.111, 0.1, 0.197, 0.107, 0.023, 0.044, 0.03, 0.121, 0.147, 0.042]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.0625
[2m[36m(func pid=157245)[0m top5: 0.47388059701492535
[2m[36m(func pid=157245)[0m f1_micro: 0.0625
[2m[36m(func pid=157245)[0m f1_macro: 0.03584310719664859
[2m[36m(func pid=157245)[0m f1_weighted: 0.04032554272049034
[2m[36m(func pid=157245)[0m f1_per_class: [0.05, 0.032, 0.0, 0.088, 0.0, 0.02, 0.0, 0.096, 0.025, 0.047]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.01166044776119403
[2m[36m(func pid=158460)[0m top5: 0.5149253731343284
[2m[36m(func pid=158460)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=158460)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=158460)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.0922 | Steps: 2 | Val loss: 2.2942 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8504 | Steps: 2 | Val loss: 2.3324 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9282 | Steps: 2 | Val loss: 2.5474 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.4739 | Steps: 2 | Val loss: 83394.7578 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 03:20:42 (running for 00:01:17.65)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  3.063 |      0.036 |                    4 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  1.85  |      0.099 |                    5 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.311 |      0.208 |                    4 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.912 |      0.002 |                    3 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.09514925373134328
[2m[36m(func pid=157619)[0m top5: 0.558768656716418
[2m[36m(func pid=157619)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=157619)[0m f1_macro: 0.09907866584351187
[2m[36m(func pid=157619)[0m f1_weighted: 0.10542043402167027
[2m[36m(func pid=157619)[0m f1_per_class: [0.116, 0.082, 0.14, 0.103, 0.025, 0.075, 0.13, 0.108, 0.147, 0.065]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.26026119402985076
[2m[36m(func pid=158041)[0m top5: 0.7486007462686567
[2m[36m(func pid=158041)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=158041)[0m f1_macro: 0.23108851611577377
[2m[36m(func pid=158041)[0m f1_weighted: 0.1954963655367716
[2m[36m(func pid=158041)[0m f1_per_class: [0.266, 0.147, 0.238, 0.406, 0.233, 0.141, 0.003, 0.394, 0.13, 0.353]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.06669776119402986
[2m[36m(func pid=157245)[0m top5: 0.4724813432835821
[2m[36m(func pid=157245)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=157245)[0m f1_macro: 0.038868083359488076
[2m[36m(func pid=157245)[0m f1_weighted: 0.04782760028951502
[2m[36m(func pid=157245)[0m f1_per_class: [0.042, 0.062, 0.0, 0.097, 0.0, 0.019, 0.0, 0.1, 0.025, 0.044]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.008861940298507462
[2m[36m(func pid=158460)[0m top5: 0.511660447761194
[2m[36m(func pid=158460)[0m f1_micro: 0.008861940298507462
[2m[36m(func pid=158460)[0m f1_macro: 0.002341343191620456
[2m[36m(func pid=158460)[0m f1_weighted: 0.0002730110997691763
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4198 | Steps: 2 | Val loss: 2.2738 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.0807 | Steps: 2 | Val loss: 2.4538 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8783 | Steps: 2 | Val loss: 2.5324 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 12.7000 | Steps: 2 | Val loss: 7643211776.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 03:20:47 (running for 00:01:22.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.928 |      0.039 |                    5 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  1.85  |      0.099 |                    5 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.081 |      0.244 |                    6 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.474 |      0.002 |                    4 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.2728544776119403
[2m[36m(func pid=158041)[0m top5: 0.7467350746268657
[2m[36m(func pid=158041)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=158041)[0m f1_macro: 0.24361920475564633
[2m[36m(func pid=158041)[0m f1_weighted: 0.21237674136558166
[2m[36m(func pid=158041)[0m f1_per_class: [0.281, 0.189, 0.242, 0.423, 0.274, 0.186, 0.0, 0.399, 0.142, 0.3]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.1455223880597015
[2m[36m(func pid=157619)[0m top5: 0.613339552238806
[2m[36m(func pid=157619)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=157619)[0m f1_macro: 0.11564310706062789
[2m[36m(func pid=157619)[0m f1_weighted: 0.16362622569091614
[2m[36m(func pid=157619)[0m f1_per_class: [0.135, 0.144, 0.109, 0.087, 0.026, 0.077, 0.319, 0.027, 0.133, 0.099]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.06809701492537314
[2m[36m(func pid=157245)[0m top5: 0.46781716417910446
[2m[36m(func pid=157245)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=157245)[0m f1_macro: 0.043570841723892625
[2m[36m(func pid=157245)[0m f1_weighted: 0.051437469859847
[2m[36m(func pid=157245)[0m f1_per_class: [0.053, 0.074, 0.0, 0.098, 0.0, 0.018, 0.0, 0.105, 0.049, 0.039]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.0417 | Steps: 2 | Val loss: 2.5763 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7797 | Steps: 2 | Val loss: 2.5142 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1308 | Steps: 2 | Val loss: 2.2166 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.5357 | Steps: 2 | Val loss: 678446759936.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=158041)[0m top1: 0.28591417910447764
[2m[36m(func pid=158041)[0m top5: 0.7658582089552238
[2m[36m(func pid=158041)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=158041)[0m f1_macro: 0.2726152428797273
[2m[36m(func pid=158041)[0m f1_weighted: 0.23393612477444392
[2m[36m(func pid=158041)[0m f1_per_class: [0.316, 0.222, 0.224, 0.429, 0.354, 0.274, 0.006, 0.408, 0.146, 0.346]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:20:53 (running for 00:01:28.07)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.78  |      0.05  |                    7 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  1.42  |      0.116 |                    6 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.042 |      0.273 |                    7 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 12.7   |      0.001 |                    5 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.07602611940298508
[2m[36m(func pid=157245)[0m top5: 0.46455223880597013
[2m[36m(func pid=157245)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=157245)[0m f1_macro: 0.050302691760018406
[2m[36m(func pid=157245)[0m f1_weighted: 0.06357830701755693
[2m[36m(func pid=157245)[0m f1_per_class: [0.058, 0.099, 0.0, 0.125, 0.0, 0.018, 0.0, 0.109, 0.046, 0.048]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m top1: 0.19309701492537312
[2m[36m(func pid=157619)[0m top5: 0.6595149253731343
[2m[36m(func pid=157619)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=157619)[0m f1_macro: 0.13706849665736706
[2m[36m(func pid=157619)[0m f1_weighted: 0.20157098355525413
[2m[36m(func pid=157619)[0m f1_per_class: [0.161, 0.163, 0.11, 0.073, 0.034, 0.088, 0.444, 0.016, 0.109, 0.174]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.0166 | Steps: 2 | Val loss: 2.6780 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8222 | Steps: 2 | Val loss: 2.1469 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6594 | Steps: 2 | Val loss: 2.4947 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.1775 | Steps: 2 | Val loss: 1319500928.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=158041)[0m top1: 0.28824626865671643
[2m[36m(func pid=158041)[0m top5: 0.7821828358208955
[2m[36m(func pid=158041)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=158041)[0m f1_macro: 0.27700043018683895
[2m[36m(func pid=158041)[0m f1_weighted: 0.24903464964376676
[2m[36m(func pid=158041)[0m f1_per_class: [0.325, 0.27, 0.202, 0.423, 0.329, 0.327, 0.015, 0.394, 0.17, 0.315]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:20:58 (running for 00:01:33.20)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.78  |      0.05  |                    7 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.822 |      0.166 |                    8 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.017 |      0.277 |                    8 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  5.536 |      0.001 |                    6 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.08069029850746269
[2m[36m(func pid=157245)[0m top5: 0.46968283582089554
[2m[36m(func pid=157245)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=157245)[0m f1_macro: 0.056427854327618765
[2m[36m(func pid=157245)[0m f1_weighted: 0.07476730694558957
[2m[36m(func pid=157245)[0m f1_per_class: [0.069, 0.11, 0.0, 0.155, 0.0, 0.022, 0.0, 0.11, 0.059, 0.04]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m top1: 0.2416044776119403
[2m[36m(func pid=157619)[0m top5: 0.7126865671641791
[2m[36m(func pid=157619)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=157619)[0m f1_macro: 0.16603704329755195
[2m[36m(func pid=157619)[0m f1_weighted: 0.24377112278405239
[2m[36m(func pid=157619)[0m f1_per_class: [0.2, 0.238, 0.117, 0.098, 0.053, 0.092, 0.512, 0.016, 0.106, 0.229]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0067 | Steps: 2 | Val loss: 2.7855 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5986 | Steps: 2 | Val loss: 2.0795 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5594 | Steps: 2 | Val loss: 2.4736 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.0996 | Steps: 2 | Val loss: 79650680.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=158041)[0m top1: 0.291044776119403
[2m[36m(func pid=158041)[0m top5: 0.8055037313432836
[2m[36m(func pid=158041)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=158041)[0m f1_macro: 0.2853703064209042
[2m[36m(func pid=158041)[0m f1_weighted: 0.2663627268877314
[2m[36m(func pid=158041)[0m f1_per_class: [0.344, 0.297, 0.185, 0.423, 0.351, 0.366, 0.049, 0.37, 0.158, 0.312]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:21:03 (running for 00:01:38.34)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.659 |      0.056 |                    8 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.599 |      0.191 |                    9 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.007 |      0.285 |                    9 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.177 |      0.001 |                    7 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.27705223880597013
[2m[36m(func pid=157619)[0m top5: 0.7635261194029851
[2m[36m(func pid=157619)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=157619)[0m f1_macro: 0.1911240244026291
[2m[36m(func pid=157619)[0m f1_weighted: 0.27707712411676055
[2m[36m(func pid=157619)[0m f1_per_class: [0.248, 0.292, 0.148, 0.14, 0.071, 0.109, 0.542, 0.016, 0.11, 0.235]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.0830223880597015
[2m[36m(func pid=157245)[0m top5: 0.466884328358209
[2m[36m(func pid=157245)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=157245)[0m f1_macro: 0.06734074003508402
[2m[36m(func pid=157245)[0m f1_weighted: 0.08111761138747624
[2m[36m(func pid=157245)[0m f1_per_class: [0.084, 0.112, 0.065, 0.165, 0.0, 0.031, 0.003, 0.112, 0.07, 0.032]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0182 | Steps: 2 | Val loss: 2.8810 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4128 | Steps: 2 | Val loss: 2.0018 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4313 | Steps: 2 | Val loss: 2.4487 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.0866 | Steps: 2 | Val loss: 29031870.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=158041)[0m top1: 0.2980410447761194
[2m[36m(func pid=158041)[0m top5: 0.8292910447761194
[2m[36m(func pid=158041)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=158041)[0m f1_macro: 0.30032953569324466
[2m[36m(func pid=158041)[0m f1_weighted: 0.28426138813104934
[2m[36m(func pid=158041)[0m f1_per_class: [0.321, 0.3, 0.222, 0.419, 0.358, 0.416, 0.094, 0.349, 0.153, 0.371]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:21:08 (running for 00:01:43.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.559 |      0.067 |                    9 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.413 |      0.215 |                   10 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.018 |      0.3   |                   10 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.1   |      0.001 |                    8 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.2994402985074627
[2m[36m(func pid=157619)[0m top5: 0.8199626865671642
[2m[36m(func pid=157619)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=157619)[0m f1_macro: 0.21472634134195462
[2m[36m(func pid=157619)[0m f1_weighted: 0.3070053111436485
[2m[36m(func pid=157619)[0m f1_per_class: [0.281, 0.322, 0.19, 0.228, 0.102, 0.118, 0.534, 0.016, 0.127, 0.229]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.09654850746268656
[2m[36m(func pid=157245)[0m top5: 0.47154850746268656
[2m[36m(func pid=157245)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=157245)[0m f1_macro: 0.08534023136639066
[2m[36m(func pid=157245)[0m f1_weighted: 0.09534378035582787
[2m[36m(func pid=157245)[0m f1_per_class: [0.087, 0.144, 0.136, 0.182, 0.0, 0.039, 0.006, 0.128, 0.091, 0.039]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0035 | Steps: 2 | Val loss: 2.9733 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3088 | Steps: 2 | Val loss: 2.4267 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3911 | Steps: 2 | Val loss: 1.9476 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5407 | Steps: 2 | Val loss: 14343245.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=158041)[0m top1: 0.3111007462686567
[2m[36m(func pid=158041)[0m top5: 0.8414179104477612
[2m[36m(func pid=158041)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=158041)[0m f1_macro: 0.32401553330220917
[2m[36m(func pid=158041)[0m f1_weighted: 0.3000631168931512
[2m[36m(func pid=158041)[0m f1_per_class: [0.332, 0.333, 0.375, 0.418, 0.344, 0.446, 0.113, 0.353, 0.156, 0.371]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.09841417910447761
[2m[36m(func pid=157245)[0m top5: 0.4780783582089552
[2m[36m(func pid=157245)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=157245)[0m f1_macro: 0.09439822927893796
[2m[36m(func pid=157245)[0m f1_weighted: 0.09825869286302992
[2m[36m(func pid=157245)[0m f1_per_class: [0.089, 0.145, 0.215, 0.174, 0.0, 0.067, 0.012, 0.13, 0.073, 0.038]
== Status ==
Current time: 2024-01-07 03:21:13 (running for 00:01:48.48)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.309 |      0.094 |                   11 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.413 |      0.215 |                   10 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.003 |      0.324 |                   11 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.087 |      0.001 |                    9 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m top1: 0.30597014925373134
[2m[36m(func pid=157619)[0m top5: 0.8535447761194029
[2m[36m(func pid=157619)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=157619)[0m f1_macro: 0.24159000079783471
[2m[36m(func pid=157619)[0m f1_weighted: 0.32360311044490586
[2m[36m(func pid=157619)[0m f1_per_class: [0.33, 0.341, 0.238, 0.287, 0.088, 0.143, 0.497, 0.06, 0.14, 0.293]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0024 | Steps: 2 | Val loss: 3.0801 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2403 | Steps: 2 | Val loss: 1.8739 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1669 | Steps: 2 | Val loss: 2.4018 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5262 | Steps: 2 | Val loss: 10387454.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=158041)[0m top1: 0.3148320895522388
[2m[36m(func pid=158041)[0m top5: 0.8465485074626866
[2m[36m(func pid=158041)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=158041)[0m f1_macro: 0.3383688705401876
[2m[36m(func pid=158041)[0m f1_weighted: 0.30806558033971515
[2m[36m(func pid=158041)[0m f1_per_class: [0.343, 0.341, 0.522, 0.395, 0.328, 0.468, 0.147, 0.353, 0.158, 0.33]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:21:18 (running for 00:01:53.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.309 |      0.094 |                   11 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.24  |      0.268 |                   12 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.002 |      0.338 |                   12 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.541 |      0.001 |                   10 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.3125
[2m[36m(func pid=157619)[0m top5: 0.8810634328358209
[2m[36m(func pid=157619)[0m f1_micro: 0.3125
[2m[36m(func pid=157619)[0m f1_macro: 0.26813757546588823
[2m[36m(func pid=157619)[0m f1_weighted: 0.3324436729349357
[2m[36m(func pid=157619)[0m f1_per_class: [0.369, 0.372, 0.289, 0.345, 0.082, 0.162, 0.416, 0.189, 0.164, 0.293]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.10074626865671642
[2m[36m(func pid=157245)[0m top5: 0.48507462686567165
[2m[36m(func pid=157245)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=157245)[0m f1_macro: 0.09439860943929915
[2m[36m(func pid=157245)[0m f1_weighted: 0.10468734234812246
[2m[36m(func pid=157245)[0m f1_per_class: [0.095, 0.158, 0.182, 0.169, 0.0, 0.07, 0.03, 0.128, 0.08, 0.032]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0023 | Steps: 2 | Val loss: 3.2438 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1405 | Steps: 2 | Val loss: 1.8312 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.1204 | Steps: 2 | Val loss: 2.3805 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9477 | Steps: 2 | Val loss: 4843186.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=158041)[0m top1: 0.31669776119402987
[2m[36m(func pid=158041)[0m top5: 0.851679104477612
[2m[36m(func pid=158041)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=158041)[0m f1_macro: 0.3434091888527733
[2m[36m(func pid=158041)[0m f1_weighted: 0.3163400189733969
[2m[36m(func pid=158041)[0m f1_per_class: [0.316, 0.357, 0.615, 0.366, 0.269, 0.486, 0.187, 0.353, 0.155, 0.33]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.10634328358208955
[2m[36m(func pid=157245)[0m top5: 0.4944029850746269
[2m[36m(func pid=157245)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=157245)[0m f1_macro: 0.09880097058695547
[2m[36m(func pid=157245)[0m f1_weighted: 0.11022429335281317
[2m[36m(func pid=157245)[0m f1_per_class: [0.105, 0.18, 0.167, 0.16, 0.0, 0.085, 0.036, 0.139, 0.08, 0.036]
[2m[36m(func pid=157245)[0m 
== Status ==
Current time: 2024-01-07 03:21:23 (running for 00:01:58.79)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  2.12  |      0.099 |                   13 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.24  |      0.268 |                   12 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.002 |      0.343 |                   13 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.526 |      0.001 |                   11 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.3208955223880597
[2m[36m(func pid=157619)[0m top5: 0.8959888059701493
[2m[36m(func pid=157619)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=157619)[0m f1_macro: 0.29661806722920125
[2m[36m(func pid=157619)[0m f1_weighted: 0.3320851222895153
[2m[36m(func pid=157619)[0m f1_per_class: [0.405, 0.399, 0.324, 0.383, 0.099, 0.186, 0.321, 0.325, 0.177, 0.346]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0211 | Steps: 2 | Val loss: 3.4239 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0835 | Steps: 2 | Val loss: 1.8141 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9266 | Steps: 2 | Val loss: 2.3601 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4794 | Steps: 2 | Val loss: 2762843.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=158041)[0m top1: 0.31763059701492535
[2m[36m(func pid=158041)[0m top5: 0.8563432835820896
[2m[36m(func pid=158041)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=158041)[0m f1_macro: 0.3497738027199131
[2m[36m(func pid=158041)[0m f1_weighted: 0.3142878455580372
[2m[36m(func pid=158041)[0m f1_per_class: [0.359, 0.373, 0.667, 0.328, 0.25, 0.47, 0.207, 0.357, 0.172, 0.316]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3255597014925373
[2m[36m(func pid=157619)[0m top5: 0.8927238805970149
[2m[36m(func pid=157619)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=157619)[0m f1_macro: 0.30614581134920205
[2m[36m(func pid=157619)[0m f1_weighted: 0.31985434866083895
[2m[36m(func pid=157619)[0m f1_per_class: [0.446, 0.418, 0.364, 0.408, 0.095, 0.173, 0.232, 0.397, 0.19, 0.339]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:21:29 (running for 00:02:04.00)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.927 |      0.097 |                   14 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.083 |      0.306 |                   14 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.021 |      0.35  |                   14 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.948 |      0.001 |                   12 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.10634328358208955
[2m[36m(func pid=157245)[0m top5: 0.5107276119402985
[2m[36m(func pid=157245)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=157245)[0m f1_macro: 0.09716465084995567
[2m[36m(func pid=157245)[0m f1_weighted: 0.11357283576033063
[2m[36m(func pid=157245)[0m f1_per_class: [0.109, 0.173, 0.143, 0.155, 0.0, 0.095, 0.055, 0.12, 0.08, 0.041]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0170 | Steps: 2 | Val loss: 3.5940 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0661 | Steps: 2 | Val loss: 1.8033 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.9438 | Steps: 2 | Val loss: 2.3412 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6061 | Steps: 2 | Val loss: 1391777.6250 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=158041)[0m top1: 0.310634328358209
[2m[36m(func pid=158041)[0m top5: 0.8526119402985075
[2m[36m(func pid=158041)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=158041)[0m f1_macro: 0.355451397925491
[2m[36m(func pid=158041)[0m f1_weighted: 0.30444784676279985
[2m[36m(func pid=158041)[0m f1_per_class: [0.392, 0.363, 0.686, 0.261, 0.267, 0.472, 0.237, 0.354, 0.175, 0.348]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3283582089552239
[2m[36m(func pid=157619)[0m top5: 0.8936567164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=157619)[0m f1_macro: 0.31562856711482457
[2m[36m(func pid=157619)[0m f1_weighted: 0.30187389656711394
[2m[36m(func pid=157619)[0m f1_per_class: [0.459, 0.435, 0.421, 0.434, 0.116, 0.176, 0.13, 0.395, 0.202, 0.387]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.10914179104477612
[2m[36m(func pid=157245)[0m top5: 0.5317164179104478
[2m[36m(func pid=157245)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=157245)[0m f1_macro: 0.09692624730479621
[2m[36m(func pid=157245)[0m f1_weighted: 0.12082504064143966
[2m[36m(func pid=157245)[0m f1_per_class: [0.103, 0.175, 0.132, 0.144, 0.0, 0.092, 0.097, 0.077, 0.1, 0.049]
== Status ==
Current time: 2024-01-07 03:21:34 (running for 00:02:09.20)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.944 |      0.097 |                   15 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.066 |      0.316 |                   15 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.017 |      0.355 |                   15 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.479 |      0.001 |                   13 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0266 | Steps: 2 | Val loss: 3.5852 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0515 | Steps: 2 | Val loss: 1.8088 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6908 | Steps: 2 | Val loss: 2.3225 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3531 | Steps: 2 | Val loss: 981363.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=158041)[0m top1: 0.314365671641791
[2m[36m(func pid=158041)[0m top5: 0.878731343283582
[2m[36m(func pid=158041)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=158041)[0m f1_macro: 0.3524287532211144
[2m[36m(func pid=158041)[0m f1_weighted: 0.30361797188471995
[2m[36m(func pid=158041)[0m f1_per_class: [0.383, 0.346, 0.706, 0.255, 0.277, 0.422, 0.263, 0.389, 0.19, 0.294]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3283582089552239
[2m[36m(func pid=157619)[0m top5: 0.8936567164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=157619)[0m f1_macro: 0.31287651213821566
[2m[36m(func pid=157619)[0m f1_weighted: 0.2911668456516492
[2m[36m(func pid=157619)[0m f1_per_class: [0.473, 0.45, 0.436, 0.447, 0.146, 0.165, 0.082, 0.371, 0.199, 0.358]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:21:39 (running for 00:02:14.39)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.691 |      0.098 |                   16 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.051 |      0.313 |                   16 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.027 |      0.352 |                   16 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.606 |      0.001 |                   14 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.11473880597014925
[2m[36m(func pid=157245)[0m top5: 0.5499067164179104
[2m[36m(func pid=157245)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=157245)[0m f1_macro: 0.09779025105560236
[2m[36m(func pid=157245)[0m f1_weighted: 0.12835694513913595
[2m[36m(func pid=157245)[0m f1_per_class: [0.108, 0.177, 0.116, 0.133, 0.006, 0.103, 0.133, 0.05, 0.094, 0.059]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0006 | Steps: 2 | Val loss: 3.7856 | Batch size: 32 | lr: 0.01 | Duration: 2.61s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0504 | Steps: 2 | Val loss: 1.8305 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.6052 | Steps: 2 | Val loss: 2.3044 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 44.2130 | Steps: 2 | Val loss: 728644.9375 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=158041)[0m top1: 0.30363805970149255
[2m[36m(func pid=158041)[0m top5: 0.8847947761194029
[2m[36m(func pid=158041)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=158041)[0m f1_macro: 0.3477198048558932
[2m[36m(func pid=158041)[0m f1_weighted: 0.2902068706476619
[2m[36m(func pid=158041)[0m f1_per_class: [0.409, 0.359, 0.774, 0.238, 0.23, 0.371, 0.242, 0.411, 0.179, 0.264]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.333955223880597
[2m[36m(func pid=157619)[0m top5: 0.8894589552238806
[2m[36m(func pid=157619)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=157619)[0m f1_macro: 0.31536582293932564
[2m[36m(func pid=157619)[0m f1_weighted: 0.28700672093480184
[2m[36m(func pid=157619)[0m f1_per_class: [0.476, 0.465, 0.444, 0.458, 0.155, 0.162, 0.048, 0.368, 0.226, 0.351]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:21:44 (running for 00:02:19.59)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.605 |      0.102 |                   17 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.05  |      0.315 |                   17 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.001 |      0.348 |                   17 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.353 |      0.001 |                   15 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.12406716417910447
[2m[36m(func pid=157245)[0m top5: 0.5722947761194029
[2m[36m(func pid=157245)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=157245)[0m f1_macro: 0.10177511236333883
[2m[36m(func pid=157245)[0m f1_weighted: 0.13882640575326455
[2m[36m(func pid=157245)[0m f1_per_class: [0.116, 0.184, 0.112, 0.125, 0.012, 0.101, 0.176, 0.014, 0.107, 0.071]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.03311567164179104
[2m[36m(func pid=158460)[0m top5: 0.5149253731343284
[2m[36m(func pid=158460)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=158460)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=158460)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0011 | Steps: 2 | Val loss: 3.9946 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1010 | Steps: 2 | Val loss: 1.8543 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.5242 | Steps: 2 | Val loss: 2.2806 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8788 | Steps: 2 | Val loss: 394856.9375 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=158041)[0m top1: 0.28638059701492535
[2m[36m(func pid=158041)[0m top5: 0.8740671641791045
[2m[36m(func pid=158041)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=158041)[0m f1_macro: 0.3438933111878807
[2m[36m(func pid=158041)[0m f1_weighted: 0.27151355642877084
[2m[36m(func pid=158041)[0m f1_per_class: [0.43, 0.361, 0.828, 0.177, 0.23, 0.322, 0.251, 0.414, 0.172, 0.254]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.33722014925373134
[2m[36m(func pid=157619)[0m top5: 0.8894589552238806
[2m[36m(func pid=157619)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=157619)[0m f1_macro: 0.3240633488291815
[2m[36m(func pid=157619)[0m f1_weighted: 0.28825871065888703
[2m[36m(func pid=157619)[0m f1_per_class: [0.504, 0.473, 0.462, 0.458, 0.163, 0.166, 0.042, 0.37, 0.225, 0.378]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:21:50 (running for 00:02:24.83)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.605 |      0.102 |                   17 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.101 |      0.324 |                   18 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.001 |      0.344 |                   18 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.879 |      0.001 |                   17 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m top1: 0.13526119402985073
[2m[36m(func pid=157245)[0m top5: 0.5979477611940298
[2m[36m(func pid=157245)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=157245)[0m f1_macro: 0.10867070363986783
[2m[36m(func pid=157245)[0m f1_weighted: 0.15243845583394303
[2m[36m(func pid=157245)[0m f1_per_class: [0.115, 0.19, 0.114, 0.122, 0.019, 0.102, 0.221, 0.014, 0.104, 0.086]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0077 | Steps: 2 | Val loss: 4.3133 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0298 | Steps: 2 | Val loss: 1.8754 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 17.7972 | Steps: 2 | Val loss: 935103.3750 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.3684 | Steps: 2 | Val loss: 2.2593 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=158041)[0m top1: 0.2691231343283582
[2m[36m(func pid=158041)[0m top5: 0.8568097014925373
[2m[36m(func pid=158041)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=158041)[0m f1_macro: 0.3221482715655497
[2m[36m(func pid=158041)[0m f1_weighted: 0.25424718361029514
[2m[36m(func pid=158041)[0m f1_per_class: [0.431, 0.363, 0.786, 0.126, 0.203, 0.281, 0.266, 0.384, 0.161, 0.221]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.34701492537313433
[2m[36m(func pid=157619)[0m top5: 0.8903917910447762
[2m[36m(func pid=157619)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=157619)[0m f1_macro: 0.32951207602686466
[2m[36m(func pid=157619)[0m f1_weighted: 0.29488484797414816
[2m[36m(func pid=157619)[0m f1_per_class: [0.524, 0.486, 0.471, 0.474, 0.182, 0.174, 0.037, 0.373, 0.229, 0.346]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:21:55 (running for 00:02:29.93)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.524 |      0.109 |                   18 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.03  |      0.33  |                   19 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.008 |      0.322 |                   19 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 17.797 |      0.001 |                   18 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m top1: 0.14505597014925373
[2m[36m(func pid=157245)[0m top5: 0.6198694029850746
[2m[36m(func pid=157245)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=157245)[0m f1_macro: 0.1125629509885215
[2m[36m(func pid=157245)[0m f1_weighted: 0.16201661724009608
[2m[36m(func pid=157245)[0m f1_per_class: [0.122, 0.188, 0.112, 0.108, 0.027, 0.113, 0.262, 0.015, 0.103, 0.076]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1002 | Steps: 2 | Val loss: 4.4773 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0342 | Steps: 2 | Val loss: 1.8885 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8759 | Steps: 2 | Val loss: 313953.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4054 | Steps: 2 | Val loss: 2.2407 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m top1: 0.2691231343283582
[2m[36m(func pid=158041)[0m top5: 0.8460820895522388
[2m[36m(func pid=158041)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=158041)[0m f1_macro: 0.301621174062154
[2m[36m(func pid=158041)[0m f1_weighted: 0.257781842335772
[2m[36m(func pid=158041)[0m f1_per_class: [0.358, 0.38, 0.786, 0.138, 0.033, 0.274, 0.267, 0.395, 0.153, 0.232]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3558768656716418
[2m[36m(func pid=157619)[0m top5: 0.8908582089552238
[2m[36m(func pid=157619)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=157619)[0m f1_macro: 0.34353421206364065
[2m[36m(func pid=157619)[0m f1_weighted: 0.2993891580508707
[2m[36m(func pid=157619)[0m f1_per_class: [0.533, 0.495, 0.545, 0.488, 0.204, 0.171, 0.031, 0.373, 0.239, 0.356]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:22:00 (running for 00:02:35.05)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.368 |      0.113 |                   19 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.034 |      0.344 |                   20 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.1   |      0.302 |                   20 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.876 |      0.001 |                   19 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m top1: 0.1599813432835821
[2m[36m(func pid=157245)[0m top5: 0.648320895522388
[2m[36m(func pid=157245)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=157245)[0m f1_macro: 0.12170019177675254
[2m[36m(func pid=157245)[0m f1_weighted: 0.176996457451042
[2m[36m(func pid=157245)[0m f1_per_class: [0.124, 0.186, 0.117, 0.103, 0.029, 0.119, 0.314, 0.015, 0.105, 0.105]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0009 | Steps: 2 | Val loss: 4.4195 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0385 | Steps: 2 | Val loss: 1.8829 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.8985 | Steps: 2 | Val loss: 128654.9766 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.1400 | Steps: 2 | Val loss: 2.2163 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=158041)[0m top1: 0.28638059701492535
[2m[36m(func pid=158041)[0m top5: 0.8325559701492538
[2m[36m(func pid=158041)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=158041)[0m f1_macro: 0.29230271697504717
[2m[36m(func pid=158041)[0m f1_weighted: 0.28546107534397924
[2m[36m(func pid=158041)[0m f1_per_class: [0.289, 0.348, 0.741, 0.168, 0.0, 0.257, 0.367, 0.381, 0.157, 0.216]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3628731343283582
[2m[36m(func pid=157619)[0m top5: 0.9039179104477612
[2m[36m(func pid=157619)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=157619)[0m f1_macro: 0.3499501303993161
[2m[36m(func pid=157619)[0m f1_weighted: 0.3070463354931434
[2m[36m(func pid=157619)[0m f1_per_class: [0.536, 0.505, 0.558, 0.503, 0.222, 0.191, 0.031, 0.36, 0.238, 0.356]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:22:05 (running for 00:02:40.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.405 |      0.122 |                   20 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.038 |      0.35  |                   21 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.001 |      0.292 |                   21 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.898 |      0.001 |                   20 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m top1: 0.17257462686567165
[2m[36m(func pid=157245)[0m top5: 0.6711753731343284
[2m[36m(func pid=157245)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=157245)[0m f1_macro: 0.12812923915062002
[2m[36m(func pid=157245)[0m f1_weighted: 0.1875673600678727
[2m[36m(func pid=157245)[0m f1_per_class: [0.131, 0.191, 0.134, 0.103, 0.032, 0.123, 0.347, 0.0, 0.1, 0.12]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0049 | Steps: 2 | Val loss: 4.6886 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0283 | Steps: 2 | Val loss: 1.8971 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7832 | Steps: 2 | Val loss: 83258.4062 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.0889 | Steps: 2 | Val loss: 2.1922 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m top1: 0.3003731343283582
[2m[36m(func pid=158041)[0m top5: 0.816231343283582
[2m[36m(func pid=158041)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=158041)[0m f1_macro: 0.27158323286073865
[2m[36m(func pid=158041)[0m f1_weighted: 0.3007259890210933
[2m[36m(func pid=158041)[0m f1_per_class: [0.262, 0.296, 0.645, 0.169, 0.0, 0.224, 0.483, 0.293, 0.138, 0.206]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.36847014925373134
[2m[36m(func pid=157619)[0m top5: 0.9071828358208955
[2m[36m(func pid=157619)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=157619)[0m f1_macro: 0.3507061966221856
[2m[36m(func pid=157619)[0m f1_weighted: 0.313940600534503
[2m[36m(func pid=157619)[0m f1_per_class: [0.541, 0.508, 0.545, 0.515, 0.23, 0.206, 0.037, 0.361, 0.231, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:22:10 (running for 00:02:45.71)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.089 |      0.141 |                   22 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.028 |      0.351 |                   22 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.005 |      0.272 |                   22 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.783 |      0.001 |                   21 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.18983208955223882
[2m[36m(func pid=157245)[0m top5: 0.6888992537313433
[2m[36m(func pid=157245)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=157245)[0m f1_macro: 0.14118325598318768
[2m[36m(func pid=157245)[0m f1_weighted: 0.20279648617684187
[2m[36m(func pid=157245)[0m f1_per_class: [0.139, 0.206, 0.138, 0.106, 0.033, 0.124, 0.383, 0.0, 0.11, 0.174]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3485 | Steps: 2 | Val loss: 4.9181 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0248 | Steps: 2 | Val loss: 1.9102 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.0638 | Steps: 2 | Val loss: 55184.2422 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.0475 | Steps: 2 | Val loss: 2.1653 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=158041)[0m top1: 0.35027985074626866
[2m[36m(func pid=158041)[0m top5: 0.8367537313432836
[2m[36m(func pid=158041)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=158041)[0m f1_macro: 0.2826257956516812
[2m[36m(func pid=158041)[0m f1_weighted: 0.3349036194110827
[2m[36m(func pid=158041)[0m f1_per_class: [0.192, 0.386, 0.8, 0.236, 0.0, 0.191, 0.528, 0.106, 0.179, 0.207]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.37080223880597013
[2m[36m(func pid=157619)[0m top5: 0.9081156716417911
[2m[36m(func pid=157619)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=157619)[0m f1_macro: 0.3534175604784492
[2m[36m(func pid=157619)[0m f1_weighted: 0.3177638458132628
[2m[36m(func pid=157619)[0m f1_per_class: [0.547, 0.515, 0.545, 0.514, 0.233, 0.218, 0.04, 0.366, 0.238, 0.318]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:22:16 (running for 00:02:50.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  1.047 |      0.148 |                   23 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.025 |      0.353 |                   23 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.349 |      0.283 |                   23 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.064 |      0.001 |                   22 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.20662313432835822
[2m[36m(func pid=157245)[0m top5: 0.7075559701492538
[2m[36m(func pid=157245)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=157245)[0m f1_macro: 0.14758823572611007
[2m[36m(func pid=157245)[0m f1_weighted: 0.2175752982365735
[2m[36m(func pid=157245)[0m f1_per_class: [0.143, 0.207, 0.153, 0.117, 0.035, 0.132, 0.418, 0.0, 0.114, 0.157]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2424 | Steps: 2 | Val loss: 5.4055 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0175 | Steps: 2 | Val loss: 1.9289 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.9855 | Steps: 2 | Val loss: 45704.8477 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.9130 | Steps: 2 | Val loss: 2.1399 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=158041)[0m top1: 0.271455223880597
[2m[36m(func pid=158041)[0m top5: 0.8218283582089553
[2m[36m(func pid=158041)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=158041)[0m f1_macro: 0.1955878082624156
[2m[36m(func pid=158041)[0m f1_weighted: 0.3040116689460303
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.231, 0.044, 0.246, 0.182, 0.23, 0.501, 0.201, 0.132, 0.189]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3670708955223881
[2m[36m(func pid=157619)[0m top5: 0.9127798507462687
[2m[36m(func pid=157619)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=157619)[0m f1_macro: 0.3500578897014829
[2m[36m(func pid=157619)[0m f1_weighted: 0.31550991833309566
[2m[36m(func pid=157619)[0m f1_per_class: [0.542, 0.496, 0.545, 0.52, 0.233, 0.22, 0.04, 0.353, 0.241, 0.311]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:22:21 (running for 00:02:56.09)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.913 |      0.156 |                   24 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.017 |      0.35  |                   24 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.242 |      0.196 |                   24 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.986 |      0.001 |                   23 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.22388059701492538
[2m[36m(func pid=157245)[0m top5: 0.7201492537313433
[2m[36m(func pid=157245)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=157245)[0m f1_macro: 0.15560935984594745
[2m[36m(func pid=157245)[0m f1_weighted: 0.23369966590460814
[2m[36m(func pid=157245)[0m f1_per_class: [0.153, 0.222, 0.143, 0.135, 0.037, 0.129, 0.445, 0.0, 0.122, 0.169]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2793 | Steps: 2 | Val loss: 13.3345 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0163 | Steps: 2 | Val loss: 1.9402 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8498 | Steps: 2 | Val loss: 30026.5176 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8406 | Steps: 2 | Val loss: 2.1154 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=158041)[0m top1: 0.20009328358208955
[2m[36m(func pid=158041)[0m top5: 0.7019589552238806
[2m[36m(func pid=158041)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=158041)[0m f1_macro: 0.1821855666819677
[2m[36m(func pid=158041)[0m f1_weighted: 0.23931130985786359
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.1, 0.024, 0.092, 0.209, 0.291, 0.452, 0.351, 0.131, 0.171]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3712686567164179
[2m[36m(func pid=157619)[0m top5: 0.9169776119402985
[2m[36m(func pid=157619)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=157619)[0m f1_macro: 0.35812163490222193
[2m[36m(func pid=157619)[0m f1_weighted: 0.31988268121637803
[2m[36m(func pid=157619)[0m f1_per_class: [0.547, 0.501, 0.558, 0.522, 0.244, 0.231, 0.042, 0.351, 0.247, 0.337]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:22:26 (running for 00:03:01.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.841 |      0.162 |                   25 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.016 |      0.358 |                   25 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.279 |      0.182 |                   25 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.85  |      0.001 |                   24 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.2392723880597015
[2m[36m(func pid=157245)[0m top5: 0.7355410447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=157245)[0m f1_macro: 0.16246534402986265
[2m[36m(func pid=157245)[0m f1_weighted: 0.24465835091065435
[2m[36m(func pid=157245)[0m f1_per_class: [0.15, 0.237, 0.158, 0.14, 0.052, 0.129, 0.469, 0.0, 0.126, 0.165]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3464 | Steps: 2 | Val loss: 9.3470 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0212 | Steps: 2 | Val loss: 1.9443 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 4.4105 | Steps: 2 | Val loss: 30603.9023 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7167 | Steps: 2 | Val loss: 2.0859 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=158041)[0m top1: 0.23833955223880596
[2m[36m(func pid=158041)[0m top5: 0.664179104477612
[2m[36m(func pid=158041)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=158041)[0m f1_macro: 0.2018723400155725
[2m[36m(func pid=158041)[0m f1_weighted: 0.2735319821898681
[2m[36m(func pid=158041)[0m f1_per_class: [0.084, 0.244, 0.038, 0.051, 0.175, 0.436, 0.482, 0.234, 0.179, 0.096]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3763992537313433
[2m[36m(func pid=157619)[0m top5: 0.9202425373134329
[2m[36m(func pid=157619)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=157619)[0m f1_macro: 0.36477233132642867
[2m[36m(func pid=157619)[0m f1_weighted: 0.32764411892843287
[2m[36m(func pid=157619)[0m f1_per_class: [0.552, 0.496, 0.558, 0.522, 0.25, 0.262, 0.057, 0.36, 0.249, 0.34]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m top1: 0.2593283582089552
[2m[36m(func pid=157245)[0m top5: 0.7513992537313433
[2m[36m(func pid=157245)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=157245)[0m f1_macro: 0.17356959053279974
[2m[36m(func pid=157245)[0m f1_weighted: 0.26319206961525893
[2m[36m(func pid=157245)[0m f1_per_class: [0.158, 0.247, 0.165, 0.167, 0.064, 0.136, 0.495, 0.0, 0.131, 0.173]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3149 | Steps: 2 | Val loss: 14.1023 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0097 | Steps: 2 | Val loss: 1.9586 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5345 | Steps: 2 | Val loss: 37498.0820 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6360 | Steps: 2 | Val loss: 2.0636 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 03:22:34 (running for 00:03:09.33)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.717 |      0.174 |                   26 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.021 |      0.365 |                   26 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.315 |      0.192 |                   27 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.411 |      0.001 |                   25 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.2224813432835821
[2m[36m(func pid=158041)[0m top5: 0.6557835820895522
[2m[36m(func pid=158041)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=158041)[0m f1_macro: 0.191896850988458
[2m[36m(func pid=158041)[0m f1_weighted: 0.19828765038340437
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.305, 0.632, 0.0, 0.067, 0.387, 0.303, 0.031, 0.131, 0.064]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m top1: 0.37919776119402987
[2m[36m(func pid=157619)[0m top5: 0.9225746268656716
[2m[36m(func pid=157619)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=157619)[0m f1_macro: 0.36938841514482024
[2m[36m(func pid=157619)[0m f1_weighted: 0.33522385130400445
[2m[36m(func pid=157619)[0m f1_per_class: [0.574, 0.496, 0.585, 0.534, 0.244, 0.262, 0.072, 0.354, 0.247, 0.326]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.27705223880597013
[2m[36m(func pid=157245)[0m top5: 0.7700559701492538
[2m[36m(func pid=157245)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=157245)[0m f1_macro: 0.18510082799174743
[2m[36m(func pid=157245)[0m f1_weighted: 0.2761505984781663
[2m[36m(func pid=157245)[0m f1_per_class: [0.167, 0.25, 0.2, 0.183, 0.078, 0.137, 0.519, 0.0, 0.138, 0.179]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.2653 | Steps: 2 | Val loss: 13.2839 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0982 | Steps: 2 | Val loss: 40219.5859 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0058 | Steps: 2 | Val loss: 1.9773 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6205 | Steps: 2 | Val loss: 2.0409 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:22:39 (running for 00:03:14.60)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.636 |      0.185 |                   27 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.01  |      0.369 |                   27 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.265 |      0.162 |                   28 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.534 |      0.001 |                   26 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.1142723880597015
[2m[36m(func pid=158041)[0m top5: 0.652518656716418
[2m[36m(func pid=158041)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=158041)[0m f1_macro: 0.16181425737211166
[2m[36m(func pid=158041)[0m f1_weighted: 0.1103645092001877
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.315, 0.667, 0.003, 0.021, 0.024, 0.097, 0.241, 0.121, 0.13]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3810634328358209
[2m[36m(func pid=157619)[0m top5: 0.9249067164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=157619)[0m f1_macro: 0.3734011411990535
[2m[36m(func pid=157619)[0m f1_weighted: 0.33891117085686184
[2m[36m(func pid=157619)[0m f1_per_class: [0.554, 0.488, 0.585, 0.535, 0.25, 0.28, 0.08, 0.361, 0.243, 0.358]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.28824626865671643
[2m[36m(func pid=157245)[0m top5: 0.7821828358208955
[2m[36m(func pid=157245)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=157245)[0m f1_macro: 0.19128897957867616
[2m[36m(func pid=157245)[0m f1_weighted: 0.2877391503527662
[2m[36m(func pid=157245)[0m f1_per_class: [0.173, 0.25, 0.212, 0.21, 0.074, 0.134, 0.532, 0.0, 0.142, 0.187]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.9386 | Steps: 2 | Val loss: 6.1093 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3049 | Steps: 2 | Val loss: 39679.1172 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0110 | Steps: 2 | Val loss: 1.9887 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5758 | Steps: 2 | Val loss: 2.0228 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:22:44 (running for 00:03:19.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.621 |      0.191 |                   28 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.006 |      0.373 |                   28 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.939 |      0.09  |                   29 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.098 |      0.001 |                   27 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.16744402985074627
[2m[36m(func pid=158041)[0m top5: 0.6893656716417911
[2m[36m(func pid=158041)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=158041)[0m f1_macro: 0.08992837538827733
[2m[36m(func pid=158041)[0m f1_weighted: 0.0902334597530015
[2m[36m(func pid=158041)[0m f1_per_class: [0.078, 0.29, 0.0, 0.045, 0.0, 0.0, 0.027, 0.192, 0.187, 0.081]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m top1: 0.38526119402985076
[2m[36m(func pid=157619)[0m top5: 0.9235074626865671
[2m[36m(func pid=157619)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=157619)[0m f1_macro: 0.3732605485734225
[2m[36m(func pid=157619)[0m f1_weighted: 0.3453225473457576
[2m[36m(func pid=157619)[0m f1_per_class: [0.551, 0.492, 0.571, 0.541, 0.25, 0.288, 0.091, 0.359, 0.244, 0.344]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.2947761194029851
[2m[36m(func pid=157245)[0m top5: 0.8003731343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=157245)[0m f1_macro: 0.196712172263294
[2m[36m(func pid=157245)[0m f1_weighted: 0.2958855347433968
[2m[36m(func pid=157245)[0m f1_per_class: [0.177, 0.249, 0.224, 0.232, 0.074, 0.139, 0.536, 0.0, 0.144, 0.192]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0240 | Steps: 2 | Val loss: 26.1911 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4294 | Steps: 2 | Val loss: 39669.3398 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0052 | Steps: 2 | Val loss: 1.9962 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4945 | Steps: 2 | Val loss: 2.0021 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=158041)[0m top1: 0.1837686567164179
[2m[36m(func pid=158041)[0m top5: 0.6147388059701493
[2m[36m(func pid=158041)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=158041)[0m f1_macro: 0.0662283985180067
[2m[36m(func pid=158041)[0m f1_weighted: 0.07281840049967576
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.296, 0.0, 0.0, 0.0, 0.0, 0.003, 0.364, 0.0, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 03:22:50 (running for 00:03:25.80)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.576 |      0.197 |                   29 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.011 |      0.373 |                   29 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.024 |      0.066 |                   30 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.429 |      0.001 |                   29 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m top1: 0.386660447761194
[2m[36m(func pid=157619)[0m top5: 0.925839552238806
[2m[36m(func pid=157619)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.3746765433044069
[2m[36m(func pid=157619)[0m f1_weighted: 0.34819364099326755
[2m[36m(func pid=157619)[0m f1_per_class: [0.553, 0.499, 0.558, 0.547, 0.268, 0.278, 0.096, 0.359, 0.234, 0.354]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.30223880597014924
[2m[36m(func pid=157245)[0m top5: 0.8111007462686567
[2m[36m(func pid=157245)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=157245)[0m f1_macro: 0.20231526314227194
[2m[36m(func pid=157245)[0m f1_weighted: 0.3048470166749384
[2m[36m(func pid=157245)[0m f1_per_class: [0.185, 0.257, 0.226, 0.256, 0.077, 0.142, 0.537, 0.0, 0.15, 0.192]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2517 | Steps: 2 | Val loss: 69.5075 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 20.7716 | Steps: 2 | Val loss: 36277.3164 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0051 | Steps: 2 | Val loss: 2.0095 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=158041)[0m top1: 0.16930970149253732
[2m[36m(func pid=158041)[0m top5: 0.3894589552238806
[2m[36m(func pid=158041)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=158041)[0m f1_macro: 0.035580404685835995
[2m[36m(func pid=158041)[0m f1_weighted: 0.05054027983087757
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.067]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4494 | Steps: 2 | Val loss: 1.9817 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:22:56 (running for 00:03:31.04)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.495 |      0.202 |                   30 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.005 |      0.375 |                   30 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.252 |      0.036 |                   31 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 20.772 |      0.001 |                   30 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m top1: 0.3833955223880597
[2m[36m(func pid=157619)[0m top5: 0.9253731343283582
[2m[36m(func pid=157619)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=157619)[0m f1_macro: 0.37352085747362274
[2m[36m(func pid=157619)[0m f1_weighted: 0.3471840051942081
[2m[36m(func pid=157619)[0m f1_per_class: [0.547, 0.492, 0.558, 0.54, 0.272, 0.278, 0.104, 0.354, 0.235, 0.354]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.30923507462686567
[2m[36m(func pid=157245)[0m top5: 0.8218283582089553
[2m[36m(func pid=157245)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=157245)[0m f1_macro: 0.21082034282299889
[2m[36m(func pid=157245)[0m f1_weighted: 0.31085669185138864
[2m[36m(func pid=157245)[0m f1_per_class: [0.196, 0.264, 0.255, 0.262, 0.079, 0.135, 0.547, 0.0, 0.153, 0.216]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.1982 | Steps: 2 | Val loss: 78.4085 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.1958 | Steps: 2 | Val loss: 31869.1680 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0029 | Steps: 2 | Val loss: 2.0122 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=158041)[0m top1: 0.16651119402985073
[2m[36m(func pid=158041)[0m top5: 0.3316231343283582
[2m[36m(func pid=158041)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=158041)[0m f1_macro: 0.04407389011092977
[2m[36m(func pid=158041)[0m f1_weighted: 0.05068650662108459
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.157]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4267 | Steps: 2 | Val loss: 1.9571 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:23:02 (running for 00:03:36.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.449 |      0.211 |                   31 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.374 |                   32 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.198 |      0.044 |                   32 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.196 |      0.001 |                   31 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.38526119402985076
[2m[36m(func pid=157619)[0m top5: 0.9281716417910447
[2m[36m(func pid=157619)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=157619)[0m f1_macro: 0.373817434344841
[2m[36m(func pid=157619)[0m f1_weighted: 0.3537010302872358
[2m[36m(func pid=157619)[0m f1_per_class: [0.538, 0.493, 0.571, 0.544, 0.278, 0.286, 0.122, 0.347, 0.232, 0.327]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.3001 | Steps: 2 | Val loss: 158.3423 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=157245)[0m top1: 0.31902985074626866
[2m[36m(func pid=157245)[0m top5: 0.8316231343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=157245)[0m f1_macro: 0.2161574433529246
[2m[36m(func pid=157245)[0m f1_weighted: 0.32053199022226614
[2m[36m(func pid=157245)[0m f1_per_class: [0.211, 0.268, 0.267, 0.291, 0.085, 0.133, 0.551, 0.0, 0.151, 0.205]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3207 | Steps: 2 | Val loss: 25328.1914 | Batch size: 32 | lr: 0.1 | Duration: 2.58s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0044 | Steps: 2 | Val loss: 2.0186 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158041)[0m top1: 0.06856343283582089
[2m[36m(func pid=158041)[0m top5: 0.5694962686567164
[2m[36m(func pid=158041)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=158041)[0m f1_macro: 0.030540248712931685
[2m[36m(func pid=158041)[0m f1_weighted: 0.04807012837831618
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3866 | Steps: 2 | Val loss: 1.9357 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:23:07 (running for 00:03:42.38)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.427 |      0.216 |                   32 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.004 |      0.381 |                   33 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.3   |      0.031 |                   33 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.321 |      0.001 |                   32 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.3927238805970149
[2m[36m(func pid=157619)[0m top5: 0.9333022388059702
[2m[36m(func pid=157619)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=157619)[0m f1_macro: 0.3810381598232952
[2m[36m(func pid=157619)[0m f1_weighted: 0.3626002391542767
[2m[36m(func pid=157619)[0m f1_per_class: [0.549, 0.497, 0.571, 0.553, 0.301, 0.304, 0.133, 0.347, 0.233, 0.321]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8676 | Steps: 2 | Val loss: 470.4408 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=157245)[0m top1: 0.32742537313432835
[2m[36m(func pid=157245)[0m top5: 0.840018656716418
[2m[36m(func pid=157245)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=157245)[0m f1_macro: 0.224827053766887
[2m[36m(func pid=157245)[0m f1_weighted: 0.3283605021038034
[2m[36m(func pid=157245)[0m f1_per_class: [0.221, 0.28, 0.27, 0.304, 0.101, 0.133, 0.556, 0.0, 0.153, 0.231]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2302 | Steps: 2 | Val loss: 23220.1777 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0031 | Steps: 2 | Val loss: 2.0272 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m top1: 0.01166044776119403
[2m[36m(func pid=158041)[0m top5: 0.5611007462686567
[2m[36m(func pid=158041)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=158041)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=158041)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3655 | Steps: 2 | Val loss: 1.9147 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5088619402985075
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:23:12 (running for 00:03:47.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.387 |      0.225 |                   33 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.387 |                   34 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.868 |      0.002 |                   34 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.23  |      0.001 |                   33 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.396455223880597
[2m[36m(func pid=157619)[0m top5: 0.9361007462686567
[2m[36m(func pid=157619)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=157619)[0m f1_macro: 0.38712054633569926
[2m[36m(func pid=157619)[0m f1_weighted: 0.366963459759445
[2m[36m(func pid=157619)[0m f1_per_class: [0.549, 0.501, 0.6, 0.562, 0.329, 0.308, 0.135, 0.344, 0.234, 0.309]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.3655 | Steps: 2 | Val loss: 601.9619 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157245)[0m top1: 0.332089552238806
[2m[36m(func pid=157245)[0m top5: 0.8493470149253731
[2m[36m(func pid=157245)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=157245)[0m f1_macro: 0.23177027769125474
[2m[36m(func pid=157245)[0m f1_weighted: 0.3339697602731202
[2m[36m(func pid=157245)[0m f1_per_class: [0.246, 0.293, 0.279, 0.316, 0.102, 0.131, 0.551, 0.015, 0.158, 0.228]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.1843 | Steps: 2 | Val loss: 17915.7559 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=158041)[0m top1: 0.012126865671641791
[2m[36m(func pid=158041)[0m top5: 0.5307835820895522
[2m[36m(func pid=158041)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=158041)[0m f1_macro: 0.002850009362711441
[2m[36m(func pid=158041)[0m f1_weighted: 0.001199609048571511
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0041 | Steps: 2 | Val loss: 2.0604 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3367 | Steps: 2 | Val loss: 1.8845 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5088619402985075
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:23:17 (running for 00:03:52.66)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.366 |      0.232 |                   34 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.004 |      0.388 |                   35 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.365 |      0.003 |                   35 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.184 |      0.001 |                   34 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.39505597014925375
[2m[36m(func pid=157619)[0m top5: 0.9370335820895522
[2m[36m(func pid=157619)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=157619)[0m f1_macro: 0.387574723152263
[2m[36m(func pid=157619)[0m f1_weighted: 0.36572677129586434
[2m[36m(func pid=157619)[0m f1_per_class: [0.549, 0.501, 0.6, 0.56, 0.342, 0.302, 0.135, 0.343, 0.232, 0.312]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4040 | Steps: 2 | Val loss: 444.9884 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=157245)[0m top1: 0.34701492537313433
[2m[36m(func pid=157245)[0m top5: 0.8628731343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=157245)[0m f1_macro: 0.2507449435337993
[2m[36m(func pid=157245)[0m f1_weighted: 0.35018177390183197
[2m[36m(func pid=157245)[0m f1_per_class: [0.254, 0.315, 0.333, 0.342, 0.104, 0.136, 0.55, 0.082, 0.174, 0.217]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 21.1771 | Steps: 2 | Val loss: 14670.7080 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=158041)[0m top1: 0.010727611940298507
[2m[36m(func pid=158041)[0m top5: 0.4916044776119403
[2m[36m(func pid=158041)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=158041)[0m f1_macro: 0.002147525676937442
[2m[36m(func pid=158041)[0m f1_weighted: 0.00025041110971751884
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.0718 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3026 | Steps: 2 | Val loss: 1.8632 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=158460)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:23:22 (running for 00:03:57.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.337 |      0.251 |                   35 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.002 |      0.388 |                   36 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.404 |      0.002 |                   36 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 21.177 |      0.001 |                   35 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.39598880597014924
[2m[36m(func pid=157619)[0m top5: 0.9365671641791045
[2m[36m(func pid=157619)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=157619)[0m f1_macro: 0.38828006650691665
[2m[36m(func pid=157619)[0m f1_weighted: 0.3725986473722423
[2m[36m(func pid=157619)[0m f1_per_class: [0.549, 0.503, 0.6, 0.557, 0.313, 0.295, 0.163, 0.34, 0.232, 0.33]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6833 | Steps: 2 | Val loss: 246.1800 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=157245)[0m top1: 0.35027985074626866
[2m[36m(func pid=157245)[0m top5: 0.8745335820895522
[2m[36m(func pid=157245)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=157245)[0m f1_macro: 0.25613781415738196
[2m[36m(func pid=157245)[0m f1_weighted: 0.35586179078663965
[2m[36m(func pid=157245)[0m f1_per_class: [0.276, 0.323, 0.338, 0.368, 0.103, 0.136, 0.538, 0.079, 0.182, 0.217]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.0399 | Steps: 2 | Val loss: 9722.6094 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
[2m[36m(func pid=158041)[0m top1: 0.010727611940298507
[2m[36m(func pid=158041)[0m top5: 0.4435634328358209
[2m[36m(func pid=158041)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=158041)[0m f1_macro: 0.002183198860939725
[2m[36m(func pid=158041)[0m f1_weighted: 0.0002545707627028597
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0041 | Steps: 2 | Val loss: 2.1049 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5065298507462687
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012070566388115134
[2m[36m(func pid=158460)[0m f1_weighted: 7.318906858465332e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2571 | Steps: 2 | Val loss: 1.8444 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:23:27 (running for 00:04:02.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.303 |      0.256 |                   36 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.004 |      0.389 |                   37 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.683 |      0.002 |                   37 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.04  |      0.001 |                   36 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.39598880597014924
[2m[36m(func pid=157619)[0m top5: 0.9365671641791045
[2m[36m(func pid=157619)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=157619)[0m f1_macro: 0.388951484428454
[2m[36m(func pid=157619)[0m f1_weighted: 0.37339555301654154
[2m[36m(func pid=157619)[0m f1_per_class: [0.533, 0.505, 0.6, 0.561, 0.333, 0.3, 0.161, 0.335, 0.228, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0609 | Steps: 2 | Val loss: 172.1312 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=157245)[0m top1: 0.355410447761194
[2m[36m(func pid=157245)[0m top5: 0.8805970149253731
[2m[36m(func pid=157245)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=157245)[0m f1_macro: 0.2770819766575111
[2m[36m(func pid=157245)[0m f1_weighted: 0.3628266188438372
[2m[36m(func pid=157245)[0m f1_per_class: [0.31, 0.346, 0.375, 0.378, 0.102, 0.128, 0.525, 0.136, 0.192, 0.279]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.1904 | Steps: 2 | Val loss: 7360.6763 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=158041)[0m top1: 0.013526119402985074
[2m[36m(func pid=158041)[0m top5: 0.3969216417910448
[2m[36m(func pid=158041)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=158041)[0m f1_macro: 0.006237309170722178
[2m[36m(func pid=158041)[0m f1_weighted: 0.0005196809314836503
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0036 | Steps: 2 | Val loss: 2.1156 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5083955223880597
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012115563839701772
[2m[36m(func pid=158460)[0m f1_weighted: 7.346190761013201e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2052 | Steps: 2 | Val loss: 1.8262 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 03:23:33 (running for 00:04:07.97)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.257 |      0.277 |                   37 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.004 |      0.393 |                   38 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.061 |      0.006 |                   38 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.19  |      0.001 |                   37 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.3987873134328358
[2m[36m(func pid=157619)[0m top5: 0.9370335820895522
[2m[36m(func pid=157619)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=157619)[0m f1_macro: 0.3932670420965627
[2m[36m(func pid=157619)[0m f1_weighted: 0.3798222591419726
[2m[36m(func pid=157619)[0m f1_per_class: [0.554, 0.501, 0.6, 0.562, 0.338, 0.305, 0.18, 0.337, 0.225, 0.33]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2496 | Steps: 2 | Val loss: 156.1850 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=157245)[0m top1: 0.36100746268656714
[2m[36m(func pid=157245)[0m top5: 0.8875932835820896
[2m[36m(func pid=157245)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=157245)[0m f1_macro: 0.28777325300273704
[2m[36m(func pid=157245)[0m f1_weighted: 0.3705748782391162
[2m[36m(func pid=157245)[0m f1_per_class: [0.325, 0.356, 0.393, 0.393, 0.103, 0.133, 0.52, 0.168, 0.201, 0.286]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3032 | Steps: 2 | Val loss: 5648.1416 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=158041)[0m top1: 0.014458955223880597
[2m[36m(func pid=158041)[0m top5: 0.427705223880597
[2m[36m(func pid=158041)[0m f1_micro: 0.014458955223880597
[2m[36m(func pid=158041)[0m f1_macro: 0.00804165602419431
[2m[36m(func pid=158041)[0m f1_weighted: 0.005975538663080827
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.01, 0.032, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0019 | Steps: 2 | Val loss: 2.1288 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5093283582089553
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012363290537327628
[2m[36m(func pid=158460)[0m f1_weighted: 7.49639818028261e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2528 | Steps: 2 | Val loss: 1.8145 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 03:23:38 (running for 00:04:13.15)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.205 |      0.288 |                   38 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.002 |      0.397 |                   39 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.25  |      0.008 |                   39 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.303 |      0.001 |                   38 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.4006529850746269
[2m[36m(func pid=157619)[0m top5: 0.9375
[2m[36m(func pid=157619)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=157619)[0m f1_macro: 0.39658716869954724
[2m[36m(func pid=157619)[0m f1_weighted: 0.38219405278292934
[2m[36m(func pid=157619)[0m f1_per_class: [0.554, 0.498, 0.6, 0.567, 0.347, 0.311, 0.181, 0.339, 0.226, 0.342]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2373 | Steps: 2 | Val loss: 165.6618 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 38.7186 | Steps: 2 | Val loss: 6038.6865 | Batch size: 32 | lr: 0.1 | Duration: 2.54s
[2m[36m(func pid=157245)[0m top1: 0.36100746268656714
[2m[36m(func pid=157245)[0m top5: 0.8931902985074627
[2m[36m(func pid=157245)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=157245)[0m f1_macro: 0.2922332486271212
[2m[36m(func pid=157245)[0m f1_weighted: 0.37500963698974554
[2m[36m(func pid=157245)[0m f1_per_class: [0.333, 0.375, 0.387, 0.405, 0.099, 0.155, 0.5, 0.194, 0.192, 0.282]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.02332089552238806
[2m[36m(func pid=158041)[0m top5: 0.4944029850746269
[2m[36m(func pid=158041)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=158041)[0m f1_macro: 0.009367696786912641
[2m[36m(func pid=158041)[0m f1_weighted: 0.010597514513134067
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.1193 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=158460)[0m top1: 0.03404850746268657
[2m[36m(func pid=158460)[0m top5: 0.4496268656716418
[2m[36m(func pid=158460)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=158460)[0m f1_macro: 0.016819923371647512
[2m[36m(func pid=158460)[0m f1_weighted: 0.026467340024017842
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.153, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1747 | Steps: 2 | Val loss: 1.7966 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 03:23:43 (running for 00:04:18.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.253 |      0.292 |                   39 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.002 |      0.396 |                   40 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.237 |      0.009 |                   40 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 38.719 |      0.017 |                   39 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.40205223880597013
[2m[36m(func pid=157619)[0m top5: 0.941231343283582
[2m[36m(func pid=157619)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=157619)[0m f1_macro: 0.39611929731912965
[2m[36m(func pid=157619)[0m f1_weighted: 0.3856762657292276
[2m[36m(func pid=157619)[0m f1_per_class: [0.541, 0.494, 0.6, 0.567, 0.351, 0.316, 0.195, 0.336, 0.224, 0.336]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.3346 | Steps: 2 | Val loss: 98.7508 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 10.8968 | Steps: 2 | Val loss: 8879.6885 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=157245)[0m top1: 0.36473880597014924
[2m[36m(func pid=157245)[0m top5: 0.8987873134328358
[2m[36m(func pid=157245)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=157245)[0m f1_macro: 0.30012541316854796
[2m[36m(func pid=157245)[0m f1_weighted: 0.3773044796626985
[2m[36m(func pid=157245)[0m f1_per_class: [0.366, 0.38, 0.407, 0.419, 0.105, 0.157, 0.487, 0.192, 0.196, 0.292]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.015391791044776119
[2m[36m(func pid=158041)[0m top5: 0.5438432835820896
[2m[36m(func pid=158041)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=158041)[0m f1_macro: 0.005467067370915289
[2m[36m(func pid=158041)[0m f1_weighted: 0.007418124476209534
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0037 | Steps: 2 | Val loss: 2.1510 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=158460)[0m top1: 0.17210820895522388
[2m[36m(func pid=158460)[0m top5: 0.566231343283582
[2m[36m(func pid=158460)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=158460)[0m f1_macro: 0.029638554216867473
[2m[36m(func pid=158460)[0m f1_weighted: 0.05101038482287359
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1516 | Steps: 2 | Val loss: 1.7855 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:23:48 (running for 00:04:23.63)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.175 |      0.3   |                   40 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.004 |      0.396 |                   41 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.335 |      0.005 |                   41 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 10.897 |      0.03  |                   40 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.40205223880597013
[2m[36m(func pid=157619)[0m top5: 0.9421641791044776
[2m[36m(func pid=157619)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=157619)[0m f1_macro: 0.3957315568667651
[2m[36m(func pid=157619)[0m f1_weighted: 0.3868815976711776
[2m[36m(func pid=157619)[0m f1_per_class: [0.537, 0.488, 0.6, 0.571, 0.361, 0.314, 0.202, 0.327, 0.224, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2664 | Steps: 2 | Val loss: 99.4458 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 8.8648 | Steps: 2 | Val loss: 8738.7080 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=157245)[0m top1: 0.365205223880597
[2m[36m(func pid=157245)[0m top5: 0.9029850746268657
[2m[36m(func pid=157245)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=157245)[0m f1_macro: 0.3072588933766895
[2m[36m(func pid=157245)[0m f1_weighted: 0.378833956800839
[2m[36m(func pid=157245)[0m f1_per_class: [0.372, 0.381, 0.407, 0.428, 0.11, 0.178, 0.467, 0.233, 0.196, 0.301]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.01585820895522388
[2m[36m(func pid=158041)[0m top5: 0.6324626865671642
[2m[36m(func pid=158041)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=158041)[0m f1_macro: 0.007419164221716894
[2m[36m(func pid=158041)[0m f1_weighted: 0.0075787639938421835
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.026, 0.025]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.06203358208955224
[2m[36m(func pid=158460)[0m top5: 0.5359141791044776
[2m[36m(func pid=158460)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=158460)[0m f1_macro: 0.015753113183452006
[2m[36m(func pid=158460)[0m f1_weighted: 0.014328859589250295
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.1771 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1880 | Steps: 2 | Val loss: 1.7686 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.9179 | Steps: 2 | Val loss: 115.4109 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:23:53 (running for 00:04:28.79)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.152 |      0.307 |                   41 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.389 |                   42 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.266 |      0.007 |                   42 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  8.865 |      0.016 |                   41 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.4006529850746269
[2m[36m(func pid=157619)[0m top5: 0.9421641791044776
[2m[36m(func pid=157619)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=157619)[0m f1_macro: 0.3887851418276738
[2m[36m(func pid=157619)[0m f1_weighted: 0.38643492774215993
[2m[36m(func pid=157619)[0m f1_per_class: [0.506, 0.484, 0.6, 0.572, 0.338, 0.311, 0.207, 0.328, 0.221, 0.321]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 4.4751 | Steps: 2 | Val loss: 4587.0557 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=157245)[0m top1: 0.37080223880597013
[2m[36m(func pid=157245)[0m top5: 0.9048507462686567
[2m[36m(func pid=157245)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=157245)[0m f1_macro: 0.31463055086894137
[2m[36m(func pid=157245)[0m f1_weighted: 0.38527232037497106
[2m[36m(func pid=157245)[0m f1_per_class: [0.391, 0.381, 0.4, 0.44, 0.109, 0.181, 0.465, 0.283, 0.195, 0.301]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.021921641791044777
[2m[36m(func pid=158041)[0m top5: 0.39365671641791045
[2m[36m(func pid=158041)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=158041)[0m f1_macro: 0.01021082999340796
[2m[36m(func pid=158041)[0m f1_weighted: 0.0035398101381631016
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.068, 0.031]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.05783582089552239
[2m[36m(func pid=158460)[0m top5: 0.5074626865671642
[2m[36m(func pid=158460)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=158460)[0m f1_macro: 0.01106648817492191
[2m[36m(func pid=158460)[0m f1_weighted: 0.006400394280271999
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0115 | Steps: 2 | Val loss: 2.2034 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1355 | Steps: 2 | Val loss: 1.7567 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.3756 | Steps: 2 | Val loss: 179.0829 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:23:59 (running for 00:04:33.98)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.188 |      0.315 |                   42 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.011 |      0.394 |                   43 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.918 |      0.01  |                   43 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.475 |      0.011 |                   42 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.4048507462686567
[2m[36m(func pid=157619)[0m top5: 0.9416977611940298
[2m[36m(func pid=157619)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=157619)[0m f1_macro: 0.3936447045069947
[2m[36m(func pid=157619)[0m f1_weighted: 0.38896719613675296
[2m[36m(func pid=157619)[0m f1_per_class: [0.519, 0.487, 0.6, 0.583, 0.361, 0.316, 0.199, 0.326, 0.226, 0.319]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7171 | Steps: 2 | Val loss: 3031.1711 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=157245)[0m top1: 0.3614738805970149
[2m[36m(func pid=157245)[0m top5: 0.9067164179104478
[2m[36m(func pid=157245)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=157245)[0m f1_macro: 0.311802613932924
[2m[36m(func pid=157245)[0m f1_weighted: 0.3737629158941157
[2m[36m(func pid=157245)[0m f1_per_class: [0.391, 0.385, 0.407, 0.447, 0.122, 0.188, 0.416, 0.278, 0.192, 0.292]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.03451492537313433
[2m[36m(func pid=158041)[0m top5: 0.3050373134328358
[2m[36m(func pid=158041)[0m f1_micro: 0.03451492537313433
[2m[36m(func pid=158041)[0m f1_macro: 0.012771868384179916
[2m[36m(func pid=158041)[0m f1_weighted: 0.0033337450632419784
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.086, 0.042]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.055970149253731345
[2m[36m(func pid=158460)[0m top5: 0.5046641791044776
[2m[36m(func pid=158460)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=158460)[0m f1_macro: 0.013143483023001095
[2m[36m(func pid=158460)[0m f1_weighted: 0.007601641300616306
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.131, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0027 | Steps: 2 | Val loss: 2.2124 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1192 | Steps: 2 | Val loss: 1.7439 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7430 | Steps: 2 | Val loss: 265.6307 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 28.5018 | Steps: 2 | Val loss: 2658.3394 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:24:04 (running for 00:04:39.10)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.135 |      0.312 |                   43 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.396 |                   44 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.376 |      0.013 |                   44 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.717 |      0.013 |                   43 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.408115671641791
[2m[36m(func pid=157619)[0m top5: 0.9440298507462687
[2m[36m(func pid=157619)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=157619)[0m f1_macro: 0.39599715287530257
[2m[36m(func pid=157619)[0m f1_weighted: 0.39461597673923127
[2m[36m(func pid=157619)[0m f1_per_class: [0.526, 0.483, 0.6, 0.581, 0.356, 0.322, 0.22, 0.326, 0.228, 0.319]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3619402985074627
[2m[36m(func pid=157245)[0m top5: 0.9127798507462687
[2m[36m(func pid=157245)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=157245)[0m f1_macro: 0.32123321191460985
[2m[36m(func pid=157245)[0m f1_weighted: 0.370917181897919
[2m[36m(func pid=157245)[0m f1_per_class: [0.409, 0.389, 0.48, 0.461, 0.122, 0.186, 0.387, 0.28, 0.205, 0.292]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.03871268656716418
[2m[36m(func pid=158041)[0m top5: 0.4962686567164179
[2m[36m(func pid=158041)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=158041)[0m f1_macro: 0.013679063320008312
[2m[36m(func pid=158041)[0m f1_weighted: 0.0033104338273996265
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.057]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.004197761194029851
[2m[36m(func pid=158460)[0m top5: 0.5069962686567164
[2m[36m(func pid=158460)[0m f1_micro: 0.004197761194029851
[2m[36m(func pid=158460)[0m f1_macro: 0.002811334235791337
[2m[36m(func pid=158460)[0m f1_weighted: 0.0013167154973407786
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.006, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0023 | Steps: 2 | Val loss: 2.2218 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1037 | Steps: 2 | Val loss: 1.7402 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1560 | Steps: 2 | Val loss: 301.7477 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7958 | Steps: 2 | Val loss: 1730.1794 | Batch size: 32 | lr: 0.1 | Duration: 2.53s
== Status ==
Current time: 2024-01-07 03:24:09 (running for 00:04:44.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.119 |      0.321 |                   44 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.002 |      0.393 |                   45 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.743 |      0.014 |                   45 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 28.502 |      0.003 |                   44 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.40671641791044777
[2m[36m(func pid=157619)[0m top5: 0.9449626865671642
[2m[36m(func pid=157619)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=157619)[0m f1_macro: 0.3928158521094149
[2m[36m(func pid=157619)[0m f1_weighted: 0.39644110148635503
[2m[36m(func pid=157619)[0m f1_per_class: [0.519, 0.484, 0.6, 0.58, 0.342, 0.315, 0.232, 0.315, 0.224, 0.316]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3605410447761194
[2m[36m(func pid=157245)[0m top5: 0.9123134328358209
[2m[36m(func pid=157245)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=157245)[0m f1_macro: 0.3227132902613761
[2m[36m(func pid=157245)[0m f1_weighted: 0.3676902547613386
[2m[36m(func pid=157245)[0m f1_per_class: [0.421, 0.397, 0.49, 0.461, 0.122, 0.183, 0.369, 0.303, 0.21, 0.272]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.04291044776119403
[2m[36m(func pid=158041)[0m top5: 0.3941231343283582
[2m[36m(func pid=158041)[0m f1_micro: 0.04291044776119403
[2m[36m(func pid=158041)[0m f1_macro: 0.024206600167821113
[2m[36m(func pid=158041)[0m f1_weighted: 0.018114421189620054
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.069, 0.087]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m top1: 0.0032649253731343282
[2m[36m(func pid=158460)[0m top5: 0.5055970149253731
[2m[36m(func pid=158460)[0m f1_micro: 0.0032649253731343282
[2m[36m(func pid=158460)[0m f1_macro: 0.003263213325573016
[2m[36m(func pid=158460)[0m f1_weighted: 0.0018327802659604611
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.2314 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1352 | Steps: 2 | Val loss: 1.7402 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.6448 | Steps: 2 | Val loss: 302.0818 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.1823 | Steps: 2 | Val loss: 1248.7572 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 03:24:14 (running for 00:04:49.23)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.104 |      0.323 |                   45 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.394 |                   46 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.156 |      0.024 |                   46 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.796 |      0.003 |                   45 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.40625
[2m[36m(func pid=157619)[0m top5: 0.9472947761194029
[2m[36m(func pid=157619)[0m f1_micro: 0.40625
[2m[36m(func pid=157619)[0m f1_macro: 0.3936696753319199
[2m[36m(func pid=157619)[0m f1_weighted: 0.3976635542268764
[2m[36m(func pid=157619)[0m f1_per_class: [0.519, 0.477, 0.6, 0.58, 0.342, 0.316, 0.24, 0.315, 0.22, 0.327]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3605410447761194
[2m[36m(func pid=157245)[0m top5: 0.9123134328358209
[2m[36m(func pid=157245)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=157245)[0m f1_macro: 0.3247589211532281
[2m[36m(func pid=157245)[0m f1_weighted: 0.36419126293457715
[2m[36m(func pid=157245)[0m f1_per_class: [0.434, 0.416, 0.49, 0.465, 0.121, 0.178, 0.34, 0.306, 0.222, 0.275]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.05363805970149254
[2m[36m(func pid=158460)[0m top5: 0.5009328358208955
[2m[36m(func pid=158460)[0m f1_micro: 0.05363805970149254
[2m[36m(func pid=158460)[0m f1_macro: 0.018078733418913136
[2m[36m(func pid=158460)[0m f1_weighted: 0.009474051437293636
[2m[36m(func pid=158460)[0m f1_per_class: [0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.154, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.058768656716417914
[2m[36m(func pid=158041)[0m top5: 0.28451492537313433
[2m[36m(func pid=158041)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=158041)[0m f1_macro: 0.02035281681382913
[2m[36m(func pid=158041)[0m f1_weighted: 0.02406555338127173
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.079, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.2428 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0921 | Steps: 2 | Val loss: 1.7309 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7185 | Steps: 2 | Val loss: 248.1836 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.9122 | Steps: 2 | Val loss: 1002.6054 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=157619)[0m top1: 0.40904850746268656
[2m[36m(func pid=157619)[0m top5: 0.9482276119402985
[2m[36m(func pid=157619)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=157619)[0m f1_macro: 0.39455445773655684
[2m[36m(func pid=157619)[0m f1_weighted: 0.4015395529109277
[2m[36m(func pid=157619)[0m f1_per_class: [0.506, 0.481, 0.6, 0.584, 0.342, 0.319, 0.247, 0.315, 0.225, 0.327]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:24:19 (running for 00:04:54.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.135 |      0.325 |                   46 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.395 |                   47 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.645 |      0.02  |                   47 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.182 |      0.018 |                   46 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3591417910447761
[2m[36m(func pid=157245)[0m top5: 0.9123134328358209
[2m[36m(func pid=157245)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=157245)[0m f1_macro: 0.32642376843746124
[2m[36m(func pid=157245)[0m f1_weighted: 0.360161601273019
[2m[36m(func pid=157245)[0m f1_per_class: [0.45, 0.418, 0.49, 0.467, 0.129, 0.175, 0.324, 0.299, 0.229, 0.283]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.0648320895522388
[2m[36m(func pid=158460)[0m top5: 0.4808768656716418
[2m[36m(func pid=158460)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=158460)[0m f1_macro: 0.020568191015783083
[2m[36m(func pid=158460)[0m f1_weighted: 0.00933714672288094
[2m[36m(func pid=158460)[0m f1_per_class: [0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.137, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.06296641791044776
[2m[36m(func pid=158041)[0m top5: 0.35634328358208955
[2m[36m(func pid=158041)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=158041)[0m f1_macro: 0.021089046252223236
[2m[36m(func pid=158041)[0m f1_weighted: 0.025292836433290153
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.079, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0038 | Steps: 2 | Val loss: 2.2363 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0837 | Steps: 2 | Val loss: 1.7245 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 6.3208 | Steps: 2 | Val loss: 980.4539 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.2782 | Steps: 2 | Val loss: 201.1498 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 03:24:24 (running for 00:04:59.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.092 |      0.326 |                   47 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.004 |      0.394 |                   48 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.718 |      0.021 |                   48 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.912 |      0.021 |                   47 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.40904850746268656
[2m[36m(func pid=157619)[0m top5: 0.9486940298507462
[2m[36m(func pid=157619)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=157619)[0m f1_macro: 0.39388330096563207
[2m[36m(func pid=157619)[0m f1_weighted: 0.4031736590659353
[2m[36m(func pid=157619)[0m f1_per_class: [0.506, 0.477, 0.6, 0.582, 0.342, 0.319, 0.258, 0.303, 0.224, 0.327]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3614738805970149
[2m[36m(func pid=157245)[0m top5: 0.9132462686567164
[2m[36m(func pid=157245)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=157245)[0m f1_macro: 0.32857573489165637
[2m[36m(func pid=157245)[0m f1_weighted: 0.36035255879649297
[2m[36m(func pid=157245)[0m f1_per_class: [0.458, 0.423, 0.5, 0.47, 0.138, 0.161, 0.322, 0.311, 0.22, 0.283]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5289179104477612
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012138188608776842
[2m[36m(func pid=158460)[0m f1_weighted: 7.359909137784467e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.07136194029850747
[2m[36m(func pid=158041)[0m top5: 0.39925373134328357
[2m[36m(func pid=158041)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=158041)[0m f1_macro: 0.023304446209768664
[2m[36m(func pid=158041)[0m f1_weighted: 0.028943890669782455
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0128 | Steps: 2 | Val loss: 2.2343 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0899 | Steps: 2 | Val loss: 1.7238 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 5.6966 | Steps: 2 | Val loss: 1168.9425 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0717 | Steps: 2 | Val loss: 199.3951 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 03:24:30 (running for 00:05:04.97)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.084 |      0.329 |                   48 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.013 |      0.394 |                   49 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.278 |      0.023 |                   49 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.321 |      0.001 |                   48 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m top1: 0.4118470149253731
[2m[36m(func pid=157619)[0m top5: 0.9505597014925373
[2m[36m(func pid=157619)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=157619)[0m f1_macro: 0.3940939060137902
[2m[36m(func pid=157619)[0m f1_weighted: 0.4094631903998972
[2m[36m(func pid=157619)[0m f1_per_class: [0.494, 0.468, 0.6, 0.584, 0.351, 0.319, 0.285, 0.297, 0.227, 0.316]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.35401119402985076
[2m[36m(func pid=157245)[0m top5: 0.9127798507462687
[2m[36m(func pid=157245)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=157245)[0m f1_macro: 0.3240083440450052
[2m[36m(func pid=157245)[0m f1_weighted: 0.34929868615804627
[2m[36m(func pid=157245)[0m f1_per_class: [0.455, 0.425, 0.5, 0.466, 0.143, 0.152, 0.292, 0.302, 0.227, 0.278]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.523320895522388
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012115563839701772
[2m[36m(func pid=158460)[0m f1_weighted: 7.346190761013201e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.09654850746268656
[2m[36m(func pid=158041)[0m top5: 0.3829291044776119
[2m[36m(func pid=158041)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=158041)[0m f1_macro: 0.03251836441288084
[2m[36m(func pid=158041)[0m f1_weighted: 0.04020943595477161
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.087, 0.023]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.2503 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.8272 | Steps: 2 | Val loss: 1301.3943 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1084 | Steps: 2 | Val loss: 1.7126 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1222 | Steps: 2 | Val loss: 236.0601 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=157619)[0m top1: 0.41651119402985076
[2m[36m(func pid=157619)[0m top5: 0.9528917910447762
[2m[36m(func pid=157619)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=157619)[0m f1_macro: 0.4032815864357713
[2m[36m(func pid=157619)[0m f1_weighted: 0.41336278140680977
[2m[36m(func pid=157619)[0m f1_per_class: [0.518, 0.479, 0.632, 0.589, 0.351, 0.325, 0.279, 0.299, 0.242, 0.319]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:24:36 (running for 00:05:11.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.09  |      0.324 |                   49 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.002 |      0.403 |                   50 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.072 |      0.033 |                   50 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.827 |      0.001 |                   50 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5279850746268657
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012218045112781955
[2m[36m(func pid=158460)[0m f1_weighted: 7.408329592638313e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m top1: 0.35774253731343286
[2m[36m(func pid=157245)[0m top5: 0.914179104477612
[2m[36m(func pid=157245)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=157245)[0m f1_macro: 0.327317744623758
[2m[36m(func pid=157245)[0m f1_weighted: 0.35025309860275944
[2m[36m(func pid=157245)[0m f1_per_class: [0.479, 0.431, 0.511, 0.472, 0.148, 0.133, 0.289, 0.325, 0.215, 0.27]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.14972014925373134
[2m[36m(func pid=158041)[0m top5: 0.3498134328358209
[2m[36m(func pid=158041)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=158041)[0m f1_macro: 0.0383937181429949
[2m[36m(func pid=158041)[0m f1_weighted: 0.05305836244456188
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.094, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.2554 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 4.5152 | Steps: 2 | Val loss: 1392.4922 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0801 | Steps: 2 | Val loss: 1.7121 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0071 | Steps: 2 | Val loss: 266.9309 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=157619)[0m top1: 0.4183768656716418
[2m[36m(func pid=157619)[0m top5: 0.9538246268656716
[2m[36m(func pid=157619)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=157619)[0m f1_macro: 0.40726775151434697
[2m[36m(func pid=157619)[0m f1_weighted: 0.4157461593270735
[2m[36m(func pid=157619)[0m f1_per_class: [0.533, 0.471, 0.649, 0.592, 0.356, 0.335, 0.285, 0.3, 0.242, 0.31]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5321828358208955
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012269938650306747
[2m[36m(func pid=158460)[0m f1_weighted: 7.439794890577785e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:24:41 (running for 00:05:16.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.08  |      0.328 |                   51 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.407 |                   51 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.122 |      0.038 |                   51 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.515 |      0.001 |                   51 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3568097014925373
[2m[36m(func pid=157245)[0m top5: 0.9160447761194029
[2m[36m(func pid=157245)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=157245)[0m f1_macro: 0.327707853759421
[2m[36m(func pid=157245)[0m f1_weighted: 0.34256071290240026
[2m[36m(func pid=157245)[0m f1_per_class: [0.47, 0.435, 0.511, 0.479, 0.15, 0.145, 0.246, 0.342, 0.225, 0.275]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m top1: 0.16324626865671643
[2m[36m(func pid=158041)[0m top5: 0.36986940298507465
[2m[36m(func pid=158041)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=158041)[0m f1_macro: 0.03649740450420815
[2m[36m(func pid=158041)[0m f1_weighted: 0.05543489448272881
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.2716 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 9.4073 | Steps: 2 | Val loss: 1317.2135 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0643 | Steps: 2 | Val loss: 1.7098 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4502 | Steps: 2 | Val loss: 247.1903 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=157619)[0m top1: 0.4193097014925373
[2m[36m(func pid=157619)[0m top5: 0.9528917910447762
[2m[36m(func pid=157619)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=157619)[0m f1_macro: 0.41391234317827175
[2m[36m(func pid=157619)[0m f1_weighted: 0.4185504288570731
[2m[36m(func pid=157619)[0m f1_per_class: [0.556, 0.478, 0.686, 0.585, 0.342, 0.331, 0.295, 0.303, 0.236, 0.327]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5293843283582089
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.0012380952380952382
[2m[36m(func pid=158460)[0m f1_weighted: 7.507107320540157e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:24:46 (running for 00:05:21.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.08  |      0.328 |                   51 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.414 |                   52 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.45  |      0.035 |                   53 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  9.407 |      0.001 |                   52 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.15764925373134328
[2m[36m(func pid=158041)[0m top5: 0.3292910447761194
[2m[36m(func pid=158041)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=158041)[0m f1_macro: 0.035334203231198937
[2m[36m(func pid=158041)[0m f1_weighted: 0.05531445845807514
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3568097014925373
[2m[36m(func pid=157245)[0m top5: 0.9165111940298507
[2m[36m(func pid=157245)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=157245)[0m f1_macro: 0.33101082321943254
[2m[36m(func pid=157245)[0m f1_weighted: 0.341288783443375
[2m[36m(func pid=157245)[0m f1_per_class: [0.49, 0.442, 0.511, 0.478, 0.157, 0.154, 0.234, 0.344, 0.219, 0.283]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.2788 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 3.6476 | Steps: 2 | Val loss: 1227.3008 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9720 | Steps: 2 | Val loss: 242.3178 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0577 | Steps: 2 | Val loss: 1.6980 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=157619)[0m top1: 0.4207089552238806
[2m[36m(func pid=157619)[0m top5: 0.9547574626865671
[2m[36m(func pid=157619)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=157619)[0m f1_macro: 0.4172702581351636
[2m[36m(func pid=157619)[0m f1_weighted: 0.4215595517243493
[2m[36m(func pid=157619)[0m f1_per_class: [0.568, 0.48, 0.706, 0.589, 0.342, 0.327, 0.3, 0.303, 0.244, 0.313]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.006063432835820896
[2m[36m(func pid=158460)[0m top5: 0.5359141791044776
[2m[36m(func pid=158460)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=158460)[0m f1_macro: 0.001244614648157013
[2m[36m(func pid=158460)[0m f1_weighted: 7.546637325578904e-05
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:24:51 (running for 00:05:26.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.064 |      0.331 |                   52 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.417 |                   53 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.972 |      0.047 |                   54 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.648 |      0.001 |                   53 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.14878731343283583
[2m[36m(func pid=158041)[0m top5: 0.376865671641791
[2m[36m(func pid=158041)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=158041)[0m f1_macro: 0.046831559471213285
[2m[36m(func pid=158041)[0m f1_weighted: 0.06718602002513263
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.381, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.36007462686567165
[2m[36m(func pid=157245)[0m top5: 0.917910447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=157245)[0m f1_macro: 0.3401064730825359
[2m[36m(func pid=157245)[0m f1_weighted: 0.341935612134142
[2m[36m(func pid=157245)[0m f1_per_class: [0.5, 0.447, 0.558, 0.486, 0.173, 0.164, 0.219, 0.337, 0.225, 0.291]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.3141 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.9986 | Steps: 2 | Val loss: 1139.4906 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7432 | Steps: 2 | Val loss: 241.4617 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0617 | Steps: 2 | Val loss: 1.6952 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=157619)[0m top1: 0.4146455223880597
[2m[36m(func pid=157619)[0m top5: 0.9542910447761194
[2m[36m(func pid=157619)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=157619)[0m f1_macro: 0.41405698282700776
[2m[36m(func pid=157619)[0m f1_weighted: 0.4155640684383535
[2m[36m(func pid=157619)[0m f1_per_class: [0.568, 0.49, 0.706, 0.579, 0.338, 0.32, 0.287, 0.299, 0.248, 0.305]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.007462686567164179
[2m[36m(func pid=158460)[0m top5: 0.5447761194029851
[2m[36m(func pid=158460)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=158460)[0m f1_macro: 0.0028424774548232667
[2m[36m(func pid=158460)[0m f1_weighted: 0.002787230075431647
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.016, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
== Status ==
Current time: 2024-01-07 03:24:56 (running for 00:05:31.78)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.058 |      0.34  |                   53 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.414 |                   54 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.743 |      0.049 |                   55 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.999 |      0.003 |                   54 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=158041)[0m top1: 0.13013059701492538
[2m[36m(func pid=158041)[0m top5: 0.45149253731343286
[2m[36m(func pid=158041)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=158041)[0m f1_macro: 0.04875793464599244
[2m[36m(func pid=158041)[0m f1_weighted: 0.06849350975944447
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.386, 0.049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053, 0.0]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3614738805970149
[2m[36m(func pid=157245)[0m top5: 0.9174440298507462
[2m[36m(func pid=157245)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=157245)[0m f1_macro: 0.34215255155213253
[2m[36m(func pid=157245)[0m f1_weighted: 0.3405250941421285
[2m[36m(func pid=157245)[0m f1_per_class: [0.519, 0.453, 0.558, 0.49, 0.178, 0.17, 0.203, 0.344, 0.226, 0.28]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.3051 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.7025 | Steps: 2 | Val loss: 1007.5159 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8498 | Steps: 2 | Val loss: 240.3693 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0992 | Steps: 2 | Val loss: 1.6944 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=157619)[0m top1: 0.42024253731343286
[2m[36m(func pid=157619)[0m top5: 0.9533582089552238
[2m[36m(func pid=157619)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=157619)[0m f1_macro: 0.4188166479636095
[2m[36m(func pid=157619)[0m f1_weighted: 0.42339596422922715
[2m[36m(func pid=157619)[0m f1_per_class: [0.568, 0.495, 0.706, 0.578, 0.354, 0.323, 0.31, 0.299, 0.241, 0.313]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.008395522388059701
[2m[36m(func pid=158460)[0m top5: 0.5424440298507462
[2m[36m(func pid=158460)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=158460)[0m f1_macro: 0.0038704711638902637
[2m[36m(func pid=158460)[0m f1_weighted: 0.004525246109876152
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.026, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.10587686567164178
[2m[36m(func pid=158041)[0m top5: 0.498134328358209
[2m[36m(func pid=158041)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=158041)[0m f1_macro: 0.047483097225006404
[2m[36m(func pid=158041)[0m f1_weighted: 0.06744323225069147
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.38, 0.042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053, 0.0]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:02 (running for 00:05:37.33)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.099 |      0.338 |                   55 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.419 |                   55 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.85  |      0.047 |                   56 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.703 |      0.004 |                   55 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3582089552238806
[2m[36m(func pid=157245)[0m top5: 0.9202425373134329
[2m[36m(func pid=157245)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=157245)[0m f1_macro: 0.337917069711297
[2m[36m(func pid=157245)[0m f1_weighted: 0.3330365803751603
[2m[36m(func pid=157245)[0m f1_per_class: [0.527, 0.454, 0.545, 0.496, 0.18, 0.167, 0.175, 0.336, 0.216, 0.283]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.3425 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.5796 | Steps: 2 | Val loss: 837.7735 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7356 | Steps: 2 | Val loss: 250.9797 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0666 | Steps: 2 | Val loss: 1.6957 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=157619)[0m top1: 0.4197761194029851
[2m[36m(func pid=157619)[0m top5: 0.9524253731343284
[2m[36m(func pid=157619)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=157619)[0m f1_macro: 0.41977811453827607
[2m[36m(func pid=157619)[0m f1_weighted: 0.42173337064441657
[2m[36m(func pid=157619)[0m f1_per_class: [0.575, 0.491, 0.727, 0.577, 0.338, 0.325, 0.304, 0.311, 0.249, 0.3]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.015391791044776119
[2m[36m(func pid=158460)[0m top5: 0.5597014925373134
[2m[36m(func pid=158460)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=158460)[0m f1_macro: 0.01038146551724138
[2m[36m(func pid=158460)[0m f1_weighted: 0.016046964128709897
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.087, 0.013, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.09048507462686567
[2m[36m(func pid=158041)[0m top5: 0.5531716417910447
[2m[36m(func pid=158041)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=158041)[0m f1_macro: 0.05932483643606675
[2m[36m(func pid=158041)[0m f1_weighted: 0.07074644149815058
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.391, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.103]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:07 (running for 00:05:42.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.067 |      0.338 |                   56 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.42  |                   56 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.736 |      0.059 |                   57 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.58  |      0.01  |                   56 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.35401119402985076
[2m[36m(func pid=157245)[0m top5: 0.9165111940298507
[2m[36m(func pid=157245)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=157245)[0m f1_macro: 0.33778251160735495
[2m[36m(func pid=157245)[0m f1_weighted: 0.32640219026652084
[2m[36m(func pid=157245)[0m f1_per_class: [0.535, 0.454, 0.571, 0.492, 0.167, 0.175, 0.154, 0.331, 0.219, 0.28]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.3443 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 6.3159 | Steps: 2 | Val loss: 710.7390 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5100 | Steps: 2 | Val loss: 281.3546 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0594 | Steps: 2 | Val loss: 1.6957 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=157619)[0m top1: 0.42024253731343286
[2m[36m(func pid=157619)[0m top5: 0.9528917910447762
[2m[36m(func pid=157619)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=157619)[0m f1_macro: 0.42586194639108593
[2m[36m(func pid=157619)[0m f1_weighted: 0.42233474645252056
[2m[36m(func pid=157619)[0m f1_per_class: [0.562, 0.489, 0.774, 0.581, 0.364, 0.318, 0.307, 0.306, 0.242, 0.316]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.028917910447761194
[2m[36m(func pid=158460)[0m top5: 0.5555037313432836
[2m[36m(func pid=158460)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=158460)[0m f1_macro: 0.016460325636241124
[2m[36m(func pid=158460)[0m f1_weighted: 0.0262840359452688
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.147, 0.015, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.06949626865671642
[2m[36m(func pid=158041)[0m top5: 0.601679104477612
[2m[36m(func pid=158041)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=158041)[0m f1_macro: 0.054117256423103596
[2m[36m(func pid=158041)[0m f1_weighted: 0.059122302208289096
[2m[36m(func pid=158041)[0m f1_per_class: [0.043, 0.319, 0.035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.066, 0.078]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:12 (running for 00:05:47.77)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.059 |      0.343 |                   57 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.426 |                   57 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.51  |      0.054 |                   58 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.316 |      0.016 |                   57 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.35634328358208955
[2m[36m(func pid=157245)[0m top5: 0.9160447761194029
[2m[36m(func pid=157245)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=157245)[0m f1_macro: 0.342512983200015
[2m[36m(func pid=157245)[0m f1_weighted: 0.3270403357395152
[2m[36m(func pid=157245)[0m f1_per_class: [0.534, 0.465, 0.571, 0.482, 0.172, 0.176, 0.154, 0.333, 0.241, 0.296]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.3641 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.3039 | Steps: 2 | Val loss: 679.7296 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.3733 | Steps: 2 | Val loss: 359.7575 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0732 | Steps: 2 | Val loss: 1.6980 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=157619)[0m top1: 0.4197761194029851
[2m[36m(func pid=157619)[0m top5: 0.9547574626865671
[2m[36m(func pid=157619)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=157619)[0m f1_macro: 0.4246070470006269
[2m[36m(func pid=157619)[0m f1_weighted: 0.4196269023172632
[2m[36m(func pid=157619)[0m f1_per_class: [0.569, 0.487, 0.774, 0.579, 0.338, 0.32, 0.299, 0.308, 0.255, 0.319]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.030783582089552237
[2m[36m(func pid=158460)[0m top5: 0.574160447761194
[2m[36m(func pid=158460)[0m f1_micro: 0.030783582089552237
[2m[36m(func pid=158460)[0m f1_macro: 0.01790487468883679
[2m[36m(func pid=158460)[0m f1_weighted: 0.028416001399941344
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.165, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.049906716417910446
[2m[36m(func pid=158041)[0m top5: 0.6063432835820896
[2m[36m(func pid=158041)[0m f1_micro: 0.04990671641791045
[2m[36m(func pid=158041)[0m f1_macro: 0.04270641132963648
[2m[36m(func pid=158041)[0m f1_weighted: 0.043832033588874234
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.235, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.059, 0.103]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:18 (running for 00:05:52.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.073 |      0.344 |                   58 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.425 |                   58 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.373 |      0.043 |                   59 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.304 |      0.018 |                   58 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.35494402985074625
[2m[36m(func pid=157245)[0m top5: 0.9165111940298507
[2m[36m(func pid=157245)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=157245)[0m f1_macro: 0.34419534721140854
[2m[36m(func pid=157245)[0m f1_weighted: 0.32285713834685514
[2m[36m(func pid=157245)[0m f1_per_class: [0.535, 0.457, 0.585, 0.484, 0.189, 0.155, 0.15, 0.34, 0.233, 0.314]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.3987 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.5627 | Steps: 2 | Val loss: 732.5084 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.5469 | Steps: 2 | Val loss: 311.0403 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0545 | Steps: 2 | Val loss: 1.6982 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=157619)[0m top1: 0.416044776119403
[2m[36m(func pid=157619)[0m top5: 0.9538246268656716
[2m[36m(func pid=157619)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=157619)[0m f1_macro: 0.42246329388548653
[2m[36m(func pid=157619)[0m f1_weighted: 0.4163053297727986
[2m[36m(func pid=157619)[0m f1_per_class: [0.577, 0.486, 0.75, 0.573, 0.337, 0.314, 0.295, 0.311, 0.245, 0.336]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.029384328358208957
[2m[36m(func pid=158460)[0m top5: 0.585820895522388
[2m[36m(func pid=158460)[0m f1_micro: 0.029384328358208953
[2m[36m(func pid=158460)[0m f1_macro: 0.02158565267434927
[2m[36m(func pid=158460)[0m f1_weighted: 0.02898559198070885
[2m[36m(func pid=158460)[0m f1_per_class: [0.038, 0.163, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.07229477611940298
[2m[36m(func pid=158041)[0m top5: 0.6730410447761194
[2m[36m(func pid=158041)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=158041)[0m f1_macro: 0.05699604653218242
[2m[36m(func pid=158041)[0m f1_weighted: 0.08346439576027667
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.232, 0.033, 0.144, 0.0, 0.0, 0.0, 0.0, 0.056, 0.105]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:23 (running for 00:05:58.10)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.054 |      0.345 |                   59 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   59 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.547 |      0.057 |                   60 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.563 |      0.022 |                   59 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.35634328358208955
[2m[36m(func pid=157245)[0m top5: 0.9165111940298507
[2m[36m(func pid=157245)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=157245)[0m f1_macro: 0.34479479531830026
[2m[36m(func pid=157245)[0m f1_weighted: 0.32539103971697686
[2m[36m(func pid=157245)[0m f1_per_class: [0.52, 0.461, 0.585, 0.484, 0.196, 0.162, 0.154, 0.337, 0.232, 0.318]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.4104 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 5.1195 | Steps: 2 | Val loss: 821.7086 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.0785 | Steps: 2 | Val loss: 258.1570 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0394 | Steps: 2 | Val loss: 1.6983 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=157619)[0m top1: 0.4183768656716418
[2m[36m(func pid=157619)[0m top5: 0.9542910447761194
[2m[36m(func pid=157619)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=157619)[0m f1_macro: 0.42362227928014357
[2m[36m(func pid=157619)[0m f1_weighted: 0.41968237393300595
[2m[36m(func pid=157619)[0m f1_per_class: [0.571, 0.479, 0.774, 0.574, 0.318, 0.313, 0.308, 0.321, 0.247, 0.33]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.027985074626865673
[2m[36m(func pid=158460)[0m top5: 0.5988805970149254
[2m[36m(func pid=158460)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=158460)[0m f1_macro: 0.02229637501340828
[2m[36m(func pid=158460)[0m f1_weighted: 0.0338428648283142
[2m[36m(func pid=158460)[0m f1_per_class: [0.036, 0.144, 0.014, 0.026, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.2042910447761194
[2m[36m(func pid=158041)[0m top5: 0.7374067164179104
[2m[36m(func pid=158041)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=158041)[0m f1_macro: 0.10920128623413392
[2m[36m(func pid=158041)[0m f1_weighted: 0.18770612287589686
[2m[36m(func pid=158041)[0m f1_per_class: [0.107, 0.19, 0.0, 0.527, 0.105, 0.0, 0.006, 0.0, 0.058, 0.099]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:28 (running for 00:06:03.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.039 |      0.349 |                   60 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.424 |                   60 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.079 |      0.109 |                   61 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  5.12  |      0.022 |                   60 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.35774253731343286
[2m[36m(func pid=157245)[0m top5: 0.9183768656716418
[2m[36m(func pid=157245)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=157245)[0m f1_macro: 0.3485973613443824
[2m[36m(func pid=157245)[0m f1_weighted: 0.3253354794831439
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.467, 0.6, 0.485, 0.198, 0.156, 0.15, 0.339, 0.239, 0.337]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.9453 | Steps: 2 | Val loss: 892.1616 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.4240 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.0396 | Steps: 2 | Val loss: 202.8986 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0350 | Steps: 2 | Val loss: 1.6971 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=157619)[0m top1: 0.4221082089552239
[2m[36m(func pid=157619)[0m top5: 0.9561567164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=157619)[0m f1_macro: 0.4292145208087247
[2m[36m(func pid=157619)[0m f1_weighted: 0.4223716083972186
[2m[36m(func pid=157619)[0m f1_per_class: [0.561, 0.489, 0.8, 0.581, 0.341, 0.32, 0.303, 0.317, 0.25, 0.33]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.03591417910447761
[2m[36m(func pid=158460)[0m top5: 0.6063432835820896
[2m[36m(func pid=158460)[0m f1_micro: 0.03591417910447761
[2m[36m(func pid=158460)[0m f1_macro: 0.022846233503028716
[2m[36m(func pid=158460)[0m f1_weighted: 0.04954070754905257
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.097, 0.014, 0.111, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.23367537313432835
[2m[36m(func pid=158041)[0m top5: 0.753731343283582
[2m[36m(func pid=158041)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=158041)[0m f1_macro: 0.15550309547471028
[2m[36m(func pid=158041)[0m f1_weighted: 0.2216811098331461
[2m[36m(func pid=158041)[0m f1_per_class: [0.155, 0.327, 0.0, 0.558, 0.381, 0.0, 0.003, 0.0, 0.062, 0.069]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:33 (running for 00:06:08.55)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.035 |      0.347 |                   61 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.429 |                   61 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.04  |      0.156 |                   62 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.945 |      0.023 |                   61 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3591417910447761
[2m[36m(func pid=157245)[0m top5: 0.9174440298507462
[2m[36m(func pid=157245)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=157245)[0m f1_macro: 0.3465517829726535
[2m[36m(func pid=157245)[0m f1_weighted: 0.3276647888018348
[2m[36m(func pid=157245)[0m f1_per_class: [0.524, 0.47, 0.6, 0.486, 0.178, 0.169, 0.15, 0.343, 0.242, 0.303]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0018 | Steps: 2 | Val loss: 2.4281 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.8573 | Steps: 2 | Val loss: 877.2698 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.8208 | Steps: 2 | Val loss: 215.5259 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0392 | Steps: 2 | Val loss: 1.7005 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=157619)[0m top1: 0.4193097014925373
[2m[36m(func pid=157619)[0m top5: 0.9542910447761194
[2m[36m(func pid=157619)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=157619)[0m f1_macro: 0.4290638010576636
[2m[36m(func pid=157619)[0m f1_weighted: 0.4227799894730816
[2m[36m(func pid=157619)[0m f1_per_class: [0.571, 0.488, 0.8, 0.566, 0.329, 0.32, 0.318, 0.316, 0.247, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.04057835820895522
[2m[36m(func pid=158460)[0m top5: 0.6208022388059702
[2m[36m(func pid=158460)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=158460)[0m f1_macro: 0.025966846042117186
[2m[36m(func pid=158460)[0m f1_weighted: 0.05287772824223379
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.145, 0.015, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.11007462686567164
[2m[36m(func pid=158041)[0m top5: 0.6567164179104478
[2m[36m(func pid=158041)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=158041)[0m f1_macro: 0.09060208435809987
[2m[36m(func pid=158041)[0m f1_weighted: 0.12646743726758583
[2m[36m(func pid=158041)[0m f1_per_class: [0.089, 0.448, 0.034, 0.153, 0.118, 0.0, 0.006, 0.0, 0.059, 0.0]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:38 (running for 00:06:13.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.039 |      0.345 |                   62 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.002 |      0.429 |                   62 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.821 |      0.091 |                   63 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.857 |      0.026 |                   62 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3558768656716418
[2m[36m(func pid=157245)[0m top5: 0.9169776119402985
[2m[36m(func pid=157245)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=157245)[0m f1_macro: 0.34532127993554745
[2m[36m(func pid=157245)[0m f1_weighted: 0.32300422303745463
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.468, 0.6, 0.482, 0.198, 0.171, 0.14, 0.338, 0.241, 0.299]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.4454 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2799 | Steps: 2 | Val loss: 817.3698 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9647 | Steps: 2 | Val loss: 325.8037 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0423 | Steps: 2 | Val loss: 1.7022 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=157619)[0m top1: 0.4193097014925373
[2m[36m(func pid=157619)[0m top5: 0.9561567164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=157619)[0m f1_macro: 0.4285874533250751
[2m[36m(func pid=157619)[0m f1_weighted: 0.42273411758015084
[2m[36m(func pid=157619)[0m f1_per_class: [0.571, 0.488, 0.8, 0.568, 0.322, 0.324, 0.315, 0.316, 0.247, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m top1: 0.04617537313432836
[2m[36m(func pid=158460)[0m top5: 0.6389925373134329
[2m[36m(func pid=158460)[0m f1_micro: 0.04617537313432836
[2m[36m(func pid=158460)[0m f1_macro: 0.031285811350711436
[2m[36m(func pid=158460)[0m f1_weighted: 0.059538227997826695
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.193, 0.015, 0.094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.07276119402985075
[2m[36m(func pid=158041)[0m top5: 0.47574626865671643
[2m[36m(func pid=158041)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=158041)[0m f1_macro: 0.0543063003237503
[2m[36m(func pid=158041)[0m f1_weighted: 0.08025450008478255
[2m[36m(func pid=158041)[0m f1_per_class: [0.037, 0.371, 0.028, 0.032, 0.0, 0.0, 0.015, 0.0, 0.06, 0.0]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:44 (running for 00:06:19.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.042 |      0.351 |                   63 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.429 |                   63 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.965 |      0.054 |                   64 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.28  |      0.031 |                   63 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3572761194029851
[2m[36m(func pid=157245)[0m top5: 0.9207089552238806
[2m[36m(func pid=157245)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=157245)[0m f1_macro: 0.3509852552153153
[2m[36m(func pid=157245)[0m f1_weighted: 0.32180024789802975
[2m[36m(func pid=157245)[0m f1_per_class: [0.515, 0.457, 0.632, 0.493, 0.2, 0.189, 0.122, 0.345, 0.249, 0.308]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6274 | Steps: 2 | Val loss: 736.0538 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.4783 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.0080 | Steps: 2 | Val loss: 329.2281 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=158460)[0m top1: 0.058768656716417914
[2m[36m(func pid=158460)[0m top5: 0.6492537313432836
[2m[36m(func pid=158460)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=158460)[0m f1_macro: 0.04272355342630263
[2m[36m(func pid=158460)[0m f1_weighted: 0.07131667624343746
[2m[36m(func pid=158460)[0m f1_per_class: [0.027, 0.289, 0.015, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0320 | Steps: 2 | Val loss: 1.6993 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=157619)[0m top1: 0.417910447761194
[2m[36m(func pid=157619)[0m top5: 0.9552238805970149
[2m[36m(func pid=157619)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.4295778351424254
[2m[36m(func pid=157619)[0m f1_weighted: 0.4218430979084975
[2m[36m(func pid=157619)[0m f1_per_class: [0.585, 0.484, 0.8, 0.567, 0.329, 0.325, 0.315, 0.308, 0.248, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.11007462686567164
[2m[36m(func pid=158041)[0m top5: 0.5415111940298507
[2m[36m(func pid=158041)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=158041)[0m f1_macro: 0.08851982175456802
[2m[36m(func pid=158041)[0m f1_weighted: 0.10747920776090117
[2m[36m(func pid=158041)[0m f1_per_class: [0.032, 0.371, 0.027, 0.01, 0.0, 0.0, 0.07, 0.27, 0.105, 0.0]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:49 (running for 00:06:24.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.032 |      0.35  |                   64 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.43  |                   64 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.008 |      0.089 |                   65 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.627 |      0.043 |                   64 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3582089552238806
[2m[36m(func pid=157245)[0m top5: 0.9193097014925373
[2m[36m(func pid=157245)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=157245)[0m f1_macro: 0.35003634972141556
[2m[36m(func pid=157245)[0m f1_weighted: 0.324394806548982
[2m[36m(func pid=157245)[0m f1_per_class: [0.511, 0.455, 0.632, 0.496, 0.2, 0.186, 0.132, 0.338, 0.245, 0.305]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 4.1686 | Steps: 2 | Val loss: 594.5187 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.4952 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8588 | Steps: 2 | Val loss: 301.6358 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=158460)[0m top1: 0.11100746268656717
[2m[36m(func pid=158460)[0m top5: 0.6291977611940298
[2m[36m(func pid=158460)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=158460)[0m f1_macro: 0.057953234075239425
[2m[36m(func pid=158460)[0m f1_weighted: 0.09179280225573351
[2m[36m(func pid=158460)[0m f1_per_class: [0.022, 0.451, 0.017, 0.045, 0.0, 0.0, 0.0, 0.0, 0.026, 0.019]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0461 | Steps: 2 | Val loss: 1.6967 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=157619)[0m top1: 0.4193097014925373
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=157619)[0m f1_macro: 0.42851730093603174
[2m[36m(func pid=157619)[0m f1_weighted: 0.42481715399191106
[2m[36m(func pid=157619)[0m f1_per_class: [0.574, 0.48, 0.8, 0.572, 0.326, 0.321, 0.325, 0.314, 0.247, 0.327]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.11473880597014925
[2m[36m(func pid=158041)[0m top5: 0.5694962686567164
[2m[36m(func pid=158041)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=158041)[0m f1_macro: 0.09042933490715113
[2m[36m(func pid=158041)[0m f1_weighted: 0.11195200297440336
[2m[36m(func pid=158041)[0m f1_per_class: [0.031, 0.398, 0.028, 0.003, 0.0, 0.0, 0.08, 0.269, 0.063, 0.032]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:54 (running for 00:06:29.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.046 |      0.355 |                   65 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.429 |                   65 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.859 |      0.09  |                   66 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  4.169 |      0.058 |                   65 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3614738805970149
[2m[36m(func pid=157245)[0m top5: 0.917910447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=157245)[0m f1_macro: 0.355465433643047
[2m[36m(func pid=157245)[0m f1_weighted: 0.33133014927520815
[2m[36m(func pid=157245)[0m f1_per_class: [0.507, 0.456, 0.667, 0.512, 0.191, 0.196, 0.137, 0.335, 0.243, 0.311]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.9473 | Steps: 2 | Val loss: 476.6760 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.5181 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5547 | Steps: 2 | Val loss: 269.2245 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=158460)[0m top1: 0.13899253731343283
[2m[36m(func pid=158460)[0m top5: 0.5592350746268657
[2m[36m(func pid=158460)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=158460)[0m f1_macro: 0.047732405093707936
[2m[36m(func pid=158460)[0m f1_weighted: 0.0754664837134868
[2m[36m(func pid=158460)[0m f1_per_class: [0.032, 0.407, 0.022, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0652 | Steps: 2 | Val loss: 1.6991 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=157619)[0m top1: 0.42024253731343286
[2m[36m(func pid=157619)[0m top5: 0.9547574626865671
[2m[36m(func pid=157619)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=157619)[0m f1_macro: 0.42994030366272595
[2m[36m(func pid=157619)[0m f1_weighted: 0.4252974178259527
[2m[36m(func pid=157619)[0m f1_per_class: [0.574, 0.481, 0.8, 0.57, 0.333, 0.319, 0.326, 0.326, 0.248, 0.321]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.09934701492537314
[2m[36m(func pid=158041)[0m top5: 0.53125
[2m[36m(func pid=158041)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=158041)[0m f1_macro: 0.08356987848032957
[2m[36m(func pid=158041)[0m f1_weighted: 0.09336305867653544
[2m[36m(func pid=158041)[0m f1_per_class: [0.024, 0.382, 0.029, 0.0, 0.0, 0.0, 0.037, 0.219, 0.074, 0.07]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:25:59 (running for 00:06:34.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.065 |      0.353 |                   66 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.43  |                   66 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.555 |      0.084 |                   67 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  3.947 |      0.048 |                   66 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3605410447761194
[2m[36m(func pid=157245)[0m top5: 0.9188432835820896
[2m[36m(func pid=157245)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=157245)[0m f1_macro: 0.3528364923610615
[2m[36m(func pid=157245)[0m f1_weighted: 0.33053888434522405
[2m[36m(func pid=157245)[0m f1_per_class: [0.519, 0.463, 0.632, 0.503, 0.189, 0.184, 0.142, 0.334, 0.248, 0.314]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6885 | Steps: 2 | Val loss: 357.4411 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.5284 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 5.3369 | Steps: 2 | Val loss: 268.7453 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=158460)[0m top1: 0.15625
[2m[36m(func pid=158460)[0m top5: 0.470615671641791
[2m[36m(func pid=158460)[0m f1_micro: 0.15625
[2m[36m(func pid=158460)[0m f1_macro: 0.03757882815916168
[2m[36m(func pid=158460)[0m f1_weighted: 0.053624582780396834
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.293, 0.073, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0378 | Steps: 2 | Val loss: 1.6977 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=157619)[0m top1: 0.42024253731343286
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=157619)[0m f1_macro: 0.42659149044218536
[2m[36m(func pid=157619)[0m f1_weighted: 0.42466092031949365
[2m[36m(func pid=157619)[0m f1_per_class: [0.564, 0.479, 0.759, 0.57, 0.346, 0.326, 0.325, 0.315, 0.249, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.10541044776119403
[2m[36m(func pid=158041)[0m top5: 0.5307835820895522
[2m[36m(func pid=158041)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=158041)[0m f1_macro: 0.10373599719780868
[2m[36m(func pid=158041)[0m f1_weighted: 0.0916821248528922
[2m[36m(func pid=158041)[0m f1_per_class: [0.04, 0.344, 0.032, 0.003, 0.143, 0.0, 0.037, 0.249, 0.081, 0.108]
[2m[36m(func pid=158041)[0m 
== Status ==
Current time: 2024-01-07 03:26:05 (running for 00:06:39.97)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.038 |      0.356 |                   67 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.427 |                   67 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  5.337 |      0.104 |                   68 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  2.689 |      0.038 |                   67 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.36007462686567165
[2m[36m(func pid=157245)[0m top5: 0.9216417910447762
[2m[36m(func pid=157245)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=157245)[0m f1_macro: 0.3561710843771418
[2m[36m(func pid=157245)[0m f1_weighted: 0.3273951326129659
[2m[36m(func pid=157245)[0m f1_per_class: [0.515, 0.465, 0.667, 0.502, 0.191, 0.189, 0.13, 0.333, 0.248, 0.323]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 13.8472 | Steps: 2 | Val loss: 422.8989 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.5431 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1732 | Steps: 2 | Val loss: 99.9458 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=158460)[0m top1: 0.12639925373134328
[2m[36m(func pid=158460)[0m top5: 0.5736940298507462
[2m[36m(func pid=158460)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=158460)[0m f1_macro: 0.05332282960130367
[2m[36m(func pid=158460)[0m f1_weighted: 0.09593063964962763
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.441, 0.021, 0.072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0356 | Steps: 2 | Val loss: 1.6947 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=157619)[0m top1: 0.42257462686567165
[2m[36m(func pid=157619)[0m top5: 0.9566231343283582
[2m[36m(func pid=157619)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=157619)[0m f1_macro: 0.42599742894916703
[2m[36m(func pid=157619)[0m f1_weighted: 0.42619448674679344
[2m[36m(func pid=157619)[0m f1_per_class: [0.559, 0.482, 0.759, 0.573, 0.329, 0.328, 0.323, 0.327, 0.246, 0.333]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.12033582089552239
[2m[36m(func pid=158041)[0m top5: 0.5228544776119403
[2m[36m(func pid=158041)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=158041)[0m f1_macro: 0.10718322087172436
[2m[36m(func pid=158041)[0m f1_weighted: 0.09654925754271887
[2m[36m(func pid=158041)[0m f1_per_class: [0.108, 0.297, 0.054, 0.0, 0.087, 0.0, 0.072, 0.296, 0.088, 0.071]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 5.7014 | Steps: 2 | Val loss: 599.4528 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:26:10 (running for 00:06:45.36)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.036 |      0.357 |                   68 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.426 |                   68 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.173 |      0.107 |                   69 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 13.847 |      0.053 |                   68 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3605410447761194
[2m[36m(func pid=157245)[0m top5: 0.9216417910447762
[2m[36m(func pid=157245)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=157245)[0m f1_macro: 0.35681447761114565
[2m[36m(func pid=157245)[0m f1_weighted: 0.32325861609650935
[2m[36m(func pid=157245)[0m f1_per_class: [0.523, 0.473, 0.667, 0.509, 0.194, 0.185, 0.106, 0.328, 0.243, 0.34]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8956 | Steps: 2 | Val loss: 52.3702 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.5767 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=158460)[0m top1: 0.06763059701492537
[2m[36m(func pid=158460)[0m top5: 0.5811567164179104
[2m[36m(func pid=158460)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=158460)[0m f1_macro: 0.04332703086074263
[2m[36m(func pid=158460)[0m f1_weighted: 0.07178685950720012
[2m[36m(func pid=158460)[0m f1_per_class: [0.023, 0.316, 0.019, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.09375
[2m[36m(func pid=158041)[0m top5: 0.427705223880597
[2m[36m(func pid=158041)[0m f1_micro: 0.09375
[2m[36m(func pid=158041)[0m f1_macro: 0.12548071405416666
[2m[36m(func pid=158041)[0m f1_weighted: 0.09502702240187225
[2m[36m(func pid=158041)[0m f1_per_class: [0.076, 0.307, 0.333, 0.0, 0.122, 0.008, 0.068, 0.23, 0.084, 0.026]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4155783582089552
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=157619)[0m f1_macro: 0.4225593871111992
[2m[36m(func pid=157619)[0m f1_weighted: 0.4194046491286911
[2m[36m(func pid=157619)[0m f1_per_class: [0.569, 0.477, 0.759, 0.561, 0.295, 0.32, 0.317, 0.33, 0.253, 0.346]
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0293 | Steps: 2 | Val loss: 1.6820 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 20.8905 | Steps: 2 | Val loss: 687.6424 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 03:26:15 (running for 00:06:50.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.029 |      0.359 |                   69 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.423 |                   69 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.896 |      0.125 |                   70 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  5.701 |      0.043 |                   69 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.363339552238806
[2m[36m(func pid=157245)[0m top5: 0.9230410447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=157245)[0m f1_macro: 0.35877133403601963
[2m[36m(func pid=157245)[0m f1_weighted: 0.3292975783385198
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.467, 0.667, 0.518, 0.212, 0.195, 0.119, 0.321, 0.246, 0.327]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2503 | Steps: 2 | Val loss: 49.2083 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.6035 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=158460)[0m top1: 0.055970149253731345
[2m[36m(func pid=158460)[0m top5: 0.5774253731343284
[2m[36m(func pid=158460)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=158460)[0m f1_macro: 0.03887670438571137
[2m[36m(func pid=158460)[0m f1_weighted: 0.06021269798866624
[2m[36m(func pid=158460)[0m f1_per_class: [0.028, 0.277, 0.019, 0.042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.12733208955223882
[2m[36m(func pid=158041)[0m top5: 0.5149253731343284
[2m[36m(func pid=158041)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=158041)[0m f1_macro: 0.1337561415834125
[2m[36m(func pid=158041)[0m f1_weighted: 0.14293838895413355
[2m[36m(func pid=158041)[0m f1_per_class: [0.078, 0.343, 0.143, 0.0, 0.118, 0.008, 0.19, 0.34, 0.09, 0.028]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0206 | Steps: 2 | Val loss: 1.6872 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=157619)[0m top1: 0.416044776119403
[2m[36m(func pid=157619)[0m top5: 0.9547574626865671
[2m[36m(func pid=157619)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=157619)[0m f1_macro: 0.42527680046606886
[2m[36m(func pid=157619)[0m f1_weighted: 0.4202708774241137
[2m[36m(func pid=157619)[0m f1_per_class: [0.564, 0.483, 0.786, 0.565, 0.298, 0.317, 0.313, 0.332, 0.25, 0.346]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 11.7470 | Steps: 2 | Val loss: 635.6557 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 03:26:21 (running for 00:06:55.92)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.021 |      0.359 |                   70 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.425 |                   70 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.25  |      0.134 |                   71 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 20.89  |      0.039 |                   70 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.36240671641791045
[2m[36m(func pid=157245)[0m top5: 0.9216417910447762
[2m[36m(func pid=157245)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=157245)[0m f1_macro: 0.3587704828376353
[2m[36m(func pid=157245)[0m f1_weighted: 0.33186189713382486
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.471, 0.667, 0.512, 0.2, 0.195, 0.131, 0.323, 0.239, 0.333]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.6176 | Steps: 2 | Val loss: 39.9175 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.6079 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158460)[0m top1: 0.043843283582089554
[2m[36m(func pid=158460)[0m top5: 0.6338619402985075
[2m[36m(func pid=158460)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=158460)[0m f1_macro: 0.033707657468801853
[2m[36m(func pid=158460)[0m f1_weighted: 0.04912528572743552
[2m[36m(func pid=158460)[0m f1_per_class: [0.037, 0.221, 0.019, 0.036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.17537313432835822
[2m[36m(func pid=158041)[0m top5: 0.5578358208955224
[2m[36m(func pid=158041)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=158041)[0m f1_macro: 0.18247536212124246
[2m[36m(func pid=158041)[0m f1_weighted: 0.1702773367977091
[2m[36m(func pid=158041)[0m f1_per_class: [0.083, 0.413, 0.421, 0.0, 0.077, 0.078, 0.202, 0.359, 0.107, 0.085]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0307 | Steps: 2 | Val loss: 1.6904 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=157619)[0m top1: 0.4137126865671642
[2m[36m(func pid=157619)[0m top5: 0.9561567164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=157619)[0m f1_macro: 0.42165553993120086
[2m[36m(func pid=157619)[0m f1_weighted: 0.4197198010041051
[2m[36m(func pid=157619)[0m f1_per_class: [0.554, 0.478, 0.786, 0.563, 0.295, 0.313, 0.32, 0.325, 0.247, 0.336]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 197.8344 | Steps: 2 | Val loss: 430.5041 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 03:26:26 (running for 00:07:01.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.031 |      0.357 |                   71 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   71 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.618 |      0.182 |                   72 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 11.747 |      0.034 |                   71 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.36613805970149255
[2m[36m(func pid=157245)[0m top5: 0.9207089552238806
[2m[36m(func pid=157245)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=157245)[0m f1_macro: 0.3568518346838977
[2m[36m(func pid=157245)[0m f1_weighted: 0.33784218022596796
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.47, 0.615, 0.514, 0.213, 0.196, 0.15, 0.327, 0.241, 0.327]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.1362 | Steps: 2 | Val loss: 37.6072 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.6320 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=158460)[0m top1: 0.04617537313432836
[2m[36m(func pid=158460)[0m top5: 0.7112873134328358
[2m[36m(func pid=158460)[0m f1_micro: 0.04617537313432836
[2m[36m(func pid=158460)[0m f1_macro: 0.0390092150964774
[2m[36m(func pid=158460)[0m f1_weighted: 0.05561545894361218
[2m[36m(func pid=158460)[0m f1_per_class: [0.09, 0.141, 0.02, 0.104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.15625
[2m[36m(func pid=158041)[0m top5: 0.49720149253731344
[2m[36m(func pid=158041)[0m f1_micro: 0.15625
[2m[36m(func pid=158041)[0m f1_macro: 0.16164515110389188
[2m[36m(func pid=158041)[0m f1_weighted: 0.1532495257919821
[2m[36m(func pid=158041)[0m f1_per_class: [0.07, 0.517, 0.462, 0.0, 0.0, 0.04, 0.134, 0.203, 0.065, 0.125]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0282 | Steps: 2 | Val loss: 1.6903 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=157619)[0m top1: 0.417910447761194
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.4246343835779734
[2m[36m(func pid=157619)[0m f1_weighted: 0.4227374074109125
[2m[36m(func pid=157619)[0m f1_per_class: [0.569, 0.479, 0.786, 0.574, 0.283, 0.32, 0.316, 0.33, 0.238, 0.353]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 20.0478 | Steps: 2 | Val loss: 471.1549 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 03:26:31 (running for 00:07:06.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.028 |      0.354 |                   72 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.425 |                   72 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   1.136 |      0.162 |                   73 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 197.834 |      0.039 |                   72 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |         |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |         |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |         |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.36240671641791045
[2m[36m(func pid=157245)[0m top5: 0.9211753731343284
[2m[36m(func pid=157245)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=157245)[0m f1_macro: 0.3537700054847707
[2m[36m(func pid=157245)[0m f1_weighted: 0.3316011860421223
[2m[36m(func pid=157245)[0m f1_per_class: [0.512, 0.469, 0.615, 0.511, 0.2, 0.198, 0.132, 0.327, 0.241, 0.333]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.0247 | Steps: 2 | Val loss: 43.2576 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.6421 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158460)[0m top1: 0.05970149253731343
[2m[36m(func pid=158460)[0m top5: 0.7047574626865671
[2m[36m(func pid=158460)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=158460)[0m f1_macro: 0.03911852913301718
[2m[36m(func pid=158460)[0m f1_weighted: 0.06697060103569834
[2m[36m(func pid=158460)[0m f1_per_class: [0.083, 0.0, 0.019, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.14039179104477612
[2m[36m(func pid=158041)[0m top5: 0.35027985074626866
[2m[36m(func pid=158041)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=158041)[0m f1_macro: 0.12110856526133143
[2m[36m(func pid=158041)[0m f1_weighted: 0.12937562081186527
[2m[36m(func pid=158041)[0m f1_per_class: [0.062, 0.499, 0.286, 0.0, 0.0, 0.008, 0.109, 0.063, 0.055, 0.129]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0291 | Steps: 2 | Val loss: 1.6835 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=157619)[0m top1: 0.41884328358208955
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=157619)[0m f1_macro: 0.4237968993131148
[2m[36m(func pid=157619)[0m f1_weighted: 0.42413918868574996
[2m[36m(func pid=157619)[0m f1_per_class: [0.534, 0.478, 0.815, 0.569, 0.262, 0.32, 0.327, 0.328, 0.252, 0.353]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 72.2711 | Steps: 2 | Val loss: 96.0907 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5218 | Steps: 2 | Val loss: 45.1655 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 03:26:36 (running for 00:07:11.77)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.029 |      0.357 |                   73 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.424 |                   73 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.025 |      0.121 |                   74 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 20.048 |      0.039 |                   73 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.36240671641791045
[2m[36m(func pid=157245)[0m top5: 0.9253731343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=157245)[0m f1_macro: 0.35655808421612656
[2m[36m(func pid=157245)[0m f1_weighted: 0.3334079963149346
[2m[36m(func pid=157245)[0m f1_per_class: [0.508, 0.469, 0.615, 0.518, 0.217, 0.197, 0.133, 0.315, 0.241, 0.352]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.6717 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=158460)[0m top1: 0.05037313432835821
[2m[36m(func pid=158460)[0m top5: 0.6231343283582089
[2m[36m(func pid=158460)[0m f1_micro: 0.05037313432835821
[2m[36m(func pid=158460)[0m f1_macro: 0.049226622167680076
[2m[36m(func pid=158460)[0m f1_weighted: 0.05969734908353863
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.027, 0.148, 0.0, 0.0, 0.003, 0.294, 0.0, 0.02]
[2m[36m(func pid=158460)[0m 
[2m[36m(func pid=158041)[0m top1: 0.11800373134328358
[2m[36m(func pid=158041)[0m top5: 0.29524253731343286
[2m[36m(func pid=158041)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=158041)[0m f1_macro: 0.10429166499823928
[2m[36m(func pid=158041)[0m f1_weighted: 0.10239503352851323
[2m[36m(func pid=158041)[0m f1_per_class: [0.055, 0.444, 0.255, 0.0, 0.0, 0.0, 0.066, 0.0, 0.041, 0.182]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.41138059701492535
[2m[36m(func pid=157619)[0m top5: 0.9547574626865671
[2m[36m(func pid=157619)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=157619)[0m f1_macro: 0.4196334712718232
[2m[36m(func pid=157619)[0m f1_weighted: 0.4161023632589011
[2m[36m(func pid=157619)[0m f1_per_class: [0.534, 0.474, 0.815, 0.553, 0.264, 0.314, 0.32, 0.327, 0.249, 0.346]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0249 | Steps: 2 | Val loss: 1.6829 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=158460)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 6.1456 | Steps: 2 | Val loss: 338.4180 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.9608 | Steps: 2 | Val loss: 47.1955 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 03:26:42 (running for 00:07:16.98)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.104
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING  | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.025 |      0.357 |                   74 |
| train_2d480_00001 | RUNNING  | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.42  |                   74 |
| train_2d480_00002 | RUNNING  | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.522 |      0.104 |                   75 |
| train_2d480_00003 | RUNNING  | 192.168.7.53:158460 | 0.1    |       0.99 |         0      | 72.271 |      0.049 |                   74 |
| train_2d480_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.363339552238806
[2m[36m(func pid=157245)[0m top5: 0.9253731343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=157245)[0m f1_macro: 0.35688362690168673
[2m[36m(func pid=157245)[0m f1_weighted: 0.33277293319108864
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.467, 0.615, 0.52, 0.238, 0.199, 0.128, 0.32, 0.242, 0.323]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.6750 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=158460)[0m top1: 0.24486940298507462
[2m[36m(func pid=158460)[0m top5: 0.6105410447761194
[2m[36m(func pid=158460)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=158460)[0m f1_macro: 0.0980271856532192
[2m[36m(func pid=158460)[0m f1_weighted: 0.24546847784516776
[2m[36m(func pid=158460)[0m f1_per_class: [0.0, 0.0, 0.03, 0.192, 0.0, 0.154, 0.583, 0.0, 0.0, 0.021]
[2m[36m(func pid=158041)[0m top1: 0.09468283582089553
[2m[36m(func pid=158041)[0m top5: 0.2756529850746269
[2m[36m(func pid=158041)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=158041)[0m f1_macro: 0.09540924954020188
[2m[36m(func pid=158041)[0m f1_weighted: 0.08037656847098898
[2m[36m(func pid=158041)[0m f1_per_class: [0.041, 0.364, 0.222, 0.0, 0.069, 0.0, 0.039, 0.0, 0.037, 0.182]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.41324626865671643
[2m[36m(func pid=157619)[0m top5: 0.9542910447761194
[2m[36m(func pid=157619)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=157619)[0m f1_macro: 0.42459094391088514
[2m[36m(func pid=157619)[0m f1_weighted: 0.4164426397457652
[2m[36m(func pid=157619)[0m f1_per_class: [0.544, 0.47, 0.815, 0.558, 0.292, 0.322, 0.314, 0.33, 0.249, 0.353]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0190 | Steps: 2 | Val loss: 1.6848 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 7.9753 | Steps: 2 | Val loss: 72.6340 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=157245)[0m top1: 0.36380597014925375
[2m[36m(func pid=157245)[0m top5: 0.9230410447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=157245)[0m f1_macro: 0.35748237380409736
[2m[36m(func pid=157245)[0m f1_weighted: 0.3350831503668755
[2m[36m(func pid=157245)[0m f1_per_class: [0.535, 0.473, 0.615, 0.509, 0.227, 0.205, 0.139, 0.324, 0.248, 0.299]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.6881 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=158041)[0m top1: 0.08442164179104478
[2m[36m(func pid=158041)[0m top5: 0.3064365671641791
[2m[36m(func pid=158041)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=158041)[0m f1_macro: 0.09149184431029753
[2m[36m(func pid=158041)[0m f1_weighted: 0.07699266336428282
[2m[36m(func pid=158041)[0m f1_per_class: [0.039, 0.278, 0.182, 0.0, 0.127, 0.0, 0.074, 0.0, 0.071, 0.143]
[2m[36m(func pid=157619)[0m top1: 0.41744402985074625
[2m[36m(func pid=157619)[0m top5: 0.9552238805970149
[2m[36m(func pid=157619)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=157619)[0m f1_macro: 0.42405224685554216
[2m[36m(func pid=157619)[0m f1_weighted: 0.4210536847320353
[2m[36m(func pid=157619)[0m f1_per_class: [0.534, 0.469, 0.815, 0.573, 0.283, 0.317, 0.318, 0.333, 0.249, 0.35]
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0161 | Steps: 2 | Val loss: 1.6855 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:26:47 (running for 00:07:22.27)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.019 |      0.357 |                   75 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.425 |                   75 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.961 |      0.095 |                   76 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=174613)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=174613)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=174613)[0m Configuration completed!
[2m[36m(func pid=174613)[0m New optimizer parameters:
[2m[36m(func pid=174613)[0m SGD (
[2m[36m(func pid=174613)[0m Parameter Group 0
[2m[36m(func pid=174613)[0m     dampening: 0
[2m[36m(func pid=174613)[0m     differentiable: False
[2m[36m(func pid=174613)[0m     foreach: None
[2m[36m(func pid=174613)[0m     lr: 0.0001
[2m[36m(func pid=174613)[0m     maximize: False
[2m[36m(func pid=174613)[0m     momentum: 0.9
[2m[36m(func pid=174613)[0m     nesterov: False
[2m[36m(func pid=174613)[0m     weight_decay: 0
[2m[36m(func pid=174613)[0m )
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:26:52 (running for 00:07:27.64)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.016 |      0.355 |                   76 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.424 |                   76 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  7.975 |      0.091 |                   77 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.36427238805970147
[2m[36m(func pid=157245)[0m top5: 0.9230410447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=157245)[0m f1_macro: 0.35541597494012084
[2m[36m(func pid=157245)[0m f1_weighted: 0.33601158395306746
[2m[36m(func pid=157245)[0m f1_per_class: [0.512, 0.473, 0.615, 0.512, 0.225, 0.205, 0.141, 0.322, 0.244, 0.305]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.4057 | Steps: 2 | Val loss: 74.1728 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.7023 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0178 | Steps: 2 | Val loss: 1.6826 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1315 | Steps: 2 | Val loss: 2.5113 | Batch size: 32 | lr: 0.0001 | Duration: 4.44s
[2m[36m(func pid=158041)[0m top1: 0.13526119402985073
[2m[36m(func pid=158041)[0m top5: 0.43843283582089554
[2m[36m(func pid=158041)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=158041)[0m f1_macro: 0.1389422032747783
[2m[36m(func pid=158041)[0m f1_weighted: 0.1451141094151415
[2m[36m(func pid=158041)[0m f1_per_class: [0.041, 0.233, 0.279, 0.0, 0.117, 0.062, 0.267, 0.192, 0.067, 0.131]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4146455223880597
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=157619)[0m f1_macro: 0.4228746480986844
[2m[36m(func pid=157619)[0m f1_weighted: 0.41885173061580105
[2m[36m(func pid=157619)[0m f1_per_class: [0.539, 0.463, 0.815, 0.568, 0.277, 0.322, 0.317, 0.328, 0.25, 0.35]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:26:57 (running for 00:07:32.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.018 |      0.355 |                   77 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.423 |                   77 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.406 |      0.139 |                   78 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3656716417910448
[2m[36m(func pid=157245)[0m top5: 0.9249067164179104
[2m[36m(func pid=157245)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=157245)[0m f1_macro: 0.3552122807621243
[2m[36m(func pid=157245)[0m f1_weighted: 0.33951190192411884
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.468, 0.615, 0.514, 0.215, 0.201, 0.156, 0.323, 0.244, 0.299]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.06809701492537314
[2m[36m(func pid=174613)[0m top5: 0.48600746268656714
[2m[36m(func pid=174613)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=174613)[0m f1_macro: 0.04057027367140782
[2m[36m(func pid=174613)[0m f1_weighted: 0.04032737009187348
[2m[36m(func pid=174613)[0m f1_per_class: [0.12, 0.01, 0.0, 0.096, 0.0, 0.019, 0.0, 0.105, 0.022, 0.034]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.8211 | Steps: 2 | Val loss: 63.6882 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0031 | Steps: 2 | Val loss: 2.7522 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0297 | Steps: 2 | Val loss: 1.6946 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1520 | Steps: 2 | Val loss: 2.5341 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=158041)[0m top1: 0.1921641791044776
[2m[36m(func pid=158041)[0m top5: 0.5013992537313433
[2m[36m(func pid=158041)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=158041)[0m f1_macro: 0.20338383333998705
[2m[36m(func pid=158041)[0m f1_weighted: 0.1869435036848961
[2m[36m(func pid=158041)[0m f1_per_class: [0.03, 0.297, 0.645, 0.0, 0.171, 0.092, 0.34, 0.248, 0.058, 0.152]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4137126865671642
[2m[36m(func pid=157619)[0m top5: 0.9566231343283582
[2m[36m(func pid=157619)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=157619)[0m f1_macro: 0.4267409376466082
[2m[36m(func pid=157619)[0m f1_weighted: 0.4176866838162681
[2m[36m(func pid=157619)[0m f1_per_class: [0.549, 0.472, 0.846, 0.571, 0.283, 0.317, 0.307, 0.321, 0.245, 0.356]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3628731343283582
[2m[36m(func pid=157245)[0m top5: 0.9230410447761194
[2m[36m(func pid=157245)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=157245)[0m f1_macro: 0.35613099902258755
[2m[36m(func pid=157245)[0m f1_weighted: 0.33567541728918027
[2m[36m(func pid=157245)[0m f1_per_class: [0.52, 0.471, 0.615, 0.506, 0.215, 0.192, 0.151, 0.322, 0.245, 0.323]
== Status ==
Current time: 2024-01-07 03:27:03 (running for 00:07:38.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.03  |      0.356 |                   78 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.427 |                   78 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.821 |      0.203 |                   79 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  3.131 |      0.041 |                    1 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.06296641791044776
[2m[36m(func pid=174613)[0m top5: 0.48134328358208955
[2m[36m(func pid=174613)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=174613)[0m f1_macro: 0.03529405175660044
[2m[36m(func pid=174613)[0m f1_weighted: 0.033899704988195244
[2m[36m(func pid=174613)[0m f1_per_class: [0.075, 0.01, 0.0, 0.076, 0.0, 0.019, 0.0, 0.102, 0.023, 0.048]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.9135 | Steps: 2 | Val loss: 86.1888 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0031 | Steps: 2 | Val loss: 2.7721 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0144 | Steps: 2 | Val loss: 1.6828 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0755 | Steps: 2 | Val loss: 2.5495 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=158041)[0m top1: 0.16744402985074627
[2m[36m(func pid=158041)[0m top5: 0.4766791044776119
[2m[36m(func pid=158041)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=158041)[0m f1_macro: 0.0901430552878401
[2m[36m(func pid=158041)[0m f1_weighted: 0.13132623786403771
[2m[36m(func pid=158041)[0m f1_per_class: [0.01, 0.342, 0.0, 0.0, 0.171, 0.008, 0.224, 0.0, 0.077, 0.069]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4137126865671642
[2m[36m(func pid=157619)[0m top5: 0.9580223880597015
[2m[36m(func pid=157619)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=157619)[0m f1_macro: 0.4235835324843902
[2m[36m(func pid=157619)[0m f1_weighted: 0.41588128857144896
[2m[36m(func pid=157619)[0m f1_per_class: [0.541, 0.454, 0.815, 0.58, 0.301, 0.325, 0.302, 0.321, 0.233, 0.364]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:08 (running for 00:07:43.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.03  |      0.356 |                   78 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.424 |                   79 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.914 |      0.09  |                   80 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  3.076 |      0.031 |                    3 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.0625
[2m[36m(func pid=174613)[0m top5: 0.4766791044776119
[2m[36m(func pid=174613)[0m f1_micro: 0.0625
[2m[36m(func pid=174613)[0m f1_macro: 0.031204873683085853
[2m[36m(func pid=174613)[0m f1_weighted: 0.03542525134859413
[2m[36m(func pid=174613)[0m f1_per_class: [0.055, 0.015, 0.0, 0.083, 0.0, 0.019, 0.0, 0.101, 0.0, 0.039]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3670708955223881
[2m[36m(func pid=157245)[0m top5: 0.9239738805970149
[2m[36m(func pid=157245)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=157245)[0m f1_macro: 0.3612031981701444
[2m[36m(func pid=157245)[0m f1_weighted: 0.34064596966999483
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.475, 0.615, 0.51, 0.227, 0.204, 0.156, 0.323, 0.245, 0.34]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 2.1117 | Steps: 2 | Val loss: 126.0372 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.7683 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0944 | Steps: 2 | Val loss: 2.5589 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0165 | Steps: 2 | Val loss: 1.6895 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=158041)[0m top1: 0.13852611940298507
[2m[36m(func pid=158041)[0m top5: 0.41324626865671643
[2m[36m(func pid=158041)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=158041)[0m f1_macro: 0.06897556671220824
[2m[36m(func pid=158041)[0m f1_weighted: 0.07698056809719424
[2m[36m(func pid=158041)[0m f1_per_class: [0.0, 0.36, 0.0, 0.0, 0.128, 0.008, 0.033, 0.0, 0.06, 0.1]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.41744402985074625
[2m[36m(func pid=157619)[0m top5: 0.9580223880597015
[2m[36m(func pid=157619)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=157619)[0m f1_macro: 0.426087386055218
[2m[36m(func pid=157619)[0m f1_weighted: 0.4193654151796886
[2m[36m(func pid=157619)[0m f1_per_class: [0.536, 0.458, 0.815, 0.583, 0.304, 0.332, 0.305, 0.324, 0.236, 0.367]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:13 (running for 00:07:48.56)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.014 |      0.361 |                   79 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.426 |                   80 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.112 |      0.069 |                   81 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  3.094 |      0.035 |                    4 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.061567164179104475
[2m[36m(func pid=174613)[0m top5: 0.4724813432835821
[2m[36m(func pid=174613)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=174613)[0m f1_macro: 0.034985642209439366
[2m[36m(func pid=174613)[0m f1_weighted: 0.038074674045703895
[2m[36m(func pid=174613)[0m f1_per_class: [0.051, 0.023, 0.0, 0.085, 0.0, 0.019, 0.0, 0.097, 0.025, 0.05]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.36473880597014924
[2m[36m(func pid=157245)[0m top5: 0.9239738805970149
[2m[36m(func pid=157245)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=157245)[0m f1_macro: 0.3589320164524822
[2m[36m(func pid=157245)[0m f1_weighted: 0.33765872635344135
[2m[36m(func pid=157245)[0m f1_per_class: [0.516, 0.47, 0.615, 0.513, 0.215, 0.201, 0.149, 0.319, 0.244, 0.348]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.9218 | Steps: 2 | Val loss: 122.2240 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.8032 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0148 | Steps: 2 | Val loss: 1.6903 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9961 | Steps: 2 | Val loss: 2.5564 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=158041)[0m top1: 0.14132462686567165
[2m[36m(func pid=158041)[0m top5: 0.42117537313432835
[2m[36m(func pid=158041)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=158041)[0m f1_macro: 0.0751852412753202
[2m[36m(func pid=158041)[0m f1_weighted: 0.08098911387382073
[2m[36m(func pid=158041)[0m f1_per_class: [0.017, 0.379, 0.0, 0.0, 0.158, 0.0, 0.036, 0.0, 0.071, 0.091]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.40951492537313433
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=157619)[0m f1_macro: 0.4220751818947154
[2m[36m(func pid=157619)[0m f1_weighted: 0.4146171039275851
[2m[36m(func pid=157619)[0m f1_per_class: [0.536, 0.469, 0.815, 0.57, 0.298, 0.317, 0.303, 0.318, 0.232, 0.364]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:18 (running for 00:07:53.69)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.015 |      0.359 |                   81 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.003 |      0.422 |                   81 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.922 |      0.075 |                   82 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  3.094 |      0.035 |                    4 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3675373134328358
[2m[36m(func pid=157245)[0m top5: 0.9253731343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=157245)[0m f1_macro: 0.3588021090031947
[2m[36m(func pid=157245)[0m f1_weighted: 0.3405199894217121
[2m[36m(func pid=157245)[0m f1_per_class: [0.512, 0.479, 0.615, 0.515, 0.215, 0.201, 0.152, 0.322, 0.245, 0.333]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.06436567164179105
[2m[36m(func pid=174613)[0m top5: 0.4724813432835821
[2m[36m(func pid=174613)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=174613)[0m f1_macro: 0.03769547826701235
[2m[36m(func pid=174613)[0m f1_weighted: 0.044361248224846855
[2m[36m(func pid=174613)[0m f1_per_class: [0.048, 0.049, 0.0, 0.092, 0.0, 0.019, 0.0, 0.098, 0.025, 0.046]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 2.7008 | Steps: 2 | Val loss: 123.8960 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8015 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0188 | Steps: 2 | Val loss: 1.6958 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9733 | Steps: 2 | Val loss: 2.5500 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=158041)[0m top1: 0.13619402985074627
[2m[36m(func pid=158041)[0m top5: 0.4118470149253731
[2m[36m(func pid=158041)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=158041)[0m f1_macro: 0.11454332719487444
[2m[36m(func pid=158041)[0m f1_weighted: 0.08603181856390811
[2m[36m(func pid=158041)[0m f1_per_class: [0.047, 0.388, 0.375, 0.0, 0.114, 0.0, 0.036, 0.015, 0.07, 0.1]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4123134328358209
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=157619)[0m f1_macro: 0.42306726578155224
[2m[36m(func pid=157619)[0m f1_weighted: 0.41560426037030457
[2m[36m(func pid=157619)[0m f1_per_class: [0.536, 0.468, 0.815, 0.571, 0.295, 0.323, 0.302, 0.328, 0.234, 0.36]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:23 (running for 00:07:58.83)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.019 |      0.362 |                   82 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.423 |                   82 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  2.701 |      0.115 |                   83 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.996 |      0.038 |                    5 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3675373134328358
[2m[36m(func pid=157245)[0m top5: 0.9253731343283582
[2m[36m(func pid=157245)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=157245)[0m f1_macro: 0.3624375827505172
[2m[36m(func pid=157245)[0m f1_weighted: 0.34015777915014817
[2m[36m(func pid=157245)[0m f1_per_class: [0.5, 0.478, 0.649, 0.521, 0.213, 0.203, 0.144, 0.318, 0.247, 0.352]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.06763059701492537
[2m[36m(func pid=174613)[0m top5: 0.4710820895522388
[2m[36m(func pid=174613)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=174613)[0m f1_macro: 0.04211872302396054
[2m[36m(func pid=174613)[0m f1_weighted: 0.05240453080070625
[2m[36m(func pid=174613)[0m f1_per_class: [0.062, 0.08, 0.0, 0.104, 0.0, 0.013, 0.0, 0.095, 0.025, 0.041]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 13.0019 | Steps: 2 | Val loss: 130.8072 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8323 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0106 | Steps: 2 | Val loss: 1.6883 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8893 | Steps: 2 | Val loss: 2.5385 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=158041)[0m top1: 0.13199626865671643
[2m[36m(func pid=158041)[0m top5: 0.39132462686567165
[2m[36m(func pid=158041)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=158041)[0m f1_macro: 0.14680629577318965
[2m[36m(func pid=158041)[0m f1_weighted: 0.08847737055612412
[2m[36m(func pid=158041)[0m f1_per_class: [0.031, 0.403, 0.72, 0.0, 0.125, 0.0, 0.03, 0.016, 0.075, 0.068]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.40578358208955223
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=157619)[0m f1_macro: 0.41962172393219366
[2m[36m(func pid=157619)[0m f1_weighted: 0.41084786502551796
[2m[36m(func pid=157619)[0m f1_per_class: [0.536, 0.463, 0.815, 0.563, 0.286, 0.312, 0.3, 0.323, 0.234, 0.364]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:29 (running for 00:08:03.95)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.011 |      0.363 |                   83 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.42  |                   83 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      | 13.002 |      0.147 |                   84 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.973 |      0.042 |                    6 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3670708955223881
[2m[36m(func pid=157245)[0m top5: 0.9267723880597015
[2m[36m(func pid=157245)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=157245)[0m f1_macro: 0.3630321318907712
[2m[36m(func pid=157245)[0m f1_weighted: 0.3403520467649911
[2m[36m(func pid=157245)[0m f1_per_class: [0.504, 0.48, 0.649, 0.523, 0.213, 0.207, 0.141, 0.314, 0.24, 0.36]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.07276119402985075
[2m[36m(func pid=174613)[0m top5: 0.4710820895522388
[2m[36m(func pid=174613)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=174613)[0m f1_macro: 0.04605538588665671
[2m[36m(func pid=174613)[0m f1_weighted: 0.05976116148659464
[2m[36m(func pid=174613)[0m f1_per_class: [0.054, 0.083, 0.0, 0.123, 0.0, 0.019, 0.0, 0.103, 0.049, 0.029]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.6660 | Steps: 2 | Val loss: 122.4201 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.8249 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0159 | Steps: 2 | Val loss: 1.6734 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8079 | Steps: 2 | Val loss: 2.5267 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=158041)[0m top1: 0.10541044776119403
[2m[36m(func pid=158041)[0m top5: 0.36380597014925375
[2m[36m(func pid=158041)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=158041)[0m f1_macro: 0.05968237262349947
[2m[36m(func pid=158041)[0m f1_weighted: 0.07587628830547763
[2m[36m(func pid=158041)[0m f1_per_class: [0.039, 0.395, 0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.076, 0.074]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.41044776119402987
[2m[36m(func pid=157619)[0m top5: 0.9589552238805971
[2m[36m(func pid=157619)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=157619)[0m f1_macro: 0.42206318836774115
[2m[36m(func pid=157619)[0m f1_weighted: 0.4160458705081462
[2m[36m(func pid=157619)[0m f1_per_class: [0.542, 0.467, 0.815, 0.572, 0.277, 0.315, 0.306, 0.326, 0.234, 0.367]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:34 (running for 00:08:09.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.016 |      0.369 |                   84 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   84 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.666 |      0.06  |                   85 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.889 |      0.046 |                    7 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.373134328358209
[2m[36m(func pid=157245)[0m top5: 0.929570895522388
[2m[36m(func pid=157245)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=157245)[0m f1_macro: 0.3685471836078419
[2m[36m(func pid=157245)[0m f1_weighted: 0.3494534982660131
[2m[36m(func pid=157245)[0m f1_per_class: [0.5, 0.482, 0.686, 0.526, 0.208, 0.208, 0.166, 0.321, 0.24, 0.348]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.07649253731343283
[2m[36m(func pid=174613)[0m top5: 0.47574626865671643
[2m[36m(func pid=174613)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=174613)[0m f1_macro: 0.04970284764558246
[2m[36m(func pid=174613)[0m f1_weighted: 0.06403444502409454
[2m[36m(func pid=174613)[0m f1_per_class: [0.063, 0.084, 0.0, 0.139, 0.0, 0.012, 0.0, 0.105, 0.047, 0.047]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.8170 | Steps: 2 | Val loss: 123.8835 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8378 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0147 | Steps: 2 | Val loss: 1.6805 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7392 | Steps: 2 | Val loss: 2.5143 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=158041)[0m top1: 0.10494402985074627
[2m[36m(func pid=158041)[0m top5: 0.3460820895522388
[2m[36m(func pid=158041)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=158041)[0m f1_macro: 0.058641395934963335
[2m[36m(func pid=158041)[0m f1_weighted: 0.07490461300400071
[2m[36m(func pid=158041)[0m f1_per_class: [0.037, 0.396, 0.0, 0.003, 0.0, 0.0, 0.006, 0.0, 0.074, 0.071]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4076492537313433
[2m[36m(func pid=157619)[0m top5: 0.9580223880597015
[2m[36m(func pid=157619)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=157619)[0m f1_macro: 0.4209641979673405
[2m[36m(func pid=157619)[0m f1_weighted: 0.4135255904661351
[2m[36m(func pid=157619)[0m f1_per_class: [0.541, 0.465, 0.815, 0.563, 0.277, 0.314, 0.307, 0.325, 0.235, 0.367]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:39 (running for 00:08:14.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.016 |      0.369 |                   84 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.421 |                   85 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.817 |      0.059 |                   86 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.739 |      0.05  |                    9 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.07789179104477612
[2m[36m(func pid=174613)[0m top5: 0.4762126865671642
[2m[36m(func pid=174613)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=174613)[0m f1_macro: 0.0500690011603529
[2m[36m(func pid=174613)[0m f1_weighted: 0.06811768870996016
[2m[36m(func pid=174613)[0m f1_per_class: [0.055, 0.097, 0.0, 0.146, 0.0, 0.012, 0.0, 0.109, 0.043, 0.039]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3689365671641791
[2m[36m(func pid=157245)[0m top5: 0.9305037313432836
[2m[36m(func pid=157245)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=157245)[0m f1_macro: 0.3645771792780718
[2m[36m(func pid=157245)[0m f1_weighted: 0.34609216346529265
[2m[36m(func pid=157245)[0m f1_per_class: [0.524, 0.477, 0.667, 0.522, 0.198, 0.207, 0.161, 0.324, 0.24, 0.327]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.7766 | Steps: 2 | Val loss: 128.3089 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8405 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=158041)[0m top1: 0.12080223880597014
[2m[36m(func pid=158041)[0m top5: 0.300839552238806
[2m[36m(func pid=158041)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=158041)[0m f1_macro: 0.05965894067463738
[2m[36m(func pid=158041)[0m f1_weighted: 0.07154684505502415
[2m[36m(func pid=158041)[0m f1_per_class: [0.034, 0.391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.069, 0.102]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7202 | Steps: 2 | Val loss: 2.5027 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0154 | Steps: 2 | Val loss: 1.6692 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=157619)[0m top1: 0.4123134328358209
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=157619)[0m f1_macro: 0.4212744693141966
[2m[36m(func pid=157619)[0m f1_weighted: 0.41700917747546973
[2m[36m(func pid=157619)[0m f1_per_class: [0.531, 0.464, 0.815, 0.573, 0.269, 0.319, 0.309, 0.326, 0.235, 0.371]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:44 (running for 00:08:19.44)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.015 |      0.365 |                   85 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.421 |                   86 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.777 |      0.06  |                   87 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.72  |      0.058 |                   10 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.08022388059701492
[2m[36m(func pid=174613)[0m top5: 0.4724813432835821
[2m[36m(func pid=174613)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=174613)[0m f1_macro: 0.057508416564995314
[2m[36m(func pid=174613)[0m f1_weighted: 0.07326072152225774
[2m[36m(func pid=174613)[0m f1_per_class: [0.069, 0.104, 0.0, 0.149, 0.0, 0.028, 0.0, 0.107, 0.081, 0.037]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.36986940298507465
[2m[36m(func pid=157245)[0m top5: 0.9319029850746269
[2m[36m(func pid=157245)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=157245)[0m f1_macro: 0.3708627584550008
[2m[36m(func pid=157245)[0m f1_weighted: 0.3477153565856164
[2m[36m(func pid=157245)[0m f1_per_class: [0.542, 0.481, 0.686, 0.516, 0.222, 0.206, 0.168, 0.321, 0.243, 0.323]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.4545 | Steps: 2 | Val loss: 108.9218 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8241 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=158041)[0m top1: 0.13013059701492538
[2m[36m(func pid=158041)[0m top5: 0.3931902985074627
[2m[36m(func pid=158041)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=158041)[0m f1_macro: 0.06775339218126933
[2m[36m(func pid=158041)[0m f1_weighted: 0.07713460164769409
[2m[36m(func pid=158041)[0m f1_per_class: [0.052, 0.389, 0.0, 0.01, 0.069, 0.0, 0.0, 0.046, 0.089, 0.023]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0194 | Steps: 2 | Val loss: 1.6563 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5975 | Steps: 2 | Val loss: 2.4872 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=157619)[0m top1: 0.41277985074626866
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=157619)[0m f1_macro: 0.42187763711019305
[2m[36m(func pid=157619)[0m f1_weighted: 0.42001796530150515
[2m[36m(func pid=157619)[0m f1_per_class: [0.531, 0.462, 0.815, 0.566, 0.275, 0.321, 0.326, 0.32, 0.239, 0.364]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:49 (running for 00:08:24.78)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.015 |      0.371 |                   86 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   87 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.455 |      0.068 |                   88 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.597 |      0.061 |                   11 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=174613)[0m top1: 0.08628731343283583

[2m[36m(func pid=174613)[0m top5: 0.47154850746268656
[2m[36m(func pid=174613)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=174613)[0m f1_macro: 0.06068863149681886
[2m[36m(func pid=174613)[0m f1_weighted: 0.0814573114808019
[2m[36m(func pid=174613)[0m f1_per_class: [0.062, 0.114, 0.0, 0.169, 0.0, 0.033, 0.0, 0.114, 0.078, 0.036]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.37593283582089554
[2m[36m(func pid=157245)[0m top5: 0.933768656716418
[2m[36m(func pid=157245)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=157245)[0m f1_macro: 0.377742367613551
[2m[36m(func pid=157245)[0m f1_weighted: 0.35689311699384224
[2m[36m(func pid=157245)[0m f1_per_class: [0.554, 0.477, 0.686, 0.532, 0.233, 0.216, 0.182, 0.319, 0.235, 0.344]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.7546 | Steps: 2 | Val loss: 93.8185 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8269 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=158041)[0m top1: 0.18516791044776118
[2m[36m(func pid=158041)[0m top5: 0.48973880597014924
[2m[36m(func pid=158041)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=158041)[0m f1_macro: 0.19058974633580122
[2m[36m(func pid=158041)[0m f1_weighted: 0.12239104190159888
[2m[36m(func pid=158041)[0m f1_per_class: [0.039, 0.398, 0.696, 0.051, 0.065, 0.0, 0.018, 0.413, 0.103, 0.123]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0087 | Steps: 2 | Val loss: 1.6558 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5494 | Steps: 2 | Val loss: 2.4757 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=157619)[0m top1: 0.4141791044776119
[2m[36m(func pid=157619)[0m top5: 0.9580223880597015
[2m[36m(func pid=157619)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=157619)[0m f1_macro: 0.42223585218918797
[2m[36m(func pid=157619)[0m f1_weighted: 0.4221915932500071
[2m[36m(func pid=157619)[0m f1_per_class: [0.531, 0.46, 0.815, 0.564, 0.272, 0.324, 0.335, 0.321, 0.244, 0.356]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:27:55 (running for 00:08:29.97)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.019 |      0.378 |                   87 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   88 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.755 |      0.191 |                   89 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.549 |      0.062 |                   12 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.37779850746268656
[2m[36m(func pid=157245)[0m top5: 0.933768656716418
[2m[36m(func pid=157245)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=157245)[0m f1_macro: 0.37733084525237987
[2m[36m(func pid=157245)[0m f1_weighted: 0.358165800455605
[2m[36m(func pid=157245)[0m f1_per_class: [0.574, 0.479, 0.667, 0.534, 0.225, 0.214, 0.182, 0.32, 0.237, 0.34]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.09001865671641791
[2m[36m(func pid=174613)[0m top5: 0.47388059701492535
[2m[36m(func pid=174613)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=174613)[0m f1_macro: 0.06249590739060085
[2m[36m(func pid=174613)[0m f1_weighted: 0.08697739588708206
[2m[36m(func pid=174613)[0m f1_per_class: [0.06, 0.121, 0.0, 0.183, 0.0, 0.037, 0.0, 0.116, 0.072, 0.035]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.9255 | Steps: 2 | Val loss: 78.7166 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8556 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=158041)[0m top1: 0.21875
[2m[36m(func pid=158041)[0m top5: 0.5261194029850746
[2m[36m(func pid=158041)[0m f1_micro: 0.21875
[2m[36m(func pid=158041)[0m f1_macro: 0.19780479238214577
[2m[36m(func pid=158041)[0m f1_weighted: 0.1570506931568429
[2m[36m(func pid=158041)[0m f1_per_class: [0.03, 0.41, 0.64, 0.078, 0.08, 0.0, 0.102, 0.415, 0.125, 0.098]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5449 | Steps: 2 | Val loss: 2.4681 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0117 | Steps: 2 | Val loss: 1.6496 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=157619)[0m top1: 0.4141791044776119
[2m[36m(func pid=157619)[0m top5: 0.9584888059701493
[2m[36m(func pid=157619)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=157619)[0m f1_macro: 0.42362525037724436
[2m[36m(func pid=157619)[0m f1_weighted: 0.42069329951856355
[2m[36m(func pid=157619)[0m f1_per_class: [0.53, 0.466, 0.815, 0.555, 0.275, 0.332, 0.331, 0.33, 0.248, 0.356]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=174613)[0m top1: 0.09095149253731344
[2m[36m(func pid=174613)[0m top5: 0.47388059701492535
[2m[36m(func pid=174613)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=174613)[0m f1_macro: 0.07149601977668686
[2m[36m(func pid=174613)[0m f1_weighted: 0.08778010793789771
[2m[36m(func pid=174613)[0m f1_per_class: [0.064, 0.122, 0.059, 0.177, 0.0, 0.048, 0.0, 0.122, 0.085, 0.038]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:28:00 (running for 00:08:35.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.012 |      0.383 |                   89 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.424 |                   89 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.926 |      0.198 |                   90 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.545 |      0.071 |                   13 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3843283582089552
[2m[36m(func pid=157245)[0m top5: 0.9347014925373134
[2m[36m(func pid=157245)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=157245)[0m f1_macro: 0.3825519953872365
[2m[36m(func pid=157245)[0m f1_weighted: 0.36841501405193533
[2m[36m(func pid=157245)[0m f1_per_class: [0.569, 0.484, 0.686, 0.534, 0.22, 0.209, 0.215, 0.328, 0.234, 0.348]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9298 | Steps: 2 | Val loss: 63.3038 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.8614 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=158041)[0m top1: 0.2751865671641791
[2m[36m(func pid=158041)[0m top5: 0.5536380597014925
[2m[36m(func pid=158041)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=158041)[0m f1_macro: 0.20315259725340856
[2m[36m(func pid=158041)[0m f1_weighted: 0.2235826760951272
[2m[36m(func pid=158041)[0m f1_per_class: [0.036, 0.421, 0.462, 0.099, 0.0, 0.0, 0.29, 0.478, 0.144, 0.102]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.3960 | Steps: 2 | Val loss: 2.4557 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0127 | Steps: 2 | Val loss: 1.6434 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=157619)[0m top1: 0.4123134328358209
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=157619)[0m f1_macro: 0.4217379399703093
[2m[36m(func pid=157619)[0m f1_weighted: 0.41832761296786325
[2m[36m(func pid=157619)[0m f1_per_class: [0.526, 0.464, 0.815, 0.555, 0.275, 0.331, 0.325, 0.332, 0.239, 0.356]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=174613)[0m top1: 0.09281716417910447
[2m[36m(func pid=174613)[0m top5: 0.4724813432835821
[2m[36m(func pid=174613)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=174613)[0m f1_macro: 0.07520210736684316
[2m[36m(func pid=174613)[0m f1_weighted: 0.09120765197390769
[2m[36m(func pid=174613)[0m f1_per_class: [0.06, 0.133, 0.095, 0.181, 0.0, 0.047, 0.003, 0.116, 0.077, 0.04]
== Status ==
Current time: 2024-01-07 03:28:05 (running for 00:08:40.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.012 |      0.383 |                   89 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   90 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.93  |      0.203 |                   91 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.396 |      0.075 |                   14 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.3880597014925373
[2m[36m(func pid=157245)[0m top5: 0.9365671641791045
[2m[36m(func pid=157245)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=157245)[0m f1_macro: 0.38064841887475404
[2m[36m(func pid=157245)[0m f1_weighted: 0.3752116916920048
[2m[36m(func pid=157245)[0m f1_per_class: [0.554, 0.48, 0.667, 0.538, 0.227, 0.213, 0.237, 0.324, 0.23, 0.337]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.7975 | Steps: 2 | Val loss: 59.4767 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0012 | Steps: 2 | Val loss: 2.8617 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0136 | Steps: 2 | Val loss: 1.6485 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3677 | Steps: 2 | Val loss: 2.4457 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m top1: 0.2989738805970149
[2m[36m(func pid=158041)[0m top5: 0.5611007462686567
[2m[36m(func pid=158041)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=158041)[0m f1_macro: 0.16800885388744488
[2m[36m(func pid=158041)[0m f1_weighted: 0.25141445584798067
[2m[36m(func pid=158041)[0m f1_per_class: [0.034, 0.417, 0.0, 0.103, 0.0, 0.0, 0.388, 0.489, 0.15, 0.098]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.417910447761194
[2m[36m(func pid=157619)[0m top5: 0.957089552238806
[2m[36m(func pid=157619)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.4265165511607479
[2m[36m(func pid=157619)[0m f1_weighted: 0.4223680948914045
[2m[36m(func pid=157619)[0m f1_per_class: [0.536, 0.472, 0.815, 0.559, 0.292, 0.332, 0.327, 0.331, 0.248, 0.353]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:28:10 (running for 00:08:45.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.013 |      0.381 |                   90 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.427 |                   91 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.798 |      0.168 |                   92 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.368 |      0.085 |                   15 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157245)[0m top1: 0.386660447761194
[2m[36m(func pid=157245)[0m top5: 0.9361007462686567
[2m[36m(func pid=157245)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=157245)[0m f1_macro: 0.3759449522167699
[2m[36m(func pid=157245)[0m f1_weighted: 0.37368950929549677
[2m[36m(func pid=157245)[0m f1_per_class: [0.542, 0.473, 0.649, 0.532, 0.206, 0.213, 0.242, 0.329, 0.236, 0.337]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m top1: 0.0960820895522388
[2m[36m(func pid=174613)[0m top5: 0.47388059701492535
[2m[36m(func pid=174613)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=174613)[0m f1_macro: 0.08523782834345041
[2m[36m(func pid=174613)[0m f1_weighted: 0.09527124241069089
[2m[36m(func pid=174613)[0m f1_per_class: [0.065, 0.136, 0.163, 0.187, 0.0, 0.056, 0.003, 0.12, 0.084, 0.039]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.0640 | Steps: 2 | Val loss: 56.2297 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.8607 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=158041)[0m top1: 0.30830223880597013
[2m[36m(func pid=158041)[0m top5: 0.570429104477612
[2m[36m(func pid=158041)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=158041)[0m f1_macro: 0.17784796866255476
[2m[36m(func pid=158041)[0m f1_weighted: 0.27269879407569847
[2m[36m(func pid=158041)[0m f1_per_class: [0.062, 0.411, 0.0, 0.111, 0.0, 0.0, 0.449, 0.521, 0.139, 0.084]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2683 | Steps: 2 | Val loss: 2.4341 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0119 | Steps: 2 | Val loss: 1.6520 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=157619)[0m top1: 0.417910447761194
[2m[36m(func pid=157619)[0m top5: 0.9580223880597015
[2m[36m(func pid=157619)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.425456686384321
[2m[36m(func pid=157619)[0m f1_weighted: 0.423725331454437
[2m[36m(func pid=157619)[0m f1_per_class: [0.532, 0.463, 0.815, 0.563, 0.277, 0.333, 0.334, 0.328, 0.245, 0.364]
[2m[36m(func pid=157619)[0m 
== Status ==
Current time: 2024-01-07 03:28:15 (running for 00:08:50.44)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.014 |      0.376 |                   91 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.425 |                   92 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.064 |      0.178 |                   93 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.268 |      0.088 |                   16 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.09748134328358209
[2m[36m(func pid=174613)[0m top5: 0.47947761194029853
[2m[36m(func pid=174613)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=174613)[0m f1_macro: 0.08781078688881333
[2m[36m(func pid=174613)[0m f1_weighted: 0.09805646583161283
[2m[36m(func pid=174613)[0m f1_per_class: [0.062, 0.153, 0.182, 0.184, 0.0, 0.059, 0.006, 0.111, 0.081, 0.04]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.384794776119403
[2m[36m(func pid=157245)[0m top5: 0.9351679104477612
[2m[36m(func pid=157245)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=157245)[0m f1_macro: 0.37672036502181144
[2m[36m(func pid=157245)[0m f1_weighted: 0.37090171221221657
[2m[36m(func pid=157245)[0m f1_per_class: [0.542, 0.482, 0.632, 0.526, 0.222, 0.214, 0.233, 0.322, 0.238, 0.356]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.0969 | Steps: 2 | Val loss: 59.2381 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8670 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2004 | Steps: 2 | Val loss: 2.4201 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=157619)[0m top1: 0.417910447761194
[2m[36m(func pid=157619)[0m top5: 0.9580223880597015
[2m[36m(func pid=157619)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.4208311158185394
[2m[36m(func pid=157619)[0m f1_weighted: 0.42318733674071335
[2m[36m(func pid=157619)[0m f1_per_class: [0.518, 0.468, 0.769, 0.56, 0.283, 0.334, 0.332, 0.333, 0.25, 0.36]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.2835820895522388
[2m[36m(func pid=158041)[0m top5: 0.5680970149253731
[2m[36m(func pid=158041)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=158041)[0m f1_macro: 0.1718798312986933
[2m[36m(func pid=158041)[0m f1_weighted: 0.2554927906050078
[2m[36m(func pid=158041)[0m f1_per_class: [0.053, 0.4, 0.0, 0.117, 0.0, 0.0, 0.392, 0.525, 0.146, 0.086]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0167 | Steps: 2 | Val loss: 1.6404 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=174613)[0m top1: 0.09934701492537314== Status ==
Current time: 2024-01-07 03:28:20 (running for 00:08:55.58)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.012 |      0.377 |                   92 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.421 |                   93 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  1.097 |      0.172 |                   94 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.2   |      0.095 |                   17 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=174613)[0m top5: 0.48740671641791045
[2m[36m(func pid=174613)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=174613)[0m f1_macro: 0.09457005879249851
[2m[36m(func pid=174613)[0m f1_weighted: 0.09975922062595645
[2m[36m(func pid=174613)[0m f1_per_class: [0.066, 0.153, 0.25, 0.189, 0.0, 0.058, 0.006, 0.112, 0.078, 0.033]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3931902985074627
[2m[36m(func pid=157245)[0m top5: 0.9361007462686567
[2m[36m(func pid=157245)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=157245)[0m f1_macro: 0.3848111052687472
[2m[36m(func pid=157245)[0m f1_weighted: 0.3788961463657383
[2m[36m(func pid=157245)[0m f1_per_class: [0.541, 0.486, 0.686, 0.538, 0.241, 0.214, 0.244, 0.325, 0.246, 0.327]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8736 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.6708 | Steps: 2 | Val loss: 63.1069 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2811 | Steps: 2 | Val loss: 2.4101 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=157619)[0m top1: 0.4239738805970149
[2m[36m(func pid=157619)[0m top5: 0.9589552238805971
[2m[36m(func pid=157619)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=157619)[0m f1_macro: 0.42551140249582203
[2m[36m(func pid=157619)[0m f1_weighted: 0.43011242030255165
[2m[36m(func pid=157619)[0m f1_per_class: [0.537, 0.473, 0.769, 0.574, 0.292, 0.333, 0.34, 0.332, 0.247, 0.36]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.23274253731343283
[2m[36m(func pid=158041)[0m top5: 0.5583022388059702
[2m[36m(func pid=158041)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=158041)[0m f1_macro: 0.14469844157515405
[2m[36m(func pid=158041)[0m f1_weighted: 0.19651053243672562
[2m[36m(func pid=158041)[0m f1_per_class: [0.045, 0.387, 0.0, 0.118, 0.0, 0.0, 0.214, 0.464, 0.138, 0.081]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0090 | Steps: 2 | Val loss: 1.6434 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:28:25 (running for 00:09:00.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.017 |      0.385 |                   93 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.426 |                   94 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.671 |      0.145 |                   95 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.281 |      0.096 |                   18 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.10121268656716417
[2m[36m(func pid=174613)[0m top5: 0.49533582089552236
[2m[36m(func pid=174613)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=174613)[0m f1_macro: 0.09623613782200253
[2m[36m(func pid=174613)[0m f1_weighted: 0.10113563907296458
[2m[36m(func pid=174613)[0m f1_per_class: [0.07, 0.16, 0.247, 0.187, 0.0, 0.066, 0.006, 0.109, 0.074, 0.045]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.8825 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=157245)[0m top1: 0.3880597014925373
[2m[36m(func pid=157245)[0m top5: 0.9365671641791045
[2m[36m(func pid=157245)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=157245)[0m f1_macro: 0.3810842621340179
[2m[36m(func pid=157245)[0m f1_weighted: 0.3731583843914447
[2m[36m(func pid=157245)[0m f1_per_class: [0.547, 0.484, 0.667, 0.537, 0.233, 0.214, 0.228, 0.322, 0.243, 0.337]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.6446 | Steps: 2 | Val loss: 60.9431 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.0502 | Steps: 2 | Val loss: 2.3975 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=157619)[0m top1: 0.417910447761194
[2m[36m(func pid=157619)[0m top5: 0.9566231343283582
[2m[36m(func pid=157619)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=157619)[0m f1_macro: 0.42232703536935323
[2m[36m(func pid=157619)[0m f1_weighted: 0.425400314257416
[2m[36m(func pid=157619)[0m f1_per_class: [0.523, 0.475, 0.769, 0.555, 0.28, 0.334, 0.343, 0.32, 0.241, 0.383]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=158041)[0m top1: 0.21175373134328357
[2m[36m(func pid=158041)[0m top5: 0.5527052238805971
[2m[36m(func pid=158041)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=158041)[0m f1_macro: 0.13039096627305205
[2m[36m(func pid=158041)[0m f1_weighted: 0.15355640975123228
[2m[36m(func pid=158041)[0m f1_per_class: [0.04, 0.391, 0.0, 0.123, 0.0, 0.0, 0.062, 0.474, 0.134, 0.081]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0077 | Steps: 2 | Val loss: 1.6354 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 03:28:30 (running for 00:09:05.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.009 |      0.381 |                   94 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.422 |                   95 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.645 |      0.13  |                   96 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.05  |      0.099 |                   19 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.10494402985074627
[2m[36m(func pid=174613)[0m top5: 0.4986007462686567
[2m[36m(func pid=174613)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=174613)[0m f1_macro: 0.09895465911692682
[2m[36m(func pid=174613)[0m f1_weighted: 0.10591967242571727
[2m[36m(func pid=174613)[0m f1_per_class: [0.088, 0.16, 0.243, 0.187, 0.0, 0.069, 0.018, 0.116, 0.069, 0.039]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9012 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.5525 | Steps: 2 | Val loss: 59.5975 | Batch size: 32 | lr: 0.01 | Duration: 2.57s
[2m[36m(func pid=157245)[0m top1: 0.39225746268656714
[2m[36m(func pid=157245)[0m top5: 0.9370335820895522
[2m[36m(func pid=157245)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=157245)[0m f1_macro: 0.3854468350526562
[2m[36m(func pid=157245)[0m f1_weighted: 0.37872223788098675
[2m[36m(func pid=157245)[0m f1_per_class: [0.566, 0.491, 0.667, 0.532, 0.238, 0.213, 0.246, 0.326, 0.242, 0.333]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.0835 | Steps: 2 | Val loss: 2.3925 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=158041)[0m top1: 0.21455223880597016
[2m[36m(func pid=158041)[0m top5: 0.5489738805970149
[2m[36m(func pid=158041)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=158041)[0m f1_macro: 0.12971190302040175
[2m[36m(func pid=158041)[0m f1_weighted: 0.1515727705097395
[2m[36m(func pid=158041)[0m f1_per_class: [0.039, 0.395, 0.0, 0.127, 0.0, 0.008, 0.048, 0.467, 0.124, 0.089]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.41651119402985076
[2m[36m(func pid=157619)[0m top5: 0.9556902985074627
[2m[36m(func pid=157619)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=157619)[0m f1_macro: 0.42138740043937517
[2m[36m(func pid=157619)[0m f1_weighted: 0.4246529340299705
[2m[36m(func pid=157619)[0m f1_per_class: [0.527, 0.48, 0.769, 0.551, 0.28, 0.332, 0.341, 0.325, 0.236, 0.371]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0091 | Steps: 2 | Val loss: 1.6461 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:28:36 (running for 00:09:10.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.385 |                   95 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.421 |                   96 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.552 |      0.13  |                   97 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.084 |      0.1   |                   20 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.10354477611940298
[2m[36m(func pid=174613)[0m top5: 0.4986007462686567
[2m[36m(func pid=174613)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=174613)[0m f1_macro: 0.10035638063107288
[2m[36m(func pid=174613)[0m f1_weighted: 0.1048095051085194
[2m[36m(func pid=174613)[0m f1_per_class: [0.099, 0.155, 0.234, 0.178, 0.0, 0.066, 0.024, 0.119, 0.086, 0.041]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.6327 | Steps: 2 | Val loss: 55.0759 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9428 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=157245)[0m top1: 0.3871268656716418
[2m[36m(func pid=157245)[0m top5: 0.9347014925373134
[2m[36m(func pid=157245)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=157245)[0m f1_macro: 0.38109993447060797
[2m[36m(func pid=157245)[0m f1_weighted: 0.37208060412336463
[2m[36m(func pid=157245)[0m f1_per_class: [0.559, 0.486, 0.667, 0.519, 0.227, 0.211, 0.239, 0.335, 0.248, 0.32]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.0112 | Steps: 2 | Val loss: 2.3815 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=158041)[0m top1: 0.22527985074626866
[2m[36m(func pid=158041)[0m top5: 0.5419776119402985
[2m[36m(func pid=158041)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=158041)[0m f1_macro: 0.14089305611419858
[2m[36m(func pid=158041)[0m f1_weighted: 0.16772722876279636
[2m[36m(func pid=158041)[0m f1_per_class: [0.043, 0.408, 0.0, 0.126, 0.032, 0.008, 0.091, 0.486, 0.12, 0.096]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.41511194029850745
[2m[36m(func pid=157619)[0m top5: 0.9561567164179104
[2m[36m(func pid=157619)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=157619)[0m f1_macro: 0.42080441497885285
[2m[36m(func pid=157619)[0m f1_weighted: 0.42354876577640144
[2m[36m(func pid=157619)[0m f1_per_class: [0.527, 0.474, 0.769, 0.551, 0.275, 0.335, 0.34, 0.325, 0.236, 0.375]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0105 | Steps: 2 | Val loss: 1.6587 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 03:28:41 (running for 00:09:16.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.009 |      0.381 |                   96 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.421 |                   97 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.633 |      0.141 |                   98 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.011 |      0.099 |                   21 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.10401119402985075
[2m[36m(func pid=174613)[0m top5: 0.5083955223880597
[2m[36m(func pid=174613)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=174613)[0m f1_macro: 0.09910657695198852
[2m[36m(func pid=174613)[0m f1_weighted: 0.10451846037820606
[2m[36m(func pid=174613)[0m f1_per_class: [0.1, 0.15, 0.22, 0.18, 0.0, 0.065, 0.024, 0.127, 0.081, 0.045]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.9815 | Steps: 2 | Val loss: 50.6014 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.9356 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=157245)[0m top1: 0.3833955223880597
[2m[36m(func pid=157245)[0m top5: 0.9328358208955224
[2m[36m(func pid=157245)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=157245)[0m f1_macro: 0.3771407862659413
[2m[36m(func pid=157245)[0m f1_weighted: 0.36835427990467146
[2m[36m(func pid=157245)[0m f1_per_class: [0.537, 0.48, 0.667, 0.519, 0.215, 0.215, 0.231, 0.329, 0.243, 0.337]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.0199 | Steps: 2 | Val loss: 2.3694 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=158041)[0m top1: 0.20942164179104478
[2m[36m(func pid=158041)[0m top5: 0.5153917910447762
[2m[36m(func pid=158041)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=158041)[0m f1_macro: 0.14155851046325235
[2m[36m(func pid=158041)[0m f1_weighted: 0.16495026665443763
[2m[36m(func pid=158041)[0m f1_per_class: [0.04, 0.423, 0.0, 0.118, 0.1, 0.0, 0.087, 0.48, 0.1, 0.069]
[2m[36m(func pid=158041)[0m 
[2m[36m(func pid=157619)[0m top1: 0.4155783582089552
[2m[36m(func pid=157619)[0m top5: 0.9589552238805971
[2m[36m(func pid=157619)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=157619)[0m f1_macro: 0.419894862971611
[2m[36m(func pid=157619)[0m f1_weighted: 0.42610864709270035
[2m[36m(func pid=157619)[0m f1_per_class: [0.542, 0.475, 0.769, 0.56, 0.262, 0.326, 0.345, 0.314, 0.239, 0.367]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0075 | Steps: 2 | Val loss: 1.6564 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 03:28:46 (running for 00:09:21.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.011 |      0.377 |                   97 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0.001 |      0.42  |                   98 |
| train_2d480_00002 | RUNNING    | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.982 |      0.142 |                   99 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  2.02  |      0.103 |                   22 |
| train_2d480_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.10774253731343283
[2m[36m(func pid=174613)[0m top5: 0.5219216417910447
[2m[36m(func pid=174613)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=174613)[0m f1_macro: 0.10268885778786789
[2m[36m(func pid=174613)[0m f1_weighted: 0.10937462216347778
[2m[36m(func pid=174613)[0m f1_per_class: [0.107, 0.157, 0.225, 0.182, 0.0, 0.08, 0.03, 0.12, 0.081, 0.046]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=158041)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.9398 | Steps: 2 | Val loss: 49.5528 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.9616 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=157245)[0m top1: 0.38619402985074625
[2m[36m(func pid=157245)[0m top5: 0.9328358208955224
[2m[36m(func pid=157245)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=157245)[0m f1_macro: 0.37944493391878004
[2m[36m(func pid=157245)[0m f1_weighted: 0.37255156004900286
[2m[36m(func pid=157245)[0m f1_per_class: [0.537, 0.483, 0.686, 0.518, 0.211, 0.215, 0.243, 0.335, 0.244, 0.324]
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.8553 | Steps: 2 | Val loss: 2.3586 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=158041)[0m top1: 0.1669776119402985
[2m[36m(func pid=158041)[0m top5: 0.4435634328358209
[2m[36m(func pid=158041)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=158041)[0m f1_macro: 0.10732581618744412
[2m[36m(func pid=158041)[0m f1_weighted: 0.12573539079817625
[2m[36m(func pid=158041)[0m f1_per_class: [0.046, 0.404, 0.0, 0.077, 0.113, 0.0, 0.053, 0.235, 0.078, 0.067]
[2m[36m(func pid=157619)[0m top1: 0.4085820895522388
[2m[36m(func pid=157619)[0m top5: 0.957089552238806
[2m[36m(func pid=157619)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=157619)[0m f1_macro: 0.4176820254121788
[2m[36m(func pid=157619)[0m f1_weighted: 0.4198319621363052
[2m[36m(func pid=157619)[0m f1_per_class: [0.552, 0.476, 0.769, 0.545, 0.255, 0.316, 0.34, 0.321, 0.232, 0.371]
[2m[36m(func pid=157619)[0m 
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0113 | Steps: 2 | Val loss: 1.6571 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=174613)[0m top1: 0.11194029850746269
[2m[36m(func pid=174613)[0m top5: 0.5293843283582089
[2m[36m(func pid=174613)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=174613)[0m f1_macro: 0.10574214141642416
[2m[36m(func pid=174613)[0m f1_weighted: 0.11326138115091358
[2m[36m(func pid=174613)[0m f1_per_class: [0.105, 0.161, 0.217, 0.182, 0.0, 0.084, 0.035, 0.129, 0.09, 0.053]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.384794776119403
[2m[36m(func pid=157245)[0m top5: 0.9328358208955224
[2m[36m(func pid=157245)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=157245)[0m f1_macro: 0.3786054332334286
[2m[36m(func pid=157245)[0m f1_weighted: 0.3719962576396855
[2m[36m(func pid=157245)[0m f1_per_class: [0.545, 0.496, 0.667, 0.512, 0.206, 0.214, 0.24, 0.331, 0.248, 0.327]
[2m[36m(func pid=157619)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.9342 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8389 | Steps: 2 | Val loss: 2.3517 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=157619)[0m top1: 0.41091417910447764
[2m[36m(func pid=157619)[0m top5: 0.9575559701492538
[2m[36m(func pid=157619)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=157619)[0m f1_macro: 0.4162298275335822
[2m[36m(func pid=157619)[0m f1_weighted: 0.4228763301744163
[2m[36m(func pid=157619)[0m f1_per_class: [0.552, 0.47, 0.769, 0.558, 0.255, 0.306, 0.348, 0.303, 0.243, 0.358]
[2m[36m(func pid=174613)[0m top1: 0.11240671641791045
[2m[36m(func pid=174613)[0m top5: 0.5335820895522388
[2m[36m(func pid=174613)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=174613)[0m f1_macro: 0.10458682559371742
[2m[36m(func pid=174613)[0m f1_weighted: 0.11475244433410017
[2m[36m(func pid=174613)[0m f1_per_class: [0.104, 0.164, 0.202, 0.182, 0.0, 0.08, 0.041, 0.131, 0.087, 0.055]
== Status ==
Current time: 2024-01-07 03:28:51 (running for 00:09:26.55)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.007 |      0.379 |                   98 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.418 |                   99 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.855 |      0.106 |                   23 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 03:28:58 (running for 00:09:33.57)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.007 |      0.379 |                   98 |
| train_2d480_00001 | RUNNING    | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.418 |                   99 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.839 |      0.105 |                   24 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180330)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=180330)[0m Configuration completed!
[2m[36m(func pid=180330)[0m New optimizer parameters:
[2m[36m(func pid=180330)[0m SGD (
[2m[36m(func pid=180330)[0m Parameter Group 0
[2m[36m(func pid=180330)[0m     dampening: 0
[2m[36m(func pid=180330)[0m     differentiable: False
[2m[36m(func pid=180330)[0m     foreach: None
[2m[36m(func pid=180330)[0m     lr: 0.001
[2m[36m(func pid=180330)[0m     maximize: False
[2m[36m(func pid=180330)[0m     momentum: 0.9
[2m[36m(func pid=180330)[0m     nesterov: False
[2m[36m(func pid=180330)[0m     weight_decay: 0
[2m[36m(func pid=180330)[0m )
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=157245)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.7906 | Steps: 2 | Val loss: 2.3405 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=157245)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0080 | Steps: 2 | Val loss: 1.6660 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1293 | Steps: 2 | Val loss: 2.4899 | Batch size: 32 | lr: 0.001 | Duration: 4.74s
[2m[36m(func pid=174613)[0m top1: 0.11520522388059702
[2m[36m(func pid=174613)[0m top5: 0.5438432835820896
[2m[36m(func pid=174613)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=174613)[0m f1_macro: 0.10559881738169725
[2m[36m(func pid=174613)[0m f1_weighted: 0.11979679519885655
[2m[36m(func pid=174613)[0m f1_per_class: [0.102, 0.165, 0.2, 0.188, 0.0, 0.081, 0.052, 0.128, 0.085, 0.055]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=157245)[0m top1: 0.3843283582089552
[2m[36m(func pid=157245)[0m top5: 0.9342350746268657
[2m[36m(func pid=157245)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=157245)[0m f1_macro: 0.3805224795528684
[2m[36m(func pid=157245)[0m f1_weighted: 0.36938737547877976
[2m[36m(func pid=157245)[0m f1_per_class: [0.559, 0.507, 0.667, 0.509, 0.211, 0.22, 0.224, 0.333, 0.246, 0.33]
[2m[36m(func pid=180330)[0m top1: 0.06996268656716417
[2m[36m(func pid=180330)[0m top5: 0.4869402985074627
[2m[36m(func pid=180330)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=180330)[0m f1_macro: 0.04560543780399535
[2m[36m(func pid=180330)[0m f1_weighted: 0.0442544719145302
[2m[36m(func pid=180330)[0m f1_per_class: [0.161, 0.01, 0.0, 0.108, 0.0, 0.018, 0.0, 0.104, 0.021, 0.034]
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.7715 | Steps: 2 | Val loss: 2.3275 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 03:29:04 (running for 00:09:38.92)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00000 | RUNNING    | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.011 |      0.379 |                   99 |
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.791 |      0.106 |                   25 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |        |            |                      |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180846)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=180846)[0m Configuration completed!
[2m[36m(func pid=180846)[0m New optimizer parameters:
[2m[36m(func pid=180846)[0m SGD (
[2m[36m(func pid=180846)[0m Parameter Group 0
[2m[36m(func pid=180846)[0m     dampening: 0
[2m[36m(func pid=180846)[0m     differentiable: False
[2m[36m(func pid=180846)[0m     foreach: None
[2m[36m(func pid=180846)[0m     lr: 0.01
[2m[36m(func pid=180846)[0m     maximize: False
[2m[36m(func pid=180846)[0m     momentum: 0.9
[2m[36m(func pid=180846)[0m     nesterov: False
[2m[36m(func pid=180846)[0m     weight_decay: 0
[2m[36m(func pid=180846)[0m )
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.11986940298507463
[2m[36m(func pid=174613)[0m top5: 0.5513059701492538
[2m[36m(func pid=174613)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=174613)[0m f1_macro: 0.10967735415361468
[2m[36m(func pid=174613)[0m f1_weighted: 0.12630085253723952
[2m[36m(func pid=174613)[0m f1_per_class: [0.109, 0.171, 0.207, 0.19, 0.0, 0.08, 0.068, 0.122, 0.089, 0.059]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9376 | Steps: 2 | Val loss: 2.4511 | Batch size: 32 | lr: 0.001 | Duration: 3.31s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.6593 | Steps: 2 | Val loss: 2.3222 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0733 | Steps: 2 | Val loss: 2.3371 | Batch size: 32 | lr: 0.01 | Duration: 4.75s
[2m[36m(func pid=180330)[0m top1: 0.07742537313432836
[2m[36m(func pid=180330)[0m top5: 0.48740671641791045
[2m[36m(func pid=180330)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=180330)[0m f1_macro: 0.057480166041196525
[2m[36m(func pid=180330)[0m f1_weighted: 0.05942202910473328
[2m[36m(func pid=180330)[0m f1_per_class: [0.153, 0.059, 0.0, 0.126, 0.0, 0.026, 0.0, 0.104, 0.04, 0.068]
[2m[36m(func pid=174613)[0m top1: 0.11986940298507463
[2m[36m(func pid=174613)[0m top5: 0.5597014925373134
[2m[36m(func pid=174613)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=174613)[0m f1_macro: 0.11010633982621931
[2m[36m(func pid=174613)[0m f1_weighted: 0.12433454099415259
[2m[36m(func pid=174613)[0m f1_per_class: [0.119, 0.172, 0.187, 0.176, 0.0, 0.084, 0.071, 0.129, 0.099, 0.065]
[2m[36m(func pid=180846)[0m top1: 0.0914179104477612
[2m[36m(func pid=180846)[0m top5: 0.5293843283582089
[2m[36m(func pid=180846)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=180846)[0m f1_macro: 0.08582815541471975
[2m[36m(func pid=180846)[0m f1_weighted: 0.09358671877457708
[2m[36m(func pid=180846)[0m f1_per_class: [0.109, 0.112, 0.113, 0.177, 0.026, 0.045, 0.021, 0.113, 0.107, 0.036]
== Status ==
Current time: 2024-01-07 03:29:09 (running for 00:09:43.96)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.771 |      0.11  |                   26 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  3.129 |      0.046 |                    1 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 03:29:16 (running for 00:09:51.01)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.659 |      0.11  |                   27 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  3.129 |      0.046 |                    1 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |        |            |                      |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=181391)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=181391)[0m Configuration completed!
[2m[36m(func pid=181391)[0m New optimizer parameters:
[2m[36m(func pid=181391)[0m SGD (
[2m[36m(func pid=181391)[0m Parameter Group 0
[2m[36m(func pid=181391)[0m     dampening: 0
[2m[36m(func pid=181391)[0m     differentiable: False
[2m[36m(func pid=181391)[0m     foreach: None
[2m[36m(func pid=181391)[0m     lr: 0.1
[2m[36m(func pid=181391)[0m     maximize: False
[2m[36m(func pid=181391)[0m     momentum: 0.9
[2m[36m(func pid=181391)[0m     nesterov: False
[2m[36m(func pid=181391)[0m     weight_decay: 0
[2m[36m(func pid=181391)[0m )
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6403 | Steps: 2 | Val loss: 2.3158 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6943 | Steps: 2 | Val loss: 2.4050 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9607 | Steps: 2 | Val loss: 2.2027 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.4333 | Steps: 2 | Val loss: 14.0241 | Batch size: 32 | lr: 0.1 | Duration: 4.49s
== Status ==
Current time: 2024-01-07 03:29:21 (running for 00:09:56.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.659 |      0.11  |                   27 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  2.938 |      0.057 |                    2 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  3.073 |      0.086 |                    1 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |        |            |                      |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.12220149253731344
[2m[36m(func pid=174613)[0m top5: 0.5625
[2m[36m(func pid=174613)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=174613)[0m f1_macro: 0.1126530377609714
[2m[36m(func pid=174613)[0m f1_weighted: 0.129687081505649
[2m[36m(func pid=174613)[0m f1_per_class: [0.119, 0.173, 0.198, 0.176, 0.0, 0.079, 0.09, 0.126, 0.1, 0.066]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m top1: 0.08395522388059702
[2m[36m(func pid=180330)[0m top5: 0.490205223880597
[2m[36m(func pid=180330)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=180330)[0m f1_macro: 0.07501309231149525
[2m[36m(func pid=180330)[0m f1_weighted: 0.07851614798092738
[2m[36m(func pid=180330)[0m f1_per_class: [0.121, 0.105, 0.08, 0.153, 0.0, 0.026, 0.006, 0.102, 0.096, 0.06]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.14272388059701493
[2m[36m(func pid=180846)[0m top5: 0.707089552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=180846)[0m f1_macro: 0.12941011087176002
[2m[36m(func pid=180846)[0m f1_weighted: 0.16416048209176845
[2m[36m(func pid=180846)[0m f1_per_class: [0.146, 0.138, 0.102, 0.275, 0.045, 0.09, 0.133, 0.069, 0.123, 0.174]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.021921641791044777
[2m[36m(func pid=181391)[0m top5: 0.4300373134328358
[2m[36m(func pid=181391)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=181391)[0m f1_macro: 0.008180325833551752
[2m[36m(func pid=181391)[0m f1_weighted: 0.0020200783545543583
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6375 | Steps: 2 | Val loss: 2.3093 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8444 | Steps: 2 | Val loss: 2.0159 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3148 | Steps: 2 | Val loss: 2.3544 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 4.2318 | Steps: 2 | Val loss: 105.1003 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 03:29:26 (running for 00:10:01.69)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.637 |      0.114 |                   29 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  2.694 |      0.075 |                    3 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  1.961 |      0.129 |                    2 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  4.433 |      0.008 |                    1 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.12639925373134328
[2m[36m(func pid=174613)[0m top5: 0.5666977611940298
[2m[36m(func pid=174613)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=174613)[0m f1_macro: 0.11375517767372131
[2m[36m(func pid=174613)[0m f1_weighted: 0.13376193897860825
[2m[36m(func pid=174613)[0m f1_per_class: [0.124, 0.18, 0.176, 0.181, 0.0, 0.083, 0.092, 0.135, 0.1, 0.066]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m top1: 0.09421641791044776
[2m[36m(func pid=180330)[0m top5: 0.511660447761194
[2m[36m(func pid=180330)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=180330)[0m f1_macro: 0.09790159025495318
[2m[36m(func pid=180330)[0m f1_weighted: 0.0986747643108803
[2m[36m(func pid=180330)[0m f1_per_class: [0.116, 0.122, 0.208, 0.178, 0.0, 0.049, 0.027, 0.106, 0.12, 0.053]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.2453358208955224
[2m[36m(func pid=180846)[0m top5: 0.8199626865671642
[2m[36m(func pid=180846)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=180846)[0m f1_macro: 0.1862830646361513
[2m[36m(func pid=180846)[0m f1_weighted: 0.2395431482034488
[2m[36m(func pid=180846)[0m f1_per_class: [0.297, 0.365, 0.123, 0.35, 0.074, 0.101, 0.159, 0.111, 0.14, 0.143]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.17210820895522388
[2m[36m(func pid=181391)[0m top5: 0.5121268656716418
[2m[36m(func pid=181391)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=181391)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=181391)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6837 | Steps: 2 | Val loss: 2.2979 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.2926 | Steps: 2 | Val loss: 1.9160 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.0717 | Steps: 2 | Val loss: 2.3038 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 5.2736 | Steps: 2 | Val loss: 19024.8711 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 03:29:32 (running for 00:10:06.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.684 |      0.119 |                   30 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  2.315 |      0.098 |                    4 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.844 |      0.186 |                    3 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  4.232 |      0.029 |                    2 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.13199626865671643
[2m[36m(func pid=174613)[0m top5: 0.5792910447761194
[2m[36m(func pid=174613)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=174613)[0m f1_macro: 0.11920428703471013
[2m[36m(func pid=174613)[0m f1_weighted: 0.14173116083841536
[2m[36m(func pid=174613)[0m f1_per_class: [0.134, 0.176, 0.194, 0.194, 0.0, 0.084, 0.107, 0.137, 0.098, 0.07]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m top1: 0.2798507462686567
[2m[36m(func pid=180846)[0m top5: 0.8488805970149254
[2m[36m(func pid=180846)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=180846)[0m f1_macro: 0.2332330135035881
[2m[36m(func pid=180846)[0m f1_weighted: 0.25458633033947903
[2m[36m(func pid=180846)[0m f1_per_class: [0.372, 0.428, 0.176, 0.342, 0.103, 0.049, 0.156, 0.268, 0.162, 0.278]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.12173507462686567
[2m[36m(func pid=180330)[0m top5: 0.5764925373134329
[2m[36m(func pid=180330)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=180330)[0m f1_macro: 0.11335729965111345
[2m[36m(func pid=180330)[0m f1_weighted: 0.13492760756522973
[2m[36m(func pid=180330)[0m f1_per_class: [0.114, 0.126, 0.182, 0.23, 0.014, 0.065, 0.086, 0.134, 0.116, 0.067]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.01166044776119403
[2m[36m(func pid=181391)[0m top5: 0.5186567164179104
[2m[36m(func pid=181391)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=181391)[0m f1_macro: 0.0023073373327180437
[2m[36m(func pid=181391)[0m f1_weighted: 0.0002690458643561152
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.6657 | Steps: 2 | Val loss: 2.2887 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.1438 | Steps: 2 | Val loss: 1.8649 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.6780 | Steps: 2 | Val loss: 2.2422 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 7.3204 | Steps: 2 | Val loss: 859225600.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:29:37 (running for 00:10:11.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.666 |      0.121 |                   31 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  2.072 |      0.113 |                    5 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.293 |      0.233 |                    4 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  5.274 |      0.002 |                    3 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.134794776119403
[2m[36m(func pid=174613)[0m top5: 0.5867537313432836
[2m[36m(func pid=174613)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=174613)[0m f1_macro: 0.12098655753337055
[2m[36m(func pid=174613)[0m f1_weighted: 0.14574886116566002
[2m[36m(func pid=174613)[0m f1_per_class: [0.124, 0.181, 0.198, 0.194, 0.0, 0.084, 0.116, 0.14, 0.101, 0.07]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m top1: 0.2957089552238806
[2m[36m(func pid=180846)[0m top5: 0.8661380597014925
[2m[36m(func pid=180846)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=180846)[0m f1_macro: 0.26498539818832634
[2m[36m(func pid=180846)[0m f1_weighted: 0.2695439117763083
[2m[36m(func pid=180846)[0m f1_per_class: [0.435, 0.439, 0.212, 0.362, 0.127, 0.029, 0.171, 0.298, 0.165, 0.412]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.15205223880597016
[2m[36m(func pid=180330)[0m top5: 0.6375932835820896
[2m[36m(func pid=180330)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=180330)[0m f1_macro: 0.12580466058058432
[2m[36m(func pid=180330)[0m f1_weighted: 0.17236159091184802
[2m[36m(func pid=180330)[0m f1_per_class: [0.134, 0.129, 0.142, 0.259, 0.016, 0.091, 0.175, 0.117, 0.114, 0.081]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.01166044776119403
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=181391)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=181391)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4891 | Steps: 2 | Val loss: 2.2786 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.0697 | Steps: 2 | Val loss: 1.8477 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.3642 | Steps: 2 | Val loss: 2.1790 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 26.9305 | Steps: 2 | Val loss: 903766592.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 03:29:42 (running for 00:10:17.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.489 |      0.126 |                   32 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  1.678 |      0.126 |                    6 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.144 |      0.265 |                    5 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  7.32  |      0.002 |                    4 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.1394589552238806
[2m[36m(func pid=174613)[0m top5: 0.6002798507462687
[2m[36m(func pid=174613)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=174613)[0m f1_macro: 0.12613413669475165
[2m[36m(func pid=174613)[0m f1_weighted: 0.1512990778178358
[2m[36m(func pid=174613)[0m f1_per_class: [0.132, 0.178, 0.22, 0.193, 0.0, 0.089, 0.136, 0.137, 0.103, 0.074]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m top1: 0.30923507462686567
[2m[36m(func pid=180846)[0m top5: 0.8782649253731343
[2m[36m(func pid=180846)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=180846)[0m f1_macro: 0.28277071335147275
[2m[36m(func pid=180846)[0m f1_weighted: 0.27948585913885815
[2m[36m(func pid=180846)[0m f1_per_class: [0.485, 0.44, 0.27, 0.39, 0.14, 0.085, 0.144, 0.326, 0.203, 0.345]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.1875
[2m[36m(func pid=180330)[0m top5: 0.7038246268656716
[2m[36m(func pid=180330)[0m f1_micro: 0.1875
[2m[36m(func pid=180330)[0m f1_macro: 0.1436005288573754
[2m[36m(func pid=180330)[0m f1_weighted: 0.20907413009742964
[2m[36m(func pid=180330)[0m f1_per_class: [0.14, 0.136, 0.158, 0.294, 0.022, 0.099, 0.258, 0.108, 0.121, 0.1]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.1142723880597015
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=181391)[0m f1_macro: 0.020510673922143157
[2m[36m(func pid=181391)[0m f1_weighted: 0.023438036897971425
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5805 | Steps: 2 | Val loss: 2.2693 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.0559 | Steps: 2 | Val loss: 1.8558 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.1868 | Steps: 2 | Val loss: 2.1226 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 9.4943 | Steps: 2 | Val loss: 163900352.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 03:29:47 (running for 00:10:22.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.58  |      0.13  |                   33 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  1.364 |      0.144 |                    7 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.07  |      0.283 |                    6 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 26.93  |      0.021 |                    5 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.14412313432835822
[2m[36m(func pid=174613)[0m top5: 0.605410447761194
[2m[36m(func pid=174613)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=174613)[0m f1_macro: 0.13018817176914643
[2m[36m(func pid=174613)[0m f1_weighted: 0.15639635949871047
[2m[36m(func pid=174613)[0m f1_per_class: [0.129, 0.187, 0.233, 0.205, 0.0, 0.085, 0.136, 0.14, 0.107, 0.079]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m top1: 0.310634328358209
[2m[36m(func pid=180846)[0m top5: 0.8871268656716418
[2m[36m(func pid=180846)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=180846)[0m f1_macro: 0.2899561639862634
[2m[36m(func pid=180846)[0m f1_weighted: 0.28281563287558736
[2m[36m(func pid=180846)[0m f1_per_class: [0.471, 0.416, 0.32, 0.421, 0.175, 0.116, 0.129, 0.319, 0.206, 0.327]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.21735074626865672
[2m[36m(func pid=180330)[0m top5: 0.7518656716417911
[2m[36m(func pid=180330)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=180330)[0m f1_macro: 0.16282407004068275
[2m[36m(func pid=180330)[0m f1_weighted: 0.23903965452018297
[2m[36m(func pid=180330)[0m f1_per_class: [0.139, 0.159, 0.192, 0.306, 0.03, 0.128, 0.322, 0.105, 0.112, 0.135]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.020988805970149255
[2m[36m(func pid=181391)[0m top5: 0.5237873134328358
[2m[36m(func pid=181391)[0m f1_micro: 0.020988805970149255
[2m[36m(func pid=181391)[0m f1_macro: 0.0059901843745854885
[2m[36m(func pid=181391)[0m f1_weighted: 0.0014692009510743139
[2m[36m(func pid=181391)[0m f1_per_class: [0.041, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.4041 | Steps: 2 | Val loss: 2.2614 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.0364 | Steps: 2 | Val loss: 1.8893 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9698 | Steps: 2 | Val loss: 2.0765 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 10.2286 | Steps: 2 | Val loss: 22710366.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 03:29:52 (running for 00:10:27.34)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.404 |      0.133 |                   34 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  1.187 |      0.163 |                    8 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.056 |      0.29  |                    7 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  9.494 |      0.006 |                    6 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m top1: 0.14878731343283583
[2m[36m(func pid=174613)[0m top5: 0.6156716417910447
[2m[36m(func pid=174613)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=174613)[0m f1_macro: 0.13336745686895563
[2m[36m(func pid=174613)[0m f1_weighted: 0.1617898930258191
[2m[36m(func pid=174613)[0m f1_per_class: [0.132, 0.186, 0.241, 0.217, 0.0, 0.09, 0.141, 0.144, 0.108, 0.075]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3125
[2m[36m(func pid=180846)[0m top5: 0.8936567164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.3125
[2m[36m(func pid=180846)[0m f1_macro: 0.30038979013234457
[2m[36m(func pid=180846)[0m f1_weighted: 0.2841188998198217
[2m[36m(func pid=180846)[0m f1_per_class: [0.454, 0.404, 0.407, 0.446, 0.185, 0.155, 0.1, 0.324, 0.215, 0.315]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.23740671641791045
[2m[36m(func pid=180330)[0m top5: 0.7826492537313433
[2m[36m(func pid=180330)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=180330)[0m f1_macro: 0.17877103414412077
[2m[36m(func pid=180330)[0m f1_weighted: 0.2530052179662934
[2m[36m(func pid=180330)[0m f1_per_class: [0.148, 0.175, 0.276, 0.352, 0.05, 0.127, 0.32, 0.069, 0.126, 0.144]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.007462686567164179
[2m[36m(func pid=181391)[0m top5: 0.5107276119402985
[2m[36m(func pid=181391)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=181391)[0m f1_macro: 0.0014814814814814816
[2m[36m(func pid=181391)[0m f1_weighted: 0.0001105583195135434
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.3765 | Steps: 2 | Val loss: 2.2528 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0125 | Steps: 2 | Val loss: 1.9318 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7314 | Steps: 2 | Val loss: 2.0527 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 10.9229 | Steps: 2 | Val loss: 3484780.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=174613)[0m top1: 0.15205223880597016
[2m[36m(func pid=174613)[0m top5: 0.6259328358208955
[2m[36m(func pid=174613)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=174613)[0m f1_macro: 0.13835940108899442
[2m[36m(func pid=174613)[0m f1_weighted: 0.1657446707593704
[2m[36m(func pid=174613)[0m f1_per_class: [0.133, 0.186, 0.278, 0.231, 0.0, 0.089, 0.141, 0.141, 0.111, 0.074]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:29:58 (running for 00:10:32.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.376 |      0.138 |                   35 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.97  |      0.179 |                    9 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.013 |      0.309 |                    9 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 10.229 |      0.001 |                    7 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.314365671641791
[2m[36m(func pid=180846)[0m top5: 0.8936567164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.3086905339198166
[2m[36m(func pid=180846)[0m f1_weighted: 0.2858269126819497
[2m[36m(func pid=180846)[0m f1_per_class: [0.447, 0.412, 0.462, 0.459, 0.183, 0.183, 0.078, 0.324, 0.206, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.24860074626865672
[2m[36m(func pid=180330)[0m top5: 0.7971082089552238
[2m[36m(func pid=180330)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=180330)[0m f1_macro: 0.1976833767195389
[2m[36m(func pid=180330)[0m f1_weighted: 0.26678293104226924
[2m[36m(func pid=180330)[0m f1_per_class: [0.168, 0.207, 0.308, 0.355, 0.061, 0.131, 0.326, 0.144, 0.125, 0.153]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.020522388059701493
[2m[36m(func pid=181391)[0m top5: 0.5237873134328358
[2m[36m(func pid=181391)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=181391)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=181391)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=181391)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.3656 | Steps: 2 | Val loss: 2.2431 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0110 | Steps: 2 | Val loss: 1.9557 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7096 | Steps: 2 | Val loss: 2.0305 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 9.4034 | Steps: 2 | Val loss: 17426700.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=174613)[0m top1: 0.1553171641791045
[2m[36m(func pid=174613)[0m top5: 0.6324626865671642
[2m[36m(func pid=174613)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=174613)[0m f1_macro: 0.13995497418719133
[2m[36m(func pid=174613)[0m f1_weighted: 0.16942620121575536
[2m[36m(func pid=174613)[0m f1_per_class: [0.143, 0.182, 0.265, 0.23, 0.0, 0.097, 0.152, 0.143, 0.113, 0.074]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:30:03 (running for 00:10:38.12)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.366 |      0.14  |                   36 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.731 |      0.198 |                   10 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.011 |      0.324 |                   10 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 10.923 |      0.004 |                    8 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.322294776119403
[2m[36m(func pid=180846)[0m top5: 0.9015858208955224
[2m[36m(func pid=180846)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.32435171759528236
[2m[36m(func pid=180846)[0m f1_weighted: 0.29734527496063934
[2m[36m(func pid=180846)[0m f1_per_class: [0.447, 0.411, 0.545, 0.484, 0.188, 0.216, 0.078, 0.32, 0.22, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.26026119402985076
[2m[36m(func pid=180330)[0m top5: 0.8064365671641791
[2m[36m(func pid=180330)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=180330)[0m f1_macro: 0.21744531540433892
[2m[36m(func pid=180330)[0m f1_weighted: 0.27754366949741904
[2m[36m(func pid=180330)[0m f1_per_class: [0.193, 0.243, 0.387, 0.369, 0.074, 0.141, 0.315, 0.171, 0.131, 0.15]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.020522388059701493
[2m[36m(func pid=181391)[0m top5: 0.5237873134328358
[2m[36m(func pid=181391)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=181391)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=181391)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=181391)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3191 | Steps: 2 | Val loss: 2.2312 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0114 | Steps: 2 | Val loss: 1.9708 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5155 | Steps: 2 | Val loss: 2.0116 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 10.4091 | Steps: 2 | Val loss: 8047855.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=174613)[0m top1: 0.16184701492537312
[2m[36m(func pid=174613)[0m top5: 0.6427238805970149
[2m[36m(func pid=174613)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=174613)[0m f1_macro: 0.14727848561232745
[2m[36m(func pid=174613)[0m f1_weighted: 0.1753957639378504
[2m[36m(func pid=174613)[0m f1_per_class: [0.144, 0.192, 0.29, 0.24, 0.011, 0.1, 0.154, 0.146, 0.113, 0.082]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:30:08 (running for 00:10:43.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.319 |      0.147 |                   37 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.71  |      0.217 |                   11 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.011 |      0.336 |                   11 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  9.403 |      0.004 |                    9 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3292910447761194
[2m[36m(func pid=180846)[0m top5: 0.902518656716418
[2m[36m(func pid=180846)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=180846)[0m f1_macro: 0.3360932210231706
[2m[36m(func pid=180846)[0m f1_weighted: 0.3055692955741892
[2m[36m(func pid=180846)[0m f1_per_class: [0.459, 0.405, 0.571, 0.497, 0.21, 0.243, 0.084, 0.326, 0.219, 0.347]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.2593283582089552
[2m[36m(func pid=180330)[0m top5: 0.8125
[2m[36m(func pid=180330)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=180330)[0m f1_macro: 0.22550833330846087
[2m[36m(func pid=180330)[0m f1_weighted: 0.2748160284113034
[2m[36m(func pid=180330)[0m f1_per_class: [0.22, 0.27, 0.407, 0.368, 0.086, 0.138, 0.282, 0.203, 0.145, 0.135]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.006063432835820896
[2m[36m(func pid=181391)[0m top5: 0.5093283582089553
[2m[36m(func pid=181391)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=181391)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=181391)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.2399 | Steps: 2 | Val loss: 2.2223 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0086 | Steps: 2 | Val loss: 1.9306 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4419 | Steps: 2 | Val loss: 2.0055 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 13.8505 | Steps: 2 | Val loss: 182362.6562 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=174613)[0m top1: 0.16324626865671643
[2m[36m(func pid=174613)[0m top5: 0.6511194029850746
[2m[36m(func pid=174613)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=174613)[0m f1_macro: 0.14969527886445014
[2m[36m(func pid=174613)[0m f1_weighted: 0.17795391364710866
[2m[36m(func pid=174613)[0m f1_per_class: [0.144, 0.186, 0.308, 0.243, 0.011, 0.099, 0.163, 0.145, 0.118, 0.079]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:30:13 (running for 00:10:48.79)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.24  |      0.15  |                   38 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.515 |      0.226 |                   12 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.009 |      0.348 |                   12 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 10.409 |      0.001 |                   10 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.345615671641791
[2m[36m(func pid=180846)[0m top5: 0.9071828358208955
[2m[36m(func pid=180846)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.3476444177192313
[2m[36m(func pid=180846)[0m f1_weighted: 0.33080462126874555
[2m[36m(func pid=180846)[0m f1_per_class: [0.452, 0.434, 0.6, 0.503, 0.19, 0.278, 0.132, 0.335, 0.222, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.25419776119402987
[2m[36m(func pid=180330)[0m top5: 0.8120335820895522
[2m[36m(func pid=180330)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=180330)[0m f1_macro: 0.23226196651048822
[2m[36m(func pid=180330)[0m f1_weighted: 0.2684741215838826
[2m[36m(func pid=180330)[0m f1_per_class: [0.251, 0.297, 0.449, 0.366, 0.057, 0.119, 0.244, 0.238, 0.164, 0.139]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.17210820895522388
[2m[36m(func pid=181391)[0m top5: 0.5727611940298507
[2m[36m(func pid=181391)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=181391)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=181391)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2552 | Steps: 2 | Val loss: 2.2148 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0057 | Steps: 2 | Val loss: 1.9083 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3405 | Steps: 2 | Val loss: 1.9904 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 30.4568 | Steps: 2 | Val loss: 447.3266 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=174613)[0m top1: 0.16791044776119404
[2m[36m(func pid=174613)[0m top5: 0.6571828358208955
[2m[36m(func pid=174613)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=174613)[0m f1_macro: 0.15014390546488346
[2m[36m(func pid=174613)[0m f1_weighted: 0.18327790896891505
[2m[36m(func pid=174613)[0m f1_per_class: [0.143, 0.186, 0.294, 0.261, 0.012, 0.096, 0.166, 0.147, 0.118, 0.079]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:30:19 (running for 00:10:53.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.255 |      0.15  |                   39 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.442 |      0.232 |                   13 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.006 |      0.356 |                   13 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 13.851 |      0.029 |                   11 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3558768656716418
[2m[36m(func pid=180846)[0m top5: 0.9104477611940298
[2m[36m(func pid=180846)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=180846)[0m f1_macro: 0.3557848574936287
[2m[36m(func pid=180846)[0m f1_weighted: 0.35162105638604996
[2m[36m(func pid=180846)[0m f1_per_class: [0.455, 0.432, 0.615, 0.509, 0.19, 0.282, 0.195, 0.335, 0.221, 0.324]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.2579291044776119
[2m[36m(func pid=180330)[0m top5: 0.8166977611940298
[2m[36m(func pid=180330)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=180330)[0m f1_macro: 0.2430313861487873
[2m[36m(func pid=180330)[0m f1_weighted: 0.26692940693449624
[2m[36m(func pid=180330)[0m f1_per_class: [0.298, 0.307, 0.478, 0.364, 0.054, 0.116, 0.224, 0.278, 0.167, 0.145]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.17210820895522388
[2m[36m(func pid=181391)[0m top5: 0.5722947761194029
[2m[36m(func pid=181391)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=181391)[0m f1_macro: 0.029378980891719746
[2m[36m(func pid=181391)[0m f1_weighted: 0.05056363782203632
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.3483 | Steps: 2 | Val loss: 2.2082 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0060 | Steps: 2 | Val loss: 1.9044 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2833 | Steps: 2 | Val loss: 1.9788 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 25.3509 | Steps: 2 | Val loss: 1238.8226 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=174613)[0m top1: 0.16791044776119404
[2m[36m(func pid=174613)[0m top5: 0.6660447761194029
[2m[36m(func pid=174613)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=174613)[0m f1_macro: 0.14947141795842542
[2m[36m(func pid=174613)[0m f1_weighted: 0.18201828170650833
[2m[36m(func pid=174613)[0m f1_per_class: [0.146, 0.192, 0.286, 0.258, 0.012, 0.099, 0.16, 0.147, 0.115, 0.08]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:30:24 (running for 00:10:59.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.348 |      0.149 |                   40 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.341 |      0.243 |                   14 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.006 |      0.358 |                   14 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 30.457 |      0.029 |                   12 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.35867537313432835
[2m[36m(func pid=180846)[0m top5: 0.9118470149253731
[2m[36m(func pid=180846)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=180846)[0m f1_macro: 0.3581156740447823
[2m[36m(func pid=180846)[0m f1_weighted: 0.35583619420870216
[2m[36m(func pid=180846)[0m f1_per_class: [0.475, 0.428, 0.615, 0.523, 0.187, 0.29, 0.198, 0.319, 0.223, 0.324]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.2621268656716418
[2m[36m(func pid=180330)[0m top5: 0.8222947761194029
[2m[36m(func pid=180330)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=180330)[0m f1_macro: 0.25186075020852167
[2m[36m(func pid=180330)[0m f1_weighted: 0.26963254993059704
[2m[36m(func pid=180330)[0m f1_per_class: [0.303, 0.33, 0.512, 0.361, 0.051, 0.127, 0.214, 0.275, 0.189, 0.157]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.012126865671641791
[2m[36m(func pid=181391)[0m top5: 0.4766791044776119
[2m[36m(func pid=181391)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=181391)[0m f1_macro: 0.0044606291904707674
[2m[36m(func pid=181391)[0m f1_weighted: 0.0006831076566477989
[2m[36m(func pid=181391)[0m f1_per_class: [0.027, 0.0, 0.0, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.1666 | Steps: 2 | Val loss: 2.2024 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0032 | Steps: 2 | Val loss: 1.8856 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2496 | Steps: 2 | Val loss: 1.9723 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 19.1015 | Steps: 2 | Val loss: 6432.9541 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=174613)[0m top1: 0.1707089552238806
[2m[36m(func pid=174613)[0m top5: 0.6702425373134329
[2m[36m(func pid=174613)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=174613)[0m f1_macro: 0.15128795765305406
[2m[36m(func pid=174613)[0m f1_weighted: 0.18565120975016422
[2m[36m(func pid=174613)[0m f1_per_class: [0.153, 0.194, 0.274, 0.254, 0.012, 0.1, 0.173, 0.147, 0.116, 0.089]
[2m[36m(func pid=174613)[0m 
== Status ==
Current time: 2024-01-07 03:30:29 (running for 00:11:04.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.167 |      0.151 |                   41 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.283 |      0.252 |                   15 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.003 |      0.365 |                   15 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 25.351 |      0.004 |                   13 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.36240671641791045
[2m[36m(func pid=180846)[0m top5: 0.9146455223880597
[2m[36m(func pid=180846)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=180846)[0m f1_macro: 0.36462697754762674
[2m[36m(func pid=180846)[0m f1_weighted: 0.3622220823692661
[2m[36m(func pid=180846)[0m f1_per_class: [0.482, 0.43, 0.649, 0.521, 0.184, 0.288, 0.218, 0.327, 0.221, 0.327]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.26259328358208955
[2m[36m(func pid=180330)[0m top5: 0.8236940298507462
[2m[36m(func pid=180330)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=180330)[0m f1_macro: 0.25481494668459787
[2m[36m(func pid=180330)[0m f1_weighted: 0.26824707343325194
[2m[36m(func pid=180330)[0m f1_per_class: [0.316, 0.342, 0.524, 0.349, 0.056, 0.116, 0.216, 0.284, 0.179, 0.166]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.007462686567164179
[2m[36m(func pid=181391)[0m top5: 0.5107276119402985
[2m[36m(func pid=181391)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=181391)[0m f1_macro: 0.0014814814814814816
[2m[36m(func pid=181391)[0m f1_weighted: 0.0001105583195135434
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2789 | Steps: 2 | Val loss: 2.1963 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0035 | Steps: 2 | Val loss: 1.8643 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=174613)[0m top1: 0.17024253731343283
[2m[36m(func pid=174613)[0m top5: 0.6772388059701493
[2m[36m(func pid=174613)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=174613)[0m f1_macro: 0.1540432099584464
[2m[36m(func pid=174613)[0m f1_weighted: 0.18640363767873955
[2m[36m(func pid=174613)[0m f1_per_class: [0.155, 0.179, 0.303, 0.258, 0.011, 0.105, 0.179, 0.14, 0.123, 0.086]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1937 | Steps: 2 | Val loss: 1.9459 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 8.9705 | Steps: 2 | Val loss: 71495.8125 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:30:34 (running for 00:11:09.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.279 |      0.154 |                   42 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.25  |      0.255 |                   16 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.004 |      0.369 |                   16 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 19.101 |      0.001 |                   14 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3712686567164179
[2m[36m(func pid=180846)[0m top5: 0.9169776119402985
[2m[36m(func pid=180846)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=180846)[0m f1_macro: 0.3685935135060279
[2m[36m(func pid=180846)[0m f1_weighted: 0.3786471063431917
[2m[36m(func pid=180846)[0m f1_per_class: [0.493, 0.464, 0.632, 0.511, 0.163, 0.284, 0.265, 0.319, 0.225, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.2728544776119403
[2m[36m(func pid=180330)[0m top5: 0.8339552238805971
[2m[36m(func pid=180330)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=180330)[0m f1_macro: 0.2675544211632045
[2m[36m(func pid=180330)[0m f1_weighted: 0.27669815664408104
[2m[36m(func pid=180330)[0m f1_per_class: [0.338, 0.35, 0.55, 0.356, 0.054, 0.125, 0.223, 0.292, 0.205, 0.183]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.03311567164179104
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=181391)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=181391)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.1939 | Steps: 2 | Val loss: 2.1949 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0026 | Steps: 2 | Val loss: 1.8561 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=174613)[0m top1: 0.16977611940298507
[2m[36m(func pid=174613)[0m top5: 0.679570895522388
[2m[36m(func pid=174613)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=174613)[0m f1_macro: 0.15516254034045876
[2m[36m(func pid=174613)[0m f1_weighted: 0.18549523105831442
[2m[36m(func pid=174613)[0m f1_per_class: [0.152, 0.182, 0.303, 0.247, 0.023, 0.109, 0.182, 0.141, 0.123, 0.089]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 12.8941 | Steps: 2 | Val loss: 60678.7969 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2120 | Steps: 2 | Val loss: 1.9296 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 03:30:40 (running for 00:11:15.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.194 |      0.155 |                   43 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.194 |      0.268 |                   17 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.003 |      0.37  |                   17 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  8.97  |      0.006 |                   15 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3736007462686567
[2m[36m(func pid=180846)[0m top5: 0.9174440298507462
[2m[36m(func pid=180846)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=180846)[0m f1_macro: 0.36966411960545686
[2m[36m(func pid=180846)[0m f1_weighted: 0.3834483849181658
[2m[36m(func pid=180846)[0m f1_per_class: [0.496, 0.458, 0.632, 0.51, 0.159, 0.294, 0.281, 0.32, 0.23, 0.316]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.03311567164179104
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=181391)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=181391)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.2789179104477612
[2m[36m(func pid=180330)[0m top5: 0.8386194029850746
[2m[36m(func pid=180330)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=180330)[0m f1_macro: 0.2722046784002524
[2m[36m(func pid=180330)[0m f1_weighted: 0.28179214231370053
[2m[36m(func pid=180330)[0m f1_per_class: [0.351, 0.356, 0.55, 0.37, 0.069, 0.113, 0.227, 0.286, 0.213, 0.187]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1596 | Steps: 2 | Val loss: 2.1936 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0024 | Steps: 2 | Val loss: 1.8358 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=174613)[0m top1: 0.17117537313432835
[2m[36m(func pid=174613)[0m top5: 0.6786380597014925
[2m[36m(func pid=174613)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=174613)[0m f1_macro: 0.15898271145119916
[2m[36m(func pid=174613)[0m f1_weighted: 0.18760689023110524
[2m[36m(func pid=174613)[0m f1_per_class: [0.155, 0.183, 0.333, 0.254, 0.022, 0.102, 0.183, 0.145, 0.123, 0.089]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 12.9042 | Steps: 2 | Val loss: 86868.3203 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2284 | Steps: 2 | Val loss: 1.9102 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:30:45 (running for 00:11:20.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.16  |      0.159 |                   44 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.212 |      0.272 |                   18 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.374 |                   18 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 12.894 |      0.006 |                   16 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.38013059701492535
[2m[36m(func pid=180846)[0m top5: 0.9183768656716418
[2m[36m(func pid=180846)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=180846)[0m f1_macro: 0.3736005378767654
[2m[36m(func pid=180846)[0m f1_weighted: 0.39404623222766144
[2m[36m(func pid=180846)[0m f1_per_class: [0.507, 0.464, 0.632, 0.505, 0.156, 0.278, 0.323, 0.322, 0.236, 0.313]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.1142723880597015
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=181391)[0m f1_macro: 0.020510673922143157
[2m[36m(func pid=181391)[0m f1_weighted: 0.023438036897971425
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1191 | Steps: 2 | Val loss: 2.1875 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=180330)[0m top1: 0.28638059701492535
[2m[36m(func pid=180330)[0m top5: 0.8488805970149254
[2m[36m(func pid=180330)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=180330)[0m f1_macro: 0.28109173552563405
[2m[36m(func pid=180330)[0m f1_weighted: 0.2920065120274336
[2m[36m(func pid=180330)[0m f1_per_class: [0.358, 0.356, 0.595, 0.378, 0.08, 0.124, 0.249, 0.278, 0.217, 0.176]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0019 | Steps: 2 | Val loss: 1.8247 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=174613)[0m top1: 0.17024253731343283
[2m[36m(func pid=174613)[0m top5: 0.6856343283582089
[2m[36m(func pid=174613)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=174613)[0m f1_macro: 0.15808747088883898
[2m[36m(func pid=174613)[0m f1_weighted: 0.18695324452901818
[2m[36m(func pid=174613)[0m f1_per_class: [0.159, 0.186, 0.333, 0.253, 0.02, 0.102, 0.182, 0.143, 0.111, 0.091]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 8.8942 | Steps: 2 | Val loss: 116005.7344 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1479 | Steps: 2 | Val loss: 1.9044 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:30:50 (running for 00:11:25.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.119 |      0.158 |                   45 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.228 |      0.281 |                   19 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.378 |                   19 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 12.904 |      0.021 |                   17 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.384794776119403
[2m[36m(func pid=180846)[0m top5: 0.917910447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.3775030081719901
[2m[36m(func pid=180846)[0m f1_weighted: 0.3993557005732711
[2m[36m(func pid=180846)[0m f1_per_class: [0.507, 0.468, 0.632, 0.513, 0.161, 0.282, 0.328, 0.321, 0.244, 0.319]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.007462686567164179
[2m[36m(func pid=181391)[0m top5: 0.5107276119402985
[2m[36m(func pid=181391)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=181391)[0m f1_macro: 0.0014814814814814816
[2m[36m(func pid=181391)[0m f1_weighted: 0.0001105583195135434
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.0600 | Steps: 2 | Val loss: 2.1833 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=180330)[0m top1: 0.28824626865671643
[2m[36m(func pid=180330)[0m top5: 0.8521455223880597
[2m[36m(func pid=180330)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=180330)[0m f1_macro: 0.2816737887441868
[2m[36m(func pid=180330)[0m f1_weighted: 0.2931552806901963
[2m[36m(func pid=180330)[0m f1_per_class: [0.358, 0.356, 0.579, 0.38, 0.077, 0.125, 0.248, 0.288, 0.215, 0.189]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0054 | Steps: 2 | Val loss: 1.8203 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=174613)[0m top1: 0.17350746268656717
[2m[36m(func pid=174613)[0m top5: 0.6875
[2m[36m(func pid=174613)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=174613)[0m f1_macro: 0.16180317578432857
[2m[36m(func pid=174613)[0m f1_weighted: 0.1897009140927526
[2m[36m(func pid=174613)[0m f1_per_class: [0.156, 0.188, 0.345, 0.26, 0.03, 0.103, 0.182, 0.149, 0.111, 0.094]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 20.7902 | Steps: 2 | Val loss: 109274.8047 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1542 | Steps: 2 | Val loss: 1.8934 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 03:30:55 (running for 00:11:30.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.06  |      0.162 |                   46 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.148 |      0.282 |                   20 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.005 |      0.382 |                   20 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  8.894 |      0.001 |                   18 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.38526119402985076
[2m[36m(func pid=180846)[0m top5: 0.9169776119402985
[2m[36m(func pid=180846)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=180846)[0m f1_macro: 0.38171245667103654
[2m[36m(func pid=180846)[0m f1_weighted: 0.4002099539834542
[2m[36m(func pid=180846)[0m f1_per_class: [0.5, 0.469, 0.686, 0.518, 0.165, 0.28, 0.326, 0.32, 0.24, 0.313]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1311 | Steps: 2 | Val loss: 2.1819 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=180330)[0m top1: 0.2873134328358209
[2m[36m(func pid=180330)[0m top5: 0.8558768656716418
[2m[36m(func pid=180330)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=180330)[0m f1_macro: 0.28294277880944046
[2m[36m(func pid=180330)[0m f1_weighted: 0.29531187366395334
[2m[36m(func pid=180330)[0m f1_per_class: [0.385, 0.354, 0.571, 0.375, 0.072, 0.126, 0.262, 0.282, 0.203, 0.2]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.01166044776119403
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=181391)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=181391)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0019 | Steps: 2 | Val loss: 1.8176 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=174613)[0m top1: 0.17630597014925373
[2m[36m(func pid=174613)[0m top5: 0.6884328358208955
[2m[36m(func pid=174613)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=174613)[0m f1_macro: 0.16471160143425698
[2m[36m(func pid=174613)[0m f1_weighted: 0.19226474805715082
[2m[36m(func pid=174613)[0m f1_per_class: [0.155, 0.184, 0.345, 0.266, 0.03, 0.104, 0.184, 0.155, 0.132, 0.093]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1193 | Steps: 2 | Val loss: 1.8862 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 9.0659 | Steps: 2 | Val loss: 28374.7324 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:31:01 (running for 00:11:35.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.131 |      0.165 |                   47 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.154 |      0.283 |                   21 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.383 |                   21 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      | 20.79  |      0.002 |                   19 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.38526119402985076
[2m[36m(func pid=180846)[0m top5: 0.9188432835820896
[2m[36m(func pid=180846)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=180846)[0m f1_macro: 0.38255110075974086
[2m[36m(func pid=180846)[0m f1_weighted: 0.40123526263179443
[2m[36m(func pid=180846)[0m f1_per_class: [0.508, 0.464, 0.686, 0.514, 0.161, 0.278, 0.338, 0.317, 0.236, 0.324]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0569 | Steps: 2 | Val loss: 2.1768 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=180330)[0m top1: 0.2947761194029851
[2m[36m(func pid=180330)[0m top5: 0.8614738805970149
[2m[36m(func pid=180330)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=180330)[0m f1_macro: 0.2896502320816431
[2m[36m(func pid=180330)[0m f1_weighted: 0.30690757158049786
[2m[36m(func pid=180330)[0m f1_per_class: [0.398, 0.36, 0.564, 0.38, 0.072, 0.13, 0.291, 0.275, 0.212, 0.216]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.05783582089552239
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=181391)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=181391)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0016 | Steps: 2 | Val loss: 1.8143 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=174613)[0m top1: 0.17863805970149255
[2m[36m(func pid=174613)[0m top5: 0.6963619402985075
[2m[36m(func pid=174613)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=174613)[0m f1_macro: 0.1675054872127853
[2m[36m(func pid=174613)[0m f1_weighted: 0.19482600537802072
[2m[36m(func pid=174613)[0m f1_per_class: [0.158, 0.187, 0.357, 0.267, 0.041, 0.103, 0.19, 0.153, 0.126, 0.093]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 6.4975 | Steps: 2 | Val loss: 25569.5352 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1284 | Steps: 2 | Val loss: 1.8707 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:31:06 (running for 00:11:41.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.057 |      0.168 |                   48 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.119 |      0.29  |                   22 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.385 |                   22 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  9.066 |      0.011 |                   20 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3871268656716418
[2m[36m(func pid=180846)[0m top5: 0.9183768656716418
[2m[36m(func pid=180846)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=180846)[0m f1_macro: 0.38479504603596804
[2m[36m(func pid=180846)[0m f1_weighted: 0.4026630643659965
[2m[36m(func pid=180846)[0m f1_per_class: [0.52, 0.469, 0.686, 0.512, 0.161, 0.264, 0.344, 0.327, 0.239, 0.327]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9582 | Steps: 2 | Val loss: 2.1703 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=181391)[0m top1: 0.05783582089552239
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=181391)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=181391)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=180330)[0m top1: 0.30223880597014924
[2m[36m(func pid=180330)[0m top5: 0.8661380597014925
[2m[36m(func pid=180330)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=180330)[0m f1_macro: 0.293468318145994
[2m[36m(func pid=180330)[0m f1_weighted: 0.31553506259938646
[2m[36m(func pid=180330)[0m f1_per_class: [0.402, 0.358, 0.564, 0.392, 0.083, 0.144, 0.305, 0.27, 0.207, 0.21]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0018 | Steps: 2 | Val loss: 1.8131 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=174613)[0m top1: 0.1828358208955224
[2m[36m(func pid=174613)[0m top5: 0.7005597014925373
[2m[36m(func pid=174613)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=174613)[0m f1_macro: 0.1734647290378622
[2m[36m(func pid=174613)[0m f1_weighted: 0.1983842755052074
[2m[36m(func pid=174613)[0m f1_per_class: [0.162, 0.198, 0.385, 0.275, 0.04, 0.106, 0.185, 0.156, 0.132, 0.096]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 5.0940 | Steps: 2 | Val loss: 19258.4141 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1190 | Steps: 2 | Val loss: 1.8668 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=180846)[0m top1: 0.38759328358208955
[2m[36m(func pid=180846)[0m top5: 0.917910447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=180846)[0m f1_macro: 0.38031323665430095
[2m[36m(func pid=180846)[0m f1_weighted: 0.40087716534442247
[2m[36m(func pid=180846)[0m f1_per_class: [0.524, 0.468, 0.649, 0.52, 0.16, 0.261, 0.331, 0.335, 0.242, 0.313]
== Status ==
Current time: 2024-01-07 03:31:11 (running for 00:11:46.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.958 |      0.173 |                   49 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.128 |      0.293 |                   23 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.38  |                   23 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  6.498 |      0.011 |                   21 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0401 | Steps: 2 | Val loss: 2.1606 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=180330)[0m top1: 0.3041044776119403
[2m[36m(func pid=180330)[0m top5: 0.867070895522388
[2m[36m(func pid=180330)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=180330)[0m f1_macro: 0.2959000055913071
[2m[36m(func pid=180330)[0m f1_weighted: 0.31742159241336015
[2m[36m(func pid=180330)[0m f1_per_class: [0.398, 0.368, 0.579, 0.392, 0.079, 0.142, 0.306, 0.272, 0.208, 0.216]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.05783582089552239
[2m[36m(func pid=181391)[0m top5: 0.5149253731343284
[2m[36m(func pid=181391)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=181391)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=181391)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8108 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=174613)[0m top1: 0.1865671641791045
[2m[36m(func pid=174613)[0m top5: 0.7098880597014925
[2m[36m(func pid=174613)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=174613)[0m f1_macro: 0.1768752000914854
[2m[36m(func pid=174613)[0m f1_weighted: 0.20164087537638684
[2m[36m(func pid=174613)[0m f1_per_class: [0.168, 0.214, 0.4, 0.278, 0.041, 0.109, 0.183, 0.15, 0.126, 0.099]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0831 | Steps: 2 | Val loss: 1.8645 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 6.0393 | Steps: 2 | Val loss: 7947.4937 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 03:31:16 (running for 00:11:51.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  1.04  |      0.177 |                   50 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.119 |      0.296 |                   24 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.382 |                   24 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  5.094 |      0.011 |                   22 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3885261194029851
[2m[36m(func pid=180846)[0m top5: 0.9193097014925373
[2m[36m(func pid=180846)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=180846)[0m f1_macro: 0.3817222174112146
[2m[36m(func pid=180846)[0m f1_weighted: 0.40353719802077825
[2m[36m(func pid=180846)[0m f1_per_class: [0.524, 0.47, 0.649, 0.518, 0.155, 0.26, 0.342, 0.328, 0.239, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.9728 | Steps: 2 | Val loss: 2.1612 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=180330)[0m top1: 0.30597014925373134
[2m[36m(func pid=180330)[0m top5: 0.8661380597014925
[2m[36m(func pid=180330)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=180330)[0m f1_macro: 0.2968571237543881
[2m[36m(func pid=180330)[0m f1_weighted: 0.3217360155944818
[2m[36m(func pid=180330)[0m f1_per_class: [0.405, 0.362, 0.564, 0.389, 0.078, 0.144, 0.326, 0.267, 0.204, 0.229]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.05783582089552239
[2m[36m(func pid=181391)[0m top5: 0.5289179104477612
[2m[36m(func pid=181391)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=181391)[0m f1_macro: 0.011056620597414177
[2m[36m(func pid=181391)[0m f1_weighted: 0.006394687285817901
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0020 | Steps: 2 | Val loss: 1.8102 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=174613)[0m top1: 0.18423507462686567
[2m[36m(func pid=174613)[0m top5: 0.707089552238806
[2m[36m(func pid=174613)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=174613)[0m f1_macro: 0.17507405037140106
[2m[36m(func pid=174613)[0m f1_weighted: 0.19896966791425058
[2m[36m(func pid=174613)[0m f1_per_class: [0.164, 0.212, 0.392, 0.272, 0.04, 0.109, 0.181, 0.145, 0.133, 0.102]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.0586 | Steps: 2 | Val loss: 2210.0811 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0856 | Steps: 2 | Val loss: 1.8743 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:31:22 (running for 00:11:57.26)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.973 |      0.175 |                   51 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.083 |      0.297 |                   25 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.382 |                   25 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  6.039 |      0.011 |                   23 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3885261194029851
[2m[36m(func pid=180846)[0m top5: 0.9188432835820896
[2m[36m(func pid=180846)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=180846)[0m f1_macro: 0.38202822512273515
[2m[36m(func pid=180846)[0m f1_weighted: 0.4045349818642307
[2m[36m(func pid=180846)[0m f1_per_class: [0.508, 0.471, 0.686, 0.522, 0.148, 0.256, 0.345, 0.312, 0.249, 0.324]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.9766 | Steps: 2 | Val loss: 2.1625 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=180330)[0m top1: 0.2989738805970149
[2m[36m(func pid=180330)[0m top5: 0.8642723880597015
[2m[36m(func pid=180330)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=180330)[0m f1_macro: 0.29179385968810145
[2m[36m(func pid=180330)[0m f1_weighted: 0.3134328133231708
[2m[36m(func pid=180330)[0m f1_per_class: [0.395, 0.363, 0.564, 0.385, 0.08, 0.135, 0.307, 0.262, 0.211, 0.217]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.0648320895522388
[2m[36m(func pid=181391)[0m top5: 0.601679104477612
[2m[36m(func pid=181391)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=181391)[0m f1_macro: 0.025625415434774913
[2m[36m(func pid=181391)[0m f1_weighted: 0.010939480314673069
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.0, 0.085, 0.0, 0.0, 0.0, 0.006, 0.126, 0.04, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m top1: 0.18563432835820895
[2m[36m(func pid=174613)[0m top5: 0.7042910447761194
[2m[36m(func pid=174613)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=174613)[0m f1_macro: 0.17682028723077692
[2m[36m(func pid=174613)[0m f1_weighted: 0.20031646770491462
[2m[36m(func pid=174613)[0m f1_per_class: [0.168, 0.205, 0.4, 0.282, 0.046, 0.107, 0.18, 0.157, 0.124, 0.099]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.8080 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.4979 | Steps: 2 | Val loss: 901.2062 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0923 | Steps: 2 | Val loss: 1.8739 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:31:27 (running for 00:12:02.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.977 |      0.177 |                   52 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.086 |      0.292 |                   26 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.38  |                   26 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  3.059 |      0.026 |                   24 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.38759328358208955
[2m[36m(func pid=180846)[0m top5: 0.9202425373134329
[2m[36m(func pid=180846)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=180846)[0m f1_macro: 0.37972313336726504
[2m[36m(func pid=180846)[0m f1_weighted: 0.3999315403713809
[2m[36m(func pid=180846)[0m f1_per_class: [0.531, 0.473, 0.649, 0.523, 0.152, 0.255, 0.324, 0.329, 0.245, 0.316]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.9668 | Steps: 2 | Val loss: 2.1538 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=181391)[0m top1: 0.27658582089552236
[2m[36m(func pid=181391)[0m top5: 0.6152052238805971
[2m[36m(func pid=181391)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=181391)[0m f1_macro: 0.067783012309307
[2m[36m(func pid=181391)[0m f1_weighted: 0.1520183579728226
[2m[36m(func pid=181391)[0m f1_per_class: [0.045, 0.0, 0.077, 0.0, 0.0, 0.0, 0.499, 0.0, 0.056, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.2994402985074627
[2m[36m(func pid=180330)[0m top5: 0.8642723880597015
[2m[36m(func pid=180330)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=180330)[0m f1_macro: 0.28902669213631993
[2m[36m(func pid=180330)[0m f1_weighted: 0.31477085544034283
[2m[36m(func pid=180330)[0m f1_per_class: [0.405, 0.354, 0.537, 0.384, 0.081, 0.131, 0.319, 0.259, 0.211, 0.21]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.19076492537313433
[2m[36m(func pid=174613)[0m top5: 0.7117537313432836
[2m[36m(func pid=174613)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=174613)[0m f1_macro: 0.17899240829670235
[2m[36m(func pid=174613)[0m f1_weighted: 0.2049961731151789
[2m[36m(func pid=174613)[0m f1_per_class: [0.169, 0.211, 0.393, 0.29, 0.04, 0.11, 0.181, 0.167, 0.126, 0.102]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.8068 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.6068 | Steps: 2 | Val loss: 599.8467 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1028 | Steps: 2 | Val loss: 1.8646 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8790 | Steps: 2 | Val loss: 2.1472 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:31:33 (running for 00:12:07.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.967 |      0.179 |                   53 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.092 |      0.289 |                   27 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.385 |                   27 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  3.498 |      0.068 |                   25 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.38899253731343286
[2m[36m(func pid=180846)[0m top5: 0.9230410447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=180846)[0m f1_macro: 0.3846686235590565
[2m[36m(func pid=180846)[0m f1_weighted: 0.39922535123257546
[2m[36m(func pid=180846)[0m f1_per_class: [0.523, 0.472, 0.686, 0.532, 0.151, 0.262, 0.31, 0.334, 0.247, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.2891791044776119
[2m[36m(func pid=181391)[0m top5: 0.6697761194029851
[2m[36m(func pid=181391)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=181391)[0m f1_macro: 0.08784791846041398
[2m[36m(func pid=181391)[0m f1_weighted: 0.18191789035089534
[2m[36m(func pid=181391)[0m f1_per_class: [0.033, 0.156, 0.101, 0.0, 0.0, 0.0, 0.507, 0.0, 0.082, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3050373134328358
[2m[36m(func pid=180330)[0m top5: 0.8675373134328358
[2m[36m(func pid=180330)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=180330)[0m f1_macro: 0.29310511890798807
[2m[36m(func pid=180330)[0m f1_weighted: 0.31770043554044075
[2m[36m(func pid=180330)[0m f1_per_class: [0.412, 0.365, 0.545, 0.404, 0.084, 0.124, 0.306, 0.259, 0.201, 0.229]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.19402985074626866
[2m[36m(func pid=174613)[0m top5: 0.7182835820895522
[2m[36m(func pid=174613)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=174613)[0m f1_macro: 0.18177286434777234
[2m[36m(func pid=174613)[0m f1_weighted: 0.20871679157372985
[2m[36m(func pid=174613)[0m f1_per_class: [0.174, 0.224, 0.407, 0.294, 0.04, 0.103, 0.186, 0.16, 0.126, 0.104]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8030 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.3986 | Steps: 2 | Val loss: 241.8218 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0755 | Steps: 2 | Val loss: 1.8499 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9317 | Steps: 2 | Val loss: 2.1456 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=181391)[0m top1: 0.3180970149253731
[2m[36m(func pid=181391)[0m top5: 0.6842350746268657
[2m[36m(func pid=181391)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=181391)[0m f1_macro: 0.11612190774438302== Status ==
Current time: 2024-01-07 03:31:38 (running for 00:12:13.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.879 |      0.182 |                   54 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.103 |      0.293 |                   28 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.385 |                   27 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  4.399 |      0.116 |                   27 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m f1_weighted: 0.22579839211950273

[2m[36m(func pid=181391)[0m f1_per_class: [0.055, 0.338, 0.138, 0.0, 0.0, 0.019, 0.541, 0.0, 0.071, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3903917910447761
[2m[36m(func pid=180846)[0m top5: 0.9230410447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=180846)[0m f1_macro: 0.3855360573360779
[2m[36m(func pid=180846)[0m f1_weighted: 0.4004759724575815
[2m[36m(func pid=180846)[0m f1_per_class: [0.523, 0.485, 0.667, 0.53, 0.158, 0.265, 0.308, 0.329, 0.249, 0.343]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.30830223880597013
[2m[36m(func pid=180330)[0m top5: 0.8698694029850746
[2m[36m(func pid=180330)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=180330)[0m f1_macro: 0.2949921065388944
[2m[36m(func pid=180330)[0m f1_weighted: 0.3219602194619419
[2m[36m(func pid=180330)[0m f1_per_class: [0.412, 0.361, 0.545, 0.411, 0.083, 0.127, 0.316, 0.255, 0.198, 0.242]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.1982276119402985
[2m[36m(func pid=174613)[0m top5: 0.7164179104477612
[2m[36m(func pid=174613)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=174613)[0m f1_macro: 0.185317852224358
[2m[36m(func pid=174613)[0m f1_weighted: 0.21199691166344042
[2m[36m(func pid=174613)[0m f1_per_class: [0.177, 0.24, 0.4, 0.295, 0.039, 0.103, 0.184, 0.167, 0.142, 0.108]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6224 | Steps: 2 | Val loss: 93.9898 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0735 | Steps: 2 | Val loss: 1.8411 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.8138 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:31:43 (running for 00:12:18.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.932 |      0.185 |                   55 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.076 |      0.295 |                   29 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.386 |                   28 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.622 |      0.106 |                   28 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.18796641791044777
[2m[36m(func pid=181391)[0m top5: 0.683768656716418
[2m[36m(func pid=181391)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=181391)[0m f1_macro: 0.10628685932267135
[2m[36m(func pid=181391)[0m f1_weighted: 0.11599721885343553
[2m[36m(func pid=181391)[0m f1_per_class: [0.112, 0.368, 0.14, 0.116, 0.0, 0.008, 0.0, 0.224, 0.094, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8172 | Steps: 2 | Val loss: 2.1426 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=180330)[0m top1: 0.31669776119402987
[2m[36m(func pid=180330)[0m top5: 0.8768656716417911
[2m[36m(func pid=180330)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=180330)[0m f1_macro: 0.29759941496470005
[2m[36m(func pid=180330)[0m f1_weighted: 0.33255630970517963
[2m[36m(func pid=180330)[0m f1_per_class: [0.405, 0.342, 0.545, 0.424, 0.091, 0.129, 0.349, 0.26, 0.204, 0.227]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.386660447761194
[2m[36m(func pid=180846)[0m top5: 0.9221082089552238
[2m[36m(func pid=180846)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=180846)[0m f1_macro: 0.38045138237864323
[2m[36m(func pid=180846)[0m f1_weighted: 0.3999737863775468
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.472, 0.649, 0.52, 0.154, 0.262, 0.326, 0.331, 0.243, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.197294776119403
[2m[36m(func pid=174613)[0m top5: 0.7196828358208955
[2m[36m(func pid=174613)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=174613)[0m f1_macro: 0.18723883783010614
[2m[36m(func pid=174613)[0m f1_weighted: 0.2094740074404588
[2m[36m(func pid=174613)[0m f1_per_class: [0.181, 0.239, 0.423, 0.297, 0.038, 0.11, 0.171, 0.163, 0.141, 0.109]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.2598 | Steps: 2 | Val loss: 135.2004 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0585 | Steps: 2 | Val loss: 1.8372 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7938 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 03:31:48 (running for 00:12:23.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.817 |      0.187 |                   56 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.073 |      0.298 |                   30 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.38  |                   29 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  3.26  |      0.088 |                   29 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.1767723880597015
[2m[36m(func pid=181391)[0m top5: 0.6879664179104478
[2m[36m(func pid=181391)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=181391)[0m f1_macro: 0.08764568666974029
[2m[36m(func pid=181391)[0m f1_weighted: 0.105494917513255
[2m[36m(func pid=181391)[0m f1_per_class: [0.082, 0.344, 0.115, 0.111, 0.0, 0.0, 0.0, 0.224, 0.0, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7777 | Steps: 2 | Val loss: 2.1341 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=180330)[0m top1: 0.31763059701492535
[2m[36m(func pid=180330)[0m top5: 0.8773320895522388
[2m[36m(func pid=180330)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=180330)[0m f1_macro: 0.2978294749356066
[2m[36m(func pid=180330)[0m f1_weighted: 0.3325745195838475
[2m[36m(func pid=180330)[0m f1_per_class: [0.402, 0.346, 0.545, 0.426, 0.091, 0.129, 0.345, 0.255, 0.205, 0.233]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3927238805970149
[2m[36m(func pid=180846)[0m top5: 0.9244402985074627
[2m[36m(func pid=180846)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=180846)[0m f1_macro: 0.38781521095649424
[2m[36m(func pid=180846)[0m f1_weighted: 0.40455609968058287
[2m[36m(func pid=180846)[0m f1_per_class: [0.504, 0.476, 0.686, 0.528, 0.157, 0.26, 0.33, 0.333, 0.248, 0.356]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.20149253731343283
[2m[36m(func pid=174613)[0m top5: 0.7290111940298507
[2m[36m(func pid=174613)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=174613)[0m f1_macro: 0.18923557032106786
[2m[36m(func pid=174613)[0m f1_weighted: 0.21522082372322662
[2m[36m(func pid=174613)[0m f1_per_class: [0.183, 0.238, 0.423, 0.301, 0.041, 0.106, 0.19, 0.16, 0.14, 0.111]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.1351 | Steps: 2 | Val loss: 127.9055 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0750 | Steps: 2 | Val loss: 1.8222 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.7939 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8138 | Steps: 2 | Val loss: 2.1293 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:31:54 (running for 00:12:29.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.778 |      0.189 |                   57 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.059 |      0.298 |                   31 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.388 |                   30 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  3.135 |      0.102 |                   30 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.16557835820895522
[2m[36m(func pid=181391)[0m top5: 0.7285447761194029
[2m[36m(func pid=181391)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=181391)[0m f1_macro: 0.10228235424581
[2m[36m(func pid=181391)[0m f1_weighted: 0.11810763970918668
[2m[36m(func pid=181391)[0m f1_per_class: [0.062, 0.36, 0.087, 0.122, 0.019, 0.0, 0.0, 0.342, 0.0, 0.031]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.396455223880597
[2m[36m(func pid=180846)[0m top5: 0.9244402985074627
[2m[36m(func pid=180846)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=180846)[0m f1_macro: 0.38807671655897147
[2m[36m(func pid=180846)[0m f1_weighted: 0.40637686057093664
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.479, 0.649, 0.539, 0.15, 0.264, 0.319, 0.343, 0.255, 0.364]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3260261194029851
[2m[36m(func pid=180330)[0m top5: 0.8801305970149254
[2m[36m(func pid=180330)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=180330)[0m f1_macro: 0.30581406273567646
[2m[36m(func pid=180330)[0m f1_weighted: 0.34044796902596364
[2m[36m(func pid=180330)[0m f1_per_class: [0.407, 0.358, 0.545, 0.424, 0.093, 0.133, 0.362, 0.265, 0.206, 0.265]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.20569029850746268
[2m[36m(func pid=174613)[0m top5: 0.7308768656716418
[2m[36m(func pid=174613)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=174613)[0m f1_macro: 0.19694570735315844
[2m[36m(func pid=174613)[0m f1_weighted: 0.2196342427142802
[2m[36m(func pid=174613)[0m f1_per_class: [0.193, 0.247, 0.458, 0.303, 0.052, 0.12, 0.19, 0.164, 0.139, 0.105]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7938 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2697 | Steps: 2 | Val loss: 97.7479 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0932 | Steps: 2 | Val loss: 1.8105 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:31:59 (running for 00:12:34.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.814 |      0.197 |                   58 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.075 |      0.306 |                   32 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.388 |                   31 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.27  |      0.084 |                   31 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7578 | Steps: 2 | Val loss: 2.1232 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=181391)[0m top1: 0.12639925373134328
[2m[36m(func pid=181391)[0m top5: 0.7789179104477612
[2m[36m(func pid=181391)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=181391)[0m f1_macro: 0.08441343757844305
[2m[36m(func pid=181391)[0m f1_weighted: 0.13560145037501803
[2m[36m(func pid=181391)[0m f1_per_class: [0.085, 0.369, 0.074, 0.242, 0.04, 0.016, 0.0, 0.0, 0.0, 0.018]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.39132462686567165
[2m[36m(func pid=180846)[0m top5: 0.9235074626865671
[2m[36m(func pid=180846)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=180846)[0m f1_macro: 0.38257624828127584
[2m[36m(func pid=180846)[0m f1_weighted: 0.40034315209021976
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.472, 0.649, 0.543, 0.156, 0.238, 0.312, 0.333, 0.255, 0.35]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3292910447761194
[2m[36m(func pid=180330)[0m top5: 0.8857276119402985
[2m[36m(func pid=180330)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=180330)[0m f1_macro: 0.30422591843019076
[2m[36m(func pid=180330)[0m f1_weighted: 0.3450772792204305
[2m[36m(func pid=180330)[0m f1_per_class: [0.407, 0.357, 0.545, 0.436, 0.098, 0.135, 0.371, 0.244, 0.199, 0.25]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.20615671641791045
[2m[36m(func pid=174613)[0m top5: 0.7388059701492538
[2m[36m(func pid=174613)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=174613)[0m f1_macro: 0.1969943909581646
[2m[36m(func pid=174613)[0m f1_weighted: 0.22092584638001758
[2m[36m(func pid=174613)[0m f1_per_class: [0.182, 0.241, 0.449, 0.3, 0.05, 0.126, 0.197, 0.171, 0.146, 0.109]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.3800 | Steps: 2 | Val loss: 53.9218 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0035 | Steps: 2 | Val loss: 1.8067 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0542 | Steps: 2 | Val loss: 1.8084 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7565 | Steps: 2 | Val loss: 2.1173 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:32:04 (running for 00:12:39.72)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.758 |      0.197 |                   59 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.093 |      0.304 |                   33 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.003 |      0.381 |                   33 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.27  |      0.084 |                   31 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180330)[0m top1: 0.3316231343283582
[2m[36m(func pid=180330)[0m top5: 0.8833955223880597
[2m[36m(func pid=180330)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=180330)[0m f1_macro: 0.30474696379016797
[2m[36m(func pid=180330)[0m f1_weighted: 0.34734203099117295
[2m[36m(func pid=180330)[0m f1_per_class: [0.385, 0.366, 0.558, 0.424, 0.105, 0.127, 0.388, 0.245, 0.206, 0.244]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3941231343283582
[2m[36m(func pid=180846)[0m top5: 0.9221082089552238
[2m[36m(func pid=180846)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=180846)[0m f1_macro: 0.38138703621135056
[2m[36m(func pid=180846)[0m f1_weighted: 0.40107127823523014
[2m[36m(func pid=180846)[0m f1_per_class: [0.538, 0.48, 0.6, 0.542, 0.152, 0.241, 0.307, 0.341, 0.26, 0.353]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.05923507462686567
[2m[36m(func pid=181391)[0m top5: 0.7635261194029851
[2m[36m(func pid=181391)[0m f1_micro: 0.05923507462686567
[2m[36m(func pid=181391)[0m f1_macro: 0.06449472811795816
[2m[36m(func pid=181391)[0m f1_weighted: 0.06072919513245918
[2m[36m(func pid=181391)[0m f1_per_class: [0.081, 0.064, 0.072, 0.109, 0.056, 0.077, 0.0, 0.09, 0.079, 0.016]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m top1: 0.20708955223880596
[2m[36m(func pid=174613)[0m top5: 0.7458022388059702
[2m[36m(func pid=174613)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=174613)[0m f1_macro: 0.1973089460846982
[2m[36m(func pid=174613)[0m f1_weighted: 0.22333502874661537
[2m[36m(func pid=174613)[0m f1_per_class: [0.185, 0.244, 0.458, 0.302, 0.051, 0.118, 0.207, 0.159, 0.14, 0.109]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.8055 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.0927 | Steps: 2 | Val loss: 24.8744 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0559 | Steps: 2 | Val loss: 1.8121 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 03:32:09 (running for 00:12:44.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.757 |      0.197 |                   60 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.054 |      0.305 |                   34 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.003 |      0.381 |                   33 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.093 |      0.096 |                   33 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8142 | Steps: 2 | Val loss: 2.1060 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=181391)[0m top1: 0.10121268656716417
[2m[36m(func pid=181391)[0m top5: 0.7882462686567164
[2m[36m(func pid=181391)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=181391)[0m f1_macro: 0.09636643272946169
[2m[36m(func pid=181391)[0m f1_weighted: 0.06344914169530098
[2m[36m(func pid=181391)[0m f1_per_class: [0.082, 0.053, 0.087, 0.013, 0.057, 0.205, 0.0, 0.371, 0.095, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.394589552238806
[2m[36m(func pid=180846)[0m top5: 0.9207089552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=180846)[0m f1_macro: 0.3828750907443392
[2m[36m(func pid=180846)[0m f1_weighted: 0.40150016495045915
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.466, 0.649, 0.546, 0.157, 0.259, 0.306, 0.341, 0.263, 0.319]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33488805970149255
[2m[36m(func pid=180330)[0m top5: 0.8801305970149254
[2m[36m(func pid=180330)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=180330)[0m f1_macro: 0.30863051565406596
[2m[36m(func pid=180330)[0m f1_weighted: 0.3499480575329435
[2m[36m(func pid=180330)[0m f1_per_class: [0.387, 0.375, 0.558, 0.427, 0.107, 0.136, 0.383, 0.25, 0.211, 0.252]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.21082089552238806
[2m[36m(func pid=174613)[0m top5: 0.7527985074626866
[2m[36m(func pid=174613)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=174613)[0m f1_macro: 0.20073925023431535
[2m[36m(func pid=174613)[0m f1_weighted: 0.2273334091759388
[2m[36m(func pid=174613)[0m f1_per_class: [0.184, 0.244, 0.478, 0.307, 0.054, 0.118, 0.216, 0.15, 0.142, 0.114]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.0249 | Steps: 2 | Val loss: 8.8432 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7894 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0503 | Steps: 2 | Val loss: 1.8007 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:32:15 (running for 00:12:49.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.814 |      0.201 |                   61 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.056 |      0.309 |                   35 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.383 |                   34 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.025 |      0.125 |                   34 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.15205223880597016
[2m[36m(func pid=181391)[0m top5: 0.7803171641791045
[2m[36m(func pid=181391)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=181391)[0m f1_macro: 0.12533588982145696
[2m[36m(func pid=181391)[0m f1_weighted: 0.14429609859362552
[2m[36m(func pid=181391)[0m f1_per_class: [0.082, 0.13, 0.058, 0.0, 0.064, 0.217, 0.23, 0.424, 0.048, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7390 | Steps: 2 | Val loss: 2.1044 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=180846)[0m top1: 0.396455223880597
[2m[36m(func pid=180846)[0m top5: 0.9230410447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=180846)[0m f1_macro: 0.3792907558726573
[2m[36m(func pid=180846)[0m f1_weighted: 0.405848316627441
[2m[36m(func pid=180846)[0m f1_per_class: [0.53, 0.472, 0.6, 0.547, 0.156, 0.254, 0.322, 0.331, 0.263, 0.319]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.34328358208955223
[2m[36m(func pid=180330)[0m top5: 0.8861940298507462
[2m[36m(func pid=180330)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=180330)[0m f1_macro: 0.3134306079826011
[2m[36m(func pid=180330)[0m f1_weighted: 0.3581666435055941
[2m[36m(func pid=180330)[0m f1_per_class: [0.377, 0.377, 0.571, 0.437, 0.112, 0.134, 0.401, 0.248, 0.219, 0.259]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.21128731343283583
[2m[36m(func pid=174613)[0m top5: 0.7513992537313433
[2m[36m(func pid=174613)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=174613)[0m f1_macro: 0.20020935313732124
[2m[36m(func pid=174613)[0m f1_weighted: 0.22763354380944364
[2m[36m(func pid=174613)[0m f1_per_class: [0.192, 0.25, 0.478, 0.308, 0.054, 0.106, 0.215, 0.162, 0.135, 0.102]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.0125 | Steps: 2 | Val loss: 5.7920 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7745 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0485 | Steps: 2 | Val loss: 1.7952 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7316 | Steps: 2 | Val loss: 2.1019 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 03:32:20 (running for 00:12:55.38)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.739 |      0.2   |                   62 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.05  |      0.313 |                   36 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.379 |                   35 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.013 |      0.131 |                   35 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.21128731343283583
[2m[36m(func pid=181391)[0m top5: 0.7667910447761194
[2m[36m(func pid=181391)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=181391)[0m f1_macro: 0.13091543695009525
[2m[36m(func pid=181391)[0m f1_weighted: 0.21571232156132816
[2m[36m(func pid=181391)[0m f1_per_class: [0.086, 0.172, 0.198, 0.0, 0.051, 0.236, 0.518, 0.0, 0.049, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.39972014925373134
[2m[36m(func pid=180846)[0m top5: 0.9244402985074627
[2m[36m(func pid=180846)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=180846)[0m f1_macro: 0.3851703983458717
[2m[36m(func pid=180846)[0m f1_weighted: 0.40962395668640617
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.483, 0.649, 0.541, 0.152, 0.242, 0.336, 0.337, 0.264, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3400186567164179
[2m[36m(func pid=180330)[0m top5: 0.8880597014925373
[2m[36m(func pid=180330)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=180330)[0m f1_macro: 0.3090170916150031
[2m[36m(func pid=180330)[0m f1_weighted: 0.35516905929487846
[2m[36m(func pid=180330)[0m f1_per_class: [0.361, 0.363, 0.558, 0.434, 0.109, 0.13, 0.405, 0.246, 0.219, 0.265]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.21315298507462688
[2m[36m(func pid=174613)[0m top5: 0.753731343283582
[2m[36m(func pid=174613)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=174613)[0m f1_macro: 0.20142081293058114
[2m[36m(func pid=174613)[0m f1_weighted: 0.22843704314242294
[2m[36m(func pid=174613)[0m f1_per_class: [0.203, 0.258, 0.478, 0.307, 0.047, 0.106, 0.215, 0.153, 0.142, 0.104]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4383 | Steps: 2 | Val loss: 4.2113 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7713 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0600 | Steps: 2 | Val loss: 1.7995 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7572 | Steps: 2 | Val loss: 2.0994 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:32:25 (running for 00:13:00.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.732 |      0.201 |                   63 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.048 |      0.309 |                   37 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.385 |                   36 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.438 |      0.182 |                   36 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.23367537313432835
[2m[36m(func pid=181391)[0m top5: 0.7705223880597015
[2m[36m(func pid=181391)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=181391)[0m f1_macro: 0.18179594132898086
[2m[36m(func pid=181391)[0m f1_weighted: 0.23056524486434932
[2m[36m(func pid=181391)[0m f1_per_class: [0.09, 0.422, 0.215, 0.0, 0.027, 0.232, 0.343, 0.415, 0.041, 0.033]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.39925373134328357
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=180846)[0m f1_macro: 0.3840972880017387
[2m[36m(func pid=180846)[0m f1_weighted: 0.408366786649232
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.482, 0.649, 0.545, 0.157, 0.236, 0.332, 0.33, 0.264, 0.328]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3376865671641791
[2m[36m(func pid=180330)[0m top5: 0.8857276119402985
[2m[36m(func pid=180330)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=180330)[0m f1_macro: 0.30578112422914666
[2m[36m(func pid=180330)[0m f1_weighted: 0.3517735103635974
[2m[36m(func pid=180330)[0m f1_per_class: [0.344, 0.365, 0.558, 0.436, 0.108, 0.135, 0.389, 0.245, 0.225, 0.252]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.21455223880597016
[2m[36m(func pid=174613)[0m top5: 0.7527985074626866
[2m[36m(func pid=174613)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=174613)[0m f1_macro: 0.2032646021095113
[2m[36m(func pid=174613)[0m f1_weighted: 0.22778504838203553
[2m[36m(func pid=174613)[0m f1_per_class: [0.21, 0.263, 0.478, 0.305, 0.049, 0.106, 0.21, 0.156, 0.147, 0.108]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.0732 | Steps: 2 | Val loss: 3.8834 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7666 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0664 | Steps: 2 | Val loss: 1.8010 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8537 | Steps: 2 | Val loss: 2.0979 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 03:32:31 (running for 00:13:05.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.757 |      0.203 |                   64 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.06  |      0.306 |                   38 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.384 |                   37 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.073 |      0.17  |                   37 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.21455223880597016
[2m[36m(func pid=181391)[0m top5: 0.7681902985074627
[2m[36m(func pid=181391)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=181391)[0m f1_macro: 0.16972333343392093
[2m[36m(func pid=181391)[0m f1_weighted: 0.19414248631460002
[2m[36m(func pid=181391)[0m f1_per_class: [0.085, 0.43, 0.202, 0.0, 0.032, 0.259, 0.211, 0.394, 0.04, 0.045]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.345615671641791
[2m[36m(func pid=180330)[0m top5: 0.8852611940298507
[2m[36m(func pid=180330)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=180330)[0m f1_macro: 0.3085269899860258
[2m[36m(func pid=180330)[0m f1_weighted: 0.3620346578703068
[2m[36m(func pid=180330)[0m f1_per_class: [0.335, 0.366, 0.558, 0.432, 0.111, 0.139, 0.425, 0.255, 0.227, 0.238]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4001865671641791
[2m[36m(func pid=180846)[0m top5: 0.9249067164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.3897980421361539
[2m[36m(func pid=180846)[0m f1_weighted: 0.4121363804574925
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.473, 0.706, 0.549, 0.16, 0.235, 0.346, 0.329, 0.257, 0.327]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.21735074626865672
[2m[36m(func pid=174613)[0m top5: 0.7597947761194029
[2m[36m(func pid=174613)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=174613)[0m f1_macro: 0.20557271781075848
[2m[36m(func pid=174613)[0m f1_weighted: 0.2309506556285273
[2m[36m(func pid=174613)[0m f1_per_class: [0.206, 0.261, 0.478, 0.304, 0.051, 0.107, 0.22, 0.165, 0.157, 0.108]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7676 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.9791 | Steps: 2 | Val loss: 2.7510 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0401 | Steps: 2 | Val loss: 1.7931 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7035 | Steps: 2 | Val loss: 2.0880 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:32:36 (running for 00:13:11.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.854 |      0.206 |                   65 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.066 |      0.309 |                   39 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.39  |                   38 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.979 |      0.134 |                   38 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.15391791044776118
[2m[36m(func pid=181391)[0m top5: 0.7677238805970149
[2m[36m(func pid=181391)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=181391)[0m f1_macro: 0.13395442398917473
[2m[36m(func pid=181391)[0m f1_weighted: 0.1461447548323354
[2m[36m(func pid=181391)[0m f1_per_class: [0.084, 0.062, 0.183, 0.0, 0.058, 0.299, 0.255, 0.359, 0.04, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.34841417910447764
[2m[36m(func pid=180330)[0m top5: 0.886660447761194
[2m[36m(func pid=180330)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=180330)[0m f1_macro: 0.30955948769962505
[2m[36m(func pid=180330)[0m f1_weighted: 0.3643500618078478
[2m[36m(func pid=180330)[0m f1_per_class: [0.329, 0.37, 0.558, 0.437, 0.115, 0.14, 0.426, 0.247, 0.231, 0.242]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.39972014925373134
[2m[36m(func pid=180846)[0m top5: 0.925839552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=180846)[0m f1_macro: 0.38960409027294446
[2m[36m(func pid=180846)[0m f1_weighted: 0.41374351609344123
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.487, 0.686, 0.537, 0.154, 0.233, 0.355, 0.331, 0.257, 0.336]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.2196828358208955
[2m[36m(func pid=174613)[0m top5: 0.761660447761194
[2m[36m(func pid=174613)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=174613)[0m f1_macro: 0.20640166742471258
[2m[36m(func pid=174613)[0m f1_weighted: 0.23337907309536887
[2m[36m(func pid=174613)[0m f1_per_class: [0.21, 0.266, 0.478, 0.309, 0.05, 0.11, 0.221, 0.157, 0.155, 0.107]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7813 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0572 | Steps: 2 | Val loss: 1.7901 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3434 | Steps: 2 | Val loss: 2.5300 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7044 | Steps: 2 | Val loss: 2.0850 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 03:32:41 (running for 00:13:16.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.703 |      0.206 |                   66 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.04  |      0.31  |                   40 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.388 |                   40 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.979 |      0.134 |                   38 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.3978544776119403
[2m[36m(func pid=180846)[0m top5: 0.9267723880597015
[2m[36m(func pid=180846)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.3878375316183968
[2m[36m(func pid=180846)[0m f1_weighted: 0.4101725930363767
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.477, 0.686, 0.543, 0.156, 0.243, 0.339, 0.336, 0.255, 0.327]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.34841417910447764
[2m[36m(func pid=180330)[0m top5: 0.8875932835820896
[2m[36m(func pid=180330)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=180330)[0m f1_macro: 0.3079159305223647
[2m[36m(func pid=180330)[0m f1_weighted: 0.36581854411320563
[2m[36m(func pid=180330)[0m f1_per_class: [0.323, 0.358, 0.558, 0.434, 0.115, 0.145, 0.439, 0.251, 0.231, 0.224]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.1455223880597015
[2m[36m(func pid=181391)[0m top5: 0.7980410447761194
[2m[36m(func pid=181391)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=181391)[0m f1_macro: 0.13611580281249533
[2m[36m(func pid=181391)[0m f1_weighted: 0.11711797765553889
[2m[36m(func pid=181391)[0m f1_per_class: [0.067, 0.0, 0.171, 0.065, 0.145, 0.338, 0.107, 0.409, 0.03, 0.029]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m top1: 0.22061567164179105
[2m[36m(func pid=174613)[0m top5: 0.7667910447761194
[2m[36m(func pid=174613)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=174613)[0m f1_macro: 0.20730167366516375
[2m[36m(func pid=174613)[0m f1_weighted: 0.23492783837405118
[2m[36m(func pid=174613)[0m f1_per_class: [0.207, 0.26, 0.478, 0.312, 0.053, 0.108, 0.225, 0.162, 0.162, 0.105]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7670 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0568 | Steps: 2 | Val loss: 1.7920 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7639 | Steps: 2 | Val loss: 2.7673 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6474 | Steps: 2 | Val loss: 2.0825 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:32:46 (running for 00:13:21.59)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.704 |      0.207 |                   67 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.057 |      0.308 |                   41 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.387 |                   41 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.343 |      0.136 |                   39 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.40111940298507465
[2m[36m(func pid=180846)[0m top5: 0.9263059701492538
[2m[36m(func pid=180846)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=180846)[0m f1_macro: 0.387040306260022
[2m[36m(func pid=180846)[0m f1_weighted: 0.41305974826052017
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.489, 0.649, 0.539, 0.155, 0.259, 0.342, 0.322, 0.26, 0.336]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3498134328358209
[2m[36m(func pid=180330)[0m top5: 0.8885261194029851
[2m[36m(func pid=180330)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=180330)[0m f1_macro: 0.3122231489737207
[2m[36m(func pid=180330)[0m f1_weighted: 0.36695795288230215
[2m[36m(func pid=180330)[0m f1_per_class: [0.327, 0.375, 0.585, 0.441, 0.117, 0.145, 0.427, 0.243, 0.234, 0.227]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.23647388059701493
[2m[36m(func pid=181391)[0m top5: 0.7761194029850746
[2m[36m(func pid=181391)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=181391)[0m f1_macro: 0.15510438108697866
[2m[36m(func pid=181391)[0m f1_weighted: 0.21094415837210156
[2m[36m(func pid=181391)[0m f1_per_class: [0.046, 0.011, 0.158, 0.479, 0.049, 0.276, 0.052, 0.443, 0.018, 0.02]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m top1: 0.2196828358208955
[2m[36m(func pid=174613)[0m top5: 0.769589552238806
[2m[36m(func pid=174613)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=174613)[0m f1_macro: 0.206322091595135
[2m[36m(func pid=174613)[0m f1_weighted: 0.23454819129748794
[2m[36m(func pid=174613)[0m f1_per_class: [0.206, 0.257, 0.468, 0.31, 0.052, 0.104, 0.23, 0.158, 0.167, 0.111]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7619 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0508 | Steps: 2 | Val loss: 1.7849 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.8720 | Steps: 2 | Val loss: 3.4095 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6961 | Steps: 2 | Val loss: 2.0817 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 03:32:52 (running for 00:13:27.27)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.647 |      0.206 |                   68 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.051 |      0.313 |                   43 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.387 |                   41 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.764 |      0.155 |                   40 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180330)[0m top1: 0.35401119402985076
[2m[36m(func pid=180330)[0m top5: 0.886660447761194
[2m[36m(func pid=180330)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=180330)[0m f1_macro: 0.31261729808358935
[2m[36m(func pid=180330)[0m f1_weighted: 0.369427520059008
[2m[36m(func pid=180330)[0m f1_per_class: [0.339, 0.369, 0.571, 0.441, 0.116, 0.136, 0.441, 0.245, 0.23, 0.236]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3987873134328358
[2m[36m(func pid=180846)[0m top5: 0.925839552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=180846)[0m f1_macro: 0.38341056114377625
[2m[36m(func pid=180846)[0m f1_weighted: 0.40961895329256226
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.476, 0.649, 0.545, 0.15, 0.253, 0.336, 0.32, 0.262, 0.321]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.18889925373134328
[2m[36m(func pid=181391)[0m top5: 0.7560634328358209
[2m[36m(func pid=181391)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=181391)[0m f1_macro: 0.14396441019553302
[2m[36m(func pid=181391)[0m f1_weighted: 0.1871363995229218
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.039, 0.153, 0.414, 0.037, 0.269, 0.019, 0.448, 0.04, 0.02]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=174613)[0m top1: 0.2234141791044776
[2m[36m(func pid=174613)[0m top5: 0.7714552238805971
[2m[36m(func pid=174613)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=174613)[0m f1_macro: 0.20840193740608148
[2m[36m(func pid=174613)[0m f1_weighted: 0.241037853591226
[2m[36m(func pid=174613)[0m f1_per_class: [0.194, 0.259, 0.468, 0.311, 0.062, 0.107, 0.248, 0.163, 0.167, 0.105]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0017 | Steps: 2 | Val loss: 1.7835 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8117 | Steps: 2 | Val loss: 3.2260 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0367 | Steps: 2 | Val loss: 1.7784 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6265 | Steps: 2 | Val loss: 2.0770 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=180846)[0m top1: 0.3978544776119403
[2m[36m(func pid=180846)[0m top5: 0.9230410447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.38255434441861924
[2m[36m(func pid=180846)[0m f1_weighted: 0.40722974460733197
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.471, 0.632, 0.541, 0.15, 0.254, 0.331, 0.336, 0.262, 0.33]
== Status ==
Current time: 2024-01-07 03:32:57 (running for 00:13:32.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.696 |      0.208 |                   69 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.051 |      0.313 |                   43 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.383 |                   43 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.872 |      0.144 |                   41 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.1646455223880597
[2m[36m(func pid=181391)[0m top5: 0.7490671641791045
[2m[36m(func pid=181391)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=181391)[0m f1_macro: 0.1437944361477767
[2m[36m(func pid=181391)[0m f1_weighted: 0.18281643481948073
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.175, 0.225, 0.387, 0.04, 0.034, 0.034, 0.467, 0.056, 0.02]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.35634328358208955
[2m[36m(func pid=180330)[0m top5: 0.8936567164179104
[2m[36m(func pid=180330)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=180330)[0m f1_macro: 0.31634331835225504
[2m[36m(func pid=180330)[0m f1_weighted: 0.3718759713989103
[2m[36m(func pid=180330)[0m f1_per_class: [0.352, 0.381, 0.571, 0.441, 0.118, 0.146, 0.437, 0.246, 0.232, 0.238]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.22527985074626866
[2m[36m(func pid=174613)[0m top5: 0.7747201492537313
[2m[36m(func pid=174613)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=174613)[0m f1_macro: 0.20965233335299885
[2m[36m(func pid=174613)[0m f1_weighted: 0.2417885668691154
[2m[36m(func pid=174613)[0m f1_per_class: [0.202, 0.256, 0.458, 0.32, 0.062, 0.115, 0.24, 0.168, 0.165, 0.11]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0023 | Steps: 2 | Val loss: 1.7603 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.8848 | Steps: 2 | Val loss: 2.6388 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0334 | Steps: 2 | Val loss: 1.7804 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6402 | Steps: 2 | Val loss: 2.0729 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 03:33:03 (running for 00:13:38.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.626 |      0.21  |                   70 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.037 |      0.316 |                   44 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.383 |                   43 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.885 |      0.185 |                   43 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.40625
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.40625
[2m[36m(func pid=180846)[0m f1_macro: 0.3878573526867991
[2m[36m(func pid=180846)[0m f1_weighted: 0.4171681250363632
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.475, 0.649, 0.557, 0.16, 0.26, 0.347, 0.322, 0.26, 0.322]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.24300373134328357
[2m[36m(func pid=181391)[0m top5: 0.7681902985074627
[2m[36m(func pid=181391)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=181391)[0m f1_macro: 0.18466743971876448
[2m[36m(func pid=181391)[0m f1_weighted: 0.2505985100313565
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.45, 0.364, 0.387, 0.029, 0.034, 0.115, 0.392, 0.057, 0.02]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3521455223880597
[2m[36m(func pid=180330)[0m top5: 0.8922574626865671
[2m[36m(func pid=180330)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=180330)[0m f1_macro: 0.315216939573849
[2m[36m(func pid=180330)[0m f1_weighted: 0.36728312262573753
[2m[36m(func pid=180330)[0m f1_per_class: [0.352, 0.384, 0.571, 0.442, 0.121, 0.143, 0.421, 0.24, 0.233, 0.244]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=174613)[0m top1: 0.22574626865671643
[2m[36m(func pid=174613)[0m top5: 0.7765858208955224
[2m[36m(func pid=174613)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=174613)[0m f1_macro: 0.20924774896074255
[2m[36m(func pid=174613)[0m f1_weighted: 0.24194125020641563
[2m[36m(func pid=174613)[0m f1_per_class: [0.204, 0.259, 0.449, 0.317, 0.065, 0.113, 0.243, 0.167, 0.165, 0.112]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.5605 | Steps: 2 | Val loss: 2.5637 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0470 | Steps: 2 | Val loss: 1.7825 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0020 | Steps: 2 | Val loss: 1.7763 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6379 | Steps: 2 | Val loss: 2.0725 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:33:08 (running for 00:13:43.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.64  |      0.209 |                   71 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.033 |      0.315 |                   45 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.388 |                   44 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.56  |      0.167 |                   44 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.2355410447761194
[2m[36m(func pid=181391)[0m top5: 0.7658582089552238
[2m[36m(func pid=181391)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=181391)[0m f1_macro: 0.1669516225183551
[2m[36m(func pid=181391)[0m f1_weighted: 0.2495322866817424
[2m[36m(func pid=181391)[0m f1_per_class: [0.068, 0.442, 0.25, 0.195, 0.0, 0.116, 0.297, 0.2, 0.079, 0.023]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.34794776119402987
[2m[36m(func pid=180330)[0m top5: 0.8922574626865671
[2m[36m(func pid=180330)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=180330)[0m f1_macro: 0.31269977804135507
[2m[36m(func pid=180330)[0m f1_weighted: 0.36217577327349837
[2m[36m(func pid=180330)[0m f1_per_class: [0.351, 0.384, 0.558, 0.438, 0.121, 0.156, 0.403, 0.241, 0.226, 0.248]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4001865671641791
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.38297296278922144
[2m[36m(func pid=180846)[0m f1_weighted: 0.41144722267606587
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.472, 0.649, 0.543, 0.154, 0.241, 0.35, 0.327, 0.261, 0.319]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.22761194029850745
[2m[36m(func pid=174613)[0m top5: 0.7765858208955224
[2m[36m(func pid=174613)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=174613)[0m f1_macro: 0.21368909970677064
[2m[36m(func pid=174613)[0m f1_weighted: 0.24314251062036868
[2m[36m(func pid=174613)[0m f1_per_class: [0.213, 0.26, 0.478, 0.323, 0.051, 0.126, 0.235, 0.165, 0.165, 0.122]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7025 | Steps: 2 | Val loss: 2.5990 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0540 | Steps: 2 | Val loss: 1.7775 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0017 | Steps: 2 | Val loss: 1.7883 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6573 | Steps: 2 | Val loss: 2.0727 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 03:33:13 (running for 00:13:48.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.638 |      0.214 |                   72 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.047 |      0.313 |                   46 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.383 |                   45 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.702 |      0.173 |                   45 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.2140858208955224
[2m[36m(func pid=181391)[0m top5: 0.7569962686567164
[2m[36m(func pid=181391)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=181391)[0m f1_macro: 0.17334408554558678
[2m[36m(func pid=181391)[0m f1_weighted: 0.19745104299929644
[2m[36m(func pid=181391)[0m f1_per_class: [0.082, 0.443, 0.204, 0.0, 0.011, 0.247, 0.204, 0.482, 0.023, 0.036]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3498134328358209
[2m[36m(func pid=180330)[0m top5: 0.8903917910447762
[2m[36m(func pid=180330)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=180330)[0m f1_macro: 0.31432162441310607
[2m[36m(func pid=180330)[0m f1_weighted: 0.3633891509730575
[2m[36m(func pid=180330)[0m f1_per_class: [0.366, 0.388, 0.558, 0.443, 0.122, 0.137, 0.407, 0.237, 0.229, 0.256]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3978544776119403
[2m[36m(func pid=180846)[0m top5: 0.9235074626865671
[2m[36m(func pid=180846)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.3835928984950695
[2m[36m(func pid=180846)[0m f1_weighted: 0.4094038983066774
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.479, 0.632, 0.539, 0.137, 0.247, 0.336, 0.345, 0.261, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.2248134328358209
[2m[36m(func pid=174613)[0m top5: 0.7761194029850746
[2m[36m(func pid=174613)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=174613)[0m f1_macro: 0.20930269487775136
[2m[36m(func pid=174613)[0m f1_weighted: 0.24029216729475836
[2m[36m(func pid=174613)[0m f1_per_class: [0.21, 0.258, 0.449, 0.32, 0.051, 0.123, 0.23, 0.165, 0.163, 0.123]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7555 | Steps: 2 | Val loss: 2.7808 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0637 | Steps: 2 | Val loss: 1.7830 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5957 | Steps: 2 | Val loss: 2.0676 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7900 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:33:18 (running for 00:13:53.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.657 |      0.209 |                   73 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.054 |      0.314 |                   47 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.384 |                   46 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.756 |      0.156 |                   46 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.16884328358208955
[2m[36m(func pid=181391)[0m top5: 0.7490671641791045
[2m[36m(func pid=181391)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=181391)[0m f1_macro: 0.1556450814744131
[2m[36m(func pid=181391)[0m f1_weighted: 0.14168273532560657
[2m[36m(func pid=181391)[0m f1_per_class: [0.086, 0.385, 0.244, 0.003, 0.021, 0.265, 0.042, 0.48, 0.0, 0.029]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3460820895522388
[2m[36m(func pid=180330)[0m top5: 0.8913246268656716
[2m[36m(func pid=180330)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=180330)[0m f1_macro: 0.3129431888613961
[2m[36m(func pid=180330)[0m f1_weighted: 0.36072302742091816
[2m[36m(func pid=180330)[0m f1_per_class: [0.366, 0.39, 0.545, 0.431, 0.117, 0.137, 0.406, 0.249, 0.224, 0.263]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.39925373134328357
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=180846)[0m f1_macro: 0.38278436663414767
[2m[36m(func pid=180846)[0m f1_weighted: 0.40994557176715296
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.477, 0.632, 0.543, 0.143, 0.232, 0.342, 0.341, 0.262, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.22761194029850745
[2m[36m(func pid=174613)[0m top5: 0.7807835820895522
[2m[36m(func pid=174613)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=174613)[0m f1_macro: 0.2113509155732171
[2m[36m(func pid=174613)[0m f1_weighted: 0.24315155989070966
[2m[36m(func pid=174613)[0m f1_per_class: [0.213, 0.264, 0.449, 0.321, 0.052, 0.123, 0.234, 0.174, 0.161, 0.122]
[2m[36m(func pid=174613)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.8329 | Steps: 2 | Val loss: 2.6573 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0413 | Steps: 2 | Val loss: 1.7876 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=174613)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6120 | Steps: 2 | Val loss: 2.0612 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7923 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:33:24 (running for 00:13:58.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.374
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00004 | RUNNING    | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.596 |      0.211 |                   74 |
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.064 |      0.313 |                   48 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.383 |                   47 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.833 |      0.141 |                   47 |
| train_2d480_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.1333955223880597
[2m[36m(func pid=181391)[0m top5: 0.7770522388059702
[2m[36m(func pid=181391)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=181391)[0m f1_macro: 0.1411868420132381
[2m[36m(func pid=181391)[0m f1_weighted: 0.10461358852836296
[2m[36m(func pid=181391)[0m f1_per_class: [0.086, 0.24, 0.281, 0.0, 0.039, 0.28, 0.003, 0.454, 0.0, 0.028]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3978544776119403
[2m[36m(func pid=180846)[0m top5: 0.9235074626865671
[2m[36m(func pid=180846)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.379921703235914
[2m[36m(func pid=180846)[0m f1_weighted: 0.41026987936221765
[2m[36m(func pid=180846)[0m f1_per_class: [0.53, 0.479, 0.6, 0.538, 0.138, 0.226, 0.349, 0.343, 0.263, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=174613)[0m top1: 0.22994402985074627
[2m[36m(func pid=174613)[0m top5: 0.784981343283582
[2m[36m(func pid=174613)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=174613)[0m f1_macro: 0.21480004504472133
[2m[36m(func pid=174613)[0m f1_weighted: 0.2449260849207107
[2m[36m(func pid=174613)[0m f1_per_class: [0.223, 0.261, 0.468, 0.324, 0.055, 0.129, 0.238, 0.164, 0.165, 0.122]
[2m[36m(func pid=180330)[0m top1: 0.3414179104477612
[2m[36m(func pid=180330)[0m top5: 0.8899253731343284
[2m[36m(func pid=180330)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=180330)[0m f1_macro: 0.3104900738349823
[2m[36m(func pid=180330)[0m f1_weighted: 0.35589834221253924
[2m[36m(func pid=180330)[0m f1_per_class: [0.363, 0.37, 0.545, 0.442, 0.113, 0.163, 0.382, 0.251, 0.221, 0.254]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1736 | Steps: 2 | Val loss: 2.5198 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0377 | Steps: 2 | Val loss: 1.7961 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7985 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=181391)[0m top1: 0.13152985074626866
[2m[36m(func pid=181391)[0m top5: 0.7840485074626866
[2m[36m(func pid=181391)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=181391)[0m f1_macro: 0.1447538675264199
[2m[36m(func pid=181391)[0m f1_weighted: 0.1202187716975616
[2m[36m(func pid=181391)[0m f1_per_class: [0.087, 0.175, 0.267, 0.01, 0.038, 0.278, 0.08, 0.482, 0.0, 0.032]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33115671641791045
[2m[36m(func pid=180330)[0m top5: 0.8894589552238806
[2m[36m(func pid=180330)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=180330)[0m f1_macro: 0.30677920423874455
[2m[36m(func pid=180330)[0m f1_weighted: 0.3440458461821182
[2m[36m(func pid=180330)[0m f1_per_class: [0.368, 0.387, 0.545, 0.436, 0.107, 0.165, 0.337, 0.256, 0.214, 0.252]
[2m[36m(func pid=180846)[0m top1: 0.396455223880597
[2m[36m(func pid=180846)[0m top5: 0.9235074626865671
[2m[36m(func pid=180846)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=180846)[0m f1_macro: 0.379746063866677
[2m[36m(func pid=180846)[0m f1_weighted: 0.4101339403666108
[2m[36m(func pid=180846)[0m f1_per_class: [0.53, 0.481, 0.6, 0.531, 0.135, 0.219, 0.357, 0.333, 0.268, 0.343]
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.9091 | Steps: 2 | Val loss: 2.6044 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 03:33:29 (running for 00:14:04.11)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.041 |      0.31  |                   49 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.38  |                   48 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.174 |      0.145 |                   48 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=4220)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=4220)[0m Configuration completed!
[2m[36m(func pid=4220)[0m New optimizer parameters:
[2m[36m(func pid=4220)[0m SGD (
[2m[36m(func pid=4220)[0m Parameter Group 0
[2m[36m(func pid=4220)[0m     dampening: 0
[2m[36m(func pid=4220)[0m     differentiable: False
[2m[36m(func pid=4220)[0m     foreach: None
[2m[36m(func pid=4220)[0m     lr: 0.0001
[2m[36m(func pid=4220)[0m     maximize: False
[2m[36m(func pid=4220)[0m     momentum: 0.99
[2m[36m(func pid=4220)[0m     nesterov: False
[2m[36m(func pid=4220)[0m     weight_decay: 0.0001
[2m[36m(func pid=4220)[0m )
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:33:34 (running for 00:14:09.33)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.038 |      0.307 |                   50 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.38  |                   49 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.909 |      0.167 |                   49 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.15111940298507462
[2m[36m(func pid=181391)[0m top5: 0.7621268656716418
[2m[36m(func pid=181391)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=181391)[0m f1_macro: 0.16743276522715392
[2m[36m(func pid=181391)[0m f1_weighted: 0.1696357758080506
[2m[36m(func pid=181391)[0m f1_per_class: [0.093, 0.223, 0.29, 0.044, 0.025, 0.265, 0.188, 0.451, 0.071, 0.023]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.7887 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0345 | Steps: 2 | Val loss: 1.7981 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8087 | Steps: 2 | Val loss: 2.6046 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1630 | Steps: 2 | Val loss: 2.5156 | Batch size: 32 | lr: 0.0001 | Duration: 4.65s
[2m[36m(func pid=180846)[0m top1: 0.3987873134328358
[2m[36m(func pid=180846)[0m top5: 0.9249067164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=180846)[0m f1_macro: 0.3833790248726788
[2m[36m(func pid=180846)[0m f1_weighted: 0.4122597162673322
[2m[36m(func pid=180846)[0m f1_per_class: [0.538, 0.477, 0.6, 0.53, 0.141, 0.242, 0.358, 0.337, 0.261, 0.35]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.32975746268656714
[2m[36m(func pid=180330)[0m top5: 0.8931902985074627
[2m[36m(func pid=180330)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=180330)[0m f1_macro: 0.3080847466059569
[2m[36m(func pid=180330)[0m f1_weighted: 0.34259277648633263
[2m[36m(func pid=180330)[0m f1_per_class: [0.372, 0.38, 0.545, 0.44, 0.103, 0.163, 0.331, 0.267, 0.217, 0.263]
[2m[36m(func pid=180330)[0m 
== Status ==
Current time: 2024-01-07 03:33:39 (running for 00:14:14.75)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.034 |      0.308 |                   51 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.383 |                   50 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.809 |      0.155 |                   50 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.17537313432835822
[2m[36m(func pid=181391)[0m top5: 0.7686567164179104
[2m[36m(func pid=181391)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=181391)[0m f1_macro: 0.1552113146643002
[2m[36m(func pid=181391)[0m f1_weighted: 0.1932663104211793
[2m[36m(func pid=181391)[0m f1_per_class: [0.1, 0.331, 0.0, 0.141, 0.022, 0.267, 0.112, 0.498, 0.049, 0.032]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=4220)[0m top1: 0.06669776119402986
[2m[36m(func pid=4220)[0m top5: 0.4878731343283582
[2m[36m(func pid=4220)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=4220)[0m f1_macro: 0.03952861439028619
[2m[36m(func pid=4220)[0m f1_weighted: 0.03863504639826287
[2m[36m(func pid=4220)[0m f1_per_class: [0.114, 0.01, 0.0, 0.091, 0.0, 0.019, 0.0, 0.104, 0.022, 0.036]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0336 | Steps: 2 | Val loss: 1.7943 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0024 | Steps: 2 | Val loss: 1.7964 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.7200 | Steps: 2 | Val loss: 2.7127 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1460 | Steps: 2 | Val loss: 2.5352 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=180846)[0m top1: 0.394589552238806
[2m[36m(func pid=180846)[0m top5: 0.9235074626865671
[2m[36m(func pid=180846)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=180846)[0m f1_macro: 0.37860754627981985
[2m[36m(func pid=180846)[0m f1_weighted: 0.40964114274797436
[2m[36m(func pid=180846)[0m f1_per_class: [0.538, 0.478, 0.6, 0.517, 0.125, 0.234, 0.366, 0.332, 0.262, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.332089552238806
[2m[36m(func pid=180330)[0m top5: 0.8899253731343284
[2m[36m(func pid=180330)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=180330)[0m f1_macro: 0.3067660692145735
[2m[36m(func pid=180330)[0m f1_weighted: 0.34636306559049235
[2m[36m(func pid=180330)[0m f1_per_class: [0.365, 0.376, 0.545, 0.445, 0.101, 0.171, 0.34, 0.26, 0.211, 0.252]
[2m[36m(func pid=180330)[0m 
== Status ==
Current time: 2024-01-07 03:33:45 (running for 00:14:19.93)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.034 |      0.307 |                   52 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.379 |                   51 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.72  |      0.161 |                   51 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  3.163 |      0.04  |                    1 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.197294776119403
[2m[36m(func pid=181391)[0m top5: 0.7705223880597015
[2m[36m(func pid=181391)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=181391)[0m f1_macro: 0.16074623008799407
[2m[36m(func pid=181391)[0m f1_weighted: 0.1942651259066824
[2m[36m(func pid=181391)[0m f1_per_class: [0.094, 0.427, 0.0, 0.152, 0.015, 0.273, 0.046, 0.498, 0.068, 0.035]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=4220)[0m top1: 0.06343283582089553
[2m[36m(func pid=4220)[0m top5: 0.4846082089552239
[2m[36m(func pid=4220)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=4220)[0m f1_macro: 0.035000417223796786
[2m[36m(func pid=4220)[0m f1_weighted: 0.0353028039734456
[2m[36m(func pid=4220)[0m f1_per_class: [0.079, 0.01, 0.0, 0.079, 0.0, 0.025, 0.0, 0.102, 0.022, 0.032]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7852 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0314 | Steps: 2 | Val loss: 1.7979 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.2908 | Steps: 2 | Val loss: 2.6443 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1222 | Steps: 2 | Val loss: 2.5509 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=180846)[0m top1: 0.39972014925373134
[2m[36m(func pid=180846)[0m top5: 0.9230410447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=180846)[0m f1_macro: 0.3805904793586441
[2m[36m(func pid=180846)[0m f1_weighted: 0.41509981638375776
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.478, 0.6, 0.523, 0.128, 0.24, 0.377, 0.33, 0.272, 0.336]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3316231343283582
[2m[36m(func pid=180330)[0m top5: 0.8899253731343284
[2m[36m(func pid=180330)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=180330)[0m f1_macro: 0.30896921092857965
[2m[36m(func pid=180330)[0m f1_weighted: 0.3464218979913725
[2m[36m(func pid=180330)[0m f1_per_class: [0.358, 0.371, 0.571, 0.444, 0.103, 0.172, 0.343, 0.256, 0.222, 0.248]
[2m[36m(func pid=180330)[0m 
== Status ==
Current time: 2024-01-07 03:33:50 (running for 00:14:24.97)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.031 |      0.309 |                   53 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.381 |                   52 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.291 |      0.184 |                   52 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  3.146 |      0.035 |                    2 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m top1: 0.19682835820895522
[2m[36m(func pid=181391)[0m top5: 0.769589552238806
[2m[36m(func pid=181391)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=181391)[0m f1_macro: 0.18414753836011327
[2m[36m(func pid=181391)[0m f1_weighted: 0.17504096141719985
[2m[36m(func pid=181391)[0m f1_per_class: [0.088, 0.447, 0.298, 0.034, 0.013, 0.32, 0.057, 0.509, 0.039, 0.036]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=4220)[0m top1: 0.06063432835820896
[2m[36m(func pid=4220)[0m top5: 0.47201492537313433
[2m[36m(func pid=4220)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=4220)[0m f1_macro: 0.03070944013011397
[2m[36m(func pid=4220)[0m f1_weighted: 0.03319357738755142
[2m[36m(func pid=4220)[0m f1_per_class: [0.056, 0.014, 0.0, 0.076, 0.0, 0.019, 0.0, 0.1, 0.0, 0.043]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7773 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0492 | Steps: 2 | Val loss: 1.7970 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.0467 | Steps: 2 | Val loss: 2.5414 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0443 | Steps: 2 | Val loss: 2.5553 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m top1: 0.40205223880597013
[2m[36m(func pid=180846)[0m top5: 0.9221082089552238
[2m[36m(func pid=180846)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=180846)[0m f1_macro: 0.3808100989456341
[2m[36m(func pid=180846)[0m f1_weighted: 0.4158962009202335
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.486, 0.6, 0.529, 0.123, 0.238, 0.368, 0.337, 0.281, 0.328]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33348880597014924
[2m[36m(func pid=180330)[0m top5: 0.886660447761194
[2m[36m(func pid=180330)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=180330)[0m f1_macro: 0.3091636328780566
[2m[36m(func pid=180330)[0m f1_weighted: 0.3484089795002486
[2m[36m(func pid=180330)[0m f1_per_class: [0.356, 0.371, 0.571, 0.439, 0.104, 0.161, 0.358, 0.261, 0.221, 0.248]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.16837686567164178
[2m[36m(func pid=181391)[0m top5: 0.7719216417910447
[2m[36m(func pid=181391)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=181391)[0m f1_macro: 0.16702817999137892
[2m[36m(func pid=181391)[0m f1_weighted: 0.15892809095956395
[2m[36m(func pid=181391)[0m f1_per_class: [0.081, 0.247, 0.255, 0.01, 0.038, 0.339, 0.136, 0.508, 0.036, 0.02]
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:33:56 (running for 00:14:31.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.049 |      0.309 |                   54 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.381 |                   53 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.047 |      0.167 |                   53 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  3.044 |      0.035 |                    4 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.06203358208955224
[2m[36m(func pid=4220)[0m top5: 0.47154850746268656
[2m[36m(func pid=4220)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=4220)[0m f1_macro: 0.03507069776762721
[2m[36m(func pid=4220)[0m f1_weighted: 0.038854889958122175
[2m[36m(func pid=4220)[0m f1_per_class: [0.048, 0.028, 0.0, 0.086, 0.0, 0.019, 0.0, 0.097, 0.024, 0.048]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7928 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0299 | Steps: 2 | Val loss: 1.7980 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0545 | Steps: 2 | Val loss: 2.4097 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9492 | Steps: 2 | Val loss: 2.5488 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=180846)[0m top1: 0.4001865671641791
[2m[36m(func pid=180846)[0m top5: 0.9216417910447762
[2m[36m(func pid=180846)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.38096099616369006
[2m[36m(func pid=180846)[0m f1_weighted: 0.4123023104496501
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.489, 0.6, 0.526, 0.129, 0.248, 0.352, 0.345, 0.277, 0.325]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33675373134328357
[2m[36m(func pid=180330)[0m top5: 0.8875932835820896
[2m[36m(func pid=180330)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=180330)[0m f1_macro: 0.31083604275978083
[2m[36m(func pid=180330)[0m f1_weighted: 0.35245894237433545
[2m[36m(func pid=180330)[0m f1_per_class: [0.361, 0.368, 0.558, 0.443, 0.103, 0.164, 0.368, 0.259, 0.228, 0.256]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.17630597014925373
[2m[36m(func pid=181391)[0m top5: 0.7793843283582089
[2m[36m(func pid=181391)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=181391)[0m f1_macro: 0.16010027650152828
[2m[36m(func pid=181391)[0m f1_weighted: 0.16291708239017563
[2m[36m(func pid=181391)[0m f1_per_class: [0.081, 0.046, 0.257, 0.022, 0.065, 0.355, 0.257, 0.467, 0.021, 0.03]
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:34:01 (running for 00:14:36.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.03  |      0.311 |                   55 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.381 |                   54 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.054 |      0.16  |                   54 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.949 |      0.039 |                    5 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.06529850746268656
[2m[36m(func pid=4220)[0m top5: 0.46781716417910446
[2m[36m(func pid=4220)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=4220)[0m f1_macro: 0.038525363149596314
[2m[36m(func pid=4220)[0m f1_weighted: 0.04565757160952939
[2m[36m(func pid=4220)[0m f1_per_class: [0.041, 0.057, 0.0, 0.092, 0.0, 0.019, 0.0, 0.099, 0.024, 0.053]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7770 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0308 | Steps: 2 | Val loss: 1.7901 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.6492 | Steps: 2 | Val loss: 2.2925 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=180846)[0m top1: 0.40111940298507465
[2m[36m(func pid=180846)[0m top5: 0.9216417910447762
[2m[36m(func pid=180846)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=180846)[0m f1_macro: 0.3831553686457384
[2m[36m(func pid=180846)[0m f1_weighted: 0.4127001456121089
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.495, 0.615, 0.53, 0.13, 0.253, 0.346, 0.333, 0.274, 0.336]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8830 | Steps: 2 | Val loss: 2.5363 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=180330)[0m top1: 0.33675373134328357
[2m[36m(func pid=180330)[0m top5: 0.8927238805970149
[2m[36m(func pid=180330)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=180330)[0m f1_macro: 0.3104771224496996
[2m[36m(func pid=180330)[0m f1_weighted: 0.35391476233505886
[2m[36m(func pid=180330)[0m f1_per_class: [0.358, 0.376, 0.558, 0.438, 0.101, 0.159, 0.378, 0.252, 0.22, 0.265]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.17957089552238806
[2m[36m(func pid=181391)[0m top5: 0.804570895522388
[2m[36m(func pid=181391)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=181391)[0m f1_macro: 0.15747419757084763
[2m[36m(func pid=181391)[0m f1_weighted: 0.17115933875842304
[2m[36m(func pid=181391)[0m f1_per_class: [0.079, 0.083, 0.18, 0.04, 0.079, 0.36, 0.25, 0.451, 0.0, 0.052]
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:34:07 (running for 00:14:42.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.031 |      0.31  |                   56 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.383 |                   55 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.649 |      0.157 |                   55 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.883 |      0.044 |                    6 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.06856343283582089
[2m[36m(func pid=4220)[0m top5: 0.46455223880597013
[2m[36m(func pid=4220)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=4220)[0m f1_macro: 0.04407525798003141
[2m[36m(func pid=4220)[0m f1_weighted: 0.05341107768248797
[2m[36m(func pid=4220)[0m f1_per_class: [0.052, 0.078, 0.0, 0.102, 0.0, 0.024, 0.0, 0.101, 0.047, 0.038]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7799 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0255 | Steps: 2 | Val loss: 1.7931 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7297 | Steps: 2 | Val loss: 2.3577 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=180846)[0m top1: 0.40111940298507465
[2m[36m(func pid=180846)[0m top5: 0.9244402985074627
[2m[36m(func pid=180846)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=180846)[0m f1_macro: 0.38373274332556456
[2m[36m(func pid=180846)[0m f1_weighted: 0.40924227103978056
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.494, 0.6, 0.533, 0.132, 0.26, 0.326, 0.351, 0.28, 0.345]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7910 | Steps: 2 | Val loss: 2.5160 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=180330)[0m top1: 0.33908582089552236
[2m[36m(func pid=180330)[0m top5: 0.8894589552238806
[2m[36m(func pid=180330)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=180330)[0m f1_macro: 0.31173917935443807
[2m[36m(func pid=180330)[0m f1_weighted: 0.35666069138234907
[2m[36m(func pid=180330)[0m f1_per_class: [0.358, 0.38, 0.558, 0.435, 0.097, 0.159, 0.387, 0.248, 0.225, 0.27]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.14505597014925373
[2m[36m(func pid=181391)[0m top5: 0.8027052238805971
[2m[36m(func pid=181391)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=181391)[0m f1_macro: 0.1475646192017877
[2m[36m(func pid=181391)[0m f1_weighted: 0.13000180021929406
[2m[36m(func pid=181391)[0m f1_per_class: [0.08, 0.16, 0.175, 0.072, 0.062, 0.327, 0.042, 0.475, 0.042, 0.041]
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:34:12 (running for 00:14:47.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.025 |      0.312 |                   57 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.384 |                   56 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.73  |      0.148 |                   56 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.791 |      0.052 |                    7 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.07835820895522388
[2m[36m(func pid=4220)[0m top5: 0.46222014925373134
[2m[36m(func pid=4220)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=4220)[0m f1_macro: 0.05196634773027929
[2m[36m(func pid=4220)[0m f1_weighted: 0.06589262631172565
[2m[36m(func pid=4220)[0m f1_per_class: [0.069, 0.095, 0.0, 0.133, 0.0, 0.023, 0.0, 0.111, 0.043, 0.045]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7728 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0250 | Steps: 2 | Val loss: 1.7988 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.8190 | Steps: 2 | Val loss: 2.6066 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7021 | Steps: 2 | Val loss: 2.4910 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=180330)[0m top1: 0.33348880597014924
[2m[36m(func pid=180330)[0m top5: 0.8871268656716418
[2m[36m(func pid=180330)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=180330)[0m f1_macro: 0.30852792100349946
[2m[36m(func pid=180330)[0m f1_weighted: 0.35104675296300925
[2m[36m(func pid=180330)[0m f1_per_class: [0.36, 0.377, 0.558, 0.435, 0.096, 0.161, 0.369, 0.25, 0.222, 0.256]
[2m[36m(func pid=180846)[0m top1: 0.40205223880597013
[2m[36m(func pid=180846)[0m top5: 0.9249067164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=180846)[0m f1_macro: 0.38363631037459844
[2m[36m(func pid=180846)[0m f1_weighted: 0.4135901310822883
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.491, 0.615, 0.539, 0.133, 0.265, 0.338, 0.337, 0.269, 0.328]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.15205223880597016
[2m[36m(func pid=181391)[0m top5: 0.7910447761194029
[2m[36m(func pid=181391)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=181391)[0m f1_macro: 0.15900335664711923
[2m[36m(func pid=181391)[0m f1_weighted: 0.13890323736882157
[2m[36m(func pid=181391)[0m f1_per_class: [0.084, 0.269, 0.219, 0.08, 0.055, 0.299, 0.006, 0.488, 0.062, 0.026]
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:34:18 (running for 00:14:53.06)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.025 |      0.309 |                   58 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.384 |                   57 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.819 |      0.159 |                   57 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.702 |      0.058 |                    8 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.08069029850746269
[2m[36m(func pid=4220)[0m top5: 0.466884328358209
[2m[36m(func pid=4220)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=4220)[0m f1_macro: 0.057785642731844056
[2m[36m(func pid=4220)[0m f1_weighted: 0.07272858721753957
[2m[36m(func pid=4220)[0m f1_per_class: [0.065, 0.098, 0.0, 0.149, 0.0, 0.027, 0.0, 0.116, 0.088, 0.036]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0017 | Steps: 2 | Val loss: 1.7771 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0350 | Steps: 2 | Val loss: 1.7970 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.8785 | Steps: 2 | Val loss: 2.4843 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5684 | Steps: 2 | Val loss: 2.4686 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=180330)[0m top1: 0.3353544776119403
[2m[36m(func pid=180330)[0m top5: 0.8894589552238806
[2m[36m(func pid=180330)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=180330)[0m f1_macro: 0.30840509499346014
[2m[36m(func pid=180330)[0m f1_weighted: 0.3535923430982107
[2m[36m(func pid=180330)[0m f1_per_class: [0.346, 0.36, 0.558, 0.448, 0.097, 0.172, 0.371, 0.258, 0.22, 0.254]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=180846)[0m top1: 0.39925373134328357
[2m[36m(func pid=180846)[0m top5: 0.9225746268656716
[2m[36m(func pid=180846)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=180846)[0m f1_macro: 0.3838904301900113
[2m[36m(func pid=180846)[0m f1_weighted: 0.41043420060914976
[2m[36m(func pid=180846)[0m f1_per_class: [0.511, 0.491, 0.649, 0.538, 0.136, 0.269, 0.329, 0.325, 0.274, 0.317]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.21222014925373134
[2m[36m(func pid=181391)[0m top5: 0.7957089552238806
[2m[36m(func pid=181391)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=181391)[0m f1_macro: 0.17433125282141756
[2m[36m(func pid=181391)[0m f1_weighted: 0.18206814827657036
[2m[36m(func pid=181391)[0m f1_per_class: [0.083, 0.454, 0.212, 0.118, 0.023, 0.31, 0.009, 0.485, 0.049, 0.0]
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:34:23 (running for 00:14:58.49)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.035 |      0.308 |                   59 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.384 |                   58 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.879 |      0.174 |                   58 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.568 |      0.061 |                    9 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.08115671641791045
[2m[36m(func pid=4220)[0m top5: 0.46455223880597013
[2m[36m(func pid=4220)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=4220)[0m f1_macro: 0.06092974015763789
[2m[36m(func pid=4220)[0m f1_weighted: 0.07818667098032545
[2m[36m(func pid=4220)[0m f1_per_class: [0.081, 0.118, 0.0, 0.157, 0.0, 0.025, 0.0, 0.108, 0.092, 0.029]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7877 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0595 | Steps: 2 | Val loss: 1.7890 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 4.2539 | Steps: 2 | Val loss: 2.3255 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=180846)[0m top1: 0.39972014925373134
[2m[36m(func pid=180846)[0m top5: 0.9211753731343284
[2m[36m(func pid=180846)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=180846)[0m f1_macro: 0.3863443699129109
[2m[36m(func pid=180846)[0m f1_weighted: 0.40926507328712525
[2m[36m(func pid=180846)[0m f1_per_class: [0.507, 0.478, 0.649, 0.543, 0.137, 0.271, 0.323, 0.348, 0.278, 0.331]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33722014925373134
[2m[36m(func pid=180330)[0m top5: 0.8950559701492538
[2m[36m(func pid=180330)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=180330)[0m f1_macro: 0.31635188577314327
[2m[36m(func pid=180330)[0m f1_weighted: 0.35612510677632236
[2m[36m(func pid=180330)[0m f1_per_class: [0.363, 0.379, 0.6, 0.444, 0.095, 0.168, 0.372, 0.256, 0.225, 0.263]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.2224813432835821
[2m[36m(func pid=181391)[0m top5: 0.8078358208955224
[2m[36m(func pid=181391)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=181391)[0m f1_macro: 0.19110249199544863
[2m[36m(func pid=181391)[0m f1_weighted: 0.21731298641675556
[2m[36m(func pid=181391)[0m f1_per_class: [0.089, 0.434, 0.23, 0.191, 0.018, 0.344, 0.054, 0.506, 0.045, 0.0]
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4274 | Steps: 2 | Val loss: 2.4463 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=181391)[0m 
== Status ==
Current time: 2024-01-07 03:34:29 (running for 00:15:03.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.06  |      0.316 |                   60 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.386 |                   59 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  4.254 |      0.191 |                   59 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.427 |      0.073 |                   10 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7675 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=4220)[0m top1: 0.08908582089552239
[2m[36m(func pid=4220)[0m top5: 0.46828358208955223
[2m[36m(func pid=4220)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=4220)[0m f1_macro: 0.07281977822473562
[2m[36m(func pid=4220)[0m f1_weighted: 0.08884345161053672
[2m[36m(func pid=4220)[0m f1_per_class: [0.086, 0.133, 0.067, 0.168, 0.0, 0.038, 0.009, 0.122, 0.077, 0.029]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0298 | Steps: 2 | Val loss: 1.7860 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.1419 | Steps: 2 | Val loss: 2.2800 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=180846)[0m top1: 0.40158582089552236
[2m[36m(func pid=180846)[0m top5: 0.9230410447761194
[2m[36m(func pid=180846)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=180846)[0m f1_macro: 0.3833662726208722
[2m[36m(func pid=180846)[0m f1_weighted: 0.41322965100173636
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.484, 0.632, 0.543, 0.131, 0.264, 0.338, 0.332, 0.275, 0.317]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3381529850746269
[2m[36m(func pid=180330)[0m top5: 0.8936567164179104
[2m[36m(func pid=180330)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=180330)[0m f1_macro: 0.31573367101624333
[2m[36m(func pid=180330)[0m f1_weighted: 0.3556906879605908
[2m[36m(func pid=180330)[0m f1_per_class: [0.368, 0.384, 0.571, 0.44, 0.096, 0.158, 0.373, 0.26, 0.231, 0.275]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.2523320895522388
[2m[36m(func pid=181391)[0m top5: 0.8143656716417911
[2m[36m(func pid=181391)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=181391)[0m f1_macro: 0.20902935057452457
[2m[36m(func pid=181391)[0m f1_weighted: 0.29605662797332793
[2m[36m(func pid=181391)[0m f1_per_class: [0.082, 0.338, 0.227, 0.264, 0.059, 0.345, 0.324, 0.391, 0.059, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3045 | Steps: 2 | Val loss: 2.4246 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7666 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 03:34:34 (running for 00:15:09.28)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.03  |      0.316 |                   61 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.383 |                   60 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.142 |      0.209 |                   60 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.304 |      0.088 |                   11 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.09654850746268656
[2m[36m(func pid=4220)[0m top5: 0.4664179104477612
[2m[36m(func pid=4220)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=4220)[0m f1_macro: 0.08783089541535498
[2m[36m(func pid=4220)[0m f1_weighted: 0.09876868119412273
[2m[36m(func pid=4220)[0m f1_per_class: [0.09, 0.145, 0.133, 0.168, 0.0, 0.07, 0.018, 0.13, 0.09, 0.035]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0243 | Steps: 2 | Val loss: 1.7916 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.6199 | Steps: 2 | Val loss: 2.5043 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=180846)[0m top1: 0.4025186567164179
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=180846)[0m f1_macro: 0.3854002251082191
[2m[36m(func pid=180846)[0m f1_weighted: 0.4125751889681888
[2m[36m(func pid=180846)[0m f1_per_class: [0.514, 0.484, 0.649, 0.545, 0.137, 0.268, 0.331, 0.339, 0.276, 0.311]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3358208955223881
[2m[36m(func pid=180330)[0m top5: 0.8927238805970149
[2m[36m(func pid=180330)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=180330)[0m f1_macro: 0.3129786270243755
[2m[36m(func pid=180330)[0m f1_weighted: 0.35130860129225444
[2m[36m(func pid=180330)[0m f1_per_class: [0.368, 0.387, 0.558, 0.443, 0.097, 0.16, 0.353, 0.264, 0.22, 0.278]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.22108208955223882
[2m[36m(func pid=181391)[0m top5: 0.7957089552238806
[2m[36m(func pid=181391)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=181391)[0m f1_macro: 0.16826833833178179
[2m[36m(func pid=181391)[0m f1_weighted: 0.24794598536390827
[2m[36m(func pid=181391)[0m f1_per_class: [0.092, 0.011, 0.214, 0.219, 0.051, 0.345, 0.415, 0.299, 0.038, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1831 | Steps: 2 | Val loss: 2.4051 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7601 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:34:40 (running for 00:15:14.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.024 |      0.313 |                   62 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.385 |                   61 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.62  |      0.168 |                   61 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.183 |      0.096 |                   12 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.09841417910447761
[2m[36m(func pid=4220)[0m top5: 0.47388059701492535
[2m[36m(func pid=4220)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=4220)[0m f1_macro: 0.09627763286052742
[2m[36m(func pid=4220)[0m f1_weighted: 0.10485766745677166
[2m[36m(func pid=4220)[0m f1_per_class: [0.087, 0.148, 0.2, 0.173, 0.0, 0.078, 0.03, 0.122, 0.089, 0.037]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0254 | Steps: 2 | Val loss: 1.8017 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.6296 | Steps: 2 | Val loss: 2.5295 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=180846)[0m top1: 0.40718283582089554
[2m[36m(func pid=180846)[0m top5: 0.9267723880597015
[2m[36m(func pid=180846)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=180846)[0m f1_macro: 0.39056033444883903
[2m[36m(func pid=180846)[0m f1_weighted: 0.4168497306894934
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.487, 0.649, 0.548, 0.15, 0.271, 0.337, 0.351, 0.272, 0.322]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33115671641791045
[2m[36m(func pid=180330)[0m top5: 0.8903917910447762
[2m[36m(func pid=180330)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=180330)[0m f1_macro: 0.3106385760132683
[2m[36m(func pid=180330)[0m f1_weighted: 0.34514579574067594
[2m[36m(func pid=180330)[0m f1_per_class: [0.374, 0.393, 0.558, 0.442, 0.091, 0.155, 0.332, 0.26, 0.231, 0.27]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=181391)[0m top1: 0.20009328358208955
[2m[36m(func pid=181391)[0m top5: 0.7952425373134329
[2m[36m(func pid=181391)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=181391)[0m f1_macro: 0.1638590845459305
[2m[36m(func pid=181391)[0m f1_weighted: 0.2191349444414879
[2m[36m(func pid=181391)[0m f1_per_class: [0.098, 0.016, 0.217, 0.171, 0.054, 0.358, 0.355, 0.287, 0.036, 0.047]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0786 | Steps: 2 | Val loss: 2.3833 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7677 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 03:34:45 (running for 00:15:20.41)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.025 |      0.311 |                   63 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.391 |                   62 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.63  |      0.164 |                   62 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.079 |      0.097 |                   13 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0234 | Steps: 2 | Val loss: 1.8056 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0660 | Steps: 2 | Val loss: 2.4483 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=4220)[0m top1: 0.09841417910447761
[2m[36m(func pid=4220)[0m top5: 0.4916044776119403
[2m[36m(func pid=4220)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=4220)[0m f1_macro: 0.09664294316575033
[2m[36m(func pid=4220)[0m f1_weighted: 0.1057057693265473
[2m[36m(func pid=4220)[0m f1_per_class: [0.091, 0.145, 0.193, 0.158, 0.0, 0.086, 0.044, 0.127, 0.081, 0.041]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4048507462686567
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=180846)[0m f1_macro: 0.3885886110701638
[2m[36m(func pid=180846)[0m f1_weighted: 0.416982687241716
[2m[36m(func pid=180846)[0m f1_per_class: [0.504, 0.486, 0.667, 0.546, 0.144, 0.255, 0.349, 0.34, 0.271, 0.325]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.15485074626865672
[2m[36m(func pid=181391)[0m top5: 0.7924440298507462
[2m[36m(func pid=181391)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=181391)[0m f1_macro: 0.14795205083599414
[2m[36m(func pid=181391)[0m f1_weighted: 0.14165048753050258
[2m[36m(func pid=181391)[0m f1_per_class: [0.09, 0.127, 0.18, 0.073, 0.058, 0.356, 0.1, 0.4, 0.059, 0.036]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3269589552238806
[2m[36m(func pid=180330)[0m top5: 0.8880597014925373
[2m[36m(func pid=180330)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=180330)[0m f1_macro: 0.30770274373003675
[2m[36m(func pid=180330)[0m f1_weighted: 0.33834325071435817
[2m[36m(func pid=180330)[0m f1_per_class: [0.372, 0.391, 0.558, 0.439, 0.093, 0.153, 0.313, 0.265, 0.232, 0.261]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.0160 | Steps: 2 | Val loss: 2.3673 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7674 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:34:50 (running for 00:15:25.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.023 |      0.308 |                   64 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.389 |                   63 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  2.066 |      0.148 |                   63 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  2.016 |      0.098 |                   14 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.5399 | Steps: 2 | Val loss: 2.3239 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=4220)[0m top1: 0.10074626865671642
[2m[36m(func pid=4220)[0m top5: 0.5074626865671642
[2m[36m(func pid=4220)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=4220)[0m f1_macro: 0.09755602906464732
[2m[36m(func pid=4220)[0m f1_weighted: 0.11190096633650919
[2m[36m(func pid=4220)[0m f1_per_class: [0.094, 0.149, 0.178, 0.149, 0.0, 0.09, 0.071, 0.115, 0.089, 0.04]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0267 | Steps: 2 | Val loss: 1.8049 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=180846)[0m top1: 0.4039179104477612
[2m[36m(func pid=180846)[0m top5: 0.9225746268656716
[2m[36m(func pid=180846)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=180846)[0m f1_macro: 0.3879291204498961
[2m[36m(func pid=180846)[0m f1_weighted: 0.41727055918503986
[2m[36m(func pid=180846)[0m f1_per_class: [0.511, 0.474, 0.667, 0.547, 0.155, 0.25, 0.359, 0.338, 0.266, 0.314]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.1912313432835821
[2m[36m(func pid=181391)[0m top5: 0.8017723880597015
[2m[36m(func pid=181391)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=181391)[0m f1_macro: 0.16687610152260487
[2m[36m(func pid=181391)[0m f1_weighted: 0.16240212101523716
[2m[36m(func pid=181391)[0m f1_per_class: [0.098, 0.378, 0.18, 0.019, 0.056, 0.357, 0.065, 0.454, 0.062, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3278917910447761
[2m[36m(func pid=180330)[0m top5: 0.8899253731343284
[2m[36m(func pid=180330)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=180330)[0m f1_macro: 0.30629925437917066
[2m[36m(func pid=180330)[0m f1_weighted: 0.33741445084474114
[2m[36m(func pid=180330)[0m f1_per_class: [0.368, 0.385, 0.545, 0.448, 0.1, 0.156, 0.305, 0.267, 0.218, 0.27]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8193 | Steps: 2 | Val loss: 2.3462 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7580 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7023 | Steps: 2 | Val loss: 2.3035 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 03:34:56 (running for 00:15:31.15)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.027 |      0.306 |                   65 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.388 |                   64 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.54  |      0.167 |                   64 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.819 |      0.096 |                   15 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.10401119402985075
[2m[36m(func pid=4220)[0m top5: 0.5307835820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=4220)[0m f1_macro: 0.09586224924028712
[2m[36m(func pid=4220)[0m f1_weighted: 0.11744363514096491
[2m[36m(func pid=4220)[0m f1_per_class: [0.097, 0.15, 0.155, 0.145, 0.006, 0.094, 0.099, 0.077, 0.084, 0.051]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0273 | Steps: 2 | Val loss: 1.8082 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=180846)[0m top1: 0.40578358208955223
[2m[36m(func pid=180846)[0m top5: 0.9225746268656716
[2m[36m(func pid=180846)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=180846)[0m f1_macro: 0.3904834241229268
[2m[36m(func pid=180846)[0m f1_weighted: 0.4207099612693256
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.478, 0.667, 0.545, 0.143, 0.251, 0.37, 0.33, 0.26, 0.339]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.166044776119403
[2m[36m(func pid=181391)[0m top5: 0.8097014925373134
[2m[36m(func pid=181391)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=181391)[0m f1_macro: 0.13827883708575
[2m[36m(func pid=181391)[0m f1_weighted: 0.14410927253961953
[2m[36m(func pid=181391)[0m f1_per_class: [0.103, 0.241, 0.0, 0.01, 0.068, 0.339, 0.1, 0.46, 0.061, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3278917910447761
[2m[36m(func pid=180330)[0m top5: 0.886660447761194
[2m[36m(func pid=180330)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=180330)[0m f1_macro: 0.30462251114063515
[2m[36m(func pid=180330)[0m f1_weighted: 0.3365372131981376
[2m[36m(func pid=180330)[0m f1_per_class: [0.366, 0.39, 0.545, 0.453, 0.098, 0.156, 0.295, 0.27, 0.214, 0.259]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6891 | Steps: 2 | Val loss: 2.3293 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.7715 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.5557 | Steps: 2 | Val loss: 2.3417 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0382 | Steps: 2 | Val loss: 1.8065 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:35:01 (running for 00:15:36.70)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.027 |      0.305 |                   66 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.39  |                   65 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.702 |      0.138 |                   65 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.689 |      0.096 |                   16 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.11007462686567164
[2m[36m(func pid=4220)[0m top5: 0.5485074626865671
[2m[36m(func pid=4220)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=4220)[0m f1_macro: 0.09587983961967238
[2m[36m(func pid=4220)[0m f1_weighted: 0.12656927669648058
[2m[36m(func pid=4220)[0m f1_per_class: [0.103, 0.151, 0.124, 0.129, 0.006, 0.099, 0.147, 0.049, 0.09, 0.061]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.40345149253731344
[2m[36m(func pid=180846)[0m top5: 0.9249067164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=180846)[0m f1_macro: 0.38460505168348497
[2m[36m(func pid=180846)[0m f1_weighted: 0.41665164643914254
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.478, 0.632, 0.547, 0.144, 0.25, 0.355, 0.34, 0.263, 0.324]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.16977611940298507
[2m[36m(func pid=181391)[0m top5: 0.8069029850746269
[2m[36m(func pid=181391)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=181391)[0m f1_macro: 0.16642404182816262
[2m[36m(func pid=181391)[0m f1_weighted: 0.1697414497599445
[2m[36m(func pid=181391)[0m f1_per_class: [0.098, 0.152, 0.241, 0.125, 0.057, 0.333, 0.126, 0.471, 0.061, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33115671641791045
[2m[36m(func pid=180330)[0m top5: 0.8908582089552238
[2m[36m(func pid=180330)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=180330)[0m f1_macro: 0.3050770233421631
[2m[36m(func pid=180330)[0m f1_weighted: 0.3376366798224445
[2m[36m(func pid=180330)[0m f1_per_class: [0.381, 0.384, 0.522, 0.464, 0.095, 0.153, 0.29, 0.28, 0.218, 0.263]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7038 | Steps: 2 | Val loss: 2.3028 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.7785 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.5151 | Steps: 2 | Val loss: 2.3752 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0346 | Steps: 2 | Val loss: 1.8033 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:35:07 (running for 00:15:42.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.038 |      0.305 |                   67 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.002 |      0.385 |                   66 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.556 |      0.166 |                   66 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.704 |      0.108 |                   17 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.12686567164179105
[2m[36m(func pid=4220)[0m top5: 0.5778917910447762
[2m[36m(func pid=4220)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=4220)[0m f1_macro: 0.10813837905735561
[2m[36m(func pid=4220)[0m f1_weighted: 0.14530276559450372
[2m[36m(func pid=4220)[0m f1_per_class: [0.115, 0.155, 0.143, 0.13, 0.023, 0.102, 0.204, 0.04, 0.097, 0.072]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3978544776119403
[2m[36m(func pid=180846)[0m top5: 0.9239738805970149
[2m[36m(func pid=180846)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.3793431497447356
[2m[36m(func pid=180846)[0m f1_weighted: 0.41106671347372603
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.474, 0.6, 0.536, 0.12, 0.255, 0.347, 0.341, 0.263, 0.336]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.21128731343283583
[2m[36m(func pid=181391)[0m top5: 0.8073694029850746
[2m[36m(func pid=181391)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=181391)[0m f1_macro: 0.1827571203816305
[2m[36m(func pid=181391)[0m f1_weighted: 0.23190703640234842
[2m[36m(func pid=181391)[0m f1_per_class: [0.099, 0.05, 0.165, 0.273, 0.064, 0.359, 0.238, 0.5, 0.079, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3316231343283582
[2m[36m(func pid=180330)[0m top5: 0.8889925373134329
[2m[36m(func pid=180330)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=180330)[0m f1_macro: 0.30504112445733356
[2m[36m(func pid=180330)[0m f1_weighted: 0.33734604496053516
[2m[36m(func pid=180330)[0m f1_per_class: [0.381, 0.382, 0.522, 0.466, 0.097, 0.146, 0.291, 0.278, 0.219, 0.268]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4190 | Steps: 2 | Val loss: 2.2820 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.7750 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4689 | Steps: 2 | Val loss: 2.4143 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0306 | Steps: 2 | Val loss: 1.7980 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:35:12 (running for 00:15:47.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.035 |      0.305 |                   68 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.379 |                   67 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.515 |      0.183 |                   67 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.419 |      0.109 |                   18 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.13619402985074627
[2m[36m(func pid=4220)[0m top5: 0.5993470149253731
[2m[36m(func pid=4220)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=4220)[0m f1_macro: 0.10918392961781838
[2m[36m(func pid=4220)[0m f1_weighted: 0.15435807331936405
[2m[36m(func pid=4220)[0m f1_per_class: [0.12, 0.17, 0.127, 0.12, 0.024, 0.104, 0.24, 0.014, 0.098, 0.076]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4025186567164179
[2m[36m(func pid=180846)[0m top5: 0.9235074626865671
[2m[36m(func pid=180846)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=180846)[0m f1_macro: 0.38563067045786636
[2m[36m(func pid=180846)[0m f1_weighted: 0.41530511465038683
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.478, 0.649, 0.548, 0.129, 0.261, 0.346, 0.335, 0.262, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.19029850746268656
[2m[36m(func pid=181391)[0m top5: 0.7994402985074627
[2m[36m(func pid=181391)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=181391)[0m f1_macro: 0.17380268423879627
[2m[36m(func pid=181391)[0m f1_weighted: 0.22213734108104627
[2m[36m(func pid=181391)[0m f1_per_class: [0.092, 0.088, 0.161, 0.133, 0.078, 0.354, 0.34, 0.373, 0.076, 0.043]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33255597014925375
[2m[36m(func pid=180330)[0m top5: 0.8931902985074627
[2m[36m(func pid=180330)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=180330)[0m f1_macro: 0.3043553590138307
[2m[36m(func pid=180330)[0m f1_weighted: 0.33946575115550176
[2m[36m(func pid=180330)[0m f1_per_class: [0.383, 0.374, 0.522, 0.471, 0.101, 0.157, 0.295, 0.276, 0.217, 0.248]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.3779 | Steps: 2 | Val loss: 2.2608 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7830 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3106 | Steps: 2 | Val loss: 2.5155 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0450 | Steps: 2 | Val loss: 1.7858 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:35:17 (running for 00:15:52.72)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.031 |      0.304 |                   69 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.386 |                   68 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.469 |      0.174 |                   68 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.378 |      0.114 |                   19 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.15065298507462688
[2m[36m(func pid=4220)[0m top5: 0.6245335820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=4220)[0m f1_macro: 0.11446280723245487
[2m[36m(func pid=4220)[0m f1_weighted: 0.16984251250523152
[2m[36m(func pid=4220)[0m f1_per_class: [0.119, 0.167, 0.116, 0.118, 0.026, 0.111, 0.294, 0.0, 0.104, 0.089]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.3978544776119403
[2m[36m(func pid=180846)[0m top5: 0.9249067164179104
[2m[36m(func pid=180846)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=180846)[0m f1_macro: 0.3798431838581363
[2m[36m(func pid=180846)[0m f1_weighted: 0.41046067401712644
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.478, 0.6, 0.539, 0.126, 0.261, 0.338, 0.333, 0.263, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.17397388059701493
[2m[36m(func pid=181391)[0m top5: 0.7919776119402985
[2m[36m(func pid=181391)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=181391)[0m f1_macro: 0.1593661780537718
[2m[36m(func pid=181391)[0m f1_weighted: 0.20548085144651482
[2m[36m(func pid=181391)[0m f1_per_class: [0.084, 0.177, 0.164, 0.049, 0.086, 0.349, 0.339, 0.257, 0.06, 0.029]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3344216417910448
[2m[36m(func pid=180330)[0m top5: 0.8931902985074627
[2m[36m(func pid=180330)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=180330)[0m f1_macro: 0.3056481373993333
[2m[36m(func pid=180330)[0m f1_weighted: 0.3411914347179219
[2m[36m(func pid=180330)[0m f1_per_class: [0.373, 0.37, 0.522, 0.477, 0.102, 0.168, 0.294, 0.269, 0.223, 0.259]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.2233 | Steps: 2 | Val loss: 2.2433 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7734 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6477 | Steps: 2 | Val loss: 2.4633 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0284 | Steps: 2 | Val loss: 1.7891 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 03:35:23 (running for 00:15:58.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.045 |      0.306 |                   70 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.38  |                   69 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.311 |      0.159 |                   69 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.223 |      0.119 |                   20 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.16511194029850745
[2m[36m(func pid=4220)[0m top5: 0.6427238805970149
[2m[36m(func pid=4220)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=4220)[0m f1_macro: 0.11948162509556884
[2m[36m(func pid=4220)[0m f1_weighted: 0.18296921291059937
[2m[36m(func pid=4220)[0m f1_per_class: [0.123, 0.156, 0.12, 0.108, 0.026, 0.124, 0.349, 0.0, 0.106, 0.082]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4001865671641791
[2m[36m(func pid=180846)[0m top5: 0.9253731343283582
[2m[36m(func pid=180846)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.3851633731674978
[2m[36m(func pid=180846)[0m f1_weighted: 0.41052769115693244
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.486, 0.649, 0.539, 0.126, 0.258, 0.333, 0.345, 0.258, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.22014925373134328
[2m[36m(func pid=181391)[0m top5: 0.8003731343283582
[2m[36m(func pid=181391)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=181391)[0m f1_macro: 0.19006696158101408
[2m[36m(func pid=181391)[0m f1_weighted: 0.25609683978407355
[2m[36m(func pid=181391)[0m f1_per_class: [0.084, 0.378, 0.169, 0.063, 0.108, 0.34, 0.379, 0.257, 0.088, 0.034]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3344216417910448
[2m[36m(func pid=180330)[0m top5: 0.8927238805970149
[2m[36m(func pid=180330)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=180330)[0m f1_macro: 0.3059235760722135
[2m[36m(func pid=180330)[0m f1_weighted: 0.3382626884807258
[2m[36m(func pid=180330)[0m f1_per_class: [0.381, 0.377, 0.522, 0.475, 0.106, 0.156, 0.287, 0.269, 0.221, 0.265]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2360 | Steps: 2 | Val loss: 2.2156 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7672 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.6951 | Steps: 2 | Val loss: 2.2912 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0220 | Steps: 2 | Val loss: 1.7886 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 03:35:28 (running for 00:16:03.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.028 |      0.306 |                   71 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.385 |                   70 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.648 |      0.19  |                   70 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.236 |      0.126 |                   21 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.17957089552238806
[2m[36m(func pid=4220)[0m top5: 0.6595149253731343
[2m[36m(func pid=4220)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=4220)[0m f1_macro: 0.12622234739886506
[2m[36m(func pid=4220)[0m f1_weighted: 0.19484939605000592
[2m[36m(func pid=4220)[0m f1_per_class: [0.127, 0.162, 0.118, 0.106, 0.027, 0.114, 0.389, 0.0, 0.108, 0.111]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.404384328358209
[2m[36m(func pid=180846)[0m top5: 0.925839552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=180846)[0m f1_macro: 0.3916491175901857
[2m[36m(func pid=180846)[0m f1_weighted: 0.4160815981854477
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.495, 0.686, 0.54, 0.123, 0.251, 0.347, 0.341, 0.26, 0.352]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.3185634328358209
[2m[36m(func pid=181391)[0m top5: 0.820429104477612
[2m[36m(func pid=181391)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=181391)[0m f1_macro: 0.22783038701148936
[2m[36m(func pid=181391)[0m f1_weighted: 0.3557811019649432
[2m[36m(func pid=181391)[0m f1_per_class: [0.097, 0.453, 0.168, 0.295, 0.071, 0.321, 0.448, 0.329, 0.073, 0.023]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33488805970149255
[2m[36m(func pid=180330)[0m top5: 0.8927238805970149
[2m[36m(func pid=180330)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=180330)[0m f1_macro: 0.3090792727210355
[2m[36m(func pid=180330)[0m f1_weighted: 0.33949995932800364
[2m[36m(func pid=180330)[0m f1_per_class: [0.383, 0.374, 0.545, 0.471, 0.107, 0.156, 0.295, 0.273, 0.216, 0.27]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.1258 | Steps: 2 | Val loss: 2.1949 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7727 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2370 | Steps: 2 | Val loss: 2.3074 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0395 | Steps: 2 | Val loss: 1.7835 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 03:35:33 (running for 00:16:08.42)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.022 |      0.309 |                   72 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.392 |                   71 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.695 |      0.228 |                   71 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  1.126 |      0.132 |                   22 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.20102611940298507
[2m[36m(func pid=4220)[0m top5: 0.6767723880597015
[2m[36m(func pid=4220)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=4220)[0m f1_macro: 0.13178521658835388
[2m[36m(func pid=4220)[0m f1_weighted: 0.2122161323330072
[2m[36m(func pid=4220)[0m f1_per_class: [0.128, 0.163, 0.122, 0.113, 0.035, 0.114, 0.441, 0.0, 0.105, 0.096]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4048507462686567
[2m[36m(func pid=180846)[0m top5: 0.9253731343283582
[2m[36m(func pid=180846)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=180846)[0m f1_macro: 0.3854771709549133
[2m[36m(func pid=180846)[0m f1_weighted: 0.4183280981864376
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.487, 0.632, 0.538, 0.12, 0.25, 0.362, 0.338, 0.265, 0.339]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.3736007462686567
[2m[36m(func pid=181391)[0m top5: 0.8278917910447762
[2m[36m(func pid=181391)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=181391)[0m f1_macro: 0.2498852805755712
[2m[36m(func pid=181391)[0m f1_weighted: 0.39609905712183685
[2m[36m(func pid=181391)[0m f1_per_class: [0.111, 0.391, 0.167, 0.466, 0.089, 0.332, 0.432, 0.452, 0.059, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.333955223880597
[2m[36m(func pid=180330)[0m top5: 0.8950559701492538
[2m[36m(func pid=180330)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=180330)[0m f1_macro: 0.30669185755531114
[2m[36m(func pid=180330)[0m f1_weighted: 0.3404482744819048
[2m[36m(func pid=180330)[0m f1_per_class: [0.389, 0.372, 0.522, 0.468, 0.105, 0.156, 0.304, 0.265, 0.214, 0.273]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.9279 | Steps: 2 | Val loss: 2.1760 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7766 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4321 | Steps: 2 | Val loss: 2.5487 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0245 | Steps: 2 | Val loss: 1.7819 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=4220)[0m top1: 0.21595149253731344
[2m[36m(func pid=4220)[0m top5: 0.6888992537313433
[2m[36m(func pid=4220)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=4220)[0m f1_macro: 0.1444240654513715
[2m[36m(func pid=4220)[0m f1_weighted: 0.224703513589411
[2m[36m(func pid=4220)[0m f1_per_class: [0.134, 0.176, 0.118, 0.118, 0.043, 0.119, 0.464, 0.0, 0.119, 0.152]
== Status ==
Current time: 2024-01-07 03:35:38 (running for 00:16:13.72)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.04  |      0.307 |                   73 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.385 |                   72 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.237 |      0.25  |                   72 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.928 |      0.144 |                   23 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4006529850746269
[2m[36m(func pid=180846)[0m top5: 0.9253731343283582
[2m[36m(func pid=180846)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=180846)[0m f1_macro: 0.3844569375327537
[2m[36m(func pid=180846)[0m f1_weighted: 0.4119170566209595
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.492, 0.632, 0.533, 0.117, 0.255, 0.34, 0.344, 0.264, 0.345]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.3138992537313433
[2m[36m(func pid=181391)[0m top5: 0.8260261194029851
[2m[36m(func pid=181391)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=181391)[0m f1_macro: 0.20841215693100829
[2m[36m(func pid=181391)[0m f1_weighted: 0.3241453007947527
[2m[36m(func pid=181391)[0m f1_per_class: [0.0, 0.299, 0.18, 0.451, 0.076, 0.336, 0.275, 0.385, 0.083, 0.0]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.33675373134328357
[2m[36m(func pid=180330)[0m top5: 0.8964552238805971
[2m[36m(func pid=180330)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=180330)[0m f1_macro: 0.30850762578124
[2m[36m(func pid=180330)[0m f1_weighted: 0.34228922966964215
[2m[36m(func pid=180330)[0m f1_per_class: [0.383, 0.377, 0.522, 0.473, 0.109, 0.165, 0.297, 0.278, 0.214, 0.268]
[2m[36m(func pid=180330)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.9368 | Steps: 2 | Val loss: 2.1563 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7700 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 3.2981 | Steps: 2 | Val loss: 2.5903 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=180330)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0200 | Steps: 2 | Val loss: 1.7871 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 03:35:44 (running for 00:16:19.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.357
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00005 | RUNNING    | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.025 |      0.309 |                   74 |
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.384 |                   73 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.432 |      0.208 |                   73 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.937 |      0.148 |                   24 |
| train_2d480_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.22434701492537312
[2m[36m(func pid=4220)[0m top5: 0.7052238805970149
[2m[36m(func pid=4220)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=4220)[0m f1_macro: 0.14847439536384358
[2m[36m(func pid=4220)[0m f1_weighted: 0.2317417550457492
[2m[36m(func pid=4220)[0m f1_per_class: [0.144, 0.175, 0.131, 0.132, 0.044, 0.124, 0.473, 0.0, 0.115, 0.146]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.40671641791044777
[2m[36m(func pid=180846)[0m top5: 0.9253731343283582
[2m[36m(func pid=180846)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=180846)[0m f1_macro: 0.38998776877560315
[2m[36m(func pid=180846)[0m f1_weighted: 0.41872505407376065
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.488, 0.649, 0.545, 0.138, 0.262, 0.35, 0.351, 0.263, 0.336]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=181391)[0m top1: 0.2868470149253731
[2m[36m(func pid=181391)[0m top5: 0.8176305970149254
[2m[36m(func pid=181391)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=181391)[0m f1_macro: 0.20148001181958436
[2m[36m(func pid=181391)[0m f1_weighted: 0.2668702058703406
[2m[36m(func pid=181391)[0m f1_per_class: [0.034, 0.35, 0.178, 0.426, 0.106, 0.378, 0.055, 0.393, 0.078, 0.017]
[2m[36m(func pid=181391)[0m 
[2m[36m(func pid=180330)[0m top1: 0.3358208955223881
[2m[36m(func pid=180330)[0m top5: 0.8950559701492538
[2m[36m(func pid=180330)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=180330)[0m f1_macro: 0.3080349485604308
[2m[36m(func pid=180330)[0m f1_weighted: 0.34220709049078857
[2m[36m(func pid=180330)[0m f1_per_class: [0.375, 0.386, 0.533, 0.465, 0.112, 0.166, 0.3, 0.269, 0.214, 0.259]
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8557 | Steps: 2 | Val loss: 2.1392 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7628 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=181391)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5920 | Steps: 2 | Val loss: 2.5560 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=4220)[0m top1: 0.23647388059701493
[2m[36m(func pid=4220)[0m top5: 0.7196828358208955
[2m[36m(func pid=4220)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=4220)[0m f1_macro: 0.15695127810522608
[2m[36m(func pid=4220)[0m f1_weighted: 0.24286701339658326
[2m[36m(func pid=4220)[0m f1_per_class: [0.148, 0.189, 0.14, 0.144, 0.053, 0.123, 0.489, 0.0, 0.129, 0.154]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m top1: 0.408115671641791
[2m[36m(func pid=180846)[0m top5: 0.9267723880597015
[2m[36m(func pid=180846)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.39245525294284966
[2m[36m(func pid=180846)[0m f1_weighted: 0.42014004949476735
[2m[36m(func pid=180846)[0m f1_per_class: [0.522, 0.488, 0.667, 0.548, 0.129, 0.263, 0.351, 0.353, 0.258, 0.345]
[2m[36m(func pid=181391)[0m top1: 0.23833955223880596
[2m[36m(func pid=181391)[0m top5: 0.7933768656716418
[2m[36m(func pid=181391)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=181391)[0m f1_macro: 0.18929983107755868
[2m[36m(func pid=181391)[0m f1_weighted: 0.23071567544319485
[2m[36m(func pid=181391)[0m f1_per_class: [0.101, 0.183, 0.213, 0.422, 0.0, 0.371, 0.018, 0.471, 0.079, 0.034]
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7468 | Steps: 2 | Val loss: 2.1198 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=4220)[0m top1: 0.24720149253731344
[2m[36m(func pid=4220)[0m top5: 0.7308768656716418
[2m[36m(func pid=4220)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=4220)[0m f1_macro: 0.16077162087225305
[2m[36m(func pid=4220)[0m f1_weighted: 0.25448998221715086
[2m[36m(func pid=4220)[0m f1_per_class: [0.151, 0.186, 0.135, 0.164, 0.054, 0.125, 0.511, 0.0, 0.124, 0.158]
== Status ==
Current time: 2024-01-07 03:35:49 (running for 00:16:24.61)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.34475
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.39  |                   74 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  3.298 |      0.201 |                   74 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.856 |      0.157 |                   25 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10089)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10089)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=10089)[0m Configuration completed!
[2m[36m(func pid=10089)[0m New optimizer parameters:
[2m[36m(func pid=10089)[0m SGD (
[2m[36m(func pid=10089)[0m Parameter Group 0
[2m[36m(func pid=10089)[0m     dampening: 0
[2m[36m(func pid=10089)[0m     differentiable: False
[2m[36m(func pid=10089)[0m     foreach: None
[2m[36m(func pid=10089)[0m     lr: 0.001
[2m[36m(func pid=10089)[0m     maximize: False
[2m[36m(func pid=10089)[0m     momentum: 0.99
[2m[36m(func pid=10089)[0m     nesterov: False
[2m[36m(func pid=10089)[0m     weight_decay: 0.0001
[2m[36m(func pid=10089)[0m )
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:35:55 (running for 00:16:30.33)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.3745
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.392 |                   75 |
| train_2d480_00007 | RUNNING    | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  3.298 |      0.201 |                   74 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.856 |      0.157 |                   25 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7601 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7567 | Steps: 2 | Val loss: 2.0997 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1979 | Steps: 2 | Val loss: 2.4912 | Batch size: 32 | lr: 0.001 | Duration: 4.81s
[2m[36m(func pid=180846)[0m top1: 0.40671641791044777
[2m[36m(func pid=180846)[0m top5: 0.9277052238805971
[2m[36m(func pid=180846)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=180846)[0m f1_macro: 0.391316771350567
[2m[36m(func pid=180846)[0m f1_weighted: 0.4169034119549628
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.499, 0.667, 0.54, 0.126, 0.26, 0.341, 0.358, 0.269, 0.34]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.2583955223880597
[2m[36m(func pid=4220)[0m top5: 0.7458022388059702
[2m[36m(func pid=4220)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=4220)[0m f1_macro: 0.1698689656427093
[2m[36m(func pid=4220)[0m f1_weighted: 0.26708464528139897
[2m[36m(func pid=4220)[0m f1_per_class: [0.158, 0.199, 0.145, 0.193, 0.056, 0.122, 0.517, 0.0, 0.129, 0.179]
[2m[36m(func pid=10089)[0m top1: 0.07042910447761194
[2m[36m(func pid=10089)[0m top5: 0.48507462686567165
[2m[36m(func pid=10089)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=10089)[0m f1_macro: 0.04586331575304473
[2m[36m(func pid=10089)[0m f1_weighted: 0.04380348466517389
[2m[36m(func pid=10089)[0m f1_per_class: [0.164, 0.01, 0.0, 0.106, 0.0, 0.017, 0.0, 0.106, 0.021, 0.034]
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7674 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:36:00 (running for 00:16:35.36)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.391 |                   76 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.747 |      0.161 |                   26 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10638)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=10638)[0m Configuration completed!
[2m[36m(func pid=10638)[0m New optimizer parameters:
[2m[36m(func pid=10638)[0m SGD (
[2m[36m(func pid=10638)[0m Parameter Group 0
[2m[36m(func pid=10638)[0m     dampening: 0
[2m[36m(func pid=10638)[0m     differentiable: False
[2m[36m(func pid=10638)[0m     foreach: None
[2m[36m(func pid=10638)[0m     lr: 0.01
[2m[36m(func pid=10638)[0m     maximize: False
[2m[36m(func pid=10638)[0m     momentum: 0.99
[2m[36m(func pid=10638)[0m     nesterov: False
[2m[36m(func pid=10638)[0m     weight_decay: 0.0001
[2m[36m(func pid=10638)[0m )
[2m[36m(func pid=10638)[0m 
== Status ==
Current time: 2024-01-07 03:36:05 (running for 00:16:40.62)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.393 |                   77 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.757 |      0.17  |                   27 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  3.198 |      0.046 |                    1 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4039179104477612
[2m[36m(func pid=180846)[0m top5: 0.9272388059701493
[2m[36m(func pid=180846)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=180846)[0m f1_macro: 0.3930398624945056
[2m[36m(func pid=180846)[0m f1_weighted: 0.4135977308081623
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.497, 0.686, 0.544, 0.14, 0.255, 0.33, 0.343, 0.266, 0.343]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.9229 | Steps: 2 | Val loss: 2.0795 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9770 | Steps: 2 | Val loss: 2.4601 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7788 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0812 | Steps: 2 | Val loss: 2.3483 | Batch size: 32 | lr: 0.01 | Duration: 4.50s
[2m[36m(func pid=4220)[0m top1: 0.26865671641791045
[2m[36m(func pid=4220)[0m top5: 0.7644589552238806
[2m[36m(func pid=4220)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=4220)[0m f1_macro: 0.17868106569574269
[2m[36m(func pid=4220)[0m f1_weighted: 0.27855215379605197
[2m[36m(func pid=4220)[0m f1_per_class: [0.16, 0.22, 0.155, 0.22, 0.059, 0.117, 0.518, 0.0, 0.132, 0.205]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.07602611940298508
[2m[36m(func pid=10089)[0m top5: 0.46875
[2m[36m(func pid=10089)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=10089)[0m f1_macro: 0.06044422610186493
[2m[36m(func pid=10089)[0m f1_weighted: 0.05542937910805411
[2m[36m(func pid=10089)[0m f1_per_class: [0.161, 0.065, 0.0, 0.101, 0.0, 0.031, 0.0, 0.108, 0.07, 0.069]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:36:11 (running for 00:16:45.87)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.387 |                   78 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.923 |      0.179 |                   28 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  2.977 |      0.06  |                    2 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4001865671641791
[2m[36m(func pid=180846)[0m top5: 0.925839552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=180846)[0m f1_macro: 0.387443780308538
[2m[36m(func pid=180846)[0m f1_weighted: 0.40778637408308865
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.496, 0.649, 0.539, 0.129, 0.259, 0.315, 0.344, 0.265, 0.352]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.07835820895522388
[2m[36m(func pid=10638)[0m top5: 0.5223880597014925
[2m[36m(func pid=10638)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=10638)[0m f1_macro: 0.08027701357462899
[2m[36m(func pid=10638)[0m f1_weighted: 0.07524573596693444
[2m[36m(func pid=10638)[0m f1_per_class: [0.109, 0.099, 0.104, 0.108, 0.025, 0.043, 0.029, 0.115, 0.127, 0.044]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5990 | Steps: 2 | Val loss: 2.4178 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6468 | Steps: 2 | Val loss: 2.0609 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7682 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9102 | Steps: 2 | Val loss: 2.3009 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10089)[0m top1: 0.07462686567164178
[2m[36m(func pid=10089)[0m top5: 0.45848880597014924
[2m[36m(func pid=10089)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=10089)[0m f1_macro: 0.07679582657751564
[2m[36m(func pid=10089)[0m f1_weighted: 0.06928725040491217
[2m[36m(func pid=10089)[0m f1_per_class: [0.126, 0.124, 0.129, 0.093, 0.0, 0.033, 0.015, 0.11, 0.108, 0.03]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.27611940298507465
[2m[36m(func pid=4220)[0m top5: 0.7747201492537313
[2m[36m(func pid=4220)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=4220)[0m f1_macro: 0.1861264216054584
[2m[36m(func pid=4220)[0m f1_weighted: 0.2862819951282924
[2m[36m(func pid=4220)[0m f1_per_class: [0.178, 0.229, 0.174, 0.231, 0.066, 0.121, 0.525, 0.0, 0.13, 0.207]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:36:16 (running for 00:16:51.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.394 |                   79 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.647 |      0.186 |                   29 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  2.599 |      0.077 |                    3 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  3.081 |      0.08  |                    1 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.41044776119402987
[2m[36m(func pid=180846)[0m top5: 0.9272388059701493
[2m[36m(func pid=180846)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=180846)[0m f1_macro: 0.3944711376250051
[2m[36m(func pid=180846)[0m f1_weighted: 0.42170748755148885
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.499, 0.667, 0.549, 0.14, 0.256, 0.351, 0.352, 0.268, 0.349]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.14272388059701493
[2m[36m(func pid=10638)[0m top5: 0.5932835820895522
[2m[36m(func pid=10638)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=10638)[0m f1_macro: 0.09707882935565162
[2m[36m(func pid=10638)[0m f1_weighted: 0.1380820833479468
[2m[36m(func pid=10638)[0m f1_per_class: [0.119, 0.021, 0.152, 0.013, 0.044, 0.09, 0.374, 0.0, 0.157, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3078 | Steps: 2 | Val loss: 2.3667 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5794 | Steps: 2 | Val loss: 2.0358 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7483 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8459 | Steps: 2 | Val loss: 2.1754 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=10089)[0m top1: 0.08022388059701492
[2m[36m(func pid=10089)[0m top5: 0.507929104477612
[2m[36m(func pid=10089)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=10089)[0m f1_macro: 0.09238847943273941
[2m[36m(func pid=10089)[0m f1_weighted: 0.08699623786401932
[2m[36m(func pid=10089)[0m f1_per_class: [0.125, 0.127, 0.174, 0.076, 0.016, 0.038, 0.083, 0.111, 0.121, 0.053]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.28824626865671643
[2m[36m(func pid=4220)[0m top5: 0.7915111940298507
[2m[36m(func pid=4220)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=4220)[0m f1_macro: 0.1940116904267651
[2m[36m(func pid=4220)[0m f1_weighted: 0.3008671492153609
[2m[36m(func pid=4220)[0m f1_per_class: [0.18, 0.239, 0.176, 0.265, 0.069, 0.122, 0.536, 0.0, 0.133, 0.22]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:36:21 (running for 00:16:56.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.398 |                   80 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.579 |      0.194 |                   30 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  2.308 |      0.092 |                    4 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.91  |      0.097 |                    2 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.41138059701492535
[2m[36m(func pid=180846)[0m top5: 0.9286380597014925
[2m[36m(func pid=180846)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=180846)[0m f1_macro: 0.3981008909900312
[2m[36m(func pid=180846)[0m f1_weighted: 0.42330084388871225
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.492, 0.706, 0.551, 0.147, 0.257, 0.358, 0.349, 0.261, 0.346]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.18236940298507462
[2m[36m(func pid=10638)[0m top5: 0.7887126865671642
[2m[36m(func pid=10638)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=10638)[0m f1_macro: 0.17365628736368335
[2m[36m(func pid=10638)[0m f1_weighted: 0.20710726143057448
[2m[36m(func pid=10638)[0m f1_per_class: [0.194, 0.166, 0.151, 0.215, 0.076, 0.154, 0.278, 0.088, 0.13, 0.286]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8993 | Steps: 2 | Val loss: 2.3150 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4915 | Steps: 2 | Val loss: 2.0067 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7587 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.2217 | Steps: 2 | Val loss: 2.2288 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=10089)[0m top1: 0.12220149253731344
[2m[36m(func pid=10089)[0m top5: 0.5592350746268657
[2m[36m(func pid=10089)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=10089)[0m f1_macro: 0.10812889066643253
[2m[36m(func pid=10089)[0m f1_weighted: 0.13917145442239284
[2m[36m(func pid=10089)[0m f1_per_class: [0.123, 0.113, 0.142, 0.069, 0.017, 0.062, 0.265, 0.094, 0.142, 0.054]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.29757462686567165
[2m[36m(func pid=4220)[0m top5: 0.8059701492537313
[2m[36m(func pid=4220)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=4220)[0m f1_macro: 0.2046976541070616
[2m[36m(func pid=4220)[0m f1_weighted: 0.3071716603512815
[2m[36m(func pid=4220)[0m f1_per_class: [0.2, 0.248, 0.211, 0.278, 0.078, 0.132, 0.532, 0.0, 0.146, 0.222]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:36:27 (running for 00:17:01.85)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.393 |                   81 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.492 |      0.205 |                   31 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  1.899 |      0.108 |                    5 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.846 |      0.174 |                    3 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4085820895522388
[2m[36m(func pid=180846)[0m top5: 0.9277052238805971
[2m[36m(func pid=180846)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=180846)[0m f1_macro: 0.39262473331183595
[2m[36m(func pid=180846)[0m f1_weighted: 0.41914970835217186
[2m[36m(func pid=180846)[0m f1_per_class: [0.507, 0.485, 0.686, 0.552, 0.156, 0.263, 0.345, 0.354, 0.259, 0.318]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.20149253731343283
[2m[36m(func pid=10638)[0m top5: 0.7593283582089553
[2m[36m(func pid=10638)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=10638)[0m f1_macro: 0.22616063508503617
[2m[36m(func pid=10638)[0m f1_weighted: 0.1884498503381471
[2m[36m(func pid=10638)[0m f1_per_class: [0.344, 0.302, 0.255, 0.29, 0.133, 0.123, 0.024, 0.272, 0.131, 0.386]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.4765 | Steps: 2 | Val loss: 2.2726 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4129 | Steps: 2 | Val loss: 1.9726 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7473 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.1327 | Steps: 2 | Val loss: 2.3613 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=10089)[0m top1: 0.17490671641791045
[2m[36m(func pid=10089)[0m top5: 0.5928171641791045
[2m[36m(func pid=10089)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=10089)[0m f1_macro: 0.1194676808787279
[2m[36m(func pid=10089)[0m f1_weighted: 0.18079658278711494
[2m[36m(func pid=10089)[0m f1_per_class: [0.116, 0.122, 0.126, 0.053, 0.022, 0.071, 0.429, 0.0, 0.133, 0.123]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.30970149253731344
[2m[36m(func pid=4220)[0m top5: 0.8246268656716418
[2m[36m(func pid=4220)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=4220)[0m f1_macro: 0.21489896639287392
[2m[36m(func pid=4220)[0m f1_weighted: 0.31664821843506274
[2m[36m(func pid=4220)[0m f1_per_class: [0.227, 0.256, 0.247, 0.297, 0.08, 0.136, 0.537, 0.0, 0.149, 0.22]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:36:32 (running for 00:17:07.06)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   82 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.413 |      0.215 |                   32 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  1.477 |      0.119 |                    6 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.222 |      0.226 |                    4 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.41044776119402987
[2m[36m(func pid=180846)[0m top5: 0.9267723880597015
[2m[36m(func pid=180846)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=180846)[0m f1_macro: 0.39549356419338577
[2m[36m(func pid=180846)[0m f1_weighted: 0.4218705791187566
[2m[36m(func pid=180846)[0m f1_per_class: [0.504, 0.487, 0.706, 0.557, 0.144, 0.267, 0.349, 0.344, 0.265, 0.333]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.23740671641791045
[2m[36m(func pid=10638)[0m top5: 0.7168843283582089
[2m[36m(func pid=10638)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=10638)[0m f1_macro: 0.26827375200507503
[2m[36m(func pid=10638)[0m f1_weighted: 0.20510373091892878
[2m[36m(func pid=10638)[0m f1_per_class: [0.44, 0.359, 0.375, 0.319, 0.2, 0.104, 0.003, 0.336, 0.142, 0.405]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.2185 | Steps: 2 | Val loss: 2.2300 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7584 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4673 | Steps: 2 | Val loss: 1.9500 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.0868 | Steps: 2 | Val loss: 2.4431 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=10089)[0m top1: 0.21641791044776118
[2m[36m(func pid=10089)[0m top5: 0.6180037313432836
[2m[36m(func pid=10089)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=10089)[0m f1_macro: 0.13588300629829977
[2m[36m(func pid=10089)[0m f1_weighted: 0.20515617854891877
[2m[36m(func pid=10089)[0m f1_per_class: [0.136, 0.13, 0.114, 0.047, 0.044, 0.077, 0.506, 0.0, 0.13, 0.175]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:36:37 (running for 00:17:12.33)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   82 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.467 |      0.223 |                   33 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  1.218 |      0.136 |                    7 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.133 |      0.268 |                    5 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4099813432835821
[2m[36m(func pid=180846)[0m top5: 0.9263059701492538
[2m[36m(func pid=180846)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=180846)[0m f1_macro: 0.39016703415683573
[2m[36m(func pid=180846)[0m f1_weighted: 0.4196878233904844
[2m[36m(func pid=180846)[0m f1_per_class: [0.493, 0.488, 0.667, 0.559, 0.142, 0.265, 0.34, 0.354, 0.265, 0.33]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.31949626865671643
[2m[36m(func pid=4220)[0m top5: 0.835820895522388
[2m[36m(func pid=4220)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=4220)[0m f1_macro: 0.22318816639558503
[2m[36m(func pid=4220)[0m f1_weighted: 0.327435852153935
[2m[36m(func pid=4220)[0m f1_per_class: [0.23, 0.285, 0.255, 0.32, 0.086, 0.136, 0.532, 0.0, 0.16, 0.227]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m top1: 0.26259328358208955
[2m[36m(func pid=10638)[0m top5: 0.746268656716418
[2m[36m(func pid=10638)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=10638)[0m f1_macro: 0.28716059162276597
[2m[36m(func pid=10638)[0m f1_weighted: 0.21522002597965942
[2m[36m(func pid=10638)[0m f1_per_class: [0.541, 0.412, 0.421, 0.306, 0.202, 0.092, 0.003, 0.396, 0.177, 0.323]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8577 | Steps: 2 | Val loss: 2.1785 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.0226 | Steps: 2 | Val loss: 2.4752 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7662 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3384 | Steps: 2 | Val loss: 1.9261 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=10089)[0m top1: 0.24347014925373134
[2m[36m(func pid=10089)[0m top5: 0.6711753731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=10089)[0m f1_macro: 0.14750606110670492
[2m[36m(func pid=10089)[0m f1_weighted: 0.22889762010225254
[2m[36m(func pid=10089)[0m f1_per_class: [0.139, 0.156, 0.118, 0.078, 0.064, 0.078, 0.54, 0.0, 0.129, 0.172]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:36:42 (running for 00:17:17.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.39  |                   83 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.467 |      0.223 |                   33 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.858 |      0.148 |                    8 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.023 |      0.302 |                    7 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.2775186567164179
[2m[36m(func pid=10638)[0m top5: 0.7966417910447762
[2m[36m(func pid=10638)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=10638)[0m f1_macro: 0.3022697486320235
[2m[36m(func pid=10638)[0m f1_weighted: 0.23292170357527658
[2m[36m(func pid=10638)[0m f1_per_class: [0.538, 0.421, 0.5, 0.301, 0.193, 0.09, 0.057, 0.412, 0.187, 0.323]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4076492537313433
[2m[36m(func pid=180846)[0m top5: 0.9277052238805971
[2m[36m(func pid=180846)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=180846)[0m f1_macro: 0.38766740321016835
[2m[36m(func pid=180846)[0m f1_weighted: 0.4155176387636121
[2m[36m(func pid=180846)[0m f1_per_class: [0.511, 0.491, 0.649, 0.557, 0.146, 0.262, 0.326, 0.357, 0.26, 0.318]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3255597014925373
[2m[36m(func pid=4220)[0m top5: 0.8470149253731343
[2m[36m(func pid=4220)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=4220)[0m f1_macro: 0.23265450515677477
[2m[36m(func pid=4220)[0m f1_weighted: 0.3332377899587365
[2m[36m(func pid=4220)[0m f1_per_class: [0.247, 0.285, 0.273, 0.335, 0.096, 0.132, 0.534, 0.014, 0.169, 0.242]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6200 | Steps: 2 | Val loss: 2.1157 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7467 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.0172 | Steps: 2 | Val loss: 2.5156 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2817 | Steps: 2 | Val loss: 1.8999 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=10089)[0m top1: 0.2667910447761194
[2m[36m(func pid=10089)[0m top5: 0.7308768656716418
[2m[36m(func pid=10089)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=10089)[0m f1_macro: 0.1675689946363212
[2m[36m(func pid=10089)[0m f1_weighted: 0.2618332903753942
[2m[36m(func pid=10089)[0m f1_per_class: [0.164, 0.206, 0.13, 0.131, 0.092, 0.11, 0.556, 0.016, 0.124, 0.147]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:36:48 (running for 00:17:23.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.391 |                   85 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.338 |      0.233 |                   34 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.62  |      0.168 |                    9 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.023 |      0.302 |                    7 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.40904850746268656
[2m[36m(func pid=180846)[0m top5: 0.9286380597014925
[2m[36m(func pid=180846)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=180846)[0m f1_macro: 0.3913746451900567
[2m[36m(func pid=180846)[0m f1_weighted: 0.420069507314917
[2m[36m(func pid=180846)[0m f1_per_class: [0.507, 0.487, 0.667, 0.551, 0.137, 0.261, 0.351, 0.346, 0.265, 0.343]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.28544776119402987
[2m[36m(func pid=10638)[0m top5: 0.8367537313432836
[2m[36m(func pid=10638)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=10638)[0m f1_macro: 0.32036404823063835
[2m[36m(func pid=10638)[0m f1_weighted: 0.2549414926004186
[2m[36m(func pid=10638)[0m f1_per_class: [0.539, 0.415, 0.649, 0.289, 0.179, 0.125, 0.14, 0.345, 0.205, 0.319]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.33955223880597013
[2m[36m(func pid=4220)[0m top5: 0.8582089552238806
[2m[36m(func pid=4220)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=4220)[0m f1_macro: 0.250519948293776
[2m[36m(func pid=4220)[0m f1_weighted: 0.34944382022826187
[2m[36m(func pid=4220)[0m f1_per_class: [0.266, 0.289, 0.304, 0.361, 0.1, 0.145, 0.54, 0.079, 0.178, 0.242]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4118 | Steps: 2 | Val loss: 2.0489 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7395 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0210 | Steps: 2 | Val loss: 2.5850 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2583 | Steps: 2 | Val loss: 1.8769 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=10089)[0m top1: 0.27705223880597013
[2m[36m(func pid=10089)[0m top5: 0.7924440298507462
[2m[36m(func pid=10089)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=10089)[0m f1_macro: 0.19341495552805302
[2m[36m(func pid=10089)[0m f1_weighted: 0.2883222060101625
[2m[36m(func pid=10089)[0m f1_per_class: [0.189, 0.267, 0.161, 0.22, 0.093, 0.128, 0.51, 0.031, 0.143, 0.19]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:36:53 (running for 00:17:28.26)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.394 |                   86 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.282 |      0.251 |                   35 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.412 |      0.193 |                   10 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.017 |      0.32  |                    8 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4123134328358209
[2m[36m(func pid=180846)[0m top5: 0.9291044776119403
[2m[36m(func pid=180846)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=180846)[0m f1_macro: 0.394344460034275
[2m[36m(func pid=180846)[0m f1_weighted: 0.42374420644945476
[2m[36m(func pid=180846)[0m f1_per_class: [0.511, 0.498, 0.667, 0.553, 0.132, 0.262, 0.355, 0.337, 0.271, 0.358]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.30923507462686567
[2m[36m(func pid=10638)[0m top5: 0.851679104477612
[2m[36m(func pid=10638)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=10638)[0m f1_macro: 0.33472009118991586
[2m[36m(func pid=10638)[0m f1_weighted: 0.2945287799209431
[2m[36m(func pid=10638)[0m f1_per_class: [0.541, 0.416, 0.649, 0.295, 0.206, 0.126, 0.271, 0.312, 0.219, 0.313]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.345615671641791
[2m[36m(func pid=4220)[0m top5: 0.8694029850746269
[2m[36m(func pid=4220)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=4220)[0m f1_macro: 0.2632704536913042
[2m[36m(func pid=4220)[0m f1_weighted: 0.35795164546691116
[2m[36m(func pid=4220)[0m f1_per_class: [0.286, 0.3, 0.32, 0.373, 0.102, 0.153, 0.535, 0.133, 0.187, 0.243]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2822 | Steps: 2 | Val loss: 1.9724 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7428 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0223 | Steps: 2 | Val loss: 2.6785 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2557 | Steps: 2 | Val loss: 1.8587 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=10089)[0m top1: 0.29524253731343286
[2m[36m(func pid=10089)[0m top5: 0.8446828358208955
[2m[36m(func pid=10089)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=10089)[0m f1_macro: 0.22344218727945134
[2m[36m(func pid=10089)[0m f1_weighted: 0.31725042029467926
[2m[36m(func pid=10089)[0m f1_per_class: [0.227, 0.283, 0.224, 0.308, 0.094, 0.187, 0.476, 0.103, 0.149, 0.186]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:36:58 (running for 00:17:33.55)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.394 |                   86 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.258 |      0.263 |                   36 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.282 |      0.223 |                   11 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.022 |      0.348 |                   10 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4123134328358209
[2m[36m(func pid=180846)[0m top5: 0.929570895522388
[2m[36m(func pid=180846)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=180846)[0m f1_macro: 0.39469459108616134
[2m[36m(func pid=180846)[0m f1_weighted: 0.42406082893595
[2m[36m(func pid=180846)[0m f1_per_class: [0.504, 0.494, 0.686, 0.546, 0.134, 0.263, 0.364, 0.341, 0.266, 0.35]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m top1: 0.32742537313432835
[2m[36m(func pid=10638)[0m top5: 0.8703358208955224
[2m[36m(func pid=10638)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=10638)[0m f1_macro: 0.3478420935681086
[2m[36m(func pid=10638)[0m f1_weighted: 0.3253235548333376
[2m[36m(func pid=10638)[0m f1_per_class: [0.514, 0.408, 0.686, 0.302, 0.224, 0.189, 0.348, 0.321, 0.224, 0.262]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.35261194029850745
[2m[36m(func pid=4220)[0m top5: 0.8759328358208955
[2m[36m(func pid=4220)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=4220)[0m f1_macro: 0.27454727404059
[2m[36m(func pid=4220)[0m f1_weighted: 0.3673810901151714
[2m[36m(func pid=4220)[0m f1_per_class: [0.3, 0.313, 0.324, 0.391, 0.099, 0.17, 0.529, 0.155, 0.196, 0.269]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2325 | Steps: 2 | Val loss: 1.9265 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7336 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0025 | Steps: 2 | Val loss: 2.8464 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2298 | Steps: 2 | Val loss: 1.8343 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=10089)[0m top1: 0.300839552238806
[2m[36m(func pid=10089)[0m top5: 0.8577425373134329
[2m[36m(func pid=10089)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=10089)[0m f1_macro: 0.2553948059353321
[2m[36m(func pid=10089)[0m f1_weighted: 0.3207338639403105
[2m[36m(func pid=10089)[0m f1_per_class: [0.29, 0.341, 0.282, 0.352, 0.091, 0.207, 0.372, 0.224, 0.156, 0.238]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.324160447761194
[2m[36m(func pid=10638)[0m top5: 0.8768656716417911
[2m[36m(func pid=10638)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.34602563030965106
[2m[36m(func pid=10638)[0m f1_weighted: 0.32273138557150094
[2m[36m(func pid=10638)[0m f1_per_class: [0.393, 0.404, 0.75, 0.295, 0.237, 0.232, 0.339, 0.316, 0.225, 0.269]
[2m[36m(func pid=10638)[0m 
== Status ==
Current time: 2024-01-07 03:37:03 (running for 00:17:38.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   87 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.256 |      0.275 |                   37 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.233 |      0.255 |                   12 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.003 |      0.346 |                   11 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.4141791044776119
[2m[36m(func pid=180846)[0m top5: 0.929570895522388
[2m[36m(func pid=180846)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=180846)[0m f1_macro: 0.3949994577761851
[2m[36m(func pid=180846)[0m f1_weighted: 0.42500760059775305
[2m[36m(func pid=180846)[0m f1_per_class: [0.514, 0.501, 0.667, 0.549, 0.136, 0.262, 0.36, 0.342, 0.274, 0.346]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.35774253731343286
[2m[36m(func pid=4220)[0m top5: 0.8857276119402985
[2m[36m(func pid=4220)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=4220)[0m f1_macro: 0.2843683911295515
[2m[36m(func pid=4220)[0m f1_weighted: 0.3749797325119995
[2m[36m(func pid=4220)[0m f1_per_class: [0.313, 0.335, 0.343, 0.407, 0.108, 0.182, 0.516, 0.184, 0.182, 0.275]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1526 | Steps: 2 | Val loss: 1.8830 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7350 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0314 | Steps: 2 | Val loss: 3.0563 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=10089)[0m top1: 0.3069029850746269
[2m[36m(func pid=10089)[0m top5: 0.8661380597014925
[2m[36m(func pid=10089)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=10089)[0m f1_macro: 0.27248578614408325
[2m[36m(func pid=10089)[0m f1_weighted: 0.31353648586592975
[2m[36m(func pid=10089)[0m f1_per_class: [0.328, 0.37, 0.324, 0.383, 0.113, 0.203, 0.282, 0.311, 0.174, 0.238]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.1928 | Steps: 2 | Val loss: 1.8108 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 03:37:09 (running for 00:17:44.06)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   88 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.23  |      0.284 |                   38 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.153 |      0.272 |                   13 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.031 |      0.345 |                   12 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=180846)[0m top1: 0.41277985074626866

[2m[36m(func pid=180846)[0m top5: 0.9314365671641791
[2m[36m(func pid=180846)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=180846)[0m f1_macro: 0.39855383809167244
[2m[36m(func pid=180846)[0m f1_weighted: 0.42125755008655363
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.506, 0.706, 0.549, 0.137, 0.262, 0.344, 0.338, 0.272, 0.353]
[2m[36m(func pid=10638)[0m top1: 0.3204291044776119
[2m[36m(func pid=10638)[0m top5: 0.8768656716417911
[2m[36m(func pid=10638)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=10638)[0m f1_macro: 0.34450422386226986
[2m[36m(func pid=10638)[0m f1_weighted: 0.3107542206115787
[2m[36m(func pid=10638)[0m f1_per_class: [0.357, 0.41, 0.75, 0.248, 0.233, 0.249, 0.329, 0.343, 0.236, 0.291]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3670708955223881
[2m[36m(func pid=4220)[0m top5: 0.8936567164179104
[2m[36m(func pid=4220)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=4220)[0m f1_macro: 0.2986376824417832
[2m[36m(func pid=4220)[0m f1_weighted: 0.38515467090946626
[2m[36m(func pid=4220)[0m f1_per_class: [0.333, 0.362, 0.381, 0.418, 0.107, 0.19, 0.511, 0.22, 0.2, 0.264]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0975 | Steps: 2 | Val loss: 1.8471 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7371 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0042 | Steps: 2 | Val loss: 3.4487 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=10089)[0m top1: 0.3204291044776119
[2m[36m(func pid=10089)[0m top5: 0.8777985074626866
[2m[36m(func pid=10089)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=10089)[0m f1_macro: 0.2930935364836148
[2m[36m(func pid=10089)[0m f1_weighted: 0.3075687736041676
[2m[36m(func pid=10089)[0m f1_per_class: [0.413, 0.404, 0.348, 0.419, 0.133, 0.198, 0.191, 0.35, 0.188, 0.286]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1659 | Steps: 2 | Val loss: 1.7952 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 03:37:14 (running for 00:17:49.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.399 |                   89 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.193 |      0.299 |                   39 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.098 |      0.293 |                   14 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.004 |      0.334 |                   13 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.2980410447761194
[2m[36m(func pid=10638)[0m top5: 0.8777985074626866
[2m[36m(func pid=10638)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.3344690559900733
[2m[36m(func pid=10638)[0m f1_weighted: 0.276439571896546
[2m[36m(func pid=10638)[0m f1_per_class: [0.333, 0.393, 0.8, 0.171, 0.209, 0.265, 0.287, 0.365, 0.22, 0.301]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4123134328358209
[2m[36m(func pid=180846)[0m top5: 0.9300373134328358
[2m[36m(func pid=180846)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=180846)[0m f1_macro: 0.39699085609211787
[2m[36m(func pid=180846)[0m f1_weighted: 0.4207956334111852
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.504, 0.706, 0.552, 0.131, 0.258, 0.341, 0.35, 0.267, 0.343]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.363339552238806
[2m[36m(func pid=4220)[0m top5: 0.9029850746268657
[2m[36m(func pid=4220)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=4220)[0m f1_macro: 0.30351824600522226
[2m[36m(func pid=4220)[0m f1_weighted: 0.3830362382284138
[2m[36m(func pid=4220)[0m f1_per_class: [0.357, 0.369, 0.387, 0.425, 0.105, 0.205, 0.481, 0.247, 0.19, 0.269]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0781 | Steps: 2 | Val loss: 1.8279 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0030 | Steps: 2 | Val loss: 3.8640 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7485 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=10089)[0m top1: 0.32369402985074625
[2m[36m(func pid=10089)[0m top5: 0.8871268656716418
[2m[36m(func pid=10089)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=10089)[0m f1_macro: 0.30192328314696076
[2m[36m(func pid=10089)[0m f1_weighted: 0.29238443278799187
[2m[36m(func pid=10089)[0m f1_per_class: [0.469, 0.421, 0.369, 0.436, 0.15, 0.207, 0.103, 0.365, 0.193, 0.306]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2003 | Steps: 2 | Val loss: 1.7811 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=10638)[0m top1: 0.28591417910447764
[2m[36m(func pid=10638)[0m top5: 0.875
[2m[36m(func pid=10638)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=10638)[0m f1_macro: 0.331458805947105
[2m[36m(func pid=10638)[0m f1_weighted: 0.2533883219519792
[2m[36m(func pid=10638)[0m f1_per_class: [0.333, 0.385, 0.857, 0.112, 0.173, 0.28, 0.262, 0.369, 0.222, 0.321]
== Status ==
Current time: 2024-01-07 03:37:19 (running for 00:17:54.63)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.397 |                   90 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.166 |      0.304 |                   40 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.078 |      0.302 |                   15 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.003 |      0.331 |                   14 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.40951492537313433
[2m[36m(func pid=180846)[0m top5: 0.929570895522388
[2m[36m(func pid=180846)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=180846)[0m f1_macro: 0.39538421086504943
[2m[36m(func pid=180846)[0m f1_weighted: 0.4186459844551593
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.502, 0.686, 0.543, 0.127, 0.251, 0.346, 0.345, 0.272, 0.364]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.36100746268656714
[2m[36m(func pid=4220)[0m top5: 0.9090485074626866
[2m[36m(func pid=4220)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=4220)[0m f1_macro: 0.3078170200239337
[2m[36m(func pid=4220)[0m f1_weighted: 0.3803669469650152
[2m[36m(func pid=4220)[0m f1_per_class: [0.398, 0.378, 0.375, 0.434, 0.103, 0.214, 0.447, 0.269, 0.2, 0.259]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0478 | Steps: 2 | Val loss: 1.8242 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7452 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0274 | Steps: 2 | Val loss: 4.3820 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1788 | Steps: 2 | Val loss: 1.7590 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=10089)[0m top1: 0.33115671641791045
[2m[36m(func pid=10089)[0m top5: 0.8885261194029851
[2m[36m(func pid=10089)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=10089)[0m f1_macro: 0.31170800403214977
[2m[36m(func pid=10089)[0m f1_weighted: 0.2910181040516977
[2m[36m(func pid=10089)[0m f1_per_class: [0.479, 0.436, 0.421, 0.45, 0.167, 0.205, 0.073, 0.367, 0.203, 0.316]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:37:25 (running for 00:17:59.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.395 |                   91 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.2   |      0.308 |                   41 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.048 |      0.312 |                   16 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.027 |      0.319 |                   15 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.27005597014925375
[2m[36m(func pid=10638)[0m top5: 0.8698694029850746
[2m[36m(func pid=10638)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=10638)[0m f1_macro: 0.31860287346428395
[2m[36m(func pid=10638)[0m f1_weighted: 0.22238959184553939
[2m[36m(func pid=10638)[0m f1_per_class: [0.302, 0.384, 0.857, 0.079, 0.187, 0.271, 0.195, 0.374, 0.213, 0.324]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4123134328358209
[2m[36m(func pid=180846)[0m top5: 0.9305037313432836
[2m[36m(func pid=180846)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=180846)[0m f1_macro: 0.39518083248547164
[2m[36m(func pid=180846)[0m f1_weighted: 0.4197483954482825
[2m[36m(func pid=180846)[0m f1_per_class: [0.514, 0.507, 0.667, 0.548, 0.132, 0.267, 0.336, 0.345, 0.279, 0.356]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3596082089552239
[2m[36m(func pid=4220)[0m top5: 0.9113805970149254
[2m[36m(func pid=4220)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=4220)[0m f1_macro: 0.3137432297566891
[2m[36m(func pid=4220)[0m f1_weighted: 0.3757250838290659
[2m[36m(func pid=4220)[0m f1_per_class: [0.418, 0.381, 0.414, 0.457, 0.109, 0.211, 0.404, 0.286, 0.195, 0.262]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0545 | Steps: 2 | Val loss: 1.8269 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0540 | Steps: 2 | Val loss: 4.7759 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7562 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1413 | Steps: 2 | Val loss: 1.7464 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=10089)[0m top1: 0.3362873134328358
[2m[36m(func pid=10089)[0m top5: 0.8885261194029851
[2m[36m(func pid=10089)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=10089)[0m f1_macro: 0.3188518872107621
[2m[36m(func pid=10089)[0m f1_weighted: 0.2884167519981039
[2m[36m(func pid=10089)[0m f1_per_class: [0.519, 0.455, 0.421, 0.456, 0.191, 0.212, 0.039, 0.373, 0.214, 0.308]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:37:30 (running for 00:18:05.00)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   92 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.179 |      0.314 |                   42 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.054 |      0.319 |                   17 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.054 |      0.316 |                   16 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.2667910447761194
[2m[36m(func pid=10638)[0m top5: 0.8563432835820896
[2m[36m(func pid=10638)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.3156641309149044
[2m[36m(func pid=10638)[0m f1_weighted: 0.22452520255732716
[2m[36m(func pid=10638)[0m f1_per_class: [0.24, 0.374, 0.857, 0.075, 0.222, 0.295, 0.208, 0.381, 0.188, 0.317]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.40718283582089554
[2m[36m(func pid=180846)[0m top5: 0.9286380597014925
[2m[36m(func pid=180846)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=180846)[0m f1_macro: 0.39474058231849973
[2m[36m(func pid=180846)[0m f1_weighted: 0.41329343258227647
[2m[36m(func pid=180846)[0m f1_per_class: [0.518, 0.504, 0.686, 0.538, 0.133, 0.259, 0.328, 0.345, 0.281, 0.355]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3605410447761194
[2m[36m(func pid=4220)[0m top5: 0.9132462686567164
[2m[36m(func pid=4220)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=4220)[0m f1_macro: 0.31650588593104234
[2m[36m(func pid=4220)[0m f1_weighted: 0.3718962464167872
[2m[36m(func pid=4220)[0m f1_per_class: [0.426, 0.389, 0.407, 0.468, 0.121, 0.216, 0.369, 0.307, 0.198, 0.264]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0619 | Steps: 2 | Val loss: 1.8342 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0051 | Steps: 2 | Val loss: 5.3719 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7537 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=10089)[0m top1: 0.3474813432835821
[2m[36m(func pid=10089)[0m top5: 0.8917910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=10089)[0m f1_macro: 0.3318211273456025
[2m[36m(func pid=10089)[0m f1_weighted: 0.29609116389357687
[2m[36m(func pid=10089)[0m f1_per_class: [0.538, 0.473, 0.444, 0.474, 0.22, 0.217, 0.033, 0.375, 0.206, 0.337]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1395 | Steps: 2 | Val loss: 1.7439 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:37:35 (running for 00:18:10.31)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.395 |                   93 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.141 |      0.317 |                   43 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.062 |      0.332 |                   18 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.005 |      0.29  |                   17 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.2523320895522388
[2m[36m(func pid=10638)[0m top5: 0.820429104477612
[2m[36m(func pid=10638)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=10638)[0m f1_macro: 0.2895490116167594
[2m[36m(func pid=10638)[0m f1_weighted: 0.22758850331691477
[2m[36m(func pid=10638)[0m f1_per_class: [0.128, 0.35, 0.857, 0.063, 0.233, 0.229, 0.296, 0.295, 0.17, 0.275]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.40951492537313433
[2m[36m(func pid=180846)[0m top5: 0.9277052238805971
[2m[36m(func pid=180846)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=180846)[0m f1_macro: 0.3928722940034294
[2m[36m(func pid=180846)[0m f1_weighted: 0.41748611314771317
[2m[36m(func pid=180846)[0m f1_per_class: [0.507, 0.507, 0.667, 0.54, 0.131, 0.266, 0.337, 0.344, 0.285, 0.345]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3558768656716418
[2m[36m(func pid=4220)[0m top5: 0.914179104477612
[2m[36m(func pid=4220)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=4220)[0m f1_macro: 0.31465295755790157
[2m[36m(func pid=4220)[0m f1_weighted: 0.3658088635265588
[2m[36m(func pid=4220)[0m f1_per_class: [0.426, 0.396, 0.436, 0.464, 0.118, 0.196, 0.357, 0.303, 0.206, 0.246]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0298 | Steps: 2 | Val loss: 1.8604 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.7574 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0019 | Steps: 2 | Val loss: 6.1783 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1182 | Steps: 2 | Val loss: 1.7355 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=10089)[0m top1: 0.3521455223880597
[2m[36m(func pid=10089)[0m top5: 0.8894589552238806
[2m[36m(func pid=10089)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=10089)[0m f1_macro: 0.3389346474670142
[2m[36m(func pid=10089)[0m f1_weighted: 0.29899415897951753
[2m[36m(func pid=10089)[0m f1_per_class: [0.574, 0.481, 0.471, 0.481, 0.235, 0.219, 0.028, 0.377, 0.213, 0.311]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:37:40 (running for 00:18:15.62)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.393 |                   94 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.14  |      0.315 |                   44 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.03  |      0.339 |                   19 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.002 |      0.229 |                   18 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.20522388059701493
[2m[36m(func pid=10638)[0m top5: 0.773320895522388
[2m[36m(func pid=10638)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=10638)[0m f1_macro: 0.22941325839929175
[2m[36m(func pid=10638)[0m f1_weighted: 0.1790135310280072
[2m[36m(func pid=10638)[0m f1_per_class: [0.044, 0.309, 0.828, 0.047, 0.235, 0.069, 0.288, 0.062, 0.137, 0.274]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.40671641791044777
[2m[36m(func pid=180846)[0m top5: 0.9272388059701493
[2m[36m(func pid=180846)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=180846)[0m f1_macro: 0.39087546230786746
[2m[36m(func pid=180846)[0m f1_weighted: 0.4141638630502912
[2m[36m(func pid=180846)[0m f1_per_class: [0.511, 0.502, 0.667, 0.541, 0.13, 0.261, 0.329, 0.346, 0.282, 0.339]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.35867537313432835
[2m[36m(func pid=4220)[0m top5: 0.9174440298507462
[2m[36m(func pid=4220)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=4220)[0m f1_macro: 0.31974972515367395
[2m[36m(func pid=4220)[0m f1_weighted: 0.36718806302004486
[2m[36m(func pid=4220)[0m f1_per_class: [0.439, 0.402, 0.462, 0.477, 0.122, 0.21, 0.34, 0.295, 0.207, 0.243]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0304 | Steps: 2 | Val loss: 1.8892 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0179 | Steps: 2 | Val loss: 6.7505 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7450 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1078 | Steps: 2 | Val loss: 1.7292 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=10089)[0m top1: 0.355410447761194
[2m[36m(func pid=10089)[0m top5: 0.8894589552238806
[2m[36m(func pid=10089)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=10089)[0m f1_macro: 0.34296384116573375
[2m[36m(func pid=10089)[0m f1_weighted: 0.30098740296733445
[2m[36m(func pid=10089)[0m f1_per_class: [0.578, 0.482, 0.471, 0.488, 0.25, 0.232, 0.022, 0.369, 0.215, 0.322]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:37:46 (running for 00:18:20.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.391 |                   95 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.118 |      0.32  |                   45 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.03  |      0.343 |                   20 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.018 |      0.226 |                   19 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.20289179104477612
[2m[36m(func pid=10638)[0m top5: 0.738339552238806
[2m[36m(func pid=10638)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=10638)[0m f1_macro: 0.22638639037177843
[2m[36m(func pid=10638)[0m f1_weighted: 0.1787271943101516
[2m[36m(func pid=10638)[0m f1_per_class: [0.163, 0.287, 0.8, 0.025, 0.194, 0.024, 0.341, 0.012, 0.128, 0.289]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4099813432835821
[2m[36m(func pid=180846)[0m top5: 0.9286380597014925
[2m[36m(func pid=180846)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=180846)[0m f1_macro: 0.3947586197567593
[2m[36m(func pid=180846)[0m f1_weighted: 0.41988466556707227
[2m[36m(func pid=180846)[0m f1_per_class: [0.526, 0.504, 0.667, 0.542, 0.129, 0.257, 0.348, 0.343, 0.273, 0.358]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.355410447761194
[2m[36m(func pid=4220)[0m top5: 0.9211753731343284
[2m[36m(func pid=4220)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=4220)[0m f1_macro: 0.32350971671446827
[2m[36m(func pid=4220)[0m f1_weighted: 0.3572956881138118
[2m[36m(func pid=4220)[0m f1_per_class: [0.476, 0.41, 0.462, 0.482, 0.131, 0.209, 0.295, 0.3, 0.199, 0.272]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0225 | Steps: 2 | Val loss: 1.9046 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1128 | Steps: 2 | Val loss: 7.2129 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7402 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1169 | Steps: 2 | Val loss: 1.7272 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=10089)[0m top1: 0.3558768656716418
[2m[36m(func pid=10089)[0m top5: 0.8922574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=10089)[0m f1_macro: 0.34684043079878235
[2m[36m(func pid=10089)[0m f1_weighted: 0.30177266442307615
[2m[36m(func pid=10089)[0m f1_per_class: [0.598, 0.485, 0.5, 0.487, 0.26, 0.228, 0.025, 0.37, 0.211, 0.306]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.15578358208955223
[2m[36m(func pid=10638)[0m top5: 0.7103544776119403
[2m[36m(func pid=10638)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=10638)[0m f1_macro: 0.21667999900376853
[2m[36m(func pid=10638)[0m f1_weighted: 0.14691095474098556
[2m[36m(func pid=10638)[0m f1_per_class: [0.213, 0.267, 0.857, 0.007, 0.135, 0.0, 0.266, 0.046, 0.102, 0.274]
[2m[36m(func pid=10638)[0m 
== Status ==
Current time: 2024-01-07 03:37:51 (running for 00:18:26.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   96 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.108 |      0.324 |                   46 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.023 |      0.347 |                   21 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.113 |      0.217 |                   20 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=180846)[0m top1: 0.41091417910447764
[2m[36m(func pid=180846)[0m top5: 0.9300373134328358
[2m[36m(func pid=180846)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=180846)[0m f1_macro: 0.39465089044745594
[2m[36m(func pid=180846)[0m f1_weighted: 0.4209896518658953
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.507, 0.667, 0.545, 0.128, 0.265, 0.346, 0.338, 0.275, 0.358]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.35634328358208955
[2m[36m(func pid=4220)[0m top5: 0.9225746268656716
[2m[36m(func pid=4220)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=4220)[0m f1_macro: 0.32403757440856285
[2m[36m(func pid=4220)[0m f1_weighted: 0.3520363181353915
[2m[36m(func pid=4220)[0m f1_per_class: [0.471, 0.425, 0.444, 0.494, 0.134, 0.21, 0.252, 0.315, 0.206, 0.289]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0403 | Steps: 2 | Val loss: 1.9341 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0432 | Steps: 2 | Val loss: 9.0286 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.7462 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=10089)[0m top1: 0.36427238805970147
[2m[36m(func pid=10089)[0m top5: 0.8978544776119403
[2m[36m(func pid=10089)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=10089)[0m f1_macro: 0.35215907619091136
[2m[36m(func pid=10089)[0m f1_weighted: 0.3093589469388448
[2m[36m(func pid=10089)[0m f1_per_class: [0.585, 0.494, 0.511, 0.505, 0.274, 0.229, 0.028, 0.369, 0.206, 0.321]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1001 | Steps: 2 | Val loss: 1.7240 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 03:37:56 (running for 00:18:31.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.395 |                   97 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.117 |      0.324 |                   47 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.04  |      0.352 |                   22 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.043 |      0.138 |                   21 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.09235074626865672
[2m[36m(func pid=10638)[0m top5: 0.6851679104477612
[2m[36m(func pid=10638)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=10638)[0m f1_macro: 0.13770482838195294
[2m[36m(func pid=10638)[0m f1_weighted: 0.07549916163792908
[2m[36m(func pid=10638)[0m f1_per_class: [0.136, 0.225, 0.471, 0.0, 0.095, 0.0, 0.082, 0.0, 0.082, 0.286]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.41277985074626866
[2m[36m(func pid=180846)[0m top5: 0.9286380597014925
[2m[36m(func pid=180846)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=180846)[0m f1_macro: 0.3953495533467807
[2m[36m(func pid=180846)[0m f1_weighted: 0.42409935796794235
[2m[36m(func pid=180846)[0m f1_per_class: [0.515, 0.504, 0.667, 0.551, 0.129, 0.262, 0.354, 0.337, 0.274, 0.362]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.36007462686567165
[2m[36m(func pid=4220)[0m top5: 0.9207089552238806
[2m[36m(func pid=4220)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=4220)[0m f1_macro: 0.32819481871092504
[2m[36m(func pid=4220)[0m f1_weighted: 0.35136620627312604
[2m[36m(func pid=4220)[0m f1_per_class: [0.476, 0.426, 0.444, 0.498, 0.148, 0.215, 0.239, 0.336, 0.202, 0.298]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0134 | Steps: 2 | Val loss: 1.9469 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2548 | Steps: 2 | Val loss: 8.6025 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7623 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0998 | Steps: 2 | Val loss: 1.7186 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=10089)[0m top1: 0.36613805970149255
[2m[36m(func pid=10089)[0m top5: 0.9048507462686567
[2m[36m(func pid=10089)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=10089)[0m f1_macro: 0.3552782576748268
[2m[36m(func pid=10089)[0m f1_weighted: 0.31409392984302886
[2m[36m(func pid=10089)[0m f1_per_class: [0.59, 0.499, 0.511, 0.509, 0.278, 0.231, 0.037, 0.364, 0.206, 0.329]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:38:01 (running for 00:18:36.55)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.395 |                   98 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.1   |      0.328 |                   48 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.013 |      0.355 |                   23 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.255 |      0.084 |                   22 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m top1: 0.08955223880597014
[2m[36m(func pid=10638)[0m top5: 0.6809701492537313
[2m[36m(func pid=10638)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=10638)[0m f1_macro: 0.08360257100299764
[2m[36m(func pid=10638)[0m f1_weighted: 0.09392270729890316
[2m[36m(func pid=10638)[0m f1_per_class: [0.034, 0.241, 0.049, 0.08, 0.061, 0.0, 0.079, 0.0, 0.081, 0.211]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.40718283582089554
[2m[36m(func pid=180846)[0m top5: 0.9244402985074627
[2m[36m(func pid=180846)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=180846)[0m f1_macro: 0.38824554233427555
[2m[36m(func pid=180846)[0m f1_weighted: 0.41972053835293793
[2m[36m(func pid=180846)[0m f1_per_class: [0.519, 0.496, 0.632, 0.546, 0.123, 0.255, 0.353, 0.336, 0.265, 0.358]
[2m[36m(func pid=180846)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3558768656716418
[2m[36m(func pid=4220)[0m top5: 0.9211753731343284
[2m[36m(func pid=4220)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=4220)[0m f1_macro: 0.33145587359484896
[2m[36m(func pid=4220)[0m f1_weighted: 0.3428693922720452
[2m[36m(func pid=4220)[0m f1_per_class: [0.486, 0.434, 0.49, 0.498, 0.157, 0.218, 0.206, 0.323, 0.199, 0.304]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0125 | Steps: 2 | Val loss: 1.9682 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1403 | Steps: 2 | Val loss: 14.6767 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=180846)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.7632 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:38:06 (running for 00:18:41.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00006 | RUNNING    | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0.001 |      0.388 |                   99 |
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.1   |      0.331 |                   49 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.013 |      0.358 |                   24 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.255 |      0.084 |                   22 |
| train_2d480_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.36800373134328357
[2m[36m(func pid=10089)[0m top5: 0.9071828358208955
[2m[36m(func pid=10089)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=10089)[0m f1_macro: 0.35795651373585535
[2m[36m(func pid=10089)[0m f1_weighted: 0.31825870187233524
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.507, 0.522, 0.51, 0.29, 0.241, 0.043, 0.359, 0.208, 0.329]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0915 | Steps: 2 | Val loss: 1.7114 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=10638)[0m top1: 0.0914179104477612
[2m[36m(func pid=10638)[0m top5: 0.6231343283582089
[2m[36m(func pid=10638)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=10638)[0m f1_macro: 0.0685842481218345
[2m[36m(func pid=10638)[0m f1_weighted: 0.10667203206280347
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.109, 0.023, 0.0, 0.065, 0.0, 0.279, 0.0, 0.076, 0.133]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=180846)[0m top1: 0.4076492537313433
[2m[36m(func pid=180846)[0m top5: 0.925839552238806
[2m[36m(func pid=180846)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=180846)[0m f1_macro: 0.38421335462159467
[2m[36m(func pid=180846)[0m f1_weighted: 0.4205633404949097
[2m[36m(func pid=180846)[0m f1_per_class: [0.5, 0.493, 0.615, 0.543, 0.13, 0.258, 0.361, 0.332, 0.27, 0.339]
[2m[36m(func pid=4220)[0m top1: 0.3558768656716418
[2m[36m(func pid=4220)[0m top5: 0.9211753731343284
[2m[36m(func pid=4220)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=4220)[0m f1_macro: 0.3312310874502224
[2m[36m(func pid=4220)[0m f1_weighted: 0.3352647433340168
[2m[36m(func pid=4220)[0m f1_per_class: [0.49, 0.441, 0.49, 0.504, 0.168, 0.203, 0.174, 0.329, 0.209, 0.304]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0119 | Steps: 2 | Val loss: 1.9885 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6271 | Steps: 2 | Val loss: 11.6765 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=10089)[0m top1: 0.3666044776119403
[2m[36m(func pid=10089)[0m top5: 0.9071828358208955
[2m[36m(func pid=10089)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.3633849947187104
[2m[36m(func pid=10089)[0m f1_weighted: 0.3189423746083783
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.497, 0.571, 0.507, 0.29, 0.243, 0.051, 0.354, 0.219, 0.329]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0667 | Steps: 2 | Val loss: 1.7080 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=10638)[0m top1: 0.30970149253731344
[2m[36m(func pid=10638)[0m top5: 0.6842350746268657
[2m[36m(func pid=10638)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=10638)[0m f1_macro: 0.14482761675390984
[2m[36m(func pid=10638)[0m f1_weighted: 0.2565844345588195
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.427, 0.044, 0.0, 0.178, 0.0, 0.594, 0.0, 0.095, 0.111]
[2m[36m(func pid=4220)[0m top1: 0.35401119402985076
[2m[36m(func pid=4220)[0m top5: 0.9230410447761194
[2m[36m(func pid=4220)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=4220)[0m f1_macro: 0.32938709139790723
[2m[36m(func pid=4220)[0m f1_weighted: 0.3326319587926623
[2m[36m(func pid=4220)[0m f1_per_class: [0.485, 0.444, 0.49, 0.504, 0.173, 0.207, 0.164, 0.322, 0.207, 0.298]
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0156 | Steps: 2 | Val loss: 2.0101 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 03:38:12 (running for 00:18:46.86)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.091 |      0.331 |                   50 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.012 |      0.363 |                   25 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.14  |      0.069 |                   23 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=16299)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=16299)[0m Configuration completed!
[2m[36m(func pid=16299)[0m New optimizer parameters:
[2m[36m(func pid=16299)[0m SGD (
[2m[36m(func pid=16299)[0m Parameter Group 0
[2m[36m(func pid=16299)[0m     dampening: 0
[2m[36m(func pid=16299)[0m     differentiable: False
[2m[36m(func pid=16299)[0m     foreach: None
[2m[36m(func pid=16299)[0m     lr: 0.1
[2m[36m(func pid=16299)[0m     maximize: False
[2m[36m(func pid=16299)[0m     momentum: 0.99
[2m[36m(func pid=16299)[0m     nesterov: False
[2m[36m(func pid=16299)[0m     weight_decay: 0.0001
[2m[36m(func pid=16299)[0m )
[2m[36m(func pid=16299)[0m 
== Status ==
Current time: 2024-01-07 03:38:17 (running for 00:18:52.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.067 |      0.329 |                   51 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.016 |      0.359 |                   26 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.627 |      0.145 |                   24 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.36427238805970147
[2m[36m(func pid=10089)[0m top5: 0.9090485074626866
[2m[36m(func pid=10089)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=10089)[0m f1_macro: 0.35942472014765126
[2m[36m(func pid=10089)[0m f1_weighted: 0.31918640623353456
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.501, 0.558, 0.505, 0.274, 0.245, 0.054, 0.341, 0.219, 0.326]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0700 | Steps: 2 | Val loss: 1.7089 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6005 | Steps: 2 | Val loss: 18.7028 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0178 | Steps: 2 | Val loss: 2.0187 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.0852 | Steps: 2 | Val loss: 5.1846 | Batch size: 32 | lr: 0.1 | Duration: 4.86s
[2m[36m(func pid=4220)[0m top1: 0.3512126865671642
[2m[36m(func pid=4220)[0m top5: 0.9225746268656716
[2m[36m(func pid=4220)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=4220)[0m f1_macro: 0.32772571114399013
[2m[36m(func pid=4220)[0m f1_weighted: 0.3255723630875848
[2m[36m(func pid=4220)[0m f1_per_class: [0.504, 0.443, 0.48, 0.5, 0.171, 0.192, 0.147, 0.329, 0.215, 0.297]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m top1: 0.026119402985074626
[2m[36m(func pid=10638)[0m top5: 0.48274253731343286
[2m[36m(func pid=10638)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=10638)[0m f1_macro: 0.09057459542477353
[2m[36m(func pid=10638)[0m f1_weighted: 0.01575080996736863
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.053, 0.533, 0.0, 0.286, 0.008, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=10638)[0m 
== Status ==
Current time: 2024-01-07 03:38:22 (running for 00:18:57.46)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.07  |      0.328 |                   52 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.016 |      0.359 |                   26 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.601 |      0.091 |                   25 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.085 |      0.016 |                    1 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.03311567164179104
[2m[36m(func pid=16299)[0m top5: 0.5895522388059702
[2m[36m(func pid=16299)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=16299)[0m f1_macro: 0.016003455927410298
[2m[36m(func pid=16299)[0m f1_weighted: 0.0438973587826519
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.147, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.36847014925373134
[2m[36m(func pid=10089)[0m top5: 0.9155783582089553
[2m[36m(func pid=10089)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=10089)[0m f1_macro: 0.366042476716077
[2m[36m(func pid=10089)[0m f1_weighted: 0.3268156503479347
[2m[36m(func pid=10089)[0m f1_per_class: [0.579, 0.504, 0.585, 0.512, 0.286, 0.245, 0.071, 0.336, 0.22, 0.322]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0580 | Steps: 2 | Val loss: 1.7022 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.0961 | Steps: 2 | Val loss: 9.2985 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 6.0406 | Steps: 2 | Val loss: 396.6711 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0082 | Steps: 2 | Val loss: 2.0255 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=10638)[0m top1: 0.04664179104477612
[2m[36m(func pid=10638)[0m top5: 0.5638992537313433
[2m[36m(func pid=10638)[0m f1_micro: 0.04664179104477612
[2m[36m(func pid=10638)[0m f1_macro: 0.07565234464686141
[2m[36m(func pid=10638)[0m f1_weighted: 0.051613679096420506
[2m[36m(func pid=10638)[0m f1_per_class: [0.014, 0.009, 0.043, 0.063, 0.34, 0.024, 0.056, 0.149, 0.027, 0.031]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3558768656716418
[2m[36m(func pid=4220)[0m top5: 0.9235074626865671
[2m[36m(func pid=4220)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=4220)[0m f1_macro: 0.33425109289228516
[2m[36m(func pid=4220)[0m f1_weighted: 0.32821992220604473
[2m[36m(func pid=4220)[0m f1_per_class: [0.526, 0.448, 0.49, 0.505, 0.173, 0.206, 0.14, 0.33, 0.215, 0.309]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:38:28 (running for 00:19:02.92)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.058 |      0.334 |                   53 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.018 |      0.366 |                   27 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.096 |      0.076 |                   26 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  6.041 |      0.005 |                    2 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.01632462686567164
[2m[36m(func pid=16299)[0m top5: 0.33488805970149255
[2m[36m(func pid=16299)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=16299)[0m f1_macro: 0.005416720033666953
[2m[36m(func pid=16299)[0m f1_weighted: 0.00477220119554825
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.37546641791044777
[2m[36m(func pid=10089)[0m top5: 0.9211753731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=10089)[0m f1_macro: 0.37172065333360715
[2m[36m(func pid=10089)[0m f1_weighted: 0.33422177031575095
[2m[36m(func pid=10089)[0m f1_per_class: [0.587, 0.508, 0.585, 0.521, 0.312, 0.263, 0.077, 0.342, 0.222, 0.3]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5351 | Steps: 2 | Val loss: 200.2560 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0707 | Steps: 2 | Val loss: 1.7078 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0069 | Steps: 2 | Val loss: 2.0581 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 9.1902 | Steps: 2 | Val loss: 24172306.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5806902985074627
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.35261194029850745
[2m[36m(func pid=4220)[0m top5: 0.9211753731343284
[2m[36m(func pid=4220)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=4220)[0m f1_macro: 0.3310610977517642
[2m[36m(func pid=4220)[0m f1_weighted: 0.32359686231576473
[2m[36m(func pid=4220)[0m f1_per_class: [0.53, 0.456, 0.48, 0.493, 0.175, 0.203, 0.134, 0.325, 0.215, 0.3]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:38:33 (running for 00:19:08.29)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.071 |      0.331 |                   54 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.008 |      0.372 |                   28 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.535 |      0.001 |                   27 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  9.19  |      0.011 |                    3 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.05783582089552239
[2m[36m(func pid=16299)[0m top5: 0.5149253731343284
[2m[36m(func pid=16299)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=16299)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=16299)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.375
[2m[36m(func pid=10089)[0m top5: 0.9239738805970149
[2m[36m(func pid=10089)[0m f1_micro: 0.375
[2m[36m(func pid=10089)[0m f1_macro: 0.3689486113833442
[2m[36m(func pid=10089)[0m f1_weighted: 0.3354923145628984
[2m[36m(func pid=10089)[0m f1_per_class: [0.595, 0.501, 0.571, 0.523, 0.282, 0.26, 0.085, 0.334, 0.222, 0.316]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0826 | Steps: 2 | Val loss: 785.8797 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0624 | Steps: 2 | Val loss: 1.7118 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0057 | Steps: 2 | Val loss: 2.0604 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 13.8659 | Steps: 2 | Val loss: 19169710080.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5125932835820896
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3568097014925373
[2m[36m(func pid=4220)[0m top5: 0.9216417910447762
[2m[36m(func pid=4220)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=4220)[0m f1_macro: 0.34091467668690284
[2m[36m(func pid=4220)[0m f1_weighted: 0.3249729183436052
[2m[36m(func pid=4220)[0m f1_per_class: [0.515, 0.468, 0.558, 0.498, 0.182, 0.196, 0.126, 0.328, 0.235, 0.303]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:38:38 (running for 00:19:13.55)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.062 |      0.341 |                   55 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.371 |                   30 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.083 |      0.001 |                   28 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  9.19  |      0.011 |                    3 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.376865671641791
[2m[36m(func pid=10089)[0m top5: 0.9281716417910447
[2m[36m(func pid=10089)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=10089)[0m f1_macro: 0.37128819788003586
[2m[36m(func pid=10089)[0m f1_weighted: 0.3398710681736506
[2m[36m(func pid=10089)[0m f1_per_class: [0.556, 0.503, 0.585, 0.527, 0.312, 0.278, 0.093, 0.323, 0.215, 0.32]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7006 | Steps: 2 | Val loss: 1248.8519 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0556 | Steps: 2 | Val loss: 1.7045 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0082 | Steps: 2 | Val loss: 2.0557 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 7.0859 | Steps: 2 | Val loss: 1846996224.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5088619402985075
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.36100746268656714
[2m[36m(func pid=4220)[0m top5: 0.9207089552238806
[2m[36m(func pid=4220)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=4220)[0m f1_macro: 0.3457393273914866
[2m[36m(func pid=4220)[0m f1_weighted: 0.32983505963673937
[2m[36m(func pid=4220)[0m f1_per_class: [0.519, 0.481, 0.585, 0.498, 0.18, 0.196, 0.134, 0.326, 0.245, 0.294]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.384794776119403
[2m[36m(func pid=10089)[0m top5: 0.9342350746268657
[2m[36m(func pid=10089)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.37704169505693846
[2m[36m(func pid=10089)[0m f1_weighted: 0.3513268627601573
[2m[36m(func pid=10089)[0m f1_per_class: [0.556, 0.513, 0.585, 0.532, 0.323, 0.295, 0.114, 0.322, 0.214, 0.316]
[2m[36m(func pid=10089)[0m 
== Status ==
Current time: 2024-01-07 03:38:44 (running for 00:19:19.13)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.056 |      0.346 |                   56 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.008 |      0.377 |                   31 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.701 |      0.001 |                   29 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 13.866 |      0.001 |                    4 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4164 | Steps: 2 | Val loss: 1238.0435 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0527 | Steps: 2 | Val loss: 1.7039 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0044 | Steps: 2 | Val loss: 2.0651 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.4292 | Steps: 2 | Val loss: 738557184.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5088619402985075
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3628731343283582
[2m[36m(func pid=4220)[0m top5: 0.9202425373134329
[2m[36m(func pid=4220)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=4220)[0m f1_macro: 0.34744330002014245
[2m[36m(func pid=4220)[0m f1_weighted: 0.32987254216389345
[2m[36m(func pid=4220)[0m f1_per_class: [0.507, 0.477, 0.585, 0.499, 0.194, 0.199, 0.132, 0.333, 0.254, 0.294]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:38:49 (running for 00:19:24.36)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.053 |      0.347 |                   57 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.382 |                   32 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.416 |      0.001 |                   30 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  7.086 |      0.001 |                    5 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.3829291044776119
[2m[36m(func pid=10089)[0m top5: 0.9347014925373134
[2m[36m(func pid=10089)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=10089)[0m f1_macro: 0.3815780571101016
[2m[36m(func pid=10089)[0m f1_weighted: 0.35319015707234314
[2m[36m(func pid=10089)[0m f1_per_class: [0.556, 0.514, 0.632, 0.522, 0.317, 0.299, 0.127, 0.317, 0.224, 0.308]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3707 | Steps: 2 | Val loss: 965.6390 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0425 | Steps: 2 | Val loss: 1.7012 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0053 | Steps: 2 | Val loss: 2.0973 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 23.2524 | Steps: 2 | Val loss: 457460064.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5158582089552238
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.36427238805970147
[2m[36m(func pid=4220)[0m top5: 0.9211753731343284
[2m[36m(func pid=4220)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=4220)[0m f1_macro: 0.3463938417960185
[2m[36m(func pid=4220)[0m f1_weighted: 0.32929839786787346
[2m[36m(func pid=4220)[0m f1_per_class: [0.511, 0.496, 0.571, 0.498, 0.194, 0.207, 0.119, 0.331, 0.241, 0.297]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:38:54 (running for 00:19:29.63)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.043 |      0.346 |                   58 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.005 |      0.382 |                   33 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.371 |      0.001 |                   31 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.429 |      0.001 |                    6 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.384794776119403
[2m[36m(func pid=10089)[0m top5: 0.9300373134328358
[2m[36m(func pid=10089)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.38231927763650864
[2m[36m(func pid=10089)[0m f1_weighted: 0.3577119854707139
[2m[36m(func pid=10089)[0m f1_per_class: [0.563, 0.516, 0.615, 0.523, 0.306, 0.324, 0.132, 0.314, 0.218, 0.312]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.9401 | Steps: 2 | Val loss: 803.1570 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0427 | Steps: 2 | Val loss: 1.6928 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0172 | Steps: 2 | Val loss: 2.1136 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.2062 | Steps: 2 | Val loss: 129960096.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5270522388059702
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3670708955223881
[2m[36m(func pid=4220)[0m top5: 0.9225746268656716
[2m[36m(func pid=4220)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=4220)[0m f1_macro: 0.3527144067272555
[2m[36m(func pid=4220)[0m f1_weighted: 0.33081311756160836
[2m[36m(func pid=4220)[0m f1_per_class: [0.5, 0.498, 0.615, 0.503, 0.202, 0.216, 0.113, 0.328, 0.255, 0.297]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:38:59 (running for 00:19:34.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.043 |      0.353 |                   59 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.017 |      0.384 |                   34 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.94  |      0.001 |                   32 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 23.252 |      0.001 |                    7 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.38619402985074625
[2m[36m(func pid=10089)[0m top5: 0.9328358208955224
[2m[36m(func pid=10089)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=10089)[0m f1_macro: 0.384031097332772
[2m[36m(func pid=10089)[0m f1_weighted: 0.36114539683486735
[2m[36m(func pid=10089)[0m f1_per_class: [0.556, 0.512, 0.6, 0.516, 0.314, 0.318, 0.152, 0.323, 0.219, 0.329]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8333 | Steps: 2 | Val loss: 1610.9923 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0466 | Steps: 2 | Val loss: 1.6863 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0027 | Steps: 2 | Val loss: 2.1327 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 6.9749 | Steps: 2 | Val loss: 49581160.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5093283582089553
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3670708955223881
[2m[36m(func pid=4220)[0m top5: 0.9244402985074627
[2m[36m(func pid=4220)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=4220)[0m f1_macro: 0.3553772353194065
[2m[36m(func pid=4220)[0m f1_weighted: 0.331771106914076
[2m[36m(func pid=4220)[0m f1_per_class: [0.5, 0.492, 0.632, 0.505, 0.214, 0.211, 0.118, 0.33, 0.26, 0.291]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:05 (running for 00:19:40.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.047 |      0.355 |                   60 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.386 |                   35 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.833 |      0.001 |                   33 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.206 |      0.001 |                    8 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.3880597014925373
[2m[36m(func pid=10089)[0m top5: 0.9342350746268657
[2m[36m(func pid=10089)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=10089)[0m f1_macro: 0.38584195333826937
[2m[36m(func pid=10089)[0m f1_weighted: 0.3650230242252388
[2m[36m(func pid=10089)[0m f1_per_class: [0.548, 0.516, 0.615, 0.52, 0.314, 0.312, 0.162, 0.322, 0.224, 0.325]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.17210820895522388
[2m[36m(func pid=16299)[0m top5: 0.5727611940298507
[2m[36m(func pid=16299)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=16299)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=16299)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2625 | Steps: 2 | Val loss: 1721.4314 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0421 | Steps: 2 | Val loss: 1.6870 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0050 | Steps: 2 | Val loss: 2.1618 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 5.4349 | Steps: 2 | Val loss: 37690248.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5093283582089553
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.36427238805970147
[2m[36m(func pid=4220)[0m top5: 0.9249067164179104
[2m[36m(func pid=4220)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=4220)[0m f1_macro: 0.35492356761956584
[2m[36m(func pid=4220)[0m f1_weighted: 0.3277695895289639
[2m[36m(func pid=4220)[0m f1_per_class: [0.507, 0.486, 0.632, 0.507, 0.222, 0.219, 0.105, 0.324, 0.253, 0.294]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:10 (running for 00:19:45.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.042 |      0.355 |                   61 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.005 |      0.392 |                   36 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.263 |      0.001 |                   34 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  6.975 |      0.029 |                    9 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.3880597014925373
[2m[36m(func pid=10089)[0m top5: 0.9328358208955224
[2m[36m(func pid=10089)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=10089)[0m f1_macro: 0.39152293712848324
[2m[36m(func pid=10089)[0m f1_weighted: 0.3649159042399455
[2m[36m(func pid=10089)[0m f1_per_class: [0.568, 0.514, 0.632, 0.516, 0.319, 0.323, 0.159, 0.326, 0.22, 0.338]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4562 | Steps: 2 | Val loss: 1101.7588 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0360 | Steps: 2 | Val loss: 1.6932 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0159 | Steps: 2 | Val loss: 2.2060 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 18.4391 | Steps: 2 | Val loss: 22950904.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=10638)[0m top1: 0.006063432835820896
[2m[36m(func pid=10638)[0m top5: 0.5153917910447762
[2m[36m(func pid=10638)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=10638)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=10638)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3619402985074627
[2m[36m(func pid=4220)[0m top5: 0.9249067164179104
[2m[36m(func pid=4220)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=4220)[0m f1_macro: 0.35468804491803746
[2m[36m(func pid=4220)[0m f1_weighted: 0.32650865121992934
[2m[36m(func pid=4220)[0m f1_per_class: [0.519, 0.49, 0.632, 0.5, 0.22, 0.22, 0.105, 0.319, 0.246, 0.297]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:16 (running for 00:19:51.12)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.036 |      0.355 |                   62 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.005 |      0.392 |                   36 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.456 |      0.001 |                   35 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 18.439 |      0.001 |                   11 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.3880597014925373
[2m[36m(func pid=10089)[0m top5: 0.9351679104477612
[2m[36m(func pid=10089)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=10089)[0m f1_macro: 0.3959799904132446
[2m[36m(func pid=10089)[0m f1_weighted: 0.36323212764834345
[2m[36m(func pid=10089)[0m f1_per_class: [0.587, 0.508, 0.667, 0.52, 0.319, 0.326, 0.152, 0.322, 0.223, 0.338]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0713 | Steps: 2 | Val loss: 660.9356 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0381 | Steps: 2 | Val loss: 1.6835 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=10638)[0m top1: 0.022388059701492536
[2m[36m(func pid=10638)[0m top5: 0.5928171641791045
[2m[36m(func pid=10638)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=10638)[0m f1_macro: 0.01786223277909739
[2m[36m(func pid=10638)[0m f1_weighted: 0.02869145956677421
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.166, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5846 | Steps: 2 | Val loss: 12303666.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0044 | Steps: 2 | Val loss: 2.1971 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=4220)[0m top1: 0.3614738805970149
[2m[36m(func pid=4220)[0m top5: 0.9267723880597015
[2m[36m(func pid=4220)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=4220)[0m f1_macro: 0.352430425796023
[2m[36m(func pid=4220)[0m f1_weighted: 0.3274570168612584
[2m[36m(func pid=4220)[0m f1_per_class: [0.504, 0.491, 0.632, 0.508, 0.212, 0.215, 0.105, 0.315, 0.243, 0.3]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:21 (running for 00:19:56.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.038 |      0.352 |                   63 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.016 |      0.396 |                   37 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.071 |      0.018 |                   36 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  2.585 |      0.001 |                   12 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.3903917910447761
[2m[36m(func pid=10089)[0m top5: 0.9402985074626866
[2m[36m(func pid=10089)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=10089)[0m f1_macro: 0.3977008421927188
[2m[36m(func pid=10089)[0m f1_weighted: 0.3669949132051258
[2m[36m(func pid=10089)[0m f1_per_class: [0.587, 0.512, 0.667, 0.525, 0.328, 0.322, 0.159, 0.311, 0.227, 0.338]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3732 | Steps: 2 | Val loss: 381.2458 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0323 | Steps: 2 | Val loss: 1.6832 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=10638)[0m top1: 0.08115671641791045
[2m[36m(func pid=10638)[0m top5: 0.7019589552238806
[2m[36m(func pid=10638)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=10638)[0m f1_macro: 0.05935853332091765
[2m[36m(func pid=10638)[0m f1_weighted: 0.07774544029150413
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.442, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.137]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6815 | Steps: 2 | Val loss: 6710389.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0017 | Steps: 2 | Val loss: 2.1918 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=4220)[0m top1: 0.36473880597014924
[2m[36m(func pid=4220)[0m top5: 0.925839552238806
[2m[36m(func pid=4220)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=4220)[0m f1_macro: 0.35592426601716676
[2m[36m(func pid=4220)[0m f1_weighted: 0.33012485059128466
[2m[36m(func pid=4220)[0m f1_per_class: [0.516, 0.489, 0.632, 0.513, 0.222, 0.221, 0.105, 0.319, 0.245, 0.297]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:26 (running for 00:20:01.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.032 |      0.356 |                   64 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.398 |                   38 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.373 |      0.059 |                   37 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  2.681 |      0.001 |                   13 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.39365671641791045
[2m[36m(func pid=10089)[0m top5: 0.9421641791044776
[2m[36m(func pid=10089)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=10089)[0m f1_macro: 0.40354088170677266
[2m[36m(func pid=10089)[0m f1_weighted: 0.3736905701415694
[2m[36m(func pid=10089)[0m f1_per_class: [0.59, 0.512, 0.686, 0.524, 0.328, 0.325, 0.181, 0.31, 0.228, 0.351]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.0774 | Steps: 2 | Val loss: 472.9677 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0442 | Steps: 2 | Val loss: 1.6869 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=10638)[0m top1: 0.009328358208955223
[2m[36m(func pid=10638)[0m top5: 0.582089552238806
[2m[36m(func pid=10638)[0m f1_micro: 0.009328358208955223
[2m[36m(func pid=10638)[0m f1_macro: 0.005322683412534608
[2m[36m(func pid=10638)[0m f1_weighted: 0.002261486130806971
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.011, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 19.9917 | Steps: 2 | Val loss: 4114403.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0012 | Steps: 2 | Val loss: 2.2106 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=4220)[0m top1: 0.363339552238806
[2m[36m(func pid=4220)[0m top5: 0.9239738805970149
[2m[36m(func pid=4220)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=4220)[0m f1_macro: 0.3535462768017104
[2m[36m(func pid=4220)[0m f1_weighted: 0.3276936904268219
[2m[36m(func pid=4220)[0m f1_per_class: [0.532, 0.5, 0.6, 0.506, 0.217, 0.215, 0.1, 0.316, 0.25, 0.3]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:32 (running for 00:20:06.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.044 |      0.354 |                   65 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.404 |                   39 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.077 |      0.005 |                   38 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 19.992 |      0.001 |                   14 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.39365671641791045
[2m[36m(func pid=10089)[0m top5: 0.9421641791044776
[2m[36m(func pid=10089)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=10089)[0m f1_macro: 0.40205935030004997
[2m[36m(func pid=10089)[0m f1_weighted: 0.37989719468513156
[2m[36m(func pid=10089)[0m f1_per_class: [0.582, 0.508, 0.686, 0.523, 0.319, 0.326, 0.208, 0.3, 0.222, 0.347]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.4766 | Steps: 2 | Val loss: 228.7548 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0391 | Steps: 2 | Val loss: 1.6813 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=10638)[0m top1: 0.0830223880597015
[2m[36m(func pid=10638)[0m top5: 0.4048507462686567
[2m[36m(func pid=10638)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=10638)[0m f1_macro: 0.03179835093448731
[2m[36m(func pid=10638)[0m f1_weighted: 0.05204599408912699
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6552 | Steps: 2 | Val loss: 2692358.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0019 | Steps: 2 | Val loss: 2.2285 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=4220)[0m top1: 0.3689365671641791
[2m[36m(func pid=4220)[0m top5: 0.9249067164179104
[2m[36m(func pid=4220)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=4220)[0m f1_macro: 0.3602274752926189
[2m[36m(func pid=4220)[0m f1_weighted: 0.33174144129711675
[2m[36m(func pid=4220)[0m f1_per_class: [0.55, 0.514, 0.615, 0.505, 0.234, 0.224, 0.1, 0.319, 0.258, 0.283]
[2m[36m(func pid=4220)[0m 
== Status ==
Current time: 2024-01-07 03:39:37 (running for 00:20:12.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.039 |      0.36  |                   66 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.402 |                   40 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  4.477 |      0.032 |                   39 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  2.655 |      0.001 |                   15 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.1184 | Steps: 2 | Val loss: 202.5026 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.3969216417910448
[2m[36m(func pid=10089)[0m top5: 0.9440298507462687
[2m[36m(func pid=10089)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=10089)[0m f1_macro: 0.404861815435858
[2m[36m(func pid=10089)[0m f1_weighted: 0.3829032285711362
[2m[36m(func pid=10089)[0m f1_per_class: [0.59, 0.502, 0.686, 0.534, 0.343, 0.325, 0.211, 0.304, 0.217, 0.338]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0289 | Steps: 2 | Val loss: 1.6749 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=10638)[0m top1: 0.15578358208955223
[2m[36m(func pid=10638)[0m top5: 0.4612873134328358
[2m[36m(func pid=10638)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=10638)[0m f1_macro: 0.03569016839746103
[2m[36m(func pid=10638)[0m f1_weighted: 0.06032675559835848
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.312, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6932 | Steps: 2 | Val loss: 1653670.1250 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.2447 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=4220)[0m top1: 0.3670708955223881
[2m[36m(func pid=4220)[0m top5: 0.9267723880597015
[2m[36m(func pid=4220)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=4220)[0m f1_macro: 0.3608105426031267
[2m[36m(func pid=4220)[0m f1_weighted: 0.33223820642389096
[2m[36m(func pid=4220)[0m f1_per_class: [0.569, 0.503, 0.615, 0.509, 0.237, 0.223, 0.105, 0.316, 0.242, 0.288]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.7593 | Steps: 2 | Val loss: 483.0910 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:39:42 (running for 00:20:17.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.029 |      0.361 |                   67 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.405 |                   41 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.118 |      0.036 |                   40 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  2.693 |      0.001 |                   16 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.3978544776119403
[2m[36m(func pid=10089)[0m top5: 0.9449626865671642
[2m[36m(func pid=10089)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.40353313985912254
[2m[36m(func pid=10089)[0m f1_weighted: 0.38637563319694435
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.5, 0.686, 0.539, 0.343, 0.334, 0.219, 0.296, 0.214, 0.333]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.17210820895522388
[2m[36m(func pid=10638)[0m top5: 0.5559701492537313
[2m[36m(func pid=10638)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=10638)[0m f1_macro: 0.03567511187482523
[2m[36m(func pid=10638)[0m f1_weighted: 0.05352596680603429
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.302, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0324 | Steps: 2 | Val loss: 1.6651 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 55.9030 | Steps: 2 | Val loss: 1708601600.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0027 | Steps: 2 | Val loss: 2.2452 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.0637 | Steps: 2 | Val loss: 210.1068 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 03:39:48 (running for 00:20:22.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.032 |      0.366 |                   68 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.404 |                   42 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  3.759 |      0.036 |                   41 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  2.693 |      0.001 |                   16 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.37033582089552236
[2m[36m(func pid=4220)[0m top5: 0.9281716417910447
[2m[36m(func pid=4220)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=4220)[0m f1_macro: 0.36620194078001356
[2m[36m(func pid=4220)[0m f1_weighted: 0.3376280728197002
[2m[36m(func pid=4220)[0m f1_per_class: [0.584, 0.514, 0.615, 0.51, 0.261, 0.222, 0.116, 0.31, 0.248, 0.283]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.39972014925373134
[2m[36m(func pid=10089)[0m top5: 0.945429104477612
[2m[36m(func pid=10089)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=10089)[0m f1_macro: 0.41012738287280737
[2m[36m(func pid=10089)[0m f1_weighted: 0.39085087607295743
[2m[36m(func pid=10089)[0m f1_per_class: [0.625, 0.498, 0.686, 0.54, 0.348, 0.328, 0.234, 0.29, 0.216, 0.338]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m top1: 0.2667910447761194
[2m[36m(func pid=10638)[0m top5: 0.5597014925373134
[2m[36m(func pid=10638)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.0899660896527532
[2m[36m(func pid=10638)[0m f1_weighted: 0.17810371142283904
[2m[36m(func pid=10638)[0m f1_per_class: [0.136, 0.26, 0.0, 0.466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0267 | Steps: 2 | Val loss: 1.6696 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.2461 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.2619 | Steps: 2 | Val loss: 96644904.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.0739 | Steps: 2 | Val loss: 173.5088 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 03:39:53 (running for 00:20:28.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.027 |      0.367 |                   69 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.41  |                   43 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  3.064 |      0.09  |                   42 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 55.903 |      0.001 |                   17 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.3694029850746269
[2m[36m(func pid=4220)[0m top5: 0.9281716417910447
[2m[36m(func pid=4220)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=4220)[0m f1_macro: 0.3667091782658377
[2m[36m(func pid=4220)[0m f1_weighted: 0.3353080280141124
[2m[36m(func pid=4220)[0m f1_per_class: [0.589, 0.515, 0.615, 0.507, 0.254, 0.225, 0.108, 0.308, 0.248, 0.297]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.40158582089552236
[2m[36m(func pid=10089)[0m top5: 0.9477611940298507
[2m[36m(func pid=10089)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=10089)[0m f1_macro: 0.41417757615753903
[2m[36m(func pid=10089)[0m f1_weighted: 0.3947981759043102
[2m[36m(func pid=10089)[0m f1_per_class: [0.617, 0.497, 0.706, 0.546, 0.343, 0.338, 0.239, 0.279, 0.221, 0.356]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.21082089552238806
[2m[36m(func pid=10638)[0m top5: 0.45009328358208955
[2m[36m(func pid=10638)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=10638)[0m f1_macro: 0.09264738645880202
[2m[36m(func pid=10638)[0m f1_weighted: 0.16353100500493178
[2m[36m(func pid=10638)[0m f1_per_class: [0.175, 0.04, 0.093, 0.524, 0.0, 0.052, 0.0, 0.0, 0.0, 0.043]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0341 | Steps: 2 | Val loss: 1.6680 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.1435 | Steps: 2 | Val loss: 25715122.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.2646 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2388 | Steps: 2 | Val loss: 249.0227 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:39:58 (running for 00:20:33.55)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.034 |      0.364 |                   70 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.414 |                   44 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.074 |      0.093 |                   43 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.262 |      0.001 |                   18 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.36800373134328357
[2m[36m(func pid=4220)[0m top5: 0.9300373134328358
[2m[36m(func pid=4220)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=4220)[0m f1_macro: 0.36392993720078615
[2m[36m(func pid=4220)[0m f1_weighted: 0.33255458967129314
[2m[36m(func pid=4220)[0m f1_per_class: [0.582, 0.514, 0.615, 0.511, 0.24, 0.222, 0.098, 0.311, 0.247, 0.3]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.40205223880597013
[2m[36m(func pid=10089)[0m top5: 0.9477611940298507
[2m[36m(func pid=10089)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=10089)[0m f1_macro: 0.41928581648885793
[2m[36m(func pid=10089)[0m f1_weighted: 0.3951697777774544
[2m[36m(func pid=10089)[0m f1_per_class: [0.617, 0.496, 0.706, 0.54, 0.358, 0.347, 0.241, 0.278, 0.226, 0.384]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.22621268656716417
[2m[36m(func pid=10638)[0m top5: 0.47901119402985076
[2m[36m(func pid=10638)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=10638)[0m f1_macro: 0.0863312977942072
[2m[36m(func pid=10638)[0m f1_weighted: 0.17382362433637594
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.084, 0.151, 0.549, 0.0, 0.041, 0.0, 0.0, 0.015, 0.024]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0262 | Steps: 2 | Val loss: 1.6599 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 28.0790 | Steps: 2 | Val loss: 10234949.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0060 | Steps: 2 | Val loss: 2.2882 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1363 | Steps: 2 | Val loss: 382.6235 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 03:40:04 (running for 00:20:38.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.026 |      0.363 |                   71 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.419 |                   45 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.239 |      0.086 |                   44 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.143 |      0.001 |                   19 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.37173507462686567
[2m[36m(func pid=4220)[0m top5: 0.9314365671641791
[2m[36m(func pid=4220)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=4220)[0m f1_macro: 0.36253391377102834
[2m[36m(func pid=4220)[0m f1_weighted: 0.34078752598877415
[2m[36m(func pid=4220)[0m f1_per_class: [0.561, 0.511, 0.585, 0.519, 0.243, 0.223, 0.121, 0.31, 0.246, 0.306]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.40158582089552236
[2m[36m(func pid=10089)[0m top5: 0.9477611940298507
[2m[36m(func pid=10089)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=10089)[0m f1_macro: 0.4166557705335359
[2m[36m(func pid=10089)[0m f1_weighted: 0.3949856476559833
[2m[36m(func pid=10089)[0m f1_per_class: [0.6, 0.496, 0.706, 0.541, 0.358, 0.336, 0.246, 0.281, 0.22, 0.384]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m top1: 0.1609141791044776
[2m[36m(func pid=10638)[0m top5: 0.37080223880597013
[2m[36m(func pid=10638)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=10638)[0m f1_macro: 0.043255580976368405
[2m[36m(func pid=10638)[0m f1_weighted: 0.06926682472943178
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.395, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.029]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0217 | Steps: 2 | Val loss: 1.6471 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.2603 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 12.3167 | Steps: 2 | Val loss: 5796146.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1929 | Steps: 2 | Val loss: 534.1535 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:40:09 (running for 00:20:44.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.022 |      0.366 |                   72 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.417 |                   46 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.136 |      0.043 |                   45 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 28.079 |      0.001 |                   20 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.375
[2m[36m(func pid=4220)[0m top5: 0.9347014925373134
[2m[36m(func pid=4220)[0m f1_micro: 0.375
[2m[36m(func pid=4220)[0m f1_macro: 0.3663892874625751
[2m[36m(func pid=4220)[0m f1_weighted: 0.34342311000504794
[2m[36m(func pid=4220)[0m f1_per_class: [0.574, 0.504, 0.585, 0.523, 0.25, 0.227, 0.126, 0.313, 0.249, 0.312]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.41277985074626866
[2m[36m(func pid=10089)[0m top5: 0.9524253731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=10089)[0m f1_macro: 0.4247699758182494
[2m[36m(func pid=10089)[0m f1_weighted: 0.40981863617395636
[2m[36m(func pid=10089)[0m f1_per_class: [0.61, 0.503, 0.706, 0.542, 0.369, 0.368, 0.278, 0.277, 0.217, 0.378]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m top1: 0.16371268656716417
[2m[36m(func pid=10638)[0m top5: 0.36007462686567165
[2m[36m(func pid=10638)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=10638)[0m f1_macro: 0.04310898400658198
[2m[36m(func pid=10638)[0m f1_weighted: 0.0685145335045919
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0195 | Steps: 2 | Val loss: 1.6454 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.2726 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 11.1394 | Steps: 2 | Val loss: 3804915.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.3573 | Steps: 2 | Val loss: 378.9589 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=4220)[0m top1: 0.3763992537313433
[2m[36m(func pid=4220)[0m top5: 0.9351679104477612
[2m[36m(func pid=4220)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=4220)[0m f1_macro: 0.3708429763547882
[2m[36m(func pid=4220)[0m f1_weighted: 0.34511425548734803
[2m[36m(func pid=4220)[0m f1_per_class: [0.587, 0.509, 0.6, 0.521, 0.254, 0.223, 0.131, 0.313, 0.245, 0.326]
== Status ==
Current time: 2024-01-07 03:40:14 (running for 00:20:49.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.02  |      0.371 |                   73 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.425 |                   47 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.193 |      0.043 |                   46 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 12.317 |      0.001 |                   21 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.41324626865671643
[2m[36m(func pid=10089)[0m top5: 0.9514925373134329
[2m[36m(func pid=10089)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=10089)[0m f1_macro: 0.42547018816441096
[2m[36m(func pid=10089)[0m f1_weighted: 0.4098397152182701
[2m[36m(func pid=10089)[0m f1_per_class: [0.602, 0.504, 0.706, 0.542, 0.369, 0.359, 0.281, 0.271, 0.219, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10638)[0m top1: 0.16324626865671643
[2m[36m(func pid=10638)[0m top5: 0.3712686567164179
[2m[36m(func pid=10638)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=10638)[0m f1_macro: 0.04895551089366251
[2m[36m(func pid=10638)[0m f1_weighted: 0.08007524923224703
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.405, 0.0, 0.032, 0.0, 0.008, 0.0, 0.0, 0.0, 0.044]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0510 | Steps: 2 | Val loss: 1.6338 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 11.5346 | Steps: 2 | Val loss: 3458718.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.2914 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7037 | Steps: 2 | Val loss: 195.8640 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 03:40:20 (running for 00:20:55.29)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.36575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.051 |      0.376 |                   74 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.425 |                   48 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.357 |      0.049 |                   47 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 11.139 |      0.001 |                   22 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.38013059701492535
[2m[36m(func pid=4220)[0m top5: 0.9365671641791045
[2m[36m(func pid=4220)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=4220)[0m f1_macro: 0.3764740488405104
[2m[36m(func pid=4220)[0m f1_weighted: 0.351691188572606
[2m[36m(func pid=4220)[0m f1_per_class: [0.598, 0.515, 0.615, 0.52, 0.254, 0.237, 0.144, 0.31, 0.256, 0.316]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4118470149253731
[2m[36m(func pid=10089)[0m top5: 0.9500932835820896
[2m[36m(func pid=10089)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=10089)[0m f1_macro: 0.42382819622020673
[2m[36m(func pid=10089)[0m f1_weighted: 0.40987683474932574
[2m[36m(func pid=10089)[0m f1_per_class: [0.619, 0.501, 0.706, 0.542, 0.358, 0.358, 0.285, 0.264, 0.222, 0.384]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.15438432835820895
[2m[36m(func pid=10638)[0m top5: 0.3805970149253731
[2m[36m(func pid=10638)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=10638)[0m f1_macro: 0.05746682544827617
[2m[36m(func pid=10638)[0m f1_weighted: 0.1057624850476123
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.412, 0.0, 0.118, 0.0, 0.015, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0183 | Steps: 2 | Val loss: 1.6327 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 6.6053 | Steps: 2 | Val loss: 2861556.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.3033 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1513 | Steps: 2 | Val loss: 198.0052 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:40:25 (running for 00:21:00.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.018 |      0.372 |                   75 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.424 |                   49 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.704 |      0.057 |                   48 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 11.535 |      0.001 |                   23 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.37966417910447764
[2m[36m(func pid=4220)[0m top5: 0.9356343283582089
[2m[36m(func pid=4220)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=4220)[0m f1_macro: 0.3717085401879053
[2m[36m(func pid=4220)[0m f1_weighted: 0.35054784430580416
[2m[36m(func pid=4220)[0m f1_per_class: [0.574, 0.513, 0.6, 0.521, 0.24, 0.233, 0.144, 0.313, 0.245, 0.333]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.416044776119403
[2m[36m(func pid=10089)[0m top5: 0.9538246268656716
[2m[36m(func pid=10089)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.42382300833174796
[2m[36m(func pid=10089)[0m f1_weighted: 0.41571351187172534
[2m[36m(func pid=10089)[0m f1_per_class: [0.619, 0.507, 0.706, 0.551, 0.353, 0.348, 0.299, 0.254, 0.218, 0.384]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.18563432835820895
[2m[36m(func pid=10638)[0m top5: 0.3969216417910448
[2m[36m(func pid=10638)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=10638)[0m f1_macro: 0.07833262076906145
[2m[36m(func pid=10638)[0m f1_weighted: 0.14447787194544012
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.43, 0.0, 0.237, 0.0, 0.029, 0.0, 0.0, 0.0, 0.088]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0215 | Steps: 2 | Val loss: 1.6368 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.8560 | Steps: 2 | Val loss: 2834120.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.3311 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1279 | Steps: 2 | Val loss: 1026.1598 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 03:40:31 (running for 00:21:05.99)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.021 |      0.378 |                   76 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.424 |                   50 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.151 |      0.078 |                   49 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  6.605 |      0.001 |                   24 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.3805970149253731
[2m[36m(func pid=4220)[0m top5: 0.9365671641791045
[2m[36m(func pid=4220)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=4220)[0m f1_macro: 0.37795158357159064
[2m[36m(func pid=4220)[0m f1_weighted: 0.3529710537743205
[2m[36m(func pid=4220)[0m f1_per_class: [0.579, 0.517, 0.649, 0.524, 0.24, 0.224, 0.15, 0.31, 0.246, 0.341]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.41511194029850745
[2m[36m(func pid=10089)[0m top5: 0.9500932835820896
[2m[36m(func pid=10089)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=10089)[0m f1_macro: 0.42069993528683475
[2m[36m(func pid=10089)[0m f1_weighted: 0.415971537604774
[2m[36m(func pid=10089)[0m f1_per_class: [0.612, 0.501, 0.706, 0.551, 0.333, 0.338, 0.307, 0.251, 0.224, 0.384]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.16651119402985073
[2m[36m(func pid=10638)[0m top5: 0.38759328358208955
[2m[36m(func pid=10638)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=10638)[0m f1_macro: 0.060744930768722935
[2m[36m(func pid=10638)[0m f1_weighted: 0.08796117554651188
[2m[36m(func pid=10638)[0m f1_per_class: [0.039, 0.377, 0.0, 0.052, 0.0, 0.059, 0.0, 0.0, 0.0, 0.08]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0166 | Steps: 2 | Val loss: 1.6352 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 14.5878 | Steps: 2 | Val loss: 1599561.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.3536 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.7204 | Steps: 2 | Val loss: 2194.3828 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:40:36 (running for 00:21:11.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.021 |      0.378 |                   76 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.421 |                   51 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.128 |      0.061 |                   50 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 14.588 |      0.001 |                   26 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m top1: 0.3810634328358209
[2m[36m(func pid=4220)[0m top5: 0.9361007462686567
[2m[36m(func pid=4220)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=4220)[0m f1_macro: 0.3761268758957702
[2m[36m(func pid=4220)[0m f1_weighted: 0.3561378937926988
[2m[36m(func pid=4220)[0m f1_per_class: [0.574, 0.522, 0.649, 0.525, 0.225, 0.224, 0.159, 0.306, 0.241, 0.337]
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.416044776119403
[2m[36m(func pid=10089)[0m top5: 0.9500932835820896
[2m[36m(func pid=10089)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.4221452991404099
[2m[36m(func pid=10089)[0m f1_weighted: 0.41649909722181994
[2m[36m(func pid=10089)[0m f1_per_class: [0.6, 0.493, 0.706, 0.556, 0.333, 0.349, 0.305, 0.256, 0.224, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.16651119402985073
[2m[36m(func pid=10638)[0m top5: 0.3978544776119403
[2m[36m(func pid=10638)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=10638)[0m f1_macro: 0.05112524210716098
[2m[36m(func pid=10638)[0m f1_weighted: 0.0791848336900552
[2m[36m(func pid=10638)[0m f1_per_class: [0.023, 0.362, 0.0, 0.019, 0.0, 0.097, 0.0, 0.0, 0.0, 0.01]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 5.9787 | Steps: 2 | Val loss: 781564.9375 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0209 | Steps: 2 | Val loss: 1.6290 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.3661 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.5999 | Steps: 2 | Val loss: 984.3617 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:40:41 (running for 00:21:16.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.017 |      0.376 |                   77 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.422 |                   52 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.72  |      0.051 |                   51 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  5.979 |      0.001 |                   27 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m top1: 0.38572761194029853
[2m[36m(func pid=4220)[0m top5: 0.9365671641791045
[2m[36m(func pid=4220)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=4220)[0m f1_macro: 0.3760378694932898
[2m[36m(func pid=4220)[0m f1_weighted: 0.36103650626348527
[2m[36m(func pid=4220)[0m f1_per_class: [0.587, 0.524, 0.6, 0.521, 0.225, 0.228, 0.173, 0.317, 0.243, 0.341]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4123134328358209
[2m[36m(func pid=10089)[0m top5: 0.9505597014925373
[2m[36m(func pid=10089)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=10089)[0m f1_macro: 0.4179736142581561
[2m[36m(func pid=10089)[0m f1_weighted: 0.4143334965028615
[2m[36m(func pid=10089)[0m f1_per_class: [0.584, 0.493, 0.706, 0.552, 0.333, 0.339, 0.309, 0.241, 0.222, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.292910447761194
[2m[36m(func pid=10638)[0m top5: 0.585820895522388
[2m[36m(func pid=10638)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.08364771394796303
[2m[36m(func pid=10638)[0m f1_weighted: 0.18487874882596586
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.04, 0.0, 0.543, 0.0, 0.23, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 5.5833 | Steps: 2 | Val loss: 623693.5625 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0256 | Steps: 2 | Val loss: 1.6297 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.3708 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0404 | Steps: 2 | Val loss: 1094.4634 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:40:46 (running for 00:21:21.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.021 |      0.376 |                   78 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.418 |                   53 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  3.6   |      0.084 |                   52 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  5.583 |      0.001 |                   28 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m top1: 0.38199626865671643
[2m[36m(func pid=4220)[0m top5: 0.9370335820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=4220)[0m f1_macro: 0.3708751266395016
[2m[36m(func pid=4220)[0m f1_weighted: 0.360499751939352
[2m[36m(func pid=4220)[0m f1_per_class: [0.558, 0.514, 0.585, 0.521, 0.243, 0.218, 0.185, 0.311, 0.239, 0.333]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10089)[0m top1: 0.41884328358208955
[2m[36m(func pid=10089)[0m top5: 0.9519589552238806
[2m[36m(func pid=10089)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=10089)[0m f1_macro: 0.42229609905511933
[2m[36m(func pid=10089)[0m f1_weighted: 0.42263124482042436
[2m[36m(func pid=10089)[0m f1_per_class: [0.584, 0.499, 0.706, 0.558, 0.338, 0.342, 0.326, 0.242, 0.217, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.32276119402985076
[2m[36m(func pid=10638)[0m top5: 0.6884328358208955
[2m[36m(func pid=10638)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=10638)[0m f1_macro: 0.11057674241757612
[2m[36m(func pid=10638)[0m f1_weighted: 0.20906174938891714
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.027, 0.0, 0.538, 0.0, 0.305, 0.057, 0.0, 0.025, 0.154]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 10.6984 | Steps: 2 | Val loss: 464613.8750 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.3892 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0270 | Steps: 2 | Val loss: 1.6417 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 4.4564 | Steps: 2 | Val loss: 579.9186 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
== Status ==
Current time: 2024-01-07 03:40:52 (running for 00:21:27.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.026 |      0.371 |                   79 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.422 |                   54 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.04  |      0.111 |                   53 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 10.698 |      0.001 |                   29 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m top1: 0.41884328358208955
[2m[36m(func pid=10089)[0m top5: 0.9510261194029851
[2m[36m(func pid=10089)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=10089)[0m f1_macro: 0.42135034673810906
[2m[36m(func pid=10089)[0m f1_weighted: 0.42332580964084104
[2m[36m(func pid=10089)[0m f1_per_class: [0.578, 0.498, 0.706, 0.558, 0.333, 0.341, 0.331, 0.236, 0.215, 0.417]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.37779850746268656
[2m[36m(func pid=4220)[0m top5: 0.9361007462686567
[2m[36m(func pid=4220)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=4220)[0m f1_macro: 0.3685409517285779
[2m[36m(func pid=4220)[0m f1_weighted: 0.3536720375834472
[2m[36m(func pid=4220)[0m f1_per_class: [0.552, 0.519, 0.585, 0.517, 0.244, 0.21, 0.166, 0.307, 0.242, 0.341]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m top1: 0.3362873134328358
[2m[36m(func pid=10638)[0m top5: 0.6968283582089553
[2m[36m(func pid=10638)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=10638)[0m f1_macro: 0.12778919349941473
[2m[36m(func pid=10638)[0m f1_weighted: 0.2546113833509198
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.016, 0.0, 0.564, 0.0, 0.269, 0.201, 0.0, 0.059, 0.168]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.9249 | Steps: 2 | Val loss: 398875.9688 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.3985 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0128 | Steps: 2 | Val loss: 1.6321 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8219 | Steps: 2 | Val loss: 1137.4358 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:40:57 (running for 00:21:32.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.027 |      0.369 |                   80 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.421 |                   55 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  4.456 |      0.128 |                   54 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.925 |      0.001 |                   30 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42024253731343286
[2m[36m(func pid=10089)[0m top5: 0.9524253731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=10089)[0m f1_macro: 0.4207990811022902
[2m[36m(func pid=10089)[0m f1_weighted: 0.4266717032709325
[2m[36m(func pid=10089)[0m f1_per_class: [0.562, 0.499, 0.706, 0.563, 0.338, 0.336, 0.341, 0.232, 0.215, 0.417]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.1333955223880597
[2m[36m(func pid=10638)[0m top5: 0.4916044776119403
[2m[36m(func pid=10638)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=10638)[0m f1_macro: 0.08316512433473873
[2m[36m(func pid=10638)[0m f1_weighted: 0.14560478664720877
[2m[36m(func pid=10638)[0m f1_per_class: [0.051, 0.011, 0.0, 0.003, 0.0, 0.169, 0.404, 0.0, 0.0, 0.194]
[2m[36m(func pid=4220)[0m top1: 0.38199626865671643
[2m[36m(func pid=4220)[0m top5: 0.9370335820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=4220)[0m f1_macro: 0.3730542624841122
[2m[36m(func pid=4220)[0m f1_weighted: 0.35904670275654066
[2m[36m(func pid=4220)[0m f1_per_class: [0.552, 0.521, 0.6, 0.521, 0.26, 0.218, 0.176, 0.311, 0.242, 0.33]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 7.6946 | Steps: 2 | Val loss: 351925.2812 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.4040 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7373 | Steps: 2 | Val loss: 976.0669 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0228 | Steps: 2 | Val loss: 1.6242 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 03:41:02 (running for 00:21:37.65)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.013 |      0.373 |                   81 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.421 |                   56 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.822 |      0.083 |                   55 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  7.695 |      0.001 |                   31 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42257462686567165
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=10089)[0m f1_macro: 0.4223700098774258
[2m[36m(func pid=10089)[0m f1_weighted: 0.4305894695686063
[2m[36m(func pid=10089)[0m f1_per_class: [0.568, 0.498, 0.727, 0.567, 0.329, 0.341, 0.35, 0.229, 0.215, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.1259328358208955
[2m[36m(func pid=10638)[0m top5: 0.5279850746268657
[2m[36m(func pid=10638)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=10638)[0m f1_macro: 0.08110712645374973
[2m[36m(func pid=10638)[0m f1_weighted: 0.1457260946573878
[2m[36m(func pid=10638)[0m f1_per_class: [0.043, 0.134, 0.0, 0.0, 0.0, 0.077, 0.372, 0.0, 0.0, 0.185]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.38199626865671643
[2m[36m(func pid=4220)[0m top5: 0.9384328358208955
[2m[36m(func pid=4220)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=4220)[0m f1_macro: 0.37339195650535156
[2m[36m(func pid=4220)[0m f1_weighted: 0.36004448356550345
[2m[36m(func pid=4220)[0m f1_per_class: [0.558, 0.507, 0.6, 0.525, 0.274, 0.233, 0.178, 0.312, 0.248, 0.3]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.5864 | Steps: 2 | Val loss: 349594.6250 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.4204 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1006 | Steps: 2 | Val loss: 416.7993 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0280 | Steps: 2 | Val loss: 1.6176 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 03:41:07 (running for 00:21:42.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.023 |      0.373 |                   82 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.422 |                   57 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.737 |      0.081 |                   56 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.586 |      0.001 |                   32 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4221082089552239
[2m[36m(func pid=10089)[0m top5: 0.9524253731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=10089)[0m f1_macro: 0.4236238819336295
[2m[36m(func pid=10089)[0m f1_weighted: 0.43209446761093195
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.496, 0.727, 0.559, 0.324, 0.346, 0.363, 0.22, 0.213, 0.417]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.10541044776119403
[2m[36m(func pid=10638)[0m top5: 0.5093283582089553
[2m[36m(func pid=10638)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=10638)[0m f1_macro: 0.05598672775120046
[2m[36m(func pid=10638)[0m f1_weighted: 0.06942636441827309
[2m[36m(func pid=10638)[0m f1_per_class: [0.052, 0.303, 0.0, 0.0, 0.0, 0.007, 0.046, 0.0, 0.0, 0.152]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.38899253731343286
[2m[36m(func pid=4220)[0m top5: 0.9384328358208955
[2m[36m(func pid=4220)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=4220)[0m f1_macro: 0.3755251587022503
[2m[36m(func pid=4220)[0m f1_weighted: 0.3663051491137714
[2m[36m(func pid=4220)[0m f1_per_class: [0.564, 0.502, 0.585, 0.529, 0.27, 0.234, 0.194, 0.322, 0.251, 0.303]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 4.0269 | Steps: 2 | Val loss: 371769.6562 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.4536 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.8412 | Steps: 2 | Val loss: 391.8668 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0115 | Steps: 2 | Val loss: 1.6195 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 03:41:13 (running for 00:21:47.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.028 |      0.376 |                   83 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.424 |                   58 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.101 |      0.056 |                   57 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.027 |      0.001 |                   33 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m top1: 0.41884328358208955
[2m[36m(func pid=10089)[0m top5: 0.9505597014925373
[2m[36m(func pid=10089)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=10089)[0m f1_macro: 0.4222957485904429
[2m[36m(func pid=10089)[0m f1_weighted: 0.4293538088625787
[2m[36m(func pid=10089)[0m f1_per_class: [0.553, 0.493, 0.75, 0.551, 0.313, 0.344, 0.364, 0.212, 0.225, 0.417]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=10638)[0m top1: 0.10167910447761194
[2m[36m(func pid=10638)[0m top5: 0.5013992537313433
[2m[36m(func pid=10638)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.05800515213233165
[2m[36m(func pid=10638)[0m f1_weighted: 0.05621984287749792
[2m[36m(func pid=10638)[0m f1_per_class: [0.075, 0.267, 0.0, 0.016, 0.07, 0.0, 0.006, 0.0, 0.0, 0.145]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=4220)[0m top1: 0.38899253731343286
[2m[36m(func pid=4220)[0m top5: 0.9398320895522388
[2m[36m(func pid=4220)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=4220)[0m f1_macro: 0.3755220570141824
[2m[36m(func pid=4220)[0m f1_weighted: 0.36667760225043
[2m[36m(func pid=4220)[0m f1_per_class: [0.552, 0.503, 0.585, 0.525, 0.274, 0.233, 0.199, 0.327, 0.247, 0.309]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 40.6793 | Steps: 2 | Val loss: 312645.3438 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5358 | Steps: 2 | Val loss: 465.0142 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.4587 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 03:41:18 (running for 00:21:52.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.011 |      0.376 |                   84 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.422 |                   59 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.841 |      0.058 |                   58 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 40.679 |      0.001 |                   34 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0198 | Steps: 2 | Val loss: 1.6104 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=10638)[0m top1: 0.09048507462686567
[2m[36m(func pid=10638)[0m top5: 0.566231343283582
[2m[36m(func pid=10638)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=10638)[0m f1_macro: 0.05610749256649945
[2m[36m(func pid=10638)[0m f1_weighted: 0.04332702415826874
[2m[36m(func pid=10638)[0m f1_per_class: [0.083, 0.225, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.233]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42677238805970147
[2m[36m(func pid=10089)[0m top5: 0.9524253731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=10089)[0m f1_macro: 0.42955258951097103
[2m[36m(func pid=10089)[0m f1_weighted: 0.43930853772493744
[2m[36m(func pid=10089)[0m f1_per_class: [0.574, 0.493, 0.774, 0.562, 0.313, 0.343, 0.387, 0.207, 0.225, 0.417]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.38992537313432835
[2m[36m(func pid=4220)[0m top5: 0.9393656716417911
[2m[36m(func pid=4220)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=4220)[0m f1_macro: 0.3816575151656283
[2m[36m(func pid=4220)[0m f1_weighted: 0.3711372342545539
[2m[36m(func pid=4220)[0m f1_per_class: [0.566, 0.501, 0.649, 0.523, 0.263, 0.218, 0.221, 0.331, 0.242, 0.303]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.3507 | Steps: 2 | Val loss: 381397.3438 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.4747 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5826 | Steps: 2 | Val loss: 350.4638 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
== Status ==
Current time: 2024-01-07 03:41:23 (running for 00:21:58.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.02  |      0.382 |                   85 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.43  |                   60 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.536 |      0.056 |                   59 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.351 |      0.001 |                   35 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0095 | Steps: 2 | Val loss: 1.6089 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=10638)[0m top1: 0.09235074626865672
[2m[36m(func pid=10638)[0m top5: 0.5769589552238806
[2m[36m(func pid=10638)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=10638)[0m f1_macro: 0.0576540700911853
[2m[36m(func pid=10638)[0m f1_weighted: 0.045271901879722506
[2m[36m(func pid=10638)[0m f1_per_class: [0.087, 0.22, 0.0, 0.01, 0.026, 0.0, 0.0, 0.0, 0.0, 0.233]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4239738805970149
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=10089)[0m f1_macro: 0.428154099746007
[2m[36m(func pid=10089)[0m f1_weighted: 0.43915933792841616
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.488, 0.774, 0.561, 0.317, 0.335, 0.395, 0.204, 0.219, 0.417]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.3931902985074627
[2m[36m(func pid=4220)[0m top5: 0.9388992537313433
[2m[36m(func pid=4220)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=4220)[0m f1_macro: 0.37648154174512993
[2m[36m(func pid=4220)[0m f1_weighted: 0.3737927166088317
[2m[36m(func pid=4220)[0m f1_per_class: [0.552, 0.507, 0.585, 0.522, 0.25, 0.235, 0.221, 0.334, 0.254, 0.303]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 6.4601 | Steps: 2 | Val loss: 439858.5938 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4612 | Steps: 2 | Val loss: 200.3544 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0072 | Steps: 2 | Val loss: 2.4835 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 03:41:28 (running for 00:22:03.61)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.01  |      0.376 |                   86 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.428 |                   61 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.583 |      0.058 |                   60 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  6.46  |      0.001 |                   36 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0203 | Steps: 2 | Val loss: 1.6186 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=10638)[0m top1: 0.10167910447761194
[2m[36m(func pid=10638)[0m top5: 0.601679104477612
[2m[36m(func pid=10638)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=10638)[0m f1_macro: 0.06432103751063696
[2m[36m(func pid=10638)[0m f1_weighted: 0.05703877182824867
[2m[36m(func pid=10638)[0m f1_per_class: [0.082, 0.216, 0.0, 0.054, 0.072, 0.0, 0.0, 0.0, 0.0, 0.218]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42630597014925375
[2m[36m(func pid=10089)[0m top5: 0.9556902985074627
[2m[36m(func pid=10089)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=10089)[0m f1_macro: 0.4287231754512654
[2m[36m(func pid=10089)[0m f1_weighted: 0.441381477204436
[2m[36m(func pid=10089)[0m f1_per_class: [0.553, 0.493, 0.774, 0.567, 0.347, 0.333, 0.397, 0.195, 0.217, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.39132462686567165
[2m[36m(func pid=4220)[0m top5: 0.9370335820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=4220)[0m f1_macro: 0.37660497731039083
[2m[36m(func pid=4220)[0m f1_weighted: 0.3705333381929237
[2m[36m(func pid=4220)[0m f1_per_class: [0.577, 0.501, 0.585, 0.519, 0.25, 0.223, 0.22, 0.333, 0.254, 0.303]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.5654 | Steps: 2 | Val loss: 566240.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.9501 | Steps: 2 | Val loss: 109.3793 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.5029 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 03:41:34 (running for 00:22:08.96)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.02  |      0.377 |                   87 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.007 |      0.429 |                   62 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.461 |      0.064 |                   61 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.565 |      0.001 |                   37 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0161 | Steps: 2 | Val loss: 1.6232 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=10638)[0m top1: 0.14738805970149255
[2m[36m(func pid=10638)[0m top5: 0.6459888059701493
[2m[36m(func pid=10638)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=10638)[0m f1_macro: 0.09288541192766969
[2m[36m(func pid=10638)[0m f1_weighted: 0.12368783615709178
[2m[36m(func pid=10638)[0m f1_per_class: [0.067, 0.348, 0.0, 0.19, 0.103, 0.0, 0.018, 0.0, 0.041, 0.162]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4291044776119403
[2m[36m(func pid=10089)[0m top5: 0.9552238805970149
[2m[36m(func pid=10089)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.4294610513492166
[2m[36m(func pid=10089)[0m f1_weighted: 0.444389790338169
[2m[36m(func pid=10089)[0m f1_per_class: [0.568, 0.486, 0.774, 0.578, 0.338, 0.324, 0.404, 0.19, 0.221, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=4220)[0m top1: 0.386660447761194
[2m[36m(func pid=4220)[0m top5: 0.9370335820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=4220)[0m f1_macro: 0.3717380438333394
[2m[36m(func pid=4220)[0m f1_weighted: 0.36555200996944165
[2m[36m(func pid=4220)[0m f1_per_class: [0.561, 0.498, 0.585, 0.52, 0.25, 0.212, 0.211, 0.333, 0.242, 0.306]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.2111 | Steps: 2 | Val loss: 725151.1875 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 4.2887 | Steps: 2 | Val loss: 176.9975 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 03:41:39 (running for 00:22:14.24)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.016 |      0.372 |                   88 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.429 |                   63 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  3.95  |      0.093 |                   62 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.211 |      0.001 |                   38 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.5269 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0102 | Steps: 2 | Val loss: 1.6095 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=10638)[0m top1: 0.11800373134328358
[2m[36m(func pid=10638)[0m top5: 0.6319962686567164
[2m[36m(func pid=10638)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=10638)[0m f1_macro: 0.07583017614848116
[2m[36m(func pid=10638)[0m f1_weighted: 0.09425977962114705
[2m[36m(func pid=10638)[0m f1_per_class: [0.075, 0.293, 0.0, 0.131, 0.03, 0.0, 0.003, 0.0, 0.093, 0.133]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4281716417910448
[2m[36m(func pid=10089)[0m top5: 0.9552238805970149
[2m[36m(func pid=10089)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=10089)[0m f1_macro: 0.4290268455904078
[2m[36m(func pid=10089)[0m f1_weighted: 0.4440040435173984
[2m[36m(func pid=10089)[0m f1_per_class: [0.581, 0.484, 0.774, 0.577, 0.325, 0.33, 0.403, 0.186, 0.218, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 540.5870 | Steps: 2 | Val loss: 564949.9375 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=4220)[0m top1: 0.3908582089552239
[2m[36m(func pid=4220)[0m top5: 0.9393656716417911
[2m[36m(func pid=4220)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=4220)[0m f1_macro: 0.3762323114744657
[2m[36m(func pid=4220)[0m f1_weighted: 0.3719960206485654
[2m[36m(func pid=4220)[0m f1_per_class: [0.566, 0.498, 0.585, 0.521, 0.25, 0.224, 0.225, 0.339, 0.245, 0.309]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.9367 | Steps: 2 | Val loss: 310.8271 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:41:44 (running for 00:22:19.52)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.01  |      0.376 |                   89 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.429 |                   64 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   4.289 |      0.076 |                   63 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 540.587 |      0.001 |                   39 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.5565 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5088619402985075
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001207617278216442
[2m[36m(func pid=16299)[0m f1_weighted: 7.322306257842233e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0079 | Steps: 2 | Val loss: 1.6131 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=10638)[0m top1: 0.09375
[2m[36m(func pid=10638)[0m top5: 0.6114738805970149
[2m[36m(func pid=10638)[0m f1_micro: 0.09375
[2m[36m(func pid=10638)[0m f1_macro: 0.03945928465516943
[2m[36m(func pid=10638)[0m f1_weighted: 0.049875739949894506
[2m[36m(func pid=10638)[0m f1_per_class: [0.08, 0.249, 0.0, 0.01, 0.0, 0.0, 0.003, 0.0, 0.053, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4291044776119403
[2m[36m(func pid=10089)[0m top5: 0.9547574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.43148574882506346
[2m[36m(func pid=10089)[0m f1_weighted: 0.44516465646184944
[2m[36m(func pid=10089)[0m f1_per_class: [0.581, 0.491, 0.774, 0.579, 0.333, 0.335, 0.399, 0.189, 0.224, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 102.2477 | Steps: 2 | Val loss: 1843240.6250 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=4220)[0m top1: 0.3903917910447761
[2m[36m(func pid=4220)[0m top5: 0.9370335820895522
[2m[36m(func pid=4220)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=4220)[0m f1_macro: 0.3743143308767777
[2m[36m(func pid=4220)[0m f1_weighted: 0.3716886949712501
[2m[36m(func pid=4220)[0m f1_per_class: [0.556, 0.505, 0.585, 0.52, 0.25, 0.223, 0.224, 0.329, 0.246, 0.306]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8442 | Steps: 2 | Val loss: 401.6877 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:41:49 (running for 00:22:24.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.008 |      0.374 |                   90 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0.001 |      0.431 |                   65 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.937 |      0.039 |                   64 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 102.248 |      0.014 |                   40 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.5776 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=16299)[0m top1: 0.043843283582089554
[2m[36m(func pid=16299)[0m top5: 0.5475746268656716
[2m[36m(func pid=16299)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=16299)[0m f1_macro: 0.013680311738795437
[2m[36m(func pid=16299)[0m f1_weighted: 0.03302354267070202
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.019, 0.118, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0130 | Steps: 2 | Val loss: 1.6082 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=10638)[0m top1: 0.1259328358208955
[2m[36m(func pid=10638)[0m top5: 0.5410447761194029
[2m[36m(func pid=10638)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=10638)[0m f1_macro: 0.05122899309161795
[2m[36m(func pid=10638)[0m f1_weighted: 0.09011242406072444
[2m[36m(func pid=10638)[0m f1_per_class: [0.073, 0.282, 0.0, 0.136, 0.0, 0.0, 0.006, 0.0, 0.016, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.427705223880597
[2m[36m(func pid=10089)[0m top5: 0.9547574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=10089)[0m f1_macro: 0.4332145593858657
[2m[36m(func pid=10089)[0m f1_weighted: 0.4412348432082506
[2m[36m(func pid=10089)[0m f1_per_class: [0.577, 0.491, 0.8, 0.58, 0.333, 0.335, 0.383, 0.192, 0.231, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 139.8283 | Steps: 2 | Val loss: 5312751.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=4220)[0m top1: 0.39225746268656714
[2m[36m(func pid=4220)[0m top5: 0.9388992537313433
[2m[36m(func pid=4220)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=4220)[0m f1_macro: 0.38407654869199853
[2m[36m(func pid=4220)[0m f1_weighted: 0.37384241491963377
[2m[36m(func pid=4220)[0m f1_per_class: [0.564, 0.502, 0.649, 0.517, 0.263, 0.222, 0.233, 0.332, 0.24, 0.319]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.1734 | Steps: 2 | Val loss: 1049.5951 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 03:41:55 (running for 00:22:30.12)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.013 |      0.384 |                   91 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.433 |                   66 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   0.844 |      0.051 |                   65 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 139.828 |      0.008 |                   41 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.6177 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=16299)[0m top1: 0.013059701492537313
[2m[36m(func pid=16299)[0m top5: 0.4916044776119403
[2m[36m(func pid=16299)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=16299)[0m f1_macro: 0.007834986015764048
[2m[36m(func pid=16299)[0m f1_weighted: 0.011399067543669903
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.066, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0106 | Steps: 2 | Val loss: 1.5961 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=10638)[0m top1: 0.15764925373134328
[2m[36m(func pid=10638)[0m top5: 0.4846082089552239
[2m[36m(func pid=10638)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=10638)[0m f1_macro: 0.07384399256822254
[2m[36m(func pid=10638)[0m f1_weighted: 0.08562515796390194
[2m[36m(func pid=10638)[0m f1_per_class: [0.007, 0.319, 0.0, 0.03, 0.008, 0.008, 0.0, 0.367, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4244402985074627
[2m[36m(func pid=10089)[0m top5: 0.9528917910447762
[2m[36m(func pid=10089)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=10089)[0m f1_macro: 0.4311994558401403
[2m[36m(func pid=10089)[0m f1_weighted: 0.44091349584210593
[2m[36m(func pid=10089)[0m f1_per_class: [0.583, 0.485, 0.8, 0.575, 0.337, 0.329, 0.394, 0.185, 0.223, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 22.3752 | Steps: 2 | Val loss: 4304002.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=4220)[0m top1: 0.3941231343283582
[2m[36m(func pid=4220)[0m top5: 0.9430970149253731
[2m[36m(func pid=4220)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=4220)[0m f1_macro: 0.3889089512056988
[2m[36m(func pid=4220)[0m f1_weighted: 0.3784009879544576
[2m[36m(func pid=4220)[0m f1_per_class: [0.566, 0.499, 0.667, 0.523, 0.278, 0.227, 0.243, 0.329, 0.235, 0.323]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.8829 | Steps: 2 | Val loss: 3383.2180 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:42:00 (running for 00:22:35.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.011 |      0.389 |                   92 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.431 |                   67 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.173 |      0.074 |                   66 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 22.375 |      0.001 |                   42 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0037 | Steps: 2 | Val loss: 2.6324 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0099 | Steps: 2 | Val loss: 1.5942 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=10638)[0m top1: 0.16511194029850745
[2m[36m(func pid=10638)[0m top5: 0.39972014925373134
[2m[36m(func pid=10638)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=10638)[0m f1_macro: 0.06681397368955962
[2m[36m(func pid=10638)[0m f1_weighted: 0.0801076778656727
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.364, 0.0, 0.0, 0.003, 0.0, 0.0, 0.301, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42677238805970147
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=10089)[0m f1_macro: 0.43279529261346034
[2m[36m(func pid=10089)[0m f1_weighted: 0.4429404605918202
[2m[36m(func pid=10089)[0m f1_per_class: [0.577, 0.487, 0.828, 0.579, 0.315, 0.327, 0.396, 0.183, 0.225, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 118.7965 | Steps: 2 | Val loss: 2436102.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=4220)[0m top1: 0.3969216417910448
[2m[36m(func pid=4220)[0m top5: 0.9444962686567164
[2m[36m(func pid=4220)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=4220)[0m f1_macro: 0.38807441366121803
[2m[36m(func pid=4220)[0m f1_weighted: 0.381429622011906
[2m[36m(func pid=4220)[0m f1_per_class: [0.561, 0.502, 0.667, 0.534, 0.263, 0.225, 0.243, 0.328, 0.232, 0.326]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6561 | Steps: 2 | Val loss: 8068.2495 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:42:05 (running for 00:22:40.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.01  |      0.388 |                   93 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0.004 |      0.433 |                   68 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   2.883 |      0.067 |                   67 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 118.796 |      0.001 |                   43 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5093283582089553
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.6458 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0102 | Steps: 2 | Val loss: 1.6027 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=10638)[0m top1: 0.008861940298507462
[2m[36m(func pid=10638)[0m top5: 0.3292910447761194
[2m[36m(func pid=10638)[0m f1_micro: 0.008861940298507462
[2m[36m(func pid=10638)[0m f1_macro: 0.0032023862678633233
[2m[36m(func pid=10638)[0m f1_weighted: 0.002602316907085423
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.014, 0.0, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4291044776119403
[2m[36m(func pid=10089)[0m top5: 0.9538246268656716
[2m[36m(func pid=10089)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=10089)[0m f1_macro: 0.4311192965132954
[2m[36m(func pid=10089)[0m f1_weighted: 0.4450009754973238
[2m[36m(func pid=10089)[0m f1_per_class: [0.581, 0.487, 0.786, 0.581, 0.329, 0.332, 0.399, 0.186, 0.23, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 174.9809 | Steps: 2 | Val loss: 961784.8125 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=4220)[0m top1: 0.3969216417910448
[2m[36m(func pid=4220)[0m top5: 0.9402985074626866
[2m[36m(func pid=4220)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=4220)[0m f1_macro: 0.3880612545726728
[2m[36m(func pid=4220)[0m f1_weighted: 0.3799073437160346
[2m[36m(func pid=4220)[0m f1_per_class: [0.569, 0.503, 0.667, 0.537, 0.263, 0.23, 0.233, 0.322, 0.234, 0.323]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7412 | Steps: 2 | Val loss: 7614.0156 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:42:11 (running for 00:22:45.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.01  |      0.388 |                   94 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.431 |                   69 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   2.656 |      0.003 |                   68 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 174.981 |      0.001 |                   44 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5130597014925373
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.6435 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0123 | Steps: 2 | Val loss: 1.6131 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=10638)[0m top1: 0.012593283582089552
[2m[36m(func pid=10638)[0m top5: 0.28638059701492535
[2m[36m(func pid=10638)[0m f1_micro: 0.012593283582089552
[2m[36m(func pid=10638)[0m f1_macro: 0.0061747678385893606
[2m[36m(func pid=10638)[0m f1_weighted: 0.00750788818587533
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.043, 0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4295708955223881
[2m[36m(func pid=10089)[0m top5: 0.9552238805970149
[2m[36m(func pid=10089)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=10089)[0m f1_macro: 0.434546248183125
[2m[36m(func pid=10089)[0m f1_weighted: 0.4474009327354249
[2m[36m(func pid=10089)[0m f1_per_class: [0.574, 0.485, 0.815, 0.581, 0.341, 0.331, 0.409, 0.186, 0.228, 0.395]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 20.3663 | Steps: 2 | Val loss: 895478.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=4220)[0m top1: 0.3941231343283582
[2m[36m(func pid=4220)[0m top5: 0.9388992537313433
[2m[36m(func pid=4220)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=4220)[0m f1_macro: 0.3813489333626582
[2m[36m(func pid=4220)[0m f1_weighted: 0.3756583124352065
[2m[36m(func pid=4220)[0m f1_per_class: [0.569, 0.505, 0.615, 0.525, 0.241, 0.212, 0.234, 0.328, 0.238, 0.345]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2321 | Steps: 2 | Val loss: 5750.6851 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 03:42:16 (running for 00:22:51.23)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.012 |      0.381 |                   95 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.435 |                   70 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.741 |      0.006 |                   69 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 20.366 |      0.003 |                   45 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.00792910447761194
[2m[36m(func pid=16299)[0m top5: 0.5214552238805971
[2m[36m(func pid=16299)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=16299)[0m f1_macro: 0.0033528562940327647
[2m[36m(func pid=16299)[0m f1_weighted: 0.003755058106462847
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.021, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0010 | Steps: 2 | Val loss: 2.6946 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0217 | Steps: 2 | Val loss: 1.5947 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=10638)[0m top1: 0.16884328358208955
[2m[36m(func pid=10638)[0m top5: 0.4193097014925373
[2m[36m(func pid=10638)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=10638)[0m f1_macro: 0.035659673659673666
[2m[36m(func pid=10638)[0m f1_weighted: 0.05859337490867342
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4230410447761194
[2m[36m(func pid=10089)[0m top5: 0.9538246268656716
[2m[36m(func pid=10089)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=10089)[0m f1_macro: 0.42955564088502624
[2m[36m(func pid=10089)[0m f1_weighted: 0.4379822338852852
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.491, 0.786, 0.576, 0.333, 0.34, 0.374, 0.193, 0.232, 0.405]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 19.7470 | Steps: 2 | Val loss: 839488.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=4220)[0m top1: 0.39925373134328357
[2m[36m(func pid=4220)[0m top5: 0.9416977611940298
[2m[36m(func pid=4220)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=4220)[0m f1_macro: 0.38597706595577397
[2m[36m(func pid=4220)[0m f1_weighted: 0.3835139956762912
[2m[36m(func pid=4220)[0m f1_per_class: [0.547, 0.508, 0.632, 0.529, 0.27, 0.237, 0.247, 0.332, 0.235, 0.323]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.6009 | Steps: 2 | Val loss: 4169.3228 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:42:21 (running for 00:22:56.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.022 |      0.386 |                   96 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.43  |                   71 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.232 |      0.036 |                   70 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 19.747 |      0.012 |                   46 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.008395522388059701
[2m[36m(func pid=16299)[0m top5: 0.5237873134328358
[2m[36m(func pid=16299)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=16299)[0m f1_macro: 0.011691925856110588
[2m[36m(func pid=16299)[0m f1_weighted: 0.004367825106325114
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.021, 0.012, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.7126 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0080 | Steps: 2 | Val loss: 1.5988 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=10638)[0m top1: 0.16651119402985073
[2m[36m(func pid=10638)[0m top5: 0.4141791044776119
[2m[36m(func pid=10638)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=10638)[0m f1_macro: 0.03573231064237776
[2m[36m(func pid=10638)[0m f1_weighted: 0.059274359267898286
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42257462686567165
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=10089)[0m f1_macro: 0.4292673494494636
[2m[36m(func pid=10089)[0m f1_weighted: 0.4377532631522782
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.486, 0.786, 0.574, 0.333, 0.337, 0.378, 0.204, 0.236, 0.395]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 25.7087 | Steps: 2 | Val loss: 1188601.6250 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=4220)[0m top1: 0.3969216417910448
[2m[36m(func pid=4220)[0m top5: 0.941231343283582
[2m[36m(func pid=4220)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=4220)[0m f1_macro: 0.3892999869157116
[2m[36m(func pid=4220)[0m f1_weighted: 0.3806363207948468
[2m[36m(func pid=4220)[0m f1_per_class: [0.569, 0.503, 0.632, 0.521, 0.274, 0.236, 0.245, 0.333, 0.24, 0.341]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2296 | Steps: 2 | Val loss: 2339.1541 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.7100 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=16299)[0m top1: 0.012593283582089552
[2m[36m(func pid=16299)[0m top5: 0.5270522388059702
[2m[36m(func pid=16299)[0m f1_micro: 0.012593283582089552
[2m[36m(func pid=16299)[0m f1_macro: 0.008396279713992382
[2m[36m(func pid=16299)[0m f1_weighted: 0.012399777285879862
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.072, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
== Status ==
Current time: 2024-01-07 03:42:27 (running for 00:23:02.00)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.008 |      0.389 |                   97 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.429 |                   72 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.601 |      0.036 |                   71 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 25.709 |      0.008 |                   47 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0061 | Steps: 2 | Val loss: 1.6009 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=10638)[0m top1: 0.18330223880597016
[2m[36m(func pid=10638)[0m top5: 0.5097947761194029
[2m[36m(func pid=10638)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=10638)[0m f1_macro: 0.051208429152133604
[2m[36m(func pid=10638)[0m f1_weighted: 0.10581013411324966
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.359, 0.0, 0.083, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.4230410447761194
[2m[36m(func pid=10089)[0m top5: 0.9547574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=10089)[0m f1_macro: 0.4294417942790038
[2m[36m(func pid=10089)[0m f1_weighted: 0.4395602739958997
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.479, 0.786, 0.576, 0.341, 0.334, 0.387, 0.2, 0.236, 0.39]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.8131 | Steps: 2 | Val loss: 1058431.8750 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=4220)[0m top1: 0.396455223880597
[2m[36m(func pid=4220)[0m top5: 0.941231343283582
[2m[36m(func pid=4220)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=4220)[0m f1_macro: 0.3837503750066983
[2m[36m(func pid=4220)[0m f1_weighted: 0.3803954482346838
[2m[36m(func pid=4220)[0m f1_per_class: [0.556, 0.504, 0.6, 0.522, 0.274, 0.231, 0.247, 0.328, 0.234, 0.341]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9497 | Steps: 2 | Val loss: 2236.6504 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.7346 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 03:42:32 (running for 00:23:07.29)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.006 |      0.384 |                   98 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.429 |                   73 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.23  |      0.051 |                   72 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.813 |      0.014 |                   48 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=16299)[0m top1: 0.018656716417910446
[2m[36m(func pid=16299)[0m top5: 0.5359141791044776
[2m[36m(func pid=16299)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=16299)[0m f1_macro: 0.014048014019491292
[2m[36m(func pid=16299)[0m f1_weighted: 0.02209922770158681
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.128, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0158 | Steps: 2 | Val loss: 1.5804 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=10638)[0m top1: 0.32509328358208955
[2m[36m(func pid=10638)[0m top5: 0.6385261194029851
[2m[36m(func pid=10638)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=10638)[0m f1_macro: 0.07664445704622166
[2m[36m(func pid=10638)[0m f1_weighted: 0.20906338695355509
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.0, 0.0, 0.48, 0.035, 0.0, 0.252, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42117537313432835
[2m[36m(func pid=10089)[0m top5: 0.9547574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=10089)[0m f1_macro: 0.43293435918300577
[2m[36m(func pid=10089)[0m f1_weighted: 0.43788711296343424
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.488, 0.815, 0.57, 0.329, 0.327, 0.381, 0.214, 0.234, 0.405]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 4.4270 | Steps: 2 | Val loss: 764997.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=4220)[0m top1: 0.4006529850746269
[2m[36m(func pid=4220)[0m top5: 0.9435634328358209
[2m[36m(func pid=4220)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=4220)[0m f1_macro: 0.3887541147301744
[2m[36m(func pid=4220)[0m f1_weighted: 0.38585607478788947
[2m[36m(func pid=4220)[0m f1_per_class: [0.569, 0.509, 0.615, 0.527, 0.282, 0.237, 0.255, 0.328, 0.244, 0.323]
[2m[36m(func pid=4220)[0m 
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3036 | Steps: 2 | Val loss: 1802.8229 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:42:37 (running for 00:23:12.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00008 | RUNNING    | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.016 |      0.389 |                   99 |
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.433 |                   74 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  0.95  |      0.077 |                   73 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.427 |      0.023 |                   49 |
| train_2d480_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.7706 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=16299)[0m top1: 0.03031716417910448
[2m[36m(func pid=16299)[0m top5: 0.5466417910447762
[2m[36m(func pid=16299)[0m f1_micro: 0.03031716417910448
[2m[36m(func pid=16299)[0m f1_macro: 0.02260051854320245
[2m[36m(func pid=16299)[0m f1_weighted: 0.03675695889441824
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.213, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=4220)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0066 | Steps: 2 | Val loss: 1.5709 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=10638)[0m top1: 0.37033582089552236
[2m[36m(func pid=10638)[0m top5: 0.6861007462686567
[2m[36m(func pid=10638)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=10638)[0m f1_macro: 0.09531718963791666
[2m[36m(func pid=10638)[0m f1_weighted: 0.2617845526757431
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.011, 0.0, 0.513, 0.0, 0.0, 0.388, 0.0, 0.042, 0.0]
[2m[36m(func pid=10638)[0m 
[2m[36m(func pid=10089)[0m top1: 0.42024253731343286
[2m[36m(func pid=10089)[0m top5: 0.9547574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=10089)[0m f1_macro: 0.4341185181284416
[2m[36m(func pid=10089)[0m f1_weighted: 0.43812031641964194
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.488, 0.815, 0.57, 0.333, 0.324, 0.382, 0.219, 0.234, 0.411]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.5673 | Steps: 2 | Val loss: 514840.7812 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=4220)[0m top1: 0.40298507462686567
[2m[36m(func pid=4220)[0m top5: 0.945429104477612
[2m[36m(func pid=4220)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=4220)[0m f1_macro: 0.39325350727032715
[2m[36m(func pid=4220)[0m f1_weighted: 0.39026997274110176
[2m[36m(func pid=4220)[0m f1_per_class: [0.571, 0.509, 0.649, 0.526, 0.278, 0.226, 0.274, 0.324, 0.246, 0.33]
[2m[36m(func pid=10638)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.3698 | Steps: 2 | Val loss: 1574.1102 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0035 | Steps: 2 | Val loss: 2.7799 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=16299)[0m top1: 0.06063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5531716417910447
[2m[36m(func pid=16299)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.03811254231422298
[2m[36m(func pid=16299)[0m f1_weighted: 0.06330696861237435
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.367, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=10638)[0m top1: 0.17350746268656717
[2m[36m(func pid=10638)[0m top5: 0.45988805970149255
[2m[36m(func pid=10638)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=10638)[0m f1_macro: 0.047556129315953106
[2m[36m(func pid=10638)[0m f1_weighted: 0.08859386541629351
[2m[36m(func pid=10638)[0m f1_per_class: [0.0, 0.363, 0.0, 0.056, 0.0, 0.0, 0.033, 0.0, 0.024, 0.0]
[2m[36m(func pid=10089)[0m top1: 0.42117537313432835
[2m[36m(func pid=10089)[0m top5: 0.9538246268656716
[2m[36m(func pid=10089)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=10089)[0m f1_macro: 0.4337728467475471
[2m[36m(func pid=10089)[0m f1_weighted: 0.4401634430639377
[2m[36m(func pid=10089)[0m f1_per_class: [0.571, 0.484, 0.815, 0.569, 0.329, 0.326, 0.391, 0.223, 0.235, 0.395]
== Status ==
Current time: 2024-01-07 03:42:42 (running for 00:23:17.78)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.387
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.434 |                   75 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.304 |      0.095 |                   74 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.427 |      0.023 |                   49 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=27371)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27371)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=27371)[0m Configuration completed!
[2m[36m(func pid=27371)[0m New optimizer parameters:
[2m[36m(func pid=27371)[0m SGD (
[2m[36m(func pid=27371)[0m Parameter Group 0
[2m[36m(func pid=27371)[0m     dampening: 0
[2m[36m(func pid=27371)[0m     differentiable: False
[2m[36m(func pid=27371)[0m     foreach: None
[2m[36m(func pid=27371)[0m     lr: 0.0001
[2m[36m(func pid=27371)[0m     maximize: False
[2m[36m(func pid=27371)[0m     momentum: 0.9
[2m[36m(func pid=27371)[0m     nesterov: False
[2m[36m(func pid=27371)[0m     weight_decay: 0.0001
[2m[36m(func pid=27371)[0m )
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:42:50 (running for 00:23:25.71)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.387
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.434 |                   75 |
| train_2d480_00010 | RUNNING    | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  2.304 |      0.095 |                   74 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.567 |      0.038 |                   50 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.8120 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 8.4394 | Steps: 2 | Val loss: 413088.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.32s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0605 | Steps: 2 | Val loss: 2.5113 | Batch size: 32 | lr: 0.0001 | Duration: 4.65s
[2m[36m(func pid=10089)[0m top1: 0.41884328358208955
[2m[36m(func pid=10089)[0m top5: 0.9533582089552238
[2m[36m(func pid=10089)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=10089)[0m f1_macro: 0.4311083022424892
[2m[36m(func pid=10089)[0m f1_weighted: 0.43654409896491103
[2m[36m(func pid=10089)[0m f1_per_class: [0.551, 0.478, 0.815, 0.565, 0.315, 0.324, 0.387, 0.23, 0.231, 0.416]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m top1: 0.07649253731343283
[2m[36m(func pid=16299)[0m top5: 0.5652985074626866
[2m[36m(func pid=16299)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=16299)[0m f1_macro: 0.04232098761692664
[2m[36m(func pid=16299)[0m f1_weighted: 0.07042202128756472
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.409, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=27371)[0m top1: 0.06763059701492537
[2m[36m(func pid=27371)[0m top5: 0.488339552238806
[2m[36m(func pid=27371)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=27371)[0m f1_macro: 0.04053699474296672
[2m[36m(func pid=27371)[0m f1_weighted: 0.038991021188175934
[2m[36m(func pid=27371)[0m f1_per_class: [0.122, 0.01, 0.0, 0.091, 0.0, 0.019, 0.0, 0.105, 0.022, 0.036]
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8248 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 03:42:56 (running for 00:23:31.20)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.431 |                   77 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.567 |      0.038 |                   50 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27838)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=27838)[0m Configuration completed!
[2m[36m(func pid=27838)[0m New optimizer parameters:
[2m[36m(func pid=27838)[0m SGD (
[2m[36m(func pid=27838)[0m Parameter Group 0
[2m[36m(func pid=27838)[0m     dampening: 0
[2m[36m(func pid=27838)[0m     differentiable: False
[2m[36m(func pid=27838)[0m     foreach: None
[2m[36m(func pid=27838)[0m     lr: 0.001
[2m[36m(func pid=27838)[0m     maximize: False
[2m[36m(func pid=27838)[0m     momentum: 0.9
[2m[36m(func pid=27838)[0m     nesterov: False
[2m[36m(func pid=27838)[0m     weight_decay: 0.0001
[2m[36m(func pid=27838)[0m )
[2m[36m(func pid=27838)[0m 
== Status ==
Current time: 2024-01-07 03:43:01 (running for 00:23:36.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.428 |                   78 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  8.439 |      0.042 |                   51 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  3.06  |      0.041 |                    1 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41511194029850745
[2m[36m(func pid=10089)[0m top5: 0.9533582089552238
[2m[36m(func pid=10089)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=10089)[0m f1_macro: 0.4280695120344646
[2m[36m(func pid=10089)[0m f1_weighted: 0.43192766634471375
[2m[36m(func pid=10089)[0m f1_per_class: [0.553, 0.478, 0.815, 0.557, 0.311, 0.327, 0.377, 0.233, 0.235, 0.395]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 10.1294 | Steps: 2 | Val loss: 295248.0312 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1569 | Steps: 2 | Val loss: 2.5336 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1372 | Steps: 2 | Val loss: 2.4891 | Batch size: 32 | lr: 0.001 | Duration: 4.72s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.8454 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=16299)[0m top1: 0.09654850746268656
[2m[36m(func pid=16299)[0m top5: 0.5792910447761194
[2m[36m(func pid=16299)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=16299)[0m f1_macro: 0.04528099528099528
[2m[36m(func pid=16299)[0m f1_weighted: 0.07529667860824577
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.437, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m top1: 0.06296641791044776
[2m[36m(func pid=27371)[0m top5: 0.4869402985074627
[2m[36m(func pid=27371)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=27371)[0m f1_macro: 0.03438914886300093
[2m[36m(func pid=27371)[0m f1_weighted: 0.034555867339729956
[2m[36m(func pid=27371)[0m f1_per_class: [0.078, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.102, 0.023, 0.034]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.06809701492537314
[2m[36m(func pid=27838)[0m top5: 0.4855410447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=27838)[0m f1_macro: 0.04397213866206567
[2m[36m(func pid=27838)[0m f1_weighted: 0.04189421165613862
[2m[36m(func pid=27838)[0m f1_per_class: [0.153, 0.01, 0.0, 0.1, 0.0, 0.018, 0.0, 0.103, 0.021, 0.035]
[2m[36m(func pid=27838)[0m 
== Status ==
Current time: 2024-01-07 03:43:07 (running for 00:23:41.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.432 |                   79 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 10.129 |      0.045 |                   52 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  3.157 |      0.034 |                    2 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  3.137 |      0.044 |                    1 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41511194029850745
[2m[36m(func pid=10089)[0m top5: 0.9528917910447762
[2m[36m(func pid=10089)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=10089)[0m f1_macro: 0.43159982529119995
[2m[36m(func pid=10089)[0m f1_weighted: 0.43111576809211466
[2m[36m(func pid=10089)[0m f1_per_class: [0.568, 0.481, 0.815, 0.557, 0.311, 0.324, 0.372, 0.24, 0.233, 0.416]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 15.3694 | Steps: 2 | Val loss: 241995.8906 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1331 | Steps: 2 | Val loss: 2.5496 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9552 | Steps: 2 | Val loss: 2.4600 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.8735 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=16299)[0m top1: 0.1044776119402985
[2m[36m(func pid=16299)[0m top5: 0.5797574626865671
[2m[36m(func pid=16299)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=16299)[0m f1_macro: 0.044958389128601894
[2m[36m(func pid=16299)[0m f1_weighted: 0.07459360688004701
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.433, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m top1: 0.06296641791044776
[2m[36m(func pid=27371)[0m top5: 0.478544776119403
[2m[36m(func pid=27371)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=27371)[0m f1_macro: 0.031586878445461264
[2m[36m(func pid=27371)[0m f1_weighted: 0.03497814553126096
[2m[36m(func pid=27371)[0m f1_per_class: [0.056, 0.014, 0.0, 0.082, 0.0, 0.019, 0.0, 0.102, 0.0, 0.043]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.07369402985074627
[2m[36m(func pid=27838)[0m top5: 0.4780783582089552
[2m[36m(func pid=27838)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=27838)[0m f1_macro: 0.05758488318437867
[2m[36m(func pid=27838)[0m f1_weighted: 0.054322637903241135
[2m[36m(func pid=27838)[0m f1_per_class: [0.161, 0.054, 0.0, 0.107, 0.0, 0.027, 0.0, 0.103, 0.059, 0.064]
[2m[36m(func pid=27838)[0m 
== Status ==
Current time: 2024-01-07 03:43:12 (running for 00:23:47.15)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.431 |                   80 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 15.369 |      0.045 |                   53 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  3.133 |      0.032 |                    3 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  2.955 |      0.058 |                    2 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41884328358208955
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=10089)[0m f1_macro: 0.4309633105887193
[2m[36m(func pid=10089)[0m f1_weighted: 0.43754064965057915
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.48, 0.815, 0.565, 0.311, 0.316, 0.389, 0.249, 0.231, 0.39]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1410.1630 | Steps: 2 | Val loss: 179861.6250 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0855 | Steps: 2 | Val loss: 2.5604 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6730 | Steps: 2 | Val loss: 2.4131 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=16299)[0m top1: 0.12453358208955224
[2m[36m(func pid=16299)[0m top5: 0.5951492537313433
[2m[36m(func pid=16299)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=16299)[0m f1_macro: 0.04603057804976278
[2m[36m(func pid=16299)[0m f1_weighted: 0.07607578233138688
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.441, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.8856 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=27371)[0m top1: 0.0625
[2m[36m(func pid=27371)[0m top5: 0.47761194029850745
[2m[36m(func pid=27371)[0m f1_micro: 0.0625
[2m[36m(func pid=27371)[0m f1_macro: 0.036201107089684006
[2m[36m(func pid=27371)[0m f1_weighted: 0.03910108027110419
[2m[36m(func pid=27371)[0m f1_per_class: [0.051, 0.032, 0.0, 0.083, 0.0, 0.02, 0.0, 0.096, 0.025, 0.055]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.07882462686567164
[2m[36m(func pid=27838)[0m top5: 0.4808768656716418
[2m[36m(func pid=27838)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=27838)[0m f1_macro: 0.06508444843547113
[2m[36m(func pid=27838)[0m f1_weighted: 0.07285517437487973
[2m[36m(func pid=27838)[0m f1_per_class: [0.129, 0.125, 0.0, 0.119, 0.0, 0.033, 0.009, 0.096, 0.084, 0.056]
[2m[36m(func pid=27838)[0m 
== Status ==
Current time: 2024-01-07 03:43:17 (running for 00:23:52.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+----------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |     loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+----------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |    0     |      0.436 |                   81 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 1410.16  |      0.046 |                   54 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |    3.085 |      0.036 |                    4 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |    2.673 |      0.065 |                    3 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |          |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |          |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |          |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |          |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |    0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |    0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |    0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |    6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |    0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |    0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |    0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |    1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+----------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.42350746268656714
[2m[36m(func pid=10089)[0m top5: 0.9552238805970149
[2m[36m(func pid=10089)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=10089)[0m f1_macro: 0.43647032160778
[2m[36m(func pid=10089)[0m f1_weighted: 0.4384317920971473
[2m[36m(func pid=10089)[0m f1_per_class: [0.557, 0.486, 0.815, 0.575, 0.322, 0.329, 0.371, 0.262, 0.234, 0.416]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 383.0446 | Steps: 2 | Val loss: 1918885.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9723 | Steps: 2 | Val loss: 2.5600 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3938 | Steps: 2 | Val loss: 2.3711 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=16299)[0m top1: 0.02658582089552239
[2m[36m(func pid=16299)[0m top5: 0.5144589552238806
[2m[36m(func pid=16299)[0m f1_micro: 0.02658582089552239
[2m[36m(func pid=16299)[0m f1_macro: 0.019381505688695233
[2m[36m(func pid=16299)[0m f1_weighted: 0.031240905358826787
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.181, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9023 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=27371)[0m top1: 0.06343283582089553
[2m[36m(func pid=27371)[0m top5: 0.4771455223880597
[2m[36m(func pid=27371)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=27371)[0m f1_macro: 0.037227446808872126
[2m[36m(func pid=27371)[0m f1_weighted: 0.04293493784708958
[2m[36m(func pid=27371)[0m f1_per_class: [0.045, 0.053, 0.0, 0.088, 0.0, 0.013, 0.0, 0.095, 0.025, 0.053]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.08675373134328358
[2m[36m(func pid=27838)[0m top5: 0.4976679104477612
[2m[36m(func pid=27838)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=27838)[0m f1_macro: 0.09269645963561511
[2m[36m(func pid=27838)[0m f1_weighted: 0.08950152599431452
[2m[36m(func pid=27838)[0m f1_per_class: [0.114, 0.158, 0.176, 0.128, 0.0, 0.034, 0.03, 0.1, 0.12, 0.066]
[2m[36m(func pid=27838)[0m 
== Status ==
Current time: 2024-01-07 03:43:23 (running for 00:23:58.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.435 |                   82 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 383.045 |      0.019 |                   55 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   2.972 |      0.037 |                    5 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   2.394 |      0.093 |                    4 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41884328358208955
[2m[36m(func pid=10089)[0m top5: 0.9538246268656716
[2m[36m(func pid=10089)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=10089)[0m f1_macro: 0.434730805838121
[2m[36m(func pid=10089)[0m f1_weighted: 0.43424558910730404
[2m[36m(func pid=10089)[0m f1_per_class: [0.581, 0.478, 0.815, 0.572, 0.322, 0.32, 0.365, 0.27, 0.23, 0.395]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 42.4370 | Steps: 2 | Val loss: 1695711.3750 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9468 | Steps: 2 | Val loss: 2.5553 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.0362 | Steps: 2 | Val loss: 2.3302 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=16299)[0m top1: 0.020522388059701493
[2m[36m(func pid=16299)[0m top5: 0.5027985074626866
[2m[36m(func pid=16299)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=16299)[0m f1_macro: 0.014213784008983052
[2m[36m(func pid=16299)[0m f1_weighted: 0.02235406896581748
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.129, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.9297 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=27371)[0m top1: 0.0648320895522388
[2m[36m(func pid=27371)[0m top5: 0.4650186567164179
[2m[36m(func pid=27371)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=27371)[0m f1_macro: 0.038105507781499806
[2m[36m(func pid=27371)[0m f1_weighted: 0.04624255576481725
[2m[36m(func pid=27371)[0m f1_per_class: [0.042, 0.072, 0.0, 0.088, 0.0, 0.013, 0.0, 0.098, 0.025, 0.044]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.10121268656716417
[2m[36m(func pid=27838)[0m top5: 0.5475746268656716
[2m[36m(func pid=27838)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=27838)[0m f1_macro: 0.1186944165806453
[2m[36m(func pid=27838)[0m f1_weighted: 0.1098499907510436
[2m[36m(func pid=27838)[0m f1_per_class: [0.125, 0.163, 0.31, 0.133, 0.017, 0.06, 0.072, 0.116, 0.134, 0.057]
[2m[36m(func pid=27838)[0m 
== Status ==
Current time: 2024-01-07 03:43:28 (running for 00:24:03.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.435 |                   83 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 42.437 |      0.014 |                   56 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.947 |      0.038 |                    6 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  2.036 |      0.119 |                    5 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4183768656716418
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=10089)[0m f1_macro: 0.435353315675526
[2m[36m(func pid=10089)[0m f1_weighted: 0.43263991053303297
[2m[36m(func pid=10089)[0m f1_per_class: [0.584, 0.481, 0.815, 0.568, 0.326, 0.328, 0.357, 0.277, 0.233, 0.385]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 15.8001 | Steps: 2 | Val loss: 2377427.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8487 | Steps: 2 | Val loss: 2.5470 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.8233 | Steps: 2 | Val loss: 2.2819 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=16299)[0m top1: 0.010261194029850746
[2m[36m(func pid=16299)[0m top5: 0.5144589552238806
[2m[36m(func pid=16299)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=16299)[0m f1_macro: 0.005925860745694438
[2m[36m(func pid=16299)[0m f1_weighted: 0.008183409984555932
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.047, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.9150 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=27371)[0m top1: 0.06902985074626866
[2m[36m(func pid=27371)[0m top5: 0.46548507462686567
[2m[36m(func pid=27371)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=27371)[0m f1_macro: 0.04407160177851724
[2m[36m(func pid=27371)[0m f1_weighted: 0.05421751426747116
[2m[36m(func pid=27371)[0m f1_per_class: [0.056, 0.081, 0.0, 0.107, 0.0, 0.013, 0.0, 0.1, 0.048, 0.036]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.12360074626865672
[2m[36m(func pid=27838)[0m top5: 0.6040111940298507
[2m[36m(func pid=27838)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=27838)[0m f1_macro: 0.12354292876803637
[2m[36m(func pid=27838)[0m f1_weighted: 0.1426551446703997
[2m[36m(func pid=27838)[0m f1_per_class: [0.122, 0.177, 0.231, 0.143, 0.021, 0.077, 0.163, 0.092, 0.138, 0.071]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 7.5306 | Steps: 2 | Val loss: 2135701.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:43:34 (running for 00:24:08.87)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.436 |                   84 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 15.8   |      0.006 |                   57 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.849 |      0.044 |                    7 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  1.823 |      0.124 |                    6 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41744402985074625
[2m[36m(func pid=10089)[0m top5: 0.9556902985074627
[2m[36m(func pid=10089)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=10089)[0m f1_macro: 0.4360360910057578
[2m[36m(func pid=10089)[0m f1_weighted: 0.4331422721432468
[2m[36m(func pid=10089)[0m f1_per_class: [0.591, 0.481, 0.815, 0.562, 0.329, 0.328, 0.365, 0.269, 0.235, 0.385]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8292 | Steps: 2 | Val loss: 2.5342 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4497 | Steps: 2 | Val loss: 2.2342 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.511660447761194
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.0012287334593572778
[2m[36m(func pid=16299)[0m f1_weighted: 7.450342803938718e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9325 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=27371)[0m top1: 0.07509328358208955
[2m[36m(func pid=27371)[0m top5: 0.470615671641791
[2m[36m(func pid=27371)[0m f1_micro: 0.07509328358208955
[2m[36m(func pid=27371)[0m f1_macro: 0.048920133423379634
[2m[36m(func pid=27371)[0m f1_weighted: 0.06099516129928989
[2m[36m(func pid=27371)[0m f1_per_class: [0.062, 0.09, 0.0, 0.124, 0.0, 0.013, 0.0, 0.105, 0.045, 0.049]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.1501865671641791
[2m[36m(func pid=27838)[0m top5: 0.6585820895522388
[2m[36m(func pid=27838)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=27838)[0m f1_macro: 0.13675414471858044
[2m[36m(func pid=27838)[0m f1_weighted: 0.1720825856986304
[2m[36m(func pid=27838)[0m f1_per_class: [0.127, 0.176, 0.211, 0.143, 0.026, 0.083, 0.258, 0.095, 0.142, 0.106]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 43.2792 | Steps: 2 | Val loss: 1772210.8750 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 03:43:39 (running for 00:24:14.21)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.435 |                   85 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  7.531 |      0.001 |                   58 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.829 |      0.049 |                    8 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  1.45  |      0.137 |                    7 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41511194029850745
[2m[36m(func pid=10089)[0m top5: 0.9552238805970149
[2m[36m(func pid=10089)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=10089)[0m f1_macro: 0.4354145837360882
[2m[36m(func pid=10089)[0m f1_weighted: 0.43205488424141064
[2m[36m(func pid=10089)[0m f1_per_class: [0.598, 0.473, 0.815, 0.548, 0.326, 0.322, 0.38, 0.277, 0.232, 0.385]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7674 | Steps: 2 | Val loss: 2.5190 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.1794 | Steps: 2 | Val loss: 2.1855 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5125932835820896
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001223529411764706
[2m[36m(func pid=16299)[0m f1_weighted: 7.418788410886743e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9544 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=27371)[0m top1: 0.07509328358208955
[2m[36m(func pid=27371)[0m top5: 0.46921641791044777
[2m[36m(func pid=27371)[0m f1_micro: 0.07509328358208955
[2m[36m(func pid=27371)[0m f1_macro: 0.04919458506645446
[2m[36m(func pid=27371)[0m f1_weighted: 0.06398837125437942
[2m[36m(func pid=27371)[0m f1_per_class: [0.056, 0.113, 0.0, 0.122, 0.0, 0.012, 0.0, 0.103, 0.044, 0.041]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.18470149253731344
[2m[36m(func pid=27838)[0m top5: 0.7042910447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=27838)[0m f1_macro: 0.15572225026664632
[2m[36m(func pid=27838)[0m f1_weighted: 0.20724570373095644
[2m[36m(func pid=27838)[0m f1_per_class: [0.138, 0.211, 0.232, 0.158, 0.034, 0.091, 0.333, 0.116, 0.143, 0.102]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 6.3676 | Steps: 2 | Val loss: 1460113.1250 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:43:44 (running for 00:24:19.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.432 |                   86 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 43.279 |      0.001 |                   59 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.767 |      0.049 |                    9 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  1.179 |      0.156 |                    8 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41277985074626866
[2m[36m(func pid=10089)[0m top5: 0.9533582089552238
[2m[36m(func pid=10089)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=10089)[0m f1_macro: 0.4315117773424492
[2m[36m(func pid=10089)[0m f1_weighted: 0.4298100938534205
[2m[36m(func pid=10089)[0m f1_per_class: [0.598, 0.464, 0.815, 0.555, 0.311, 0.313, 0.375, 0.278, 0.231, 0.375]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6821 | Steps: 2 | Val loss: 2.5080 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0514 | Steps: 2 | Val loss: 2.1332 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5130597014925373
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.0012081784386617103
[2m[36m(func pid=16299)[0m f1_weighted: 7.325708816512236e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9915 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=27371)[0m top1: 0.07602611940298508
[2m[36m(func pid=27371)[0m top5: 0.4664179104477612
[2m[36m(func pid=27371)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=27371)[0m f1_macro: 0.05495542576867947
[2m[36m(func pid=27371)[0m f1_weighted: 0.06610310181603077
[2m[36m(func pid=27371)[0m f1_per_class: [0.069, 0.117, 0.0, 0.119, 0.0, 0.018, 0.0, 0.105, 0.082, 0.039]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.21315298507462688
[2m[36m(func pid=27838)[0m top5: 0.746268656716418
[2m[36m(func pid=27838)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=27838)[0m f1_macro: 0.1741745779032838
[2m[36m(func pid=27838)[0m f1_weighted: 0.23553369486814296
[2m[36m(func pid=27838)[0m f1_per_class: [0.157, 0.222, 0.232, 0.211, 0.05, 0.108, 0.357, 0.138, 0.161, 0.105]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 158.5931 | Steps: 2 | Val loss: 888389.8125 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:43:49 (running for 00:24:24.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.428 |                   87 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  6.368 |      0.001 |                   60 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.682 |      0.055 |                   10 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  1.051 |      0.174 |                    9 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.40671641791044777
[2m[36m(func pid=10089)[0m top5: 0.9528917910447762
[2m[36m(func pid=10089)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=10089)[0m f1_macro: 0.4275372034800136
[2m[36m(func pid=10089)[0m f1_weighted: 0.4232815653998857
[2m[36m(func pid=10089)[0m f1_per_class: [0.598, 0.466, 0.815, 0.553, 0.304, 0.306, 0.358, 0.268, 0.232, 0.375]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5624 | Steps: 2 | Val loss: 2.4943 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7805 | Steps: 2 | Val loss: 2.0938 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=16299)[0m top1: 0.00792910447761194
[2m[36m(func pid=16299)[0m top5: 0.5223880597014925
[2m[36m(func pid=16299)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=16299)[0m f1_macro: 0.0033477775540290923
[2m[36m(func pid=16299)[0m f1_weighted: 0.003754750160473819
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.021, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9967 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=27371)[0m top1: 0.07975746268656717
[2m[36m(func pid=27371)[0m top5: 0.466884328358209
[2m[36m(func pid=27371)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=27371)[0m f1_macro: 0.05757007279864464
[2m[36m(func pid=27371)[0m f1_weighted: 0.07160147763456334
[2m[36m(func pid=27371)[0m f1_per_class: [0.061, 0.127, 0.0, 0.127, 0.0, 0.034, 0.0, 0.108, 0.079, 0.04]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.23507462686567165
[2m[36m(func pid=27838)[0m top5: 0.7737873134328358
[2m[36m(func pid=27838)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=27838)[0m f1_macro: 0.19750864652039707
[2m[36m(func pid=27838)[0m f1_weighted: 0.25930165316284837
[2m[36m(func pid=27838)[0m f1_per_class: [0.182, 0.247, 0.262, 0.274, 0.067, 0.119, 0.348, 0.177, 0.169, 0.13]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 5.4385 | Steps: 2 | Val loss: 802181.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 03:43:55 (running for 00:24:29.95)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.426 |                   88 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 158.593 |      0.003 |                   61 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   2.562 |      0.058 |                   11 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.781 |      0.198 |                   10 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.40718283582089554
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=10089)[0m f1_macro: 0.42645917224036217
[2m[36m(func pid=10089)[0m f1_weighted: 0.4252746624823733
[2m[36m(func pid=10089)[0m f1_per_class: [0.593, 0.465, 0.815, 0.554, 0.298, 0.302, 0.367, 0.27, 0.23, 0.37]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5760 | Steps: 2 | Val loss: 2.4820 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7786 | Steps: 2 | Val loss: 2.0671 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=16299)[0m top1: 0.0065298507462686565
[2m[36m(func pid=16299)[0m top5: 0.5293843283582089
[2m[36m(func pid=16299)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=16299)[0m f1_macro: 0.0017452516868185926
[2m[36m(func pid=16299)[0m f1_weighted: 0.0009985360139505937
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0062 | Steps: 2 | Val loss: 2.9808 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=27371)[0m top1: 0.08768656716417911
[2m[36m(func pid=27371)[0m top5: 0.470615671641791
[2m[36m(func pid=27371)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=27371)[0m f1_macro: 0.06317121597243433
[2m[36m(func pid=27371)[0m f1_weighted: 0.08070412165499821
[2m[36m(func pid=27371)[0m f1_per_class: [0.057, 0.151, 0.0, 0.14, 0.0, 0.038, 0.0, 0.117, 0.088, 0.04]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.2462686567164179
[2m[36m(func pid=27838)[0m top5: 0.789179104477612
[2m[36m(func pid=27838)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=27838)[0m f1_macro: 0.21581066319387165
[2m[36m(func pid=27838)[0m f1_weighted: 0.26828368498577276
[2m[36m(func pid=27838)[0m f1_per_class: [0.2, 0.266, 0.319, 0.285, 0.091, 0.143, 0.338, 0.199, 0.18, 0.136]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 206.5641 | Steps: 2 | Val loss: 2332803.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=10089)[0m top1: 0.40904850746268656
[2m[36m(func pid=10089)[0m top5: 0.9547574626865671
[2m[36m(func pid=10089)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=10089)[0m f1_macro: 0.4249309556055594
[2m[36m(func pid=10089)[0m f1_weighted: 0.4260994784637644
[2m[36m(func pid=10089)[0m f1_per_class: [0.557, 0.468, 0.815, 0.551, 0.308, 0.296, 0.374, 0.273, 0.238, 0.37]
== Status ==
Current time: 2024-01-07 03:44:00 (running for 00:24:35.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.425 |                   89 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  5.438 |      0.002 |                   62 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.576 |      0.063 |                   12 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.779 |      0.216 |                   11 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4347 | Steps: 2 | Val loss: 2.4704 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4780 | Steps: 2 | Val loss: 2.0418 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=16299)[0m top1: 0.0065298507462686565
[2m[36m(func pid=16299)[0m top5: 0.5181902985074627
[2m[36m(func pid=16299)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=16299)[0m f1_macro: 0.001747597179352054
[2m[36m(func pid=16299)[0m f1_weighted: 0.0010035037115858636
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9934 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=27838)[0m top1: 0.240205223880597
[2m[36m(func pid=27838)[0m top5: 0.8031716417910447
[2m[36m(func pid=27838)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=27838)[0m f1_macro: 0.22390712006114072
[2m[36m(func pid=27838)[0m f1_weighted: 0.2551435313167079
[2m[36m(func pid=27838)[0m f1_per_class: [0.232, 0.287, 0.393, 0.3, 0.094, 0.157, 0.259, 0.203, 0.182, 0.134]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.09048507462686567
[2m[36m(func pid=27371)[0m top5: 0.4710820895522388
[2m[36m(func pid=27371)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=27371)[0m f1_macro: 0.06549078713468064
[2m[36m(func pid=27371)[0m f1_weighted: 0.08330228703091025
[2m[36m(func pid=27371)[0m f1_per_class: [0.061, 0.151, 0.0, 0.145, 0.0, 0.043, 0.0, 0.123, 0.094, 0.037]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 44.1410 | Steps: 2 | Val loss: 2065866.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:44:05 (running for 00:24:40.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.426 |                   90 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 206.564 |      0.002 |                   63 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   2.435 |      0.065 |                   13 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.478 |      0.224 |                   12 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.41091417910447764
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=10089)[0m f1_macro: 0.42614069371273694
[2m[36m(func pid=10089)[0m f1_weighted: 0.4275694305805891
[2m[36m(func pid=10089)[0m f1_per_class: [0.557, 0.475, 0.815, 0.552, 0.301, 0.302, 0.371, 0.275, 0.239, 0.375]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4115 | Steps: 2 | Val loss: 2.0202 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4640 | Steps: 2 | Val loss: 2.4616 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5102611940298507
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=16299)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.9987 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=27838)[0m top1: 0.2416044776119403
[2m[36m(func pid=27838)[0m top5: 0.8129664179104478
[2m[36m(func pid=27838)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=27838)[0m f1_macro: 0.23413866713253687
[2m[36m(func pid=27838)[0m f1_weighted: 0.2488730379622521
[2m[36m(func pid=27838)[0m f1_per_class: [0.265, 0.316, 0.468, 0.325, 0.089, 0.154, 0.192, 0.213, 0.19, 0.129]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.09235074626865672
[2m[36m(func pid=27371)[0m top5: 0.4673507462686567
[2m[36m(func pid=27371)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=27371)[0m f1_macro: 0.07209784607271053
[2m[36m(func pid=27371)[0m f1_weighted: 0.08707137649824977
[2m[36m(func pid=27371)[0m f1_per_class: [0.065, 0.148, 0.054, 0.154, 0.0, 0.048, 0.003, 0.126, 0.087, 0.037]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:44:10 (running for 00:24:45.55)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.428 |                   91 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 44.141 |      0.001 |                   64 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.464 |      0.072 |                   14 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.411 |      0.234 |                   13 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4099813432835821
[2m[36m(func pid=10089)[0m top5: 0.9542910447761194
[2m[36m(func pid=10089)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=10089)[0m f1_macro: 0.42777515914185765
[2m[36m(func pid=10089)[0m f1_weighted: 0.42635611988349875
[2m[36m(func pid=10089)[0m f1_per_class: [0.557, 0.48, 0.815, 0.541, 0.301, 0.318, 0.368, 0.277, 0.237, 0.385]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 62.8334 | Steps: 2 | Val loss: 1247427.8750 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4309 | Steps: 2 | Val loss: 1.9982 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3252 | Steps: 2 | Val loss: 2.4506 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=16299)[0m top1: 0.006063432835820896
[2m[36m(func pid=16299)[0m top5: 0.5027985074626866
[2m[36m(func pid=16299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=16299)[0m f1_macro: 0.001208740120874012
[2m[36m(func pid=16299)[0m f1_weighted: 7.329114538881603e-05
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.9955 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=27838)[0m top1: 0.25652985074626866
[2m[36m(func pid=27838)[0m top5: 0.824160447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=27838)[0m f1_macro: 0.2464046641131913
[2m[36m(func pid=27838)[0m f1_weighted: 0.25539593264159827
[2m[36m(func pid=27838)[0m f1_per_class: [0.286, 0.334, 0.478, 0.361, 0.089, 0.158, 0.155, 0.263, 0.201, 0.139]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.09235074626865672
[2m[36m(func pid=27371)[0m top5: 0.47201492537313433
[2m[36m(func pid=27371)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=27371)[0m f1_macro: 0.07596414554778738
[2m[36m(func pid=27371)[0m f1_weighted: 0.0886592356884015
[2m[36m(func pid=27371)[0m f1_per_class: [0.061, 0.153, 0.098, 0.157, 0.0, 0.047, 0.003, 0.123, 0.086, 0.031]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1252.7887 | Steps: 2 | Val loss: 861187.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=10089)[0m top1: 0.4118470149253731
[2m[36m(func pid=10089)[0m top5: 0.9538246268656716
[2m[36m(func pid=10089)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=10089)[0m f1_macro: 0.42392126992537954
[2m[36m(func pid=10089)[0m f1_weighted: 0.4267389377479861
[2m[36m(func pid=10089)[0m f1_per_class: [0.545, 0.483, 0.769, 0.541, 0.308, 0.32, 0.367, 0.281, 0.241, 0.385]
== Status ==
Current time: 2024-01-07 03:44:16 (running for 00:24:50.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                   92 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 62.833 |      0.001 |                   65 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.325 |      0.076 |                   15 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.431 |      0.246 |                   14 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3105 | Steps: 2 | Val loss: 1.9682 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3637 | Steps: 2 | Val loss: 2.4388 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=16299)[0m top1: 0.006996268656716418
[2m[36m(func pid=16299)[0m top5: 0.4939365671641791
[2m[36m(func pid=16299)[0m f1_micro: 0.006996268656716418
[2m[36m(func pid=16299)[0m f1_macro: 0.0020595311769225242
[2m[36m(func pid=16299)[0m f1_weighted: 0.001917682760035634
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0029 | Steps: 2 | Val loss: 3.0708 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=27838)[0m top1: 0.2677238805970149
[2m[36m(func pid=27838)[0m top5: 0.8362873134328358
[2m[36m(func pid=27838)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=27838)[0m f1_macro: 0.25702129209437
[2m[36m(func pid=27838)[0m f1_weighted: 0.261935091918336
[2m[36m(func pid=27838)[0m f1_per_class: [0.306, 0.321, 0.512, 0.404, 0.095, 0.17, 0.135, 0.279, 0.192, 0.157]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.0960820895522388
[2m[36m(func pid=27371)[0m top5: 0.4710820895522388
[2m[36m(func pid=27371)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=27371)[0m f1_macro: 0.07831513652340308
[2m[36m(func pid=27371)[0m f1_weighted: 0.09177118928785802
[2m[36m(func pid=27371)[0m f1_per_class: [0.069, 0.169, 0.093, 0.155, 0.0, 0.045, 0.006, 0.129, 0.079, 0.038]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 4.4088 | Steps: 2 | Val loss: 431503.3438 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:44:21 (running for 00:24:56.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+----------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |     loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+----------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |    0.003 |      0.424 |                   93 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 1252.79  |      0.002 |                   66 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |    2.364 |      0.078 |                   16 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |    0.31  |      0.257 |                   15 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |          |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |          |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |          |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |          |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |          |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |    0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |    0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |    0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |    6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |    0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |    0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |    0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |    1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+----------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4099813432835821
[2m[36m(func pid=10089)[0m top5: 0.9524253731343284
[2m[36m(func pid=10089)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=10089)[0m f1_macro: 0.4244210115747893
[2m[36m(func pid=10089)[0m f1_weighted: 0.4253527371684055
[2m[36m(func pid=10089)[0m f1_per_class: [0.568, 0.474, 0.769, 0.548, 0.308, 0.317, 0.36, 0.287, 0.238, 0.375]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2654 | Steps: 2 | Val loss: 1.9516 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2450 | Steps: 2 | Val loss: 2.4285 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=16299)[0m top1: 0.00792910447761194
[2m[36m(func pid=16299)[0m top5: 0.4724813432835821
[2m[36m(func pid=16299)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=16299)[0m f1_macro: 0.002893045398617229
[2m[36m(func pid=16299)[0m f1_weighted: 0.0036733519411117125
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.01, 0.013, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.0909 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=27838)[0m top1: 0.26865671641791045
[2m[36m(func pid=27838)[0m top5: 0.8432835820895522
[2m[36m(func pid=27838)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=27838)[0m f1_macro: 0.2627797574167926
[2m[36m(func pid=27838)[0m f1_weighted: 0.25834370360412096
[2m[36m(func pid=27838)[0m f1_per_class: [0.301, 0.319, 0.55, 0.41, 0.1, 0.176, 0.116, 0.279, 0.182, 0.196]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.09794776119402986
[2m[36m(func pid=27371)[0m top5: 0.47574626865671643
[2m[36m(func pid=27371)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=27371)[0m f1_macro: 0.09164338712848838
[2m[36m(func pid=27371)[0m f1_weighted: 0.09223151833458194
[2m[36m(func pid=27371)[0m f1_per_class: [0.078, 0.173, 0.2, 0.145, 0.0, 0.053, 0.006, 0.135, 0.087, 0.038]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 12.1667 | Steps: 2 | Val loss: 225366.3438 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=10089)[0m top1: 0.40718283582089554
[2m[36m(func pid=10089)[0m top5: 0.9519589552238806
[2m[36m(func pid=10089)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=10089)[0m f1_macro: 0.42040690237188355
[2m[36m(func pid=10089)[0m f1_weighted: 0.42181973929706346
[2m[36m(func pid=10089)[0m f1_per_class: [0.557, 0.472, 0.769, 0.543, 0.301, 0.316, 0.355, 0.291, 0.234, 0.366]
== Status ==
Current time: 2024-01-07 03:44:26 (running for 00:25:01.44)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.42  |                   94 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  4.409 |      0.003 |                   67 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.245 |      0.092 |                   17 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.265 |      0.263 |                   16 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2425 | Steps: 2 | Val loss: 1.9358 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=16299)[0m top1: 0.010727611940298507
[2m[36m(func pid=16299)[0m top5: 0.4528917910447761
[2m[36m(func pid=16299)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=16299)[0m f1_macro: 0.004583020078478463
[2m[36m(func pid=16299)[0m f1_weighted: 0.009113772585504262
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.005, 0.013, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2269 | Steps: 2 | Val loss: 2.4224 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.1157 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=27838)[0m top1: 0.279384328358209
[2m[36m(func pid=27838)[0m top5: 0.8484141791044776
[2m[36m(func pid=27838)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=27838)[0m f1_macro: 0.27099997398985654
[2m[36m(func pid=27838)[0m f1_weighted: 0.2662118118897497
[2m[36m(func pid=27838)[0m f1_per_class: [0.331, 0.322, 0.55, 0.433, 0.107, 0.184, 0.111, 0.292, 0.176, 0.204]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.09654850746268656
[2m[36m(func pid=27371)[0m top5: 0.47901119402985076
[2m[36m(func pid=27371)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=27371)[0m f1_macro: 0.094769884104998
[2m[36m(func pid=27371)[0m f1_weighted: 0.09022419675106445
[2m[36m(func pid=27371)[0m f1_per_class: [0.078, 0.171, 0.241, 0.138, 0.0, 0.058, 0.006, 0.135, 0.082, 0.039]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 3.3734 | Steps: 2 | Val loss: 129688.7734 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 03:44:31 (running for 00:25:06.82)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.422 |                   95 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 12.167 |      0.005 |                   68 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.227 |      0.095 |                   18 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.243 |      0.271 |                   17 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.40205223880597013
[2m[36m(func pid=10089)[0m top5: 0.9528917910447762
[2m[36m(func pid=10089)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=10089)[0m f1_macro: 0.42185632322219
[2m[36m(func pid=10089)[0m f1_weighted: 0.4189644410827434
[2m[36m(func pid=10089)[0m f1_per_class: [0.574, 0.471, 0.769, 0.535, 0.295, 0.311, 0.355, 0.283, 0.23, 0.395]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1784 | Steps: 2 | Val loss: 1.9210 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=16299)[0m top1: 0.009328358208955223
[2m[36m(func pid=16299)[0m top5: 0.43236940298507465
[2m[36m(func pid=16299)[0m f1_micro: 0.009328358208955223
[2m[36m(func pid=16299)[0m f1_macro: 0.004170736976124072
[2m[36m(func pid=16299)[0m f1_weighted: 0.0062693590205818595
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.014, 0.015, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1135 | Steps: 2 | Val loss: 2.4100 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0003 | Steps: 2 | Val loss: 3.1459 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=27838)[0m top1: 0.28638059701492535
[2m[36m(func pid=27838)[0m top5: 0.8549440298507462
[2m[36m(func pid=27838)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=27838)[0m f1_macro: 0.27709237119458907
[2m[36m(func pid=27838)[0m f1_weighted: 0.2759108805456608
[2m[36m(func pid=27838)[0m f1_per_class: [0.323, 0.325, 0.55, 0.434, 0.117, 0.192, 0.137, 0.291, 0.182, 0.22]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 28.1768 | Steps: 2 | Val loss: 85837.2656 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=27371)[0m top1: 0.1021455223880597
[2m[36m(func pid=27371)[0m top5: 0.4846082089552239
[2m[36m(func pid=27371)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=27371)[0m f1_macro: 0.09931712370161537
[2m[36m(func pid=27371)[0m f1_weighted: 0.09646998611907263
[2m[36m(func pid=27371)[0m f1_per_class: [0.084, 0.175, 0.242, 0.143, 0.0, 0.062, 0.015, 0.141, 0.09, 0.041]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:44:37 (running for 00:25:11.99)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.42  |                   96 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.373 |      0.004 |                   69 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.113 |      0.099 |                   19 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.178 |      0.277 |                   18 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4025186567164179
[2m[36m(func pid=10089)[0m top5: 0.9496268656716418
[2m[36m(func pid=10089)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=10089)[0m f1_macro: 0.4203798040425039
[2m[36m(func pid=10089)[0m f1_weighted: 0.42152254477817985
[2m[36m(func pid=10089)[0m f1_per_class: [0.568, 0.47, 0.769, 0.53, 0.283, 0.305, 0.373, 0.282, 0.23, 0.395]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1549 | Steps: 2 | Val loss: 1.9052 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=16299)[0m top1: 0.014925373134328358
[2m[36m(func pid=16299)[0m top5: 0.4510261194029851
[2m[36m(func pid=16299)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=16299)[0m f1_macro: 0.0095571037886045
[2m[36m(func pid=16299)[0m f1_weighted: 0.013385861364600233
[2m[36m(func pid=16299)[0m f1_per_class: [0.022, 0.0, 0.019, 0.0, 0.0, 0.0, 0.043, 0.0, 0.0, 0.013]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.0830 | Steps: 2 | Val loss: 2.4010 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=27838)[0m top1: 0.2887126865671642
[2m[36m(func pid=27838)[0m top5: 0.8652052238805971
[2m[36m(func pid=27838)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=27838)[0m f1_macro: 0.27950980455994573
[2m[36m(func pid=27838)[0m f1_weighted: 0.28089867020887993
[2m[36m(func pid=27838)[0m f1_per_class: [0.333, 0.33, 0.55, 0.439, 0.122, 0.198, 0.145, 0.281, 0.175, 0.222]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0001 | Steps: 2 | Val loss: 3.1511 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.0774 | Steps: 2 | Val loss: 52689.8633 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=27371)[0m top1: 0.10401119402985075
[2m[36m(func pid=27371)[0m top5: 0.4925373134328358
[2m[36m(func pid=27371)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=27371)[0m f1_macro: 0.10178575615282774
[2m[36m(func pid=27371)[0m f1_weighted: 0.0982625261468768
[2m[36m(func pid=27371)[0m f1_per_class: [0.099, 0.176, 0.25, 0.143, 0.0, 0.062, 0.021, 0.138, 0.088, 0.042]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:44:42 (running for 00:25:17.31)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                   97 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 28.177 |      0.01  |                   70 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.083 |      0.102 |                   20 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.155 |      0.28  |                   19 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4039179104477612
[2m[36m(func pid=10089)[0m top5: 0.9496268656716418
[2m[36m(func pid=10089)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=10089)[0m f1_macro: 0.4244845471241967
[2m[36m(func pid=10089)[0m f1_weighted: 0.42285433033245695
[2m[36m(func pid=10089)[0m f1_per_class: [0.584, 0.468, 0.769, 0.534, 0.292, 0.311, 0.37, 0.283, 0.228, 0.405]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1472 | Steps: 2 | Val loss: 1.8849 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=16299)[0m top1: 0.027985074626865673
[2m[36m(func pid=16299)[0m top5: 0.4766791044776119
[2m[36m(func pid=16299)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=16299)[0m f1_macro: 0.017114242340517335
[2m[36m(func pid=16299)[0m f1_weighted: 0.03573856189169917
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.116, 0.016, 0.0, 0.01]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.0516 | Steps: 2 | Val loss: 2.3883 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=27838)[0m top1: 0.2971082089552239
[2m[36m(func pid=27838)[0m top5: 0.8782649253731343
[2m[36m(func pid=27838)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=27838)[0m f1_macro: 0.28447515721162736
[2m[36m(func pid=27838)[0m f1_weighted: 0.29297922421946515
[2m[36m(func pid=27838)[0m f1_per_class: [0.331, 0.335, 0.537, 0.446, 0.128, 0.199, 0.176, 0.283, 0.176, 0.234]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0001 | Steps: 2 | Val loss: 3.1728 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 6.0323 | Steps: 2 | Val loss: 37141.7969 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=27371)[0m top1: 0.10354477611940298
[2m[36m(func pid=27371)[0m top5: 0.4986007462686567
[2m[36m(func pid=27371)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=27371)[0m f1_macro: 0.10250872303725533
[2m[36m(func pid=27371)[0m f1_weighted: 0.09978914648024934
[2m[36m(func pid=27371)[0m f1_per_class: [0.096, 0.17, 0.261, 0.146, 0.0, 0.069, 0.024, 0.134, 0.087, 0.037]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:44:47 (running for 00:25:22.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.418 |                   98 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  3.077 |      0.017 |                   71 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  2.052 |      0.103 |                   21 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.147 |      0.284 |                   20 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4006529850746269
[2m[36m(func pid=10089)[0m top5: 0.9500932835820896
[2m[36m(func pid=10089)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=10089)[0m f1_macro: 0.41761680273709956
[2m[36m(func pid=10089)[0m f1_weighted: 0.41969560217203256
[2m[36m(func pid=10089)[0m f1_per_class: [0.565, 0.46, 0.769, 0.531, 0.283, 0.306, 0.371, 0.285, 0.226, 0.38]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1197 | Steps: 2 | Val loss: 1.8698 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=16299)[0m top1: 0.03264925373134328
[2m[36m(func pid=16299)[0m top5: 0.4962686567164179
[2m[36m(func pid=16299)[0m f1_micro: 0.03264925373134328
[2m[36m(func pid=16299)[0m f1_macro: 0.018867006442233148
[2m[36m(func pid=16299)[0m f1_weighted: 0.03475356996865729
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.014, 0.045, 0.0, 0.0, 0.0, 0.107, 0.0, 0.0, 0.023]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27838)[0m top1: 0.300839552238806
[2m[36m(func pid=27838)[0m top5: 0.8824626865671642
[2m[36m(func pid=27838)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=27838)[0m f1_macro: 0.2872155294035929
[2m[36m(func pid=27838)[0m f1_weighted: 0.29824272639474203
[2m[36m(func pid=27838)[0m f1_per_class: [0.374, 0.336, 0.537, 0.443, 0.13, 0.198, 0.195, 0.272, 0.176, 0.211]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8964 | Steps: 2 | Val loss: 2.3782 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.1642 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 41.0669 | Steps: 2 | Val loss: 27411.7031 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=27371)[0m top1: 0.10541044776119403
[2m[36m(func pid=27371)[0m top5: 0.507929104477612
[2m[36m(func pid=27371)[0m f1_micro: 0.10541044776119404
[2m[36m(func pid=27371)[0m f1_macro: 0.10342580907570605
[2m[36m(func pid=27371)[0m f1_weighted: 0.10211277070183135
[2m[36m(func pid=27371)[0m f1_per_class: [0.096, 0.18, 0.25, 0.143, 0.0, 0.077, 0.027, 0.132, 0.086, 0.044]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:44:53 (running for 00:25:27.87)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.426 |                   99 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  6.032 |      0.019 |                   72 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.896 |      0.103 |                   22 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.12  |      0.287 |                   21 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=10089)[0m top1: 0.4006529850746269
[2m[36m(func pid=10089)[0m top5: 0.9496268656716418
[2m[36m(func pid=10089)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=10089)[0m f1_macro: 0.4256113881245266
[2m[36m(func pid=10089)[0m f1_weighted: 0.4202315678345858
[2m[36m(func pid=10089)[0m f1_per_class: [0.593, 0.462, 0.815, 0.521, 0.269, 0.312, 0.377, 0.282, 0.225, 0.4]
[2m[36m(func pid=10089)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1072 | Steps: 2 | Val loss: 1.8645 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=16299)[0m top1: 0.025186567164179104
[2m[36m(func pid=16299)[0m top5: 0.5
[2m[36m(func pid=16299)[0m f1_micro: 0.025186567164179104
[2m[36m(func pid=16299)[0m f1_macro: 0.015752629817215914
[2m[36m(func pid=16299)[0m f1_weighted: 0.02204678507289607
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.022, 0.055, 0.0, 0.0, 0.0, 0.059, 0.0, 0.0, 0.021]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27838)[0m top1: 0.30363805970149255
[2m[36m(func pid=27838)[0m top5: 0.8847947761194029
[2m[36m(func pid=27838)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=27838)[0m f1_macro: 0.29133423426294436
[2m[36m(func pid=27838)[0m f1_weighted: 0.30286423558554587
[2m[36m(func pid=27838)[0m f1_per_class: [0.38, 0.331, 0.537, 0.438, 0.15, 0.193, 0.217, 0.283, 0.178, 0.207]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.9248 | Steps: 2 | Val loss: 2.3644 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=10089)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0000 | Steps: 2 | Val loss: 3.2158 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 8.0215 | Steps: 2 | Val loss: 24797.2188 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:44:58 (running for 00:25:33.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00009 | RUNNING    | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.426 |                   99 |
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 41.067 |      0.016 |                   73 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.925 |      0.105 |                   23 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.107 |      0.291 |                   22 |
| train_2d480_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=27371)[0m top1: 0.10634328358208955
[2m[36m(func pid=27371)[0m top5: 0.5186567164179104
[2m[36m(func pid=27371)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=27371)[0m f1_macro: 0.1054446262355088
[2m[36m(func pid=27371)[0m f1_weighted: 0.10354828857192729
[2m[36m(func pid=27371)[0m f1_per_class: [0.099, 0.18, 0.25, 0.141, 0.0, 0.085, 0.03, 0.129, 0.095, 0.046]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1149 | Steps: 2 | Val loss: 1.8587 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=10089)[0m top1: 0.4001865671641791
[2m[36m(func pid=10089)[0m top5: 0.9486940298507462
[2m[36m(func pid=10089)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=10089)[0m f1_macro: 0.42378795442005285
[2m[36m(func pid=10089)[0m f1_weighted: 0.41917989659107346
[2m[36m(func pid=10089)[0m f1_per_class: [0.587, 0.466, 0.815, 0.521, 0.262, 0.307, 0.372, 0.289, 0.23, 0.39]
[2m[36m(func pid=16299)[0m top1: 0.017257462686567165
[2m[36m(func pid=16299)[0m top5: 0.5
[2m[36m(func pid=16299)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=16299)[0m f1_macro: 0.00990614455323827
[2m[36m(func pid=16299)[0m f1_weighted: 0.010969998304227367
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.013, 0.041, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0, 0.017]
[2m[36m(func pid=16299)[0m 
[2m[36m(func pid=27838)[0m top1: 0.30830223880597013
[2m[36m(func pid=27838)[0m top5: 0.8847947761194029
[2m[36m(func pid=27838)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=27838)[0m f1_macro: 0.2921427390799765
[2m[36m(func pid=27838)[0m f1_weighted: 0.3104969426160764
[2m[36m(func pid=27838)[0m f1_per_class: [0.361, 0.336, 0.537, 0.441, 0.144, 0.191, 0.24, 0.282, 0.182, 0.209]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8446 | Steps: 2 | Val loss: 2.3554 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=16299)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 203.9755 | Steps: 2 | Val loss: 21113.6855 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=27371)[0m top1: 0.10867537313432836
[2m[36m(func pid=27371)[0m top5: 0.5209888059701493
[2m[36m(func pid=27371)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=27371)[0m f1_macro: 0.1067572541631387
[2m[36m(func pid=27371)[0m f1_weighted: 0.1067790704816427
[2m[36m(func pid=27371)[0m f1_per_class: [0.107, 0.184, 0.243, 0.141, 0.0, 0.083, 0.038, 0.135, 0.092, 0.044]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1003 | Steps: 2 | Val loss: 1.8406 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=16299)[0m top1: 0.013992537313432836
[2m[36m(func pid=16299)[0m top5: 0.5032649253731343
[2m[36m(func pid=16299)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=16299)[0m f1_macro: 0.00925753561689984
[2m[36m(func pid=16299)[0m f1_weighted: 0.009674605048357099
[2m[36m(func pid=16299)[0m f1_per_class: [0.0, 0.038, 0.037, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0, 0.008]
[2m[36m(func pid=27838)[0m top1: 0.3148320895522388
[2m[36m(func pid=27838)[0m top5: 0.8899253731343284
[2m[36m(func pid=27838)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=27838)[0m f1_macro: 0.2964791784330084
[2m[36m(func pid=27838)[0m f1_weighted: 0.31712172243931314
[2m[36m(func pid=27838)[0m f1_per_class: [0.36, 0.349, 0.537, 0.447, 0.144, 0.193, 0.248, 0.267, 0.201, 0.218]
== Status ==
Current time: 2024-01-07 03:45:03 (running for 00:25:38.50)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.382
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00011 | RUNNING    | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 |  8.021 |      0.01  |                   74 |
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.845 |      0.107 |                   24 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.115 |      0.292 |                   23 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33107)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=33107)[0m Configuration completed!
[2m[36m(func pid=33107)[0m New optimizer parameters:
[2m[36m(func pid=33107)[0m SGD (
[2m[36m(func pid=33107)[0m Parameter Group 0
[2m[36m(func pid=33107)[0m     dampening: 0
[2m[36m(func pid=33107)[0m     differentiable: False
[2m[36m(func pid=33107)[0m     foreach: None
[2m[36m(func pid=33107)[0m     lr: 0.01
[2m[36m(func pid=33107)[0m     maximize: False
[2m[36m(func pid=33107)[0m     momentum: 0.9
[2m[36m(func pid=33107)[0m     nesterov: False
[2m[36m(func pid=33107)[0m     weight_decay: 0.0001
[2m[36m(func pid=33107)[0m )
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.7785 | Steps: 2 | Val loss: 2.3500 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0829 | Steps: 2 | Val loss: 1.8362 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=27371)[0m top1: 0.11007462686567164
[2m[36m(func pid=27371)[0m top5: 0.5293843283582089
[2m[36m(func pid=27371)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=27371)[0m f1_macro: 0.1076633882753375
[2m[36m(func pid=27371)[0m f1_weighted: 0.10887093928505347
[2m[36m(func pid=27371)[0m f1_per_class: [0.11, 0.179, 0.231, 0.145, 0.0, 0.09, 0.041, 0.134, 0.094, 0.053]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0861 | Steps: 2 | Val loss: 2.3618 | Batch size: 32 | lr: 0.01 | Duration: 4.70s
[2m[36m(func pid=27838)[0m top1: 0.31296641791044777
[2m[36m(func pid=27838)[0m top5: 0.8894589552238806
[2m[36m(func pid=27838)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=27838)[0m f1_macro: 0.2940227728841104
[2m[36m(func pid=27838)[0m f1_weighted: 0.31730202775764504
[2m[36m(func pid=27838)[0m f1_per_class: [0.365, 0.347, 0.524, 0.436, 0.138, 0.192, 0.262, 0.266, 0.192, 0.218]
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.8232 | Steps: 2 | Val loss: 2.3344 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=33107)[0m top1: 0.08908582089552239
[2m[36m(func pid=33107)[0m top5: 0.5041977611940298
[2m[36m(func pid=33107)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=33107)[0m f1_macro: 0.08027080664511987
[2m[36m(func pid=33107)[0m f1_weighted: 0.07750092906341915
[2m[36m(func pid=33107)[0m f1_per_class: [0.112, 0.078, 0.092, 0.14, 0.028, 0.03, 0.021, 0.14, 0.104, 0.057]
== Status ==
Current time: 2024-01-07 03:45:09 (running for 00:25:44.20)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.779 |      0.108 |                   25 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.1   |      0.296 |                   24 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 03:45:14 (running for 00:25:49.28)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.779 |      0.108 |                   25 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.083 |      0.294 |                   25 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33680)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=33680)[0m Configuration completed!
[2m[36m(func pid=33680)[0m New optimizer parameters:
[2m[36m(func pid=33680)[0m SGD (
[2m[36m(func pid=33680)[0m Parameter Group 0
[2m[36m(func pid=33680)[0m     dampening: 0
[2m[36m(func pid=33680)[0m     differentiable: False
[2m[36m(func pid=33680)[0m     foreach: None
[2m[36m(func pid=33680)[0m     lr: 0.1
[2m[36m(func pid=33680)[0m     maximize: False
[2m[36m(func pid=33680)[0m     momentum: 0.9
[2m[36m(func pid=33680)[0m     nesterov: False
[2m[36m(func pid=33680)[0m     weight_decay: 0.0001
[2m[36m(func pid=33680)[0m )
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.1142723880597015
[2m[36m(func pid=27371)[0m top5: 0.5424440298507462
[2m[36m(func pid=27371)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=27371)[0m f1_macro: 0.11220573932426578
[2m[36m(func pid=27371)[0m f1_weighted: 0.11690237117367035
[2m[36m(func pid=27371)[0m f1_per_class: [0.121, 0.175, 0.247, 0.149, 0.0, 0.091, 0.066, 0.131, 0.095, 0.048]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9866 | Steps: 2 | Val loss: 2.2523 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0929 | Steps: 2 | Val loss: 1.8364 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.6929 | Steps: 2 | Val loss: 2.3260 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.5016 | Steps: 2 | Val loss: 10.1732 | Batch size: 32 | lr: 0.1 | Duration: 4.47s
== Status ==
Current time: 2024-01-07 03:45:19 (running for 00:25:54.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.823 |      0.112 |                   26 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.083 |      0.294 |                   25 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  3.086 |      0.08  |                    1 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27838)[0m top1: 0.31576492537313433
[2m[36m(func pid=27838)[0m top5: 0.886660447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=27838)[0m f1_macro: 0.29797268216438966
[2m[36m(func pid=27838)[0m f1_weighted: 0.3222120951468391
[2m[36m(func pid=27838)[0m f1_per_class: [0.369, 0.348, 0.537, 0.436, 0.12, 0.193, 0.274, 0.271, 0.204, 0.228]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m top1: 0.15858208955223882
[2m[36m(func pid=33107)[0m top5: 0.6506529850746269
[2m[36m(func pid=33107)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=33107)[0m f1_macro: 0.12767315240869803
[2m[36m(func pid=33107)[0m f1_weighted: 0.17154951851215322
[2m[36m(func pid=33107)[0m f1_per_class: [0.112, 0.039, 0.167, 0.167, 0.05, 0.029, 0.333, 0.102, 0.129, 0.149]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m top1: 0.11893656716417911
[2m[36m(func pid=27371)[0m top5: 0.5438432835820896
[2m[36m(func pid=27371)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=27371)[0m f1_macro: 0.11558033035947142
[2m[36m(func pid=27371)[0m f1_weighted: 0.12163948470521983
[2m[36m(func pid=27371)[0m f1_per_class: [0.12, 0.178, 0.237, 0.153, 0.0, 0.09, 0.074, 0.14, 0.098, 0.065]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.03871268656716418
[2m[36m(func pid=33680)[0m top5: 0.34048507462686567
[2m[36m(func pid=33680)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=33680)[0m f1_macro: 0.014594909583067032
[2m[36m(func pid=33680)[0m f1_weighted: 0.0039309969429891034
[2m[36m(func pid=33680)[0m f1_per_class: [0.072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.074, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9639 | Steps: 2 | Val loss: 2.1530 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1018 | Steps: 2 | Val loss: 1.8278 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6482 | Steps: 2 | Val loss: 2.3193 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.4146 | Steps: 2 | Val loss: 106.2905 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 03:45:24 (running for 00:25:59.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.693 |      0.116 |                   27 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.093 |      0.298 |                   26 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.964 |      0.167 |                    3 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.502 |      0.015 |                    1 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.19263059701492538
[2m[36m(func pid=33107)[0m top5: 0.753731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=33107)[0m f1_macro: 0.1674461747057407
[2m[36m(func pid=33107)[0m f1_weighted: 0.200257797667361
[2m[36m(func pid=33107)[0m f1_per_class: [0.141, 0.096, 0.333, 0.305, 0.062, 0.015, 0.249, 0.182, 0.152, 0.138]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3162313432835821
[2m[36m(func pid=27838)[0m top5: 0.8894589552238806
[2m[36m(func pid=27838)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=27838)[0m f1_macro: 0.2993083155862081
[2m[36m(func pid=27838)[0m f1_weighted: 0.32578205220032963
[2m[36m(func pid=27838)[0m f1_per_class: [0.377, 0.353, 0.55, 0.431, 0.133, 0.181, 0.294, 0.262, 0.199, 0.211]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.12126865671641791
[2m[36m(func pid=27371)[0m top5: 0.5522388059701493
[2m[36m(func pid=27371)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=27371)[0m f1_macro: 0.1173818439366566
[2m[36m(func pid=27371)[0m f1_weighted: 0.12577876905064342
[2m[36m(func pid=27371)[0m f1_per_class: [0.12, 0.177, 0.234, 0.156, 0.0, 0.091, 0.084, 0.141, 0.104, 0.067]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.06949626865671642
[2m[36m(func pid=33680)[0m top5: 0.23460820895522388
[2m[36m(func pid=33680)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=33680)[0m f1_macro: 0.10758048455774882
[2m[36m(func pid=33680)[0m f1_weighted: 0.048559370443189254
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.018, 0.066, 0.32, 0.0, 0.0, 0.398, 0.066, 0.207]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.3219 | Steps: 2 | Val loss: 2.1225 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0810 | Steps: 2 | Val loss: 1.8174 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6029 | Steps: 2 | Val loss: 2.3129 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.2664 | Steps: 2 | Val loss: 8028.5479 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 03:45:30 (running for 00:26:05.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.648 |      0.117 |                   28 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.102 |      0.299 |                   27 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.322 |      0.237 |                    4 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.415 |      0.108 |                    2 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.22574626865671643
[2m[36m(func pid=33107)[0m top5: 0.7807835820895522
[2m[36m(func pid=33107)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=33107)[0m f1_macro: 0.23669344996658487
[2m[36m(func pid=33107)[0m f1_weighted: 0.21392925719769743
[2m[36m(func pid=33107)[0m f1_per_class: [0.244, 0.287, 0.387, 0.315, 0.098, 0.131, 0.086, 0.314, 0.186, 0.32]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.31669776119402987
[2m[36m(func pid=27838)[0m top5: 0.8941231343283582
[2m[36m(func pid=27838)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=27838)[0m f1_macro: 0.30093257492287473
[2m[36m(func pid=27838)[0m f1_weighted: 0.3247019933337796
[2m[36m(func pid=27838)[0m f1_per_class: [0.367, 0.357, 0.55, 0.426, 0.128, 0.175, 0.294, 0.263, 0.205, 0.243]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.1226679104477612
[2m[36m(func pid=27371)[0m top5: 0.5625
[2m[36m(func pid=27371)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=27371)[0m f1_macro: 0.11670789988418098
[2m[36m(func pid=27371)[0m f1_weighted: 0.12788490872486857
[2m[36m(func pid=27371)[0m f1_per_class: [0.124, 0.171, 0.214, 0.164, 0.0, 0.091, 0.087, 0.143, 0.103, 0.069]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.01166044776119403
[2m[36m(func pid=33680)[0m top5: 0.5149253731343284
[2m[36m(func pid=33680)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=33680)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=33680)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.1459 | Steps: 2 | Val loss: 2.1288 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0981 | Steps: 2 | Val loss: 1.8170 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6060 | Steps: 2 | Val loss: 2.3040 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 11.0115 | Steps: 2 | Val loss: 160215712.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 03:45:35 (running for 00:26:10.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.603 |      0.117 |                   29 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.081 |      0.301 |                   28 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.146 |      0.274 |                    5 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.266 |      0.002 |                    3 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.24347014925373134
[2m[36m(func pid=33107)[0m top5: 0.7840485074626866
[2m[36m(func pid=33107)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=33107)[0m f1_macro: 0.27434547095118234
[2m[36m(func pid=33107)[0m f1_weighted: 0.21562860632788006
[2m[36m(func pid=33107)[0m f1_per_class: [0.373, 0.329, 0.421, 0.286, 0.174, 0.23, 0.037, 0.353, 0.184, 0.358]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3185634328358209
[2m[36m(func pid=27838)[0m top5: 0.8936567164179104
[2m[36m(func pid=27838)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=27838)[0m f1_macro: 0.30035441742520247
[2m[36m(func pid=27838)[0m f1_weighted: 0.326807206420323
[2m[36m(func pid=27838)[0m f1_per_class: [0.341, 0.369, 0.537, 0.418, 0.121, 0.174, 0.302, 0.268, 0.214, 0.259]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.125
[2m[36m(func pid=27371)[0m top5: 0.5750932835820896
[2m[36m(func pid=27371)[0m f1_micro: 0.125
[2m[36m(func pid=27371)[0m f1_macro: 0.11865950496314356
[2m[36m(func pid=27371)[0m f1_weighted: 0.13040944637068447
[2m[36m(func pid=27371)[0m f1_per_class: [0.125, 0.175, 0.209, 0.163, 0.0, 0.091, 0.092, 0.146, 0.118, 0.067]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.0790 | Steps: 2 | Val loss: 2.0676 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0706 | Steps: 2 | Val loss: 1.8118 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.6254 | Steps: 2 | Val loss: 2.3000 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 11.7857 | Steps: 2 | Val loss: 29363798016000.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=33107)[0m top1: 0.2667910447761194
[2m[36m(func pid=33107)[0m top5: 0.8255597014925373
[2m[36m(func pid=33107)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=33107)[0m f1_macro: 0.29703308223697245
[2m[36m(func pid=33107)[0m f1_weighted: 0.23584708740814803
[2m[36m(func pid=33107)[0m f1_per_class: [0.483, 0.353, 0.4, 0.304, 0.211, 0.293, 0.042, 0.344, 0.194, 0.346]
[2m[36m(func pid=33107)[0m 
== Status ==
Current time: 2024-01-07 03:45:40 (running for 00:26:15.47)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.606 |      0.119 |                   30 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.098 |      0.3   |                   29 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.079 |      0.297 |                    6 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 11.012 |      0.001 |                    4 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27838)[0m top1: 0.31949626865671643
[2m[36m(func pid=27838)[0m top5: 0.8922574626865671
[2m[36m(func pid=27838)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=27838)[0m f1_macro: 0.30138526940058885
[2m[36m(func pid=27838)[0m f1_weighted: 0.32733453085212777
[2m[36m(func pid=27838)[0m f1_per_class: [0.367, 0.374, 0.545, 0.414, 0.12, 0.173, 0.308, 0.251, 0.206, 0.254]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.125
[2m[36m(func pid=27371)[0m top5: 0.5783582089552238
[2m[36m(func pid=27371)[0m f1_micro: 0.125
[2m[36m(func pid=27371)[0m f1_macro: 0.11958503217411867
[2m[36m(func pid=27371)[0m f1_weighted: 0.13100411101692225
[2m[36m(func pid=27371)[0m f1_per_class: [0.129, 0.171, 0.22, 0.163, 0.0, 0.093, 0.097, 0.142, 0.116, 0.066]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.0530 | Steps: 2 | Val loss: 2.0275 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0799 | Steps: 2 | Val loss: 1.8117 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.2975 | Steps: 2 | Val loss: 10166243328.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4713 | Steps: 2 | Val loss: 2.2903 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 03:45:45 (running for 00:26:20.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.625 |      0.12  |                   31 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.071 |      0.301 |                   30 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.053 |      0.31  |                    7 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 11.786 |      0.001 |                    5 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.2728544776119403
[2m[36m(func pid=33107)[0m top5: 0.8535447761194029
[2m[36m(func pid=33107)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=33107)[0m f1_macro: 0.30998858473797664
[2m[36m(func pid=33107)[0m f1_weighted: 0.24172090325226173
[2m[36m(func pid=33107)[0m f1_per_class: [0.571, 0.374, 0.444, 0.286, 0.196, 0.304, 0.054, 0.357, 0.194, 0.318]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3208955223880597
[2m[36m(func pid=27838)[0m top5: 0.8908582089552238
[2m[36m(func pid=27838)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=27838)[0m f1_macro: 0.30257741583028647
[2m[36m(func pid=27838)[0m f1_weighted: 0.3296102351062273
[2m[36m(func pid=27838)[0m f1_per_class: [0.38, 0.385, 0.545, 0.392, 0.124, 0.169, 0.33, 0.252, 0.209, 0.238]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.13152985074626866
[2m[36m(func pid=27371)[0m top5: 0.5946828358208955
[2m[36m(func pid=27371)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=27371)[0m f1_macro: 0.12428549482633142
[2m[36m(func pid=27371)[0m f1_weighted: 0.13838111621520824
[2m[36m(func pid=27371)[0m f1_per_class: [0.133, 0.182, 0.222, 0.179, 0.0, 0.093, 0.099, 0.143, 0.123, 0.07]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.0414 | Steps: 2 | Val loss: 2.0150 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0741 | Steps: 2 | Val loss: 1.8170 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.2129 | Steps: 2 | Val loss: 294658112.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5096 | Steps: 2 | Val loss: 2.2804 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=33107)[0m top1: 0.28078358208955223
[2m[36m(func pid=33107)[0m top5: 0.8680037313432836
[2m[36m(func pid=33107)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=33107)[0m f1_macro: 0.3107250241129399
[2m[36m(func pid=33107)[0m f1_weighted: 0.25328451945708147
[2m[36m(func pid=33107)[0m f1_per_class: [0.487, 0.387, 0.444, 0.3, 0.202, 0.304, 0.076, 0.358, 0.203, 0.346]
[2m[36m(func pid=33107)[0m 
== Status ==
Current time: 2024-01-07 03:45:51 (running for 00:26:25.97)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.471 |      0.124 |                   32 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.08  |      0.303 |                   31 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.041 |      0.311 |                    8 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  5.298 |      0.001 |                    6 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27838)[0m top1: 0.3185634328358209
[2m[36m(func pid=27838)[0m top5: 0.8833955223880597
[2m[36m(func pid=27838)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=27838)[0m f1_macro: 0.29993234137605507
[2m[36m(func pid=27838)[0m f1_weighted: 0.328512566272157
[2m[36m(func pid=27838)[0m f1_per_class: [0.376, 0.377, 0.545, 0.393, 0.119, 0.164, 0.331, 0.258, 0.21, 0.226]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.13526119402985073
[2m[36m(func pid=27371)[0m top5: 0.6021455223880597
[2m[36m(func pid=27371)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=27371)[0m f1_macro: 0.1268000601514462
[2m[36m(func pid=27371)[0m f1_weighted: 0.14351126072279494
[2m[36m(func pid=27371)[0m f1_per_class: [0.13, 0.179, 0.231, 0.187, 0.0, 0.094, 0.109, 0.151, 0.115, 0.072]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0203 | Steps: 2 | Val loss: 1.9764 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0771 | Steps: 2 | Val loss: 1.8083 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8620 | Steps: 2 | Val loss: 22651876.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5478 | Steps: 2 | Val loss: 2.2713 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:45:56 (running for 00:26:31.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.51  |      0.127 |                   33 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.074 |      0.3   |                   32 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.02  |      0.322 |                    9 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.213 |      0.001 |                    7 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.2947761194029851
[2m[36m(func pid=33107)[0m top5: 0.8857276119402985
[2m[36m(func pid=33107)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=33107)[0m f1_macro: 0.32246219055943176
[2m[36m(func pid=33107)[0m f1_weighted: 0.2811295912950294
[2m[36m(func pid=33107)[0m f1_per_class: [0.487, 0.402, 0.49, 0.33, 0.188, 0.284, 0.142, 0.345, 0.199, 0.357]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.322294776119403
[2m[36m(func pid=27838)[0m top5: 0.8885261194029851
[2m[36m(func pid=27838)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=27838)[0m f1_macro: 0.3006528455265999
[2m[36m(func pid=27838)[0m f1_weighted: 0.3332238144950719
[2m[36m(func pid=27838)[0m f1_per_class: [0.37, 0.376, 0.545, 0.406, 0.126, 0.151, 0.342, 0.255, 0.21, 0.226]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.13805970149253732
[2m[36m(func pid=27371)[0m top5: 0.6096082089552238
[2m[36m(func pid=27371)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=27371)[0m f1_macro: 0.12923984145246936
[2m[36m(func pid=27371)[0m f1_weighted: 0.1468187762018419
[2m[36m(func pid=27371)[0m f1_per_class: [0.139, 0.183, 0.234, 0.191, 0.0, 0.092, 0.113, 0.148, 0.124, 0.068]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0123 | Steps: 2 | Val loss: 1.9534 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0823 | Steps: 2 | Val loss: 1.8031 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8444 | Steps: 2 | Val loss: 3528051.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.3878 | Steps: 2 | Val loss: 2.2632 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 03:46:01 (running for 00:26:36.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.548 |      0.129 |                   34 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.077 |      0.301 |                   33 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.012 |      0.336 |                   10 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.862 |      0.001 |                    8 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3087686567164179
[2m[36m(func pid=33107)[0m top5: 0.8964552238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=33107)[0m f1_macro: 0.3361363423148312
[2m[36m(func pid=33107)[0m f1_weighted: 0.30153681517840264
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.415, 0.522, 0.341, 0.189, 0.268, 0.195, 0.35, 0.207, 0.375]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.32369402985074625
[2m[36m(func pid=27838)[0m top5: 0.8880597014925373
[2m[36m(func pid=27838)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=27838)[0m f1_macro: 0.30360794882209935
[2m[36m(func pid=27838)[0m f1_weighted: 0.3354320498267933
[2m[36m(func pid=27838)[0m f1_per_class: [0.371, 0.373, 0.558, 0.412, 0.14, 0.153, 0.344, 0.254, 0.213, 0.219]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.14272388059701493
[2m[36m(func pid=27371)[0m top5: 0.6189365671641791
[2m[36m(func pid=27371)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=27371)[0m f1_macro: 0.13346382622457367
[2m[36m(func pid=27371)[0m f1_weighted: 0.15325163880810758
[2m[36m(func pid=27371)[0m f1_per_class: [0.139, 0.178, 0.243, 0.201, 0.011, 0.093, 0.128, 0.148, 0.122, 0.071]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0097 | Steps: 2 | Val loss: 1.8987 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0487 | Steps: 2 | Val loss: 1.8047 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6377 | Steps: 2 | Val loss: 918622.9375 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.4122 | Steps: 2 | Val loss: 2.2568 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 03:46:06 (running for 00:26:41.44)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.388 |      0.133 |                   35 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.082 |      0.304 |                   34 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.01  |      0.347 |                   11 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.844 |      0.001 |                    9 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3278917910447761
[2m[36m(func pid=33107)[0m top5: 0.9020522388059702
[2m[36m(func pid=33107)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=33107)[0m f1_macro: 0.34704387788585567
[2m[36m(func pid=33107)[0m f1_weighted: 0.3284753332389095
[2m[36m(func pid=33107)[0m f1_per_class: [0.533, 0.433, 0.522, 0.377, 0.183, 0.254, 0.247, 0.334, 0.217, 0.37]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.324160447761194
[2m[36m(func pid=27838)[0m top5: 0.8908582089552238
[2m[36m(func pid=27838)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=27838)[0m f1_macro: 0.3053364125530179
[2m[36m(func pid=27838)[0m f1_weighted: 0.33368700610847807
[2m[36m(func pid=27838)[0m f1_per_class: [0.382, 0.374, 0.558, 0.416, 0.141, 0.154, 0.33, 0.264, 0.213, 0.221]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.14925373134328357
[2m[36m(func pid=27371)[0m top5: 0.6291977611940298
[2m[36m(func pid=27371)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=27371)[0m f1_macro: 0.13888602401564282
[2m[36m(func pid=27371)[0m f1_weighted: 0.16150092611152508
[2m[36m(func pid=27371)[0m f1_per_class: [0.142, 0.183, 0.263, 0.217, 0.011, 0.092, 0.138, 0.15, 0.122, 0.072]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0050 | Steps: 2 | Val loss: 1.8642 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0597 | Steps: 2 | Val loss: 1.7986 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7022 | Steps: 2 | Val loss: 267039.6875 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:46:11 (running for 00:26:46.51)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.412 |      0.139 |                   36 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.049 |      0.305 |                   35 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.005 |      0.354 |                   12 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.638 |      0.001 |                   10 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3400186567164179
[2m[36m(func pid=33107)[0m top5: 0.9090485074626866
[2m[36m(func pid=33107)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=33107)[0m f1_macro: 0.35434414866317593
[2m[36m(func pid=33107)[0m f1_weighted: 0.34483083949256105
[2m[36m(func pid=33107)[0m f1_per_class: [0.526, 0.438, 0.571, 0.416, 0.18, 0.232, 0.271, 0.33, 0.216, 0.361]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3428 | Steps: 2 | Val loss: 2.2479 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=27838)[0m top1: 0.32882462686567165
[2m[36m(func pid=27838)[0m top5: 0.8922574626865671
[2m[36m(func pid=27838)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=27838)[0m f1_macro: 0.30679300072066557
[2m[36m(func pid=27838)[0m f1_weighted: 0.3369223171934668
[2m[36m(func pid=27838)[0m f1_per_class: [0.393, 0.392, 0.545, 0.419, 0.137, 0.162, 0.325, 0.267, 0.202, 0.226]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.17210820895522388
[2m[36m(func pid=33680)[0m top5: 0.5727611940298507
[2m[36m(func pid=33680)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=33680)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=33680)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.15391791044776118
[2m[36m(func pid=27371)[0m top5: 0.6333955223880597
[2m[36m(func pid=27371)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=27371)[0m f1_macro: 0.14000859451436815
[2m[36m(func pid=27371)[0m f1_weighted: 0.1669576260386434
[2m[36m(func pid=27371)[0m f1_per_class: [0.142, 0.186, 0.25, 0.223, 0.011, 0.09, 0.148, 0.156, 0.119, 0.076]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0043 | Steps: 2 | Val loss: 1.8276 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0539 | Steps: 2 | Val loss: 1.8065 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7611 | Steps: 2 | Val loss: 110405.4375 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:46:16 (running for 00:26:51.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.343 |      0.14  |                   37 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.06  |      0.307 |                   36 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.004 |      0.361 |                   13 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.702 |      0.029 |                   11 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.365205223880597
[2m[36m(func pid=33107)[0m top5: 0.9123134328358209
[2m[36m(func pid=33107)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=33107)[0m f1_macro: 0.3610697474527025
[2m[36m(func pid=33107)[0m f1_weighted: 0.37402159646807787
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.462, 0.558, 0.445, 0.176, 0.235, 0.327, 0.344, 0.219, 0.345]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3949 | Steps: 2 | Val loss: 2.2422 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=27838)[0m top1: 0.32509328358208955
[2m[36m(func pid=27838)[0m top5: 0.8880597014925373
[2m[36m(func pid=27838)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=27838)[0m f1_macro: 0.3064732750330172
[2m[36m(func pid=27838)[0m f1_weighted: 0.33317338946355796
[2m[36m(func pid=27838)[0m f1_per_class: [0.4, 0.39, 0.558, 0.412, 0.139, 0.158, 0.322, 0.262, 0.208, 0.216]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.15391791044776118
[2m[36m(func pid=27371)[0m top5: 0.6357276119402985
[2m[36m(func pid=27371)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=27371)[0m f1_macro: 0.14158758512555386
[2m[36m(func pid=27371)[0m f1_weighted: 0.16837505652909235
[2m[36m(func pid=27371)[0m f1_per_class: [0.142, 0.177, 0.274, 0.228, 0.011, 0.091, 0.154, 0.148, 0.117, 0.074]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0048 | Steps: 2 | Val loss: 1.8169 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0456 | Steps: 2 | Val loss: 1.8069 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.3721 | Steps: 2 | Val loss: 50201.1562 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 03:46:22 (running for 00:26:56.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.395 |      0.142 |                   38 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.054 |      0.306 |                   37 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.005 |      0.37  |                   14 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.761 |      0.001 |                   12 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.37033582089552236
[2m[36m(func pid=33107)[0m top5: 0.9132462686567164
[2m[36m(func pid=33107)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=33107)[0m f1_macro: 0.3704535678633734
[2m[36m(func pid=33107)[0m f1_weighted: 0.38055402997192883
[2m[36m(func pid=33107)[0m f1_per_class: [0.51, 0.467, 0.615, 0.45, 0.191, 0.221, 0.348, 0.324, 0.224, 0.356]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.3281 | Steps: 2 | Val loss: 2.2386 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=27838)[0m top1: 0.324160447761194
[2m[36m(func pid=27838)[0m top5: 0.8894589552238806
[2m[36m(func pid=27838)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=27838)[0m f1_macro: 0.3075287008062216
[2m[36m(func pid=27838)[0m f1_weighted: 0.3315990436755232
[2m[36m(func pid=27838)[0m f1_per_class: [0.405, 0.393, 0.558, 0.408, 0.13, 0.167, 0.314, 0.264, 0.207, 0.229]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0022 | Steps: 2 | Val loss: 1.7832 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=27371)[0m top1: 0.15345149253731344
[2m[36m(func pid=27371)[0m top5: 0.6399253731343284
[2m[36m(func pid=27371)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=27371)[0m f1_macro: 0.13961317222158448
[2m[36m(func pid=27371)[0m f1_weighted: 0.16859013401235554
[2m[36m(func pid=27371)[0m f1_per_class: [0.146, 0.18, 0.253, 0.228, 0.01, 0.09, 0.156, 0.133, 0.117, 0.082]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0699 | Steps: 2 | Val loss: 1.8077 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.5436 | Steps: 2 | Val loss: 186768.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 03:46:27 (running for 00:27:02.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.328 |      0.14  |                   39 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.046 |      0.308 |                   38 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.374 |                   15 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.372 |      0.001 |                   13 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3810634328358209
[2m[36m(func pid=33107)[0m top5: 0.9169776119402985
[2m[36m(func pid=33107)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=33107)[0m f1_macro: 0.3743222818534919
[2m[36m(func pid=33107)[0m f1_weighted: 0.3930023394991964
[2m[36m(func pid=33107)[0m f1_per_class: [0.514, 0.473, 0.6, 0.461, 0.19, 0.208, 0.38, 0.315, 0.233, 0.37]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.32276119402985076
[2m[36m(func pid=27838)[0m top5: 0.8899253731343284
[2m[36m(func pid=27838)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=27838)[0m f1_macro: 0.3027544137039851
[2m[36m(func pid=27838)[0m f1_weighted: 0.3312415190021825
[2m[36m(func pid=27838)[0m f1_per_class: [0.412, 0.397, 0.524, 0.413, 0.126, 0.162, 0.31, 0.258, 0.205, 0.221]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.3278 | Steps: 2 | Val loss: 2.2324 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5093283582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0031 | Steps: 2 | Val loss: 1.7816 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=27371)[0m top1: 0.15625
[2m[36m(func pid=27371)[0m top5: 0.6464552238805971
[2m[36m(func pid=27371)[0m f1_micro: 0.15625
[2m[36m(func pid=27371)[0m f1_macro: 0.14382641000654578
[2m[36m(func pid=27371)[0m f1_weighted: 0.17073674650359147
[2m[36m(func pid=27371)[0m f1_per_class: [0.148, 0.184, 0.267, 0.232, 0.019, 0.099, 0.153, 0.139, 0.113, 0.085]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0477 | Steps: 2 | Val loss: 1.8054 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.5293 | Steps: 2 | Val loss: 29570.3242 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 03:46:32 (running for 00:27:07.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.328 |      0.144 |                   40 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.07  |      0.303 |                   39 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.003 |      0.37  |                   16 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.544 |      0.001 |                   14 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3833955223880597
[2m[36m(func pid=33107)[0m top5: 0.9197761194029851
[2m[36m(func pid=33107)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=33107)[0m f1_macro: 0.3702191767287157
[2m[36m(func pid=33107)[0m f1_weighted: 0.3955055382671784
[2m[36m(func pid=33107)[0m f1_per_class: [0.505, 0.467, 0.571, 0.456, 0.178, 0.201, 0.4, 0.321, 0.24, 0.364]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5074626865671642
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=33680)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3283582089552239
[2m[36m(func pid=27838)[0m top5: 0.886660447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=27838)[0m f1_macro: 0.3069107221046716
[2m[36m(func pid=27838)[0m f1_weighted: 0.33896667501196837
[2m[36m(func pid=27838)[0m f1_per_class: [0.405, 0.391, 0.558, 0.417, 0.129, 0.161, 0.337, 0.252, 0.204, 0.214]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.2791 | Steps: 2 | Val loss: 2.2291 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.7651 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.1590 | Steps: 2 | Val loss: 8087.6050 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=27371)[0m top1: 0.15904850746268656
[2m[36m(func pid=27371)[0m top5: 0.6487873134328358
[2m[36m(func pid=27371)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=27371)[0m f1_macro: 0.14665487218912768
[2m[36m(func pid=27371)[0m f1_weighted: 0.17376204279596935
[2m[36m(func pid=27371)[0m f1_per_class: [0.148, 0.188, 0.267, 0.233, 0.019, 0.102, 0.156, 0.149, 0.12, 0.085]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0516 | Steps: 2 | Val loss: 1.7963 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 03:46:37 (running for 00:27:12.46)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.279 |      0.147 |                   41 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.048 |      0.307 |                   40 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.375 |                   17 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.529 |      0.001 |                   15 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3908582089552239
[2m[36m(func pid=33107)[0m top5: 0.9211753731343284
[2m[36m(func pid=33107)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=33107)[0m f1_macro: 0.375143858794737
[2m[36m(func pid=33107)[0m f1_weighted: 0.403000078754109
[2m[36m(func pid=33107)[0m f1_per_class: [0.518, 0.474, 0.571, 0.47, 0.19, 0.204, 0.407, 0.308, 0.24, 0.369]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.006063432835820896
[2m[36m(func pid=33680)[0m top5: 0.5321828358208955
[2m[36m(func pid=33680)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=33680)[0m f1_macro: 0.0012064965197215777
[2m[36m(func pid=33680)[0m f1_weighted: 7.315510613983447e-05
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.33115671641791045
[2m[36m(func pid=27838)[0m top5: 0.8917910447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=27838)[0m f1_macro: 0.31009106028358185
[2m[36m(func pid=27838)[0m f1_weighted: 0.3405484074700256
[2m[36m(func pid=27838)[0m f1_per_class: [0.412, 0.391, 0.558, 0.426, 0.13, 0.162, 0.33, 0.263, 0.207, 0.221]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2135 | Steps: 2 | Val loss: 2.2163 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0021 | Steps: 2 | Val loss: 1.7674 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7526 | Steps: 2 | Val loss: 2817.8699 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=27371)[0m top1: 0.1623134328358209
[2m[36m(func pid=27371)[0m top5: 0.6623134328358209
[2m[36m(func pid=27371)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=27371)[0m f1_macro: 0.14909351308023017
[2m[36m(func pid=27371)[0m f1_weighted: 0.17885770157659842
[2m[36m(func pid=27371)[0m f1_per_class: [0.147, 0.187, 0.294, 0.239, 0.011, 0.104, 0.17, 0.135, 0.12, 0.085]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0610 | Steps: 2 | Val loss: 1.7921 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=33107)[0m top1: 0.39225746268656714
[2m[36m(func pid=33107)[0m top5: 0.9225746268656716
[2m[36m(func pid=33107)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=33107)[0m f1_macro: 0.37417292842983907
[2m[36m(func pid=33107)[0m f1_weighted: 0.40320639605990194
[2m[36m(func pid=33107)[0m f1_per_class: [0.522, 0.479, 0.558, 0.467, 0.194, 0.198, 0.41, 0.306, 0.246, 0.362]
[2m[36m(func pid=33107)[0m 
== Status ==
Current time: 2024-01-07 03:46:42 (running for 00:27:17.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.213 |      0.149 |                   42 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.052 |      0.31  |                   41 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.374 |                   18 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.159 |      0.001 |                   16 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.03404850746268657
[2m[36m(func pid=33680)[0m top5: 0.5615671641791045
[2m[36m(func pid=33680)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=33680)[0m f1_macro: 0.02398626294829768
[2m[36m(func pid=33680)[0m f1_weighted: 0.03912050272881319
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.227, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.32975746268656714
[2m[36m(func pid=27838)[0m top5: 0.8936567164179104
[2m[36m(func pid=27838)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=27838)[0m f1_macro: 0.30145178222599167
[2m[36m(func pid=27838)[0m f1_weighted: 0.3411227803418517
[2m[36m(func pid=27838)[0m f1_per_class: [0.402, 0.374, 0.524, 0.438, 0.13, 0.155, 0.342, 0.233, 0.203, 0.214]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.1314 | Steps: 2 | Val loss: 2.2080 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.7697 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8520 | Steps: 2 | Val loss: 987.5713 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=27371)[0m top1: 0.16557835820895522
[2m[36m(func pid=27371)[0m top5: 0.667910447761194
[2m[36m(func pid=27371)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=27371)[0m f1_macro: 0.15310212016589392
[2m[36m(func pid=27371)[0m f1_weighted: 0.18072529161660944
[2m[36m(func pid=27371)[0m f1_per_class: [0.149, 0.201, 0.308, 0.242, 0.022, 0.092, 0.167, 0.141, 0.118, 0.091]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0476 | Steps: 2 | Val loss: 1.7963 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:46:47 (running for 00:27:22.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.131 |      0.153 |                   43 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.061 |      0.301 |                   42 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.372 |                   19 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.753 |      0.024 |                   17 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3908582089552239
[2m[36m(func pid=33107)[0m top5: 0.9239738805970149
[2m[36m(func pid=33107)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=33107)[0m f1_macro: 0.371972699100958
[2m[36m(func pid=33107)[0m f1_weighted: 0.4018989659268691
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.476, 0.558, 0.466, 0.182, 0.204, 0.408, 0.303, 0.245, 0.362]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.1259328358208955
[2m[36m(func pid=33680)[0m top5: 0.42863805970149255
[2m[36m(func pid=33680)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=33680)[0m f1_macro: 0.03720643499313091
[2m[36m(func pid=33680)[0m f1_weighted: 0.061091135906835695
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.338, 0.024, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3283582089552239
[2m[36m(func pid=27838)[0m top5: 0.8917910447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=27838)[0m f1_macro: 0.30400622928255017
[2m[36m(func pid=27838)[0m f1_weighted: 0.3381631166715533
[2m[36m(func pid=27838)[0m f1_per_class: [0.398, 0.367, 0.524, 0.443, 0.145, 0.161, 0.323, 0.262, 0.205, 0.213]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1877 | Steps: 2 | Val loss: 2.2020 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0019 | Steps: 2 | Val loss: 1.7624 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.2110 | Steps: 2 | Val loss: 322.6549 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=27371)[0m top1: 0.16837686567164178
[2m[36m(func pid=27371)[0m top5: 0.6721082089552238
[2m[36m(func pid=27371)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=27371)[0m f1_macro: 0.15600427306716425
[2m[36m(func pid=27371)[0m f1_weighted: 0.18464144719653824
[2m[36m(func pid=27371)[0m f1_per_class: [0.152, 0.188, 0.308, 0.246, 0.022, 0.104, 0.178, 0.148, 0.128, 0.087]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0644 | Steps: 2 | Val loss: 1.8063 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 03:46:53 (running for 00:27:27.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.188 |      0.156 |                   44 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.048 |      0.304 |                   43 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.374 |                   20 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.852 |      0.037 |                   18 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3927238805970149
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=33107)[0m f1_macro: 0.3736908199751632
[2m[36m(func pid=33107)[0m f1_weighted: 0.4032007740726913
[2m[36m(func pid=33107)[0m f1_per_class: [0.522, 0.476, 0.558, 0.469, 0.185, 0.215, 0.404, 0.306, 0.248, 0.355]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.15578358208955223
[2m[36m(func pid=33680)[0m top5: 0.46968283582089554
[2m[36m(func pid=33680)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=33680)[0m f1_macro: 0.0344997842037117
[2m[36m(func pid=33680)[0m f1_weighted: 0.06792173680582844
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.265, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3255597014925373
[2m[36m(func pid=27838)[0m top5: 0.8908582089552238
[2m[36m(func pid=27838)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=27838)[0m f1_macro: 0.3050362119761517
[2m[36m(func pid=27838)[0m f1_weighted: 0.33413600838505514
[2m[36m(func pid=27838)[0m f1_per_class: [0.398, 0.38, 0.545, 0.428, 0.13, 0.172, 0.312, 0.257, 0.207, 0.221]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1370 | Steps: 2 | Val loss: 2.1939 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0020 | Steps: 2 | Val loss: 1.7325 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 10.9701 | Steps: 2 | Val loss: 185.7609 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0391 | Steps: 2 | Val loss: 1.8120 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=27371)[0m top1: 0.16930970149253732
[2m[36m(func pid=27371)[0m top5: 0.6819029850746269
[2m[36m(func pid=27371)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=27371)[0m f1_macro: 0.1566450565351625
[2m[36m(func pid=27371)[0m f1_weighted: 0.18552720731218633
[2m[36m(func pid=27371)[0m f1_per_class: [0.15, 0.192, 0.317, 0.249, 0.022, 0.101, 0.178, 0.144, 0.123, 0.09]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:46:58 (running for 00:27:33.09)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.137 |      0.157 |                   45 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.064 |      0.305 |                   44 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.377 |                   21 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.211 |      0.034 |                   19 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.4001865671641791
[2m[36m(func pid=33107)[0m top5: 0.9291044776119403
[2m[36m(func pid=33107)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=33107)[0m f1_macro: 0.37746270700617385
[2m[36m(func pid=33107)[0m f1_weighted: 0.4110632518156611
[2m[36m(func pid=33107)[0m f1_per_class: [0.53, 0.475, 0.558, 0.489, 0.191, 0.213, 0.413, 0.301, 0.248, 0.355]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.07276119402985075
[2m[36m(func pid=33680)[0m top5: 0.4818097014925373
[2m[36m(func pid=33680)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=33680)[0m f1_macro: 0.03721387893989062
[2m[36m(func pid=33680)[0m f1_weighted: 0.09119601847646934
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.037, 0.018, 0.147, 0.0, 0.0, 0.144, 0.0, 0.026, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.322294776119403
[2m[36m(func pid=27838)[0m top5: 0.8885261194029851
[2m[36m(func pid=27838)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=27838)[0m f1_macro: 0.30104223323898566
[2m[36m(func pid=27838)[0m f1_weighted: 0.330156178119811
[2m[36m(func pid=27838)[0m f1_per_class: [0.393, 0.383, 0.533, 0.425, 0.123, 0.161, 0.303, 0.262, 0.208, 0.217]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.0666 | Steps: 2 | Val loss: 2.1867 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0019 | Steps: 2 | Val loss: 1.7499 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6012 | Steps: 2 | Val loss: 227.3160 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0540 | Steps: 2 | Val loss: 1.8099 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=27371)[0m top1: 0.17723880597014927
[2m[36m(func pid=27371)[0m top5: 0.6865671641791045
[2m[36m(func pid=27371)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=27371)[0m f1_macro: 0.16302781048717768
[2m[36m(func pid=27371)[0m f1_weighted: 0.19406404903839075
[2m[36m(func pid=27371)[0m f1_per_class: [0.154, 0.195, 0.333, 0.271, 0.022, 0.114, 0.178, 0.142, 0.13, 0.091]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:47:03 (running for 00:27:38.24)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.067 |      0.163 |                   46 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.039 |      0.301 |                   45 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.378 |                   22 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 10.97  |      0.037 |                   20 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.39552238805970147
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=33107)[0m f1_macro: 0.37751256783320053
[2m[36m(func pid=33107)[0m f1_weighted: 0.40605420457555536
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.472, 0.6, 0.488, 0.195, 0.203, 0.403, 0.302, 0.245, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.1021455223880597
[2m[36m(func pid=33680)[0m top5: 0.5237873134328358
[2m[36m(func pid=33680)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=33680)[0m f1_macro: 0.03800540130000585
[2m[36m(func pid=33680)[0m f1_weighted: 0.0849382230988203
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.027, 0.0, 0.0, 0.0, 0.276, 0.0, 0.077, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.32136194029850745
[2m[36m(func pid=27838)[0m top5: 0.8847947761194029
[2m[36m(func pid=27838)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=27838)[0m f1_macro: 0.3003722289137188
[2m[36m(func pid=27838)[0m f1_weighted: 0.3307988954043724
[2m[36m(func pid=27838)[0m f1_per_class: [0.386, 0.377, 0.545, 0.43, 0.118, 0.147, 0.31, 0.27, 0.196, 0.224]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1451 | Steps: 2 | Val loss: 2.1773 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0022 | Steps: 2 | Val loss: 1.7435 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.9476 | Steps: 2 | Val loss: 145.1964 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0440 | Steps: 2 | Val loss: 1.8130 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=27371)[0m top1: 0.1814365671641791
[2m[36m(func pid=27371)[0m top5: 0.6940298507462687
[2m[36m(func pid=27371)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=27371)[0m f1_macro: 0.16926970557270893
[2m[36m(func pid=27371)[0m f1_weighted: 0.19709679343513806
[2m[36m(func pid=27371)[0m f1_per_class: [0.163, 0.209, 0.364, 0.27, 0.023, 0.11, 0.18, 0.146, 0.133, 0.096]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:47:08 (running for 00:27:43.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.145 |      0.169 |                   47 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.054 |      0.3   |                   46 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.373 |                   23 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.601 |      0.038 |                   21 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.39598880597014924
[2m[36m(func pid=33107)[0m top5: 0.9286380597014925
[2m[36m(func pid=33107)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=33107)[0m f1_macro: 0.3726957484512182
[2m[36m(func pid=33107)[0m f1_weighted: 0.40544586919354303
[2m[36m(func pid=33107)[0m f1_per_class: [0.521, 0.476, 0.571, 0.487, 0.19, 0.181, 0.41, 0.291, 0.255, 0.345]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.07882462686567164
[2m[36m(func pid=33680)[0m top5: 0.6767723880597015
[2m[36m(func pid=33680)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=33680)[0m f1_macro: 0.032819256453868274
[2m[36m(func pid=33680)[0m f1_weighted: 0.06540357708338854
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.041, 0.0, 0.0, 0.0, 0.21, 0.0, 0.077, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3199626865671642
[2m[36m(func pid=27838)[0m top5: 0.8847947761194029
[2m[36m(func pid=27838)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=27838)[0m f1_macro: 0.3016407718887014
[2m[36m(func pid=27838)[0m f1_weighted: 0.3298118883878377
[2m[36m(func pid=27838)[0m f1_per_class: [0.393, 0.391, 0.533, 0.412, 0.119, 0.164, 0.309, 0.265, 0.199, 0.231]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0645 | Steps: 2 | Val loss: 2.1748 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7362 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.0625 | Steps: 2 | Val loss: 74.1983 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0373 | Steps: 2 | Val loss: 1.8129 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=27371)[0m top1: 0.18516791044776118
[2m[36m(func pid=27371)[0m top5: 0.6963619402985075
[2m[36m(func pid=27371)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=27371)[0m f1_macro: 0.17484591073593211
[2m[36m(func pid=27371)[0m f1_weighted: 0.2005187529522512
[2m[36m(func pid=27371)[0m f1_per_class: [0.168, 0.221, 0.393, 0.27, 0.024, 0.116, 0.18, 0.149, 0.139, 0.089]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:47:13 (running for 00:27:48.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.064 |      0.175 |                   48 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.044 |      0.302 |                   47 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.378 |                   24 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.948 |      0.033 |                   22 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3969216417910448
[2m[36m(func pid=33107)[0m top5: 0.9286380597014925
[2m[36m(func pid=33107)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=33107)[0m f1_macro: 0.37758963348961117
[2m[36m(func pid=33107)[0m f1_weighted: 0.40655313439310475
[2m[36m(func pid=33107)[0m f1_per_class: [0.529, 0.474, 0.615, 0.487, 0.195, 0.181, 0.414, 0.288, 0.257, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.15951492537313433
[2m[36m(func pid=33680)[0m top5: 0.6222014925373134
[2m[36m(func pid=33680)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=33680)[0m f1_macro: 0.05391566046213707
[2m[36m(func pid=33680)[0m f1_weighted: 0.1184034947584046
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.053, 0.0, 0.0, 0.0, 0.385, 0.0, 0.101, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3185634328358209
[2m[36m(func pid=27838)[0m top5: 0.8885261194029851
[2m[36m(func pid=27838)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=27838)[0m f1_macro: 0.30220175597526316
[2m[36m(func pid=27838)[0m f1_weighted: 0.3273428296154247
[2m[36m(func pid=27838)[0m f1_per_class: [0.393, 0.379, 0.545, 0.43, 0.114, 0.16, 0.29, 0.275, 0.195, 0.24]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0692 | Steps: 2 | Val loss: 2.1708 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7332 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7513 | Steps: 2 | Val loss: 31.9337 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0314 | Steps: 2 | Val loss: 1.8198 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=33107)[0m top1: 0.3969216417910448
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=33107)[0m f1_macro: 0.3726849128276531
[2m[36m(func pid=33107)[0m f1_weighted: 0.40807749672514754
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.475, 0.571, 0.489, 0.193, 0.189, 0.416, 0.286, 0.254, 0.336]
[2m[36m(func pid=33107)[0m 
== Status ==
Current time: 2024-01-07 03:47:19 (running for 00:27:53.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.069 |      0.174 |                   49 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.037 |      0.302 |                   48 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.373 |                   25 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.062 |      0.054 |                   23 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27371)[0m top1: 0.18610074626865672
[2m[36m(func pid=27371)[0m top5: 0.7024253731343284
[2m[36m(func pid=27371)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=27371)[0m f1_macro: 0.1742116376176899
[2m[36m(func pid=27371)[0m f1_weighted: 0.202114852771649
[2m[36m(func pid=27371)[0m f1_per_class: [0.169, 0.217, 0.373, 0.268, 0.023, 0.115, 0.189, 0.153, 0.141, 0.094]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.016791044776119403
[2m[36m(func pid=33680)[0m top5: 0.6068097014925373
[2m[36m(func pid=33680)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=33680)[0m f1_macro: 0.018584096476893252
[2m[36m(func pid=33680)[0m f1_weighted: 0.004031585147683481
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.08, 0.0, 0.019, 0.006, 0.0, 0.0, 0.081, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3199626865671642
[2m[36m(func pid=27838)[0m top5: 0.886660447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=27838)[0m f1_macro: 0.3053682214736314
[2m[36m(func pid=27838)[0m f1_weighted: 0.3268929819188883
[2m[36m(func pid=27838)[0m f1_per_class: [0.4, 0.388, 0.545, 0.43, 0.112, 0.16, 0.281, 0.279, 0.201, 0.256]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.9210 | Steps: 2 | Val loss: 2.1632 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7279 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8325 | Steps: 2 | Val loss: 14.4216 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0365 | Steps: 2 | Val loss: 1.8241 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:47:24 (running for 00:27:59.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.069 |      0.174 |                   49 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.031 |      0.305 |                   49 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   26 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.751 |      0.019 |                   24 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3978544776119403
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=33107)[0m f1_macro: 0.3768282285708228
[2m[36m(func pid=33107)[0m f1_weighted: 0.40871850864763276
[2m[36m(func pid=33107)[0m f1_per_class: [0.529, 0.475, 0.6, 0.488, 0.2, 0.181, 0.421, 0.282, 0.263, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m top1: 0.19029850746268656
[2m[36m(func pid=27371)[0m top5: 0.7066231343283582
[2m[36m(func pid=27371)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=27371)[0m f1_macro: 0.1765077244425721
[2m[36m(func pid=27371)[0m f1_weighted: 0.20653355903220127
[2m[36m(func pid=27371)[0m f1_per_class: [0.168, 0.225, 0.386, 0.273, 0.023, 0.106, 0.198, 0.158, 0.131, 0.096]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.03544776119402985
[2m[36m(func pid=33680)[0m top5: 0.7136194029850746
[2m[36m(func pid=33680)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=33680)[0m f1_macro: 0.03341796154483614
[2m[36m(func pid=33680)[0m f1_weighted: 0.02703652499246777
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.15, 0.166, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.322294776119403
[2m[36m(func pid=27838)[0m top5: 0.8847947761194029
[2m[36m(func pid=27838)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=27838)[0m f1_macro: 0.30525454225771986
[2m[36m(func pid=27838)[0m f1_weighted: 0.3290204391290925
[2m[36m(func pid=27838)[0m f1_per_class: [0.391, 0.388, 0.545, 0.439, 0.111, 0.168, 0.277, 0.274, 0.208, 0.25]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7292 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.9765 | Steps: 2 | Val loss: 2.1584 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6663 | Steps: 2 | Val loss: 7.3172 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0408 | Steps: 2 | Val loss: 1.8245 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 03:47:29 (running for 00:28:04.52)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.921 |      0.177 |                   50 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.037 |      0.305 |                   50 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.38  |                   27 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.832 |      0.033 |                   25 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.39738805970149255
[2m[36m(func pid=33107)[0m top5: 0.9272388059701493
[2m[36m(func pid=33107)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=33107)[0m f1_macro: 0.38037829256798095
[2m[36m(func pid=33107)[0m f1_weighted: 0.40817330528017237
[2m[36m(func pid=33107)[0m f1_per_class: [0.538, 0.482, 0.632, 0.481, 0.202, 0.184, 0.419, 0.28, 0.264, 0.323]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.16884328358208955
[2m[36m(func pid=33680)[0m top5: 0.7313432835820896
[2m[36m(func pid=33680)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=33680)[0m f1_macro: 0.05555267254800208
[2m[36m(func pid=33680)[0m f1_weighted: 0.05106211127806737
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.287, 0.268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.19169776119402984
[2m[36m(func pid=27371)[0m top5: 0.7126865671641791
[2m[36m(func pid=27371)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=27371)[0m f1_macro: 0.17865678649542138
[2m[36m(func pid=27371)[0m f1_weighted: 0.20789800519449791
[2m[36m(func pid=27371)[0m f1_per_class: [0.17, 0.232, 0.386, 0.277, 0.033, 0.103, 0.195, 0.157, 0.137, 0.096]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3199626865671642
[2m[36m(func pid=27838)[0m top5: 0.8847947761194029
[2m[36m(func pid=27838)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=27838)[0m f1_macro: 0.3054595551750278
[2m[36m(func pid=27838)[0m f1_weighted: 0.32466939170548514
[2m[36m(func pid=27838)[0m f1_per_class: [0.372, 0.389, 0.545, 0.434, 0.116, 0.17, 0.265, 0.279, 0.216, 0.268]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.0560 | Steps: 2 | Val loss: 2.0855 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7373 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0134 | Steps: 2 | Val loss: 2.1527 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0406 | Steps: 2 | Val loss: 1.8300 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=33107)[0m top1: 0.39365671641791045
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=33107)[0m f1_macro: 0.3770167576055963
[2m[36m(func pid=33107)[0m f1_weighted: 0.40354286454709226
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.479, 0.632, 0.473, 0.204, 0.185, 0.413, 0.282, 0.263, 0.315]
[2m[36m(func pid=33107)[0m 
== Status ==
Current time: 2024-01-07 03:47:34 (running for 00:28:09.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.976 |      0.179 |                   51 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.041 |      0.305 |                   51 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   28 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.666 |      0.056 |                   26 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.3111007462686567
[2m[36m(func pid=33680)[0m top5: 0.6226679104477612
[2m[36m(func pid=33680)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=33680)[0m f1_macro: 0.10247712365613704
[2m[36m(func pid=33680)[0m f1_weighted: 0.18000492036816418
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.184, 0.35, 0.0, 0.0, 0.0, 0.49, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.19263059701492538
[2m[36m(func pid=27371)[0m top5: 0.7154850746268657
[2m[36m(func pid=27371)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=27371)[0m f1_macro: 0.1778343091203986
[2m[36m(func pid=27371)[0m f1_weighted: 0.20973930934155216
[2m[36m(func pid=27371)[0m f1_per_class: [0.167, 0.236, 0.386, 0.279, 0.023, 0.1, 0.199, 0.157, 0.139, 0.093]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.31529850746268656
[2m[36m(func pid=27838)[0m top5: 0.8843283582089553
[2m[36m(func pid=27838)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=27838)[0m f1_macro: 0.3011642608162574
[2m[36m(func pid=27838)[0m f1_weighted: 0.3223019887792787
[2m[36m(func pid=27838)[0m f1_per_class: [0.374, 0.378, 0.545, 0.436, 0.112, 0.166, 0.265, 0.274, 0.212, 0.248]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 45.1152 | Steps: 2 | Val loss: 2.1073 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0013 | Steps: 2 | Val loss: 1.7191 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0002 | Steps: 2 | Val loss: 2.1415 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0474 | Steps: 2 | Val loss: 1.8238 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:47:40 (running for 00:28:14.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1.013 |      0.178 |                   52 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.041 |      0.301 |                   52 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   28 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 45.115 |      0.111 |                   28 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.3003731343283582
[2m[36m(func pid=33680)[0m top5: 0.6156716417910447
[2m[36m(func pid=33680)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=33680)[0m f1_macro: 0.11142488003894066
[2m[36m(func pid=33680)[0m f1_weighted: 0.17124136737772805
[2m[36m(func pid=33680)[0m f1_per_class: [0.136, 0.0, 0.316, 0.068, 0.0, 0.0, 0.491, 0.0, 0.0, 0.103]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m top1: 0.39598880597014924
[2m[36m(func pid=33107)[0m top5: 0.9291044776119403
[2m[36m(func pid=33107)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=33107)[0m f1_macro: 0.3807325681897375
[2m[36m(func pid=33107)[0m f1_weighted: 0.40603759809994766
[2m[36m(func pid=33107)[0m f1_per_class: [0.53, 0.478, 0.649, 0.489, 0.212, 0.174, 0.411, 0.277, 0.266, 0.323]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m top1: 0.2019589552238806
[2m[36m(func pid=27371)[0m top5: 0.7229477611940298
[2m[36m(func pid=27371)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=27371)[0m f1_macro: 0.1873078057738263
[2m[36m(func pid=27371)[0m f1_weighted: 0.21966860372209276
[2m[36m(func pid=27371)[0m f1_per_class: [0.173, 0.242, 0.415, 0.286, 0.036, 0.109, 0.217, 0.159, 0.141, 0.095]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3180970149253731
[2m[36m(func pid=27838)[0m top5: 0.8880597014925373
[2m[36m(func pid=27838)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=27838)[0m f1_macro: 0.30576106739880726
[2m[36m(func pid=27838)[0m f1_weighted: 0.3230141009068468
[2m[36m(func pid=27838)[0m f1_per_class: [0.378, 0.38, 0.545, 0.436, 0.113, 0.176, 0.262, 0.273, 0.212, 0.283]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 63.2565 | Steps: 2 | Val loss: 2.2445 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0013 | Steps: 2 | Val loss: 1.7250 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9333 | Steps: 2 | Val loss: 2.1346 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0360 | Steps: 2 | Val loss: 1.8210 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:47:45 (running for 00:28:20.28)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  1     |      0.187 |                   53 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.047 |      0.306 |                   53 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.381 |                   29 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 63.256 |      0.056 |                   29 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.39738805970149255
[2m[36m(func pid=33107)[0m top5: 0.9286380597014925
[2m[36m(func pid=33107)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=33107)[0m f1_macro: 0.3805503293283413
[2m[36m(func pid=33107)[0m f1_weighted: 0.40873218719782417
[2m[36m(func pid=33107)[0m f1_per_class: [0.554, 0.477, 0.649, 0.499, 0.198, 0.175, 0.411, 0.275, 0.257, 0.31]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.28078358208955223
[2m[36m(func pid=33680)[0m top5: 0.5130597014925373
[2m[36m(func pid=33680)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=33680)[0m f1_macro: 0.05569018961364164
[2m[36m(func pid=33680)[0m f1_weighted: 0.1490185853258802
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.059]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.2042910447761194
[2m[36m(func pid=27371)[0m top5: 0.726679104477612
[2m[36m(func pid=27371)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=27371)[0m f1_macro: 0.1878987529837921
[2m[36m(func pid=27371)[0m f1_weighted: 0.22238981467919772
[2m[36m(func pid=27371)[0m f1_per_class: [0.165, 0.244, 0.415, 0.291, 0.036, 0.109, 0.22, 0.16, 0.14, 0.098]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.31949626865671643
[2m[36m(func pid=27838)[0m top5: 0.8899253731343284
[2m[36m(func pid=27838)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=27838)[0m f1_macro: 0.308626757100596
[2m[36m(func pid=27838)[0m f1_weighted: 0.32320641953039303
[2m[36m(func pid=27838)[0m f1_per_class: [0.389, 0.388, 0.545, 0.428, 0.119, 0.17, 0.265, 0.275, 0.215, 0.291]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0043 | Steps: 2 | Val loss: 1.7188 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 18.7139 | Steps: 2 | Val loss: 2.6117 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9052 | Steps: 2 | Val loss: 2.1325 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0385 | Steps: 2 | Val loss: 1.8220 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:47:50 (running for 00:28:25.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.933 |      0.188 |                   54 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.036 |      0.309 |                   54 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.381 |                   30 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 18.714 |      0.057 |                   30 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3978544776119403
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=33107)[0m f1_macro: 0.3842753210301028
[2m[36m(func pid=33107)[0m f1_weighted: 0.41143355717703106
[2m[36m(func pid=33107)[0m f1_per_class: [0.534, 0.476, 0.686, 0.501, 0.196, 0.195, 0.413, 0.269, 0.262, 0.31]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.24347014925373134
[2m[36m(func pid=33680)[0m top5: 0.5223880597014925
[2m[36m(func pid=33680)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=33680)[0m f1_macro: 0.056737522414823674
[2m[36m(func pid=33680)[0m f1_weighted: 0.1575947413123043
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.527, 0.0, 0.0, 0.04]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3208955223880597
[2m[36m(func pid=27838)[0m top5: 0.8899253731343284
[2m[36m(func pid=27838)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=27838)[0m f1_macro: 0.30914485631280714
[2m[36m(func pid=27838)[0m f1_weighted: 0.325059666747921
[2m[36m(func pid=27838)[0m f1_per_class: [0.393, 0.395, 0.545, 0.426, 0.121, 0.175, 0.267, 0.277, 0.214, 0.278]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.2042910447761194
[2m[36m(func pid=27371)[0m top5: 0.7350746268656716
[2m[36m(func pid=27371)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=27371)[0m f1_macro: 0.1911375790199324
[2m[36m(func pid=27371)[0m f1_weighted: 0.22234383316736997
[2m[36m(func pid=27371)[0m f1_per_class: [0.185, 0.239, 0.431, 0.291, 0.036, 0.109, 0.22, 0.162, 0.143, 0.094]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 19.0771 | Steps: 2 | Val loss: 4.1327 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0018 | Steps: 2 | Val loss: 1.7111 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0571 | Steps: 2 | Val loss: 1.8167 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9517 | Steps: 2 | Val loss: 2.1244 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 03:47:55 (running for 00:28:30.72)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.905 |      0.191 |                   55 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.038 |      0.309 |                   55 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.004 |      0.384 |                   31 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 19.077 |      0.054 |                   31 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.20522388059701493
[2m[36m(func pid=33680)[0m top5: 0.5359141791044776
[2m[36m(func pid=33680)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=33680)[0m f1_macro: 0.054247932496536665
[2m[36m(func pid=33680)[0m f1_weighted: 0.15020592406549596
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.502, 0.0, 0.0, 0.04]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m top1: 0.4001865671641791
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=33107)[0m f1_macro: 0.3806047400978751
[2m[36m(func pid=33107)[0m f1_weighted: 0.4146875090350967
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.473, 0.649, 0.5, 0.19, 0.198, 0.427, 0.27, 0.266, 0.317]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.322294776119403
[2m[36m(func pid=27838)[0m top5: 0.8908582089552238
[2m[36m(func pid=27838)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=27838)[0m f1_macro: 0.3098258205382423
[2m[36m(func pid=27838)[0m f1_weighted: 0.3264711502032741
[2m[36m(func pid=27838)[0m f1_per_class: [0.398, 0.394, 0.545, 0.431, 0.118, 0.156, 0.275, 0.275, 0.215, 0.291]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.20662313432835822
[2m[36m(func pid=27371)[0m top5: 0.7411380597014925
[2m[36m(func pid=27371)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=27371)[0m f1_macro: 0.19483466439816036
[2m[36m(func pid=27371)[0m f1_weighted: 0.22452549587416692
[2m[36m(func pid=27371)[0m f1_per_class: [0.186, 0.246, 0.449, 0.287, 0.036, 0.113, 0.226, 0.156, 0.158, 0.093]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 4.1993 | Steps: 2 | Val loss: 6.7073 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7155 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0273 | Steps: 2 | Val loss: 1.8250 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9157 | Steps: 2 | Val loss: 2.1263 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 03:48:01 (running for 00:28:35.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.952 |      0.195 |                   56 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.057 |      0.31  |                   56 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.381 |                   32 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.199 |      0.056 |                   32 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.23367537313432835
[2m[36m(func pid=33680)[0m top5: 0.5699626865671642
[2m[36m(func pid=33680)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=33680)[0m f1_macro: 0.055986629652710895
[2m[36m(func pid=33680)[0m f1_weighted: 0.15480500555124777
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.518, 0.0, 0.0, 0.042]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m top1: 0.396455223880597
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=33107)[0m f1_macro: 0.37678209379751426
[2m[36m(func pid=33107)[0m f1_weighted: 0.4101561830836374
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.475, 0.632, 0.502, 0.191, 0.193, 0.411, 0.271, 0.268, 0.317]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.31949626865671643
[2m[36m(func pid=27838)[0m top5: 0.8852611940298507
[2m[36m(func pid=27838)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=27838)[0m f1_macro: 0.30538575000459495
[2m[36m(func pid=27838)[0m f1_weighted: 0.32376150455836145
[2m[36m(func pid=27838)[0m f1_per_class: [0.37, 0.382, 0.545, 0.434, 0.121, 0.158, 0.269, 0.284, 0.219, 0.27]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.20569029850746268
[2m[36m(func pid=27371)[0m top5: 0.7318097014925373
[2m[36m(func pid=27371)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=27371)[0m f1_macro: 0.19382393255355373
[2m[36m(func pid=27371)[0m f1_weighted: 0.22252444817276007
[2m[36m(func pid=27371)[0m f1_per_class: [0.19, 0.248, 0.449, 0.288, 0.036, 0.109, 0.219, 0.154, 0.149, 0.097]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 36.7086 | Steps: 2 | Val loss: 6.5384 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7132 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0344 | Steps: 2 | Val loss: 1.8197 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 03:48:06 (running for 00:28:41.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.916 |      0.194 |                   57 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.027 |      0.305 |                   57 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   33 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 36.709 |      0.053 |                   33 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8124 | Steps: 2 | Val loss: 2.1241 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=33680)[0m top1: 0.2574626865671642
[2m[36m(func pid=33680)[0m top5: 0.6478544776119403
[2m[36m(func pid=33680)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=33680)[0m f1_macro: 0.05286609107838158
[2m[36m(func pid=33680)[0m f1_weighted: 0.15148791024562308
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.507, 0.0, 0.0, 0.021]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m top1: 0.4001865671641791
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=33107)[0m f1_macro: 0.3787270746341031
[2m[36m(func pid=33107)[0m f1_weighted: 0.41419536218125097
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.474, 0.615, 0.5, 0.2, 0.202, 0.422, 0.28, 0.262, 0.308]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.3204291044776119
[2m[36m(func pid=27838)[0m top5: 0.8875932835820896
[2m[36m(func pid=27838)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=27838)[0m f1_macro: 0.3068016838977161
[2m[36m(func pid=27838)[0m f1_weighted: 0.32533657200927313
[2m[36m(func pid=27838)[0m f1_per_class: [0.383, 0.381, 0.545, 0.43, 0.122, 0.162, 0.276, 0.288, 0.214, 0.265]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m top1: 0.20475746268656717
[2m[36m(func pid=27371)[0m top5: 0.7350746268656716
[2m[36m(func pid=27371)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=27371)[0m f1_macro: 0.19334898847274345
[2m[36m(func pid=27371)[0m f1_weighted: 0.22089047926588343
[2m[36m(func pid=27371)[0m f1_per_class: [0.185, 0.247, 0.449, 0.285, 0.035, 0.107, 0.216, 0.165, 0.149, 0.096]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 4.8381 | Steps: 2 | Val loss: 3.9636 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0020 | Steps: 2 | Val loss: 1.7215 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0527 | Steps: 2 | Val loss: 1.8132 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=33680)[0m top1: 0.28638059701492535
[2m[36m(func pid=33680)[0m top5: 0.6450559701492538
[2m[36m(func pid=33680)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=33680)[0m f1_macro: 0.05519133787520688
[2m[36m(func pid=33680)[0m f1_weighted: 0.15087183531033774
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.028, 0.0, 0.0, 0.0, 0.0, 0.488, 0.0, 0.0, 0.035]
[2m[36m(func pid=33680)[0m 
== Status ==
Current time: 2024-01-07 03:48:11 (running for 00:28:46.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.812 |      0.193 |                   58 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.034 |      0.307 |                   58 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.002 |      0.375 |                   35 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.838 |      0.055 |                   34 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.38992537313432835
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=33107)[0m f1_macro: 0.3754375179428619
[2m[36m(func pid=33107)[0m f1_weighted: 0.4033875013231094
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.471, 0.632, 0.49, 0.193, 0.201, 0.398, 0.271, 0.265, 0.317]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7933 | Steps: 2 | Val loss: 2.1203 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=27838)[0m top1: 0.31902985074626866
[2m[36m(func pid=27838)[0m top5: 0.8908582089552238
[2m[36m(func pid=27838)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=27838)[0m f1_macro: 0.30502663580334666
[2m[36m(func pid=27838)[0m f1_weighted: 0.32666188110692623
[2m[36m(func pid=27838)[0m f1_per_class: [0.375, 0.379, 0.558, 0.425, 0.122, 0.169, 0.285, 0.291, 0.206, 0.24]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 8.8158 | Steps: 2 | Val loss: 11295.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=27371)[0m top1: 0.2080223880597015
[2m[36m(func pid=27371)[0m top5: 0.7397388059701493
[2m[36m(func pid=27371)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=27371)[0m f1_macro: 0.19822912911249524
[2m[36m(func pid=27371)[0m f1_weighted: 0.2228501705087689
[2m[36m(func pid=27371)[0m f1_per_class: [0.184, 0.251, 0.458, 0.29, 0.046, 0.114, 0.21, 0.166, 0.162, 0.1]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7097 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=33680)[0m top1: 0.01166044776119403
[2m[36m(func pid=33680)[0m top5: 0.5149253731343284
[2m[36m(func pid=33680)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=33680)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=33680)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0308 | Steps: 2 | Val loss: 1.8200 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 03:48:16 (running for 00:28:51.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.793 |      0.198 |                   59 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.053 |      0.305 |                   59 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   36 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  8.816 |      0.002 |                   35 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3987873134328358
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=33107)[0m f1_macro: 0.37690584754204304
[2m[36m(func pid=33107)[0m f1_weighted: 0.4134028356437631
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.475, 0.615, 0.502, 0.176, 0.209, 0.414, 0.273, 0.271, 0.32]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7809 | Steps: 2 | Val loss: 2.1131 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=27838)[0m top1: 0.31949626865671643
[2m[36m(func pid=27838)[0m top5: 0.886660447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=27838)[0m f1_macro: 0.30634446608718735
[2m[36m(func pid=27838)[0m f1_weighted: 0.3253756042856935
[2m[36m(func pid=27838)[0m f1_per_class: [0.377, 0.391, 0.571, 0.422, 0.129, 0.167, 0.279, 0.279, 0.225, 0.224]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 60.2402 | Steps: 2 | Val loss: 4279.5537 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.6966 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=27371)[0m top1: 0.20848880597014927
[2m[36m(func pid=27371)[0m top5: 0.7490671641791045
[2m[36m(func pid=27371)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=27371)[0m f1_macro: 0.1956084809535567
[2m[36m(func pid=27371)[0m f1_weighted: 0.22310272100047226
[2m[36m(func pid=27371)[0m f1_per_class: [0.19, 0.254, 0.449, 0.292, 0.037, 0.11, 0.212, 0.16, 0.148, 0.105]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.01166044776119403
[2m[36m(func pid=33680)[0m top5: 0.5149253731343284
[2m[36m(func pid=33680)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=33680)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=33680)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0488 | Steps: 2 | Val loss: 1.8223 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 03:48:22 (running for 00:28:57.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.781 |      0.196 |                   60 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.031 |      0.306 |                   60 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.379 |                   37 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 60.24  |      0.002 |                   36 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.40205223880597013
[2m[36m(func pid=33107)[0m top5: 0.9286380597014925
[2m[36m(func pid=33107)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=33107)[0m f1_macro: 0.3792682086785083
[2m[36m(func pid=33107)[0m f1_weighted: 0.4156276541498307
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.474, 0.615, 0.504, 0.188, 0.209, 0.419, 0.279, 0.268, 0.323]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8248 | Steps: 2 | Val loss: 2.1100 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 32.6007 | Steps: 2 | Val loss: 24376.4180 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=27838)[0m top1: 0.31949626865671643
[2m[36m(func pid=27838)[0m top5: 0.886660447761194
[2m[36m(func pid=27838)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=27838)[0m f1_macro: 0.3049042736530069
[2m[36m(func pid=27838)[0m f1_weighted: 0.3248769105468635
[2m[36m(func pid=27838)[0m f1_per_class: [0.377, 0.396, 0.545, 0.413, 0.125, 0.161, 0.283, 0.289, 0.224, 0.236]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0042 | Steps: 2 | Val loss: 1.6989 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=27371)[0m top1: 0.21082089552238806
[2m[36m(func pid=27371)[0m top5: 0.7555970149253731
[2m[36m(func pid=27371)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=27371)[0m f1_macro: 0.19963295375442255
[2m[36m(func pid=27371)[0m f1_weighted: 0.22576261573224157
[2m[36m(func pid=27371)[0m f1_per_class: [0.192, 0.256, 0.458, 0.294, 0.037, 0.11, 0.214, 0.171, 0.16, 0.104]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.01166044776119403
[2m[36m(func pid=33680)[0m top5: 0.38619402985074625
[2m[36m(func pid=33680)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=33680)[0m f1_macro: 0.0023169601482854493
[2m[36m(func pid=33680)[0m f1_weighted: 0.0002701679277385086
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0360 | Steps: 2 | Val loss: 1.8223 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 03:48:27 (running for 00:29:02.52)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.825 |      0.2   |                   61 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.049 |      0.305 |                   61 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.004 |      0.38  |                   38 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 32.601 |      0.002 |                   37 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.4048507462686567
[2m[36m(func pid=33107)[0m top5: 0.929570895522388
[2m[36m(func pid=33107)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=33107)[0m f1_macro: 0.37965597270548496
[2m[36m(func pid=33107)[0m f1_weighted: 0.41717304109237363
[2m[36m(func pid=33107)[0m f1_per_class: [0.538, 0.485, 0.571, 0.501, 0.176, 0.223, 0.413, 0.289, 0.263, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7856 | Steps: 2 | Val loss: 2.1107 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 24.3313 | Steps: 2 | Val loss: 7636.5298 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=27838)[0m top1: 0.3204291044776119
[2m[36m(func pid=27838)[0m top5: 0.8857276119402985
[2m[36m(func pid=27838)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=27838)[0m f1_macro: 0.3040945259621228
[2m[36m(func pid=27838)[0m f1_weighted: 0.32367696369667986
[2m[36m(func pid=27838)[0m f1_per_class: [0.374, 0.394, 0.558, 0.428, 0.129, 0.157, 0.269, 0.283, 0.222, 0.227]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7067 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=27371)[0m top1: 0.21222014925373134
[2m[36m(func pid=27371)[0m top5: 0.7527985074626866
[2m[36m(func pid=27371)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=27371)[0m f1_macro: 0.20277171345495656
[2m[36m(func pid=27371)[0m f1_weighted: 0.22725473670576862
[2m[36m(func pid=27371)[0m f1_per_class: [0.194, 0.253, 0.468, 0.298, 0.058, 0.108, 0.217, 0.17, 0.158, 0.103]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.036380597014925374
[2m[36m(func pid=33680)[0m top5: 0.4183768656716418
[2m[36m(func pid=33680)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=33680)[0m f1_macro: 0.008758388746767105
[2m[36m(func pid=33680)[0m f1_weighted: 0.008461610999462084
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.019, 0.0, 0.0, 0.003, 0.0, 0.065, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0220 | Steps: 2 | Val loss: 1.8220 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 03:48:32 (running for 00:29:07.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.786 |      0.203 |                   62 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.036 |      0.304 |                   62 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.376 |                   39 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 24.331 |      0.009 |                   38 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.4006529850746269
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=33107)[0m f1_macro: 0.37622631652394806
[2m[36m(func pid=33107)[0m f1_weighted: 0.41371687904742477
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.477, 0.571, 0.498, 0.177, 0.225, 0.41, 0.287, 0.267, 0.325]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8035 | Steps: 2 | Val loss: 2.1048 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 13.8719 | Steps: 2 | Val loss: 3119.1465 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=27838)[0m top1: 0.31669776119402987
[2m[36m(func pid=27838)[0m top5: 0.8833955223880597
[2m[36m(func pid=27838)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=27838)[0m f1_macro: 0.29949263028938483
[2m[36m(func pid=27838)[0m f1_weighted: 0.32127113150297254
[2m[36m(func pid=27838)[0m f1_per_class: [0.354, 0.381, 0.558, 0.427, 0.129, 0.158, 0.272, 0.275, 0.223, 0.217]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.7046 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=27371)[0m top1: 0.2150186567164179
[2m[36m(func pid=27371)[0m top5: 0.7597947761194029
[2m[36m(func pid=27371)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=27371)[0m f1_macro: 0.20460114206503185
[2m[36m(func pid=27371)[0m f1_weighted: 0.22914656302067932
[2m[36m(func pid=27371)[0m f1_per_class: [0.199, 0.258, 0.468, 0.294, 0.05, 0.111, 0.222, 0.165, 0.168, 0.11]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m top1: 0.07929104477611941
[2m[36m(func pid=33680)[0m top5: 0.5881529850746269
[2m[36m(func pid=33680)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=33680)[0m f1_macro: 0.024077668983541977
[2m[36m(func pid=33680)[0m f1_weighted: 0.047289457024219626
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.081, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0427 | Steps: 2 | Val loss: 1.8117 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=33107)[0m top1: 0.40158582089552236
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=33107)[0m f1_macro: 0.37557824680170393
[2m[36m(func pid=33107)[0m f1_weighted: 0.4138039167824368
[2m[36m(func pid=33107)[0m f1_per_class: [0.529, 0.475, 0.558, 0.493, 0.193, 0.218, 0.42, 0.286, 0.265, 0.32]
[2m[36m(func pid=33107)[0m 
== Status ==
Current time: 2024-01-07 03:48:37 (running for 00:29:12.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.803 |      0.205 |                   63 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.022 |      0.299 |                   63 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.376 |                   40 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 13.872 |      0.024 |                   39 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 8.9652 | Steps: 2 | Val loss: 875.2845 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6720 | Steps: 2 | Val loss: 2.0982 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=27838)[0m top1: 0.32369402985074625
[2m[36m(func pid=27838)[0m top5: 0.8852611940298507
[2m[36m(func pid=27838)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=27838)[0m f1_macro: 0.3029422620087501
[2m[36m(func pid=27838)[0m f1_weighted: 0.3310341358013659
[2m[36m(func pid=27838)[0m f1_per_class: [0.347, 0.388, 0.571, 0.429, 0.131, 0.163, 0.299, 0.271, 0.221, 0.208]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7091 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=33680)[0m top1: 0.17817164179104478
[2m[36m(func pid=33680)[0m top5: 0.7294776119402985
[2m[36m(func pid=33680)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=33680)[0m f1_macro: 0.05236377434650354
[2m[36m(func pid=33680)[0m f1_weighted: 0.14410031742624987
[2m[36m(func pid=33680)[0m f1_per_class: [0.008, 0.0, 0.0, 0.516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.21595149253731344
[2m[36m(func pid=27371)[0m top5: 0.761660447761194
[2m[36m(func pid=27371)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=27371)[0m f1_macro: 0.20161618127906383
[2m[36m(func pid=27371)[0m f1_weighted: 0.22981236974071437
[2m[36m(func pid=27371)[0m f1_per_class: [0.201, 0.263, 0.449, 0.296, 0.038, 0.112, 0.222, 0.161, 0.163, 0.112]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0261 | Steps: 2 | Val loss: 1.8195 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:48:43 (running for 00:29:18.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.672 |      0.202 |                   64 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.043 |      0.303 |                   64 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.374 |                   41 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  8.965 |      0.052 |                   40 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.40298507462686567
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=33107)[0m f1_macro: 0.37398377640077785
[2m[36m(func pid=33107)[0m f1_weighted: 0.41597553910919094
[2m[36m(func pid=33107)[0m f1_per_class: [0.52, 0.475, 0.558, 0.489, 0.188, 0.204, 0.437, 0.279, 0.269, 0.32]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 9.4585 | Steps: 2 | Val loss: 352.2501 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=27838)[0m top1: 0.3204291044776119
[2m[36m(func pid=27838)[0m top5: 0.882929104477612
[2m[36m(func pid=27838)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=27838)[0m f1_macro: 0.3003414261546023
[2m[36m(func pid=27838)[0m f1_weighted: 0.3256953772595718
[2m[36m(func pid=27838)[0m f1_per_class: [0.362, 0.389, 0.558, 0.431, 0.124, 0.155, 0.281, 0.27, 0.222, 0.211]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7451 | Steps: 2 | Val loss: 2.0917 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7042 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=33680)[0m top1: 0.14972014925373134
[2m[36m(func pid=33680)[0m top5: 0.7486007462686567
[2m[36m(func pid=33680)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=33680)[0m f1_macro: 0.05456489458146772
[2m[36m(func pid=33680)[0m f1_weighted: 0.13685287327578854
[2m[36m(func pid=33680)[0m f1_per_class: [0.031, 0.011, 0.0, 0.481, 0.022, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.2178171641791045
[2m[36m(func pid=27371)[0m top5: 0.7658582089552238
[2m[36m(func pid=27371)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=27371)[0m f1_macro: 0.20447789804796024
[2m[36m(func pid=27371)[0m f1_weighted: 0.23146107542569208
[2m[36m(func pid=27371)[0m f1_per_class: [0.206, 0.263, 0.458, 0.299, 0.039, 0.118, 0.221, 0.163, 0.165, 0.112]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0316 | Steps: 2 | Val loss: 1.8272 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:48:48 (running for 00:29:23.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.745 |      0.204 |                   65 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.026 |      0.3   |                   65 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.378 |                   42 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  9.458 |      0.055 |                   41 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.4048507462686567
[2m[36m(func pid=33107)[0m top5: 0.9291044776119403
[2m[36m(func pid=33107)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=33107)[0m f1_macro: 0.3778186269790492
[2m[36m(func pid=33107)[0m f1_weighted: 0.4177261523162346
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.475, 0.585, 0.49, 0.186, 0.214, 0.437, 0.284, 0.264, 0.317]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.2120 | Steps: 2 | Val loss: 152.4667 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=27838)[0m top1: 0.3199626865671642
[2m[36m(func pid=27838)[0m top5: 0.878731343283582
[2m[36m(func pid=27838)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=27838)[0m f1_macro: 0.29989995406432074
[2m[36m(func pid=27838)[0m f1_weighted: 0.32601174855836124
[2m[36m(func pid=27838)[0m f1_per_class: [0.349, 0.387, 0.558, 0.427, 0.125, 0.157, 0.285, 0.276, 0.228, 0.207]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7906 | Steps: 2 | Val loss: 2.0906 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7037 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=33680)[0m top1: 0.07369402985074627
[2m[36m(func pid=33680)[0m top5: 0.6674440298507462
[2m[36m(func pid=33680)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=33680)[0m f1_macro: 0.044007480379269596
[2m[36m(func pid=33680)[0m f1_weighted: 0.08646379683399634
[2m[36m(func pid=33680)[0m f1_per_class: [0.088, 0.072, 0.0, 0.258, 0.022, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.21735074626865672
[2m[36m(func pid=27371)[0m top5: 0.7653917910447762
[2m[36m(func pid=27371)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=27371)[0m f1_macro: 0.20418214195058693
[2m[36m(func pid=27371)[0m f1_weighted: 0.2308960484949435
[2m[36m(func pid=27371)[0m f1_per_class: [0.201, 0.257, 0.458, 0.302, 0.039, 0.114, 0.221, 0.162, 0.168, 0.119]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0246 | Steps: 2 | Val loss: 1.8268 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 03:48:53 (running for 00:29:28.49)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.791 |      0.204 |                   66 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.032 |      0.3   |                   66 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.373 |                   43 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  5.212 |      0.044 |                   42 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.40158582089552236
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=33107)[0m f1_macro: 0.37327402736022386
[2m[36m(func pid=33107)[0m f1_weighted: 0.4129143765683295
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.478, 0.558, 0.495, 0.188, 0.205, 0.418, 0.291, 0.263, 0.325]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 13.0087 | Steps: 2 | Val loss: 22.7659 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=27838)[0m top1: 0.31763059701492535
[2m[36m(func pid=27838)[0m top5: 0.8801305970149254
[2m[36m(func pid=27838)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=27838)[0m f1_macro: 0.2974121700667745
[2m[36m(func pid=27838)[0m f1_weighted: 0.32375025996579276
[2m[36m(func pid=27838)[0m f1_per_class: [0.338, 0.387, 0.545, 0.428, 0.123, 0.157, 0.278, 0.276, 0.226, 0.216]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6754 | Steps: 2 | Val loss: 2.0868 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.6976 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=33680)[0m top1: 0.02332089552238806
[2m[36m(func pid=33680)[0m top5: 0.6469216417910447
[2m[36m(func pid=33680)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=33680)[0m f1_macro: 0.031069121365479874
[2m[36m(func pid=33680)[0m f1_weighted: 0.018541227324325116
[2m[36m(func pid=33680)[0m f1_per_class: [0.143, 0.086, 0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.063]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m top1: 0.22108208955223882
[2m[36m(func pid=27371)[0m top5: 0.7658582089552238
[2m[36m(func pid=27371)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=27371)[0m f1_macro: 0.20856768549405072
[2m[36m(func pid=27371)[0m f1_weighted: 0.23496307674707992
[2m[36m(func pid=27371)[0m f1_per_class: [0.198, 0.254, 0.478, 0.314, 0.041, 0.118, 0.222, 0.159, 0.181, 0.12]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0277 | Steps: 2 | Val loss: 1.8171 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:48:58 (running for 00:29:33.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.675 |      0.209 |                   67 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.025 |      0.297 |                   67 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   44 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 | 13.009 |      0.031 |                   43 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.404384328358209
[2m[36m(func pid=33107)[0m top5: 0.9300373134328358
[2m[36m(func pid=33107)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=33107)[0m f1_macro: 0.3769017040004975
[2m[36m(func pid=33107)[0m f1_weighted: 0.4156512339972071
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.482, 0.558, 0.5, 0.183, 0.205, 0.418, 0.294, 0.261, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.3366 | Steps: 2 | Val loss: 4.1021 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=27838)[0m top1: 0.32276119402985076
[2m[36m(func pid=27838)[0m top5: 0.8852611940298507
[2m[36m(func pid=27838)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=27838)[0m f1_macro: 0.2957309935414941
[2m[36m(func pid=27838)[0m f1_weighted: 0.3294661554944024
[2m[36m(func pid=27838)[0m f1_per_class: [0.35, 0.391, 0.511, 0.436, 0.122, 0.161, 0.289, 0.269, 0.209, 0.221]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7368 | Steps: 2 | Val loss: 2.0882 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.7110 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=33680)[0m top1: 0.01585820895522388
[2m[36m(func pid=33680)[0m top5: 0.707089552238806
[2m[36m(func pid=33680)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=33680)[0m f1_macro: 0.03302599934178882
[2m[36m(func pid=33680)[0m f1_weighted: 0.007849569838964968
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.026, 0.027, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.26]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0230 | Steps: 2 | Val loss: 1.8051 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=27371)[0m top1: 0.22061567164179105
[2m[36m(func pid=27371)[0m top5: 0.761660447761194
[2m[36m(func pid=27371)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=27371)[0m f1_macro: 0.20894600522933288
[2m[36m(func pid=27371)[0m f1_weighted: 0.2334588764761484
[2m[36m(func pid=27371)[0m f1_per_class: [0.196, 0.256, 0.468, 0.313, 0.068, 0.112, 0.219, 0.165, 0.179, 0.114]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:49:04 (running for 00:29:39.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.737 |      0.209 |                   68 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.028 |      0.296 |                   68 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.377 |                   45 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.337 |      0.033 |                   44 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.4025186567164179
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=33107)[0m f1_macro: 0.37721544469261703
[2m[36m(func pid=33107)[0m f1_weighted: 0.4148991424765311
[2m[36m(func pid=33107)[0m f1_per_class: [0.516, 0.474, 0.585, 0.495, 0.179, 0.216, 0.422, 0.289, 0.265, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6160 | Steps: 2 | Val loss: 2.8629 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=27838)[0m top1: 0.3283582089552239
[2m[36m(func pid=27838)[0m top5: 0.8899253731343284
[2m[36m(func pid=27838)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=27838)[0m f1_macro: 0.3002718139476654
[2m[36m(func pid=27838)[0m f1_weighted: 0.3366070057554744
[2m[36m(func pid=27838)[0m f1_per_class: [0.349, 0.385, 0.545, 0.455, 0.123, 0.163, 0.298, 0.256, 0.216, 0.211]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6700 | Steps: 2 | Val loss: 2.0841 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.7118 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=33680)[0m top1: 0.27425373134328357
[2m[36m(func pid=33680)[0m top5: 0.7611940298507462
[2m[36m(func pid=33680)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=33680)[0m f1_macro: 0.10176121121699122
[2m[36m(func pid=33680)[0m f1_weighted: 0.1727220543563383
[2m[36m(func pid=33680)[0m f1_per_class: [0.042, 0.074, 0.074, 0.0, 0.0, 0.0, 0.521, 0.0, 0.0, 0.308]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0211 | Steps: 2 | Val loss: 1.7998 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=27371)[0m top1: 0.22014925373134328
[2m[36m(func pid=27371)[0m top5: 0.7691231343283582
[2m[36m(func pid=27371)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=27371)[0m f1_macro: 0.20924421132156237
[2m[36m(func pid=27371)[0m f1_weighted: 0.23417997824161807
[2m[36m(func pid=27371)[0m f1_per_class: [0.193, 0.258, 0.468, 0.305, 0.069, 0.117, 0.225, 0.167, 0.178, 0.113]
[2m[36m(func pid=27371)[0m 
== Status ==
Current time: 2024-01-07 03:49:09 (running for 00:29:44.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.67  |      0.209 |                   69 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.023 |      0.3   |                   69 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.378 |                   46 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.616 |      0.102 |                   45 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.4048507462686567
[2m[36m(func pid=33107)[0m top5: 0.9291044776119403
[2m[36m(func pid=33107)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=33107)[0m f1_macro: 0.3775910319329289
[2m[36m(func pid=33107)[0m f1_weighted: 0.41592600549953396
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.488, 0.558, 0.49, 0.19, 0.22, 0.42, 0.292, 0.264, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 5.1049 | Steps: 2 | Val loss: 2.5568 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=27838)[0m top1: 0.3278917910447761
[2m[36m(func pid=27838)[0m top5: 0.8936567164179104
[2m[36m(func pid=27838)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=27838)[0m f1_macro: 0.30050641617256335
[2m[36m(func pid=27838)[0m f1_weighted: 0.3362577422748552
[2m[36m(func pid=27838)[0m f1_per_class: [0.352, 0.378, 0.545, 0.455, 0.124, 0.17, 0.299, 0.254, 0.214, 0.213]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6929 | Steps: 2 | Val loss: 2.0811 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7222 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=33680)[0m top1: 0.2957089552238806
[2m[36m(func pid=33680)[0m top5: 0.5769589552238806
[2m[36m(func pid=33680)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=33680)[0m f1_macro: 0.09281617366219812
[2m[36m(func pid=33680)[0m f1_weighted: 0.1758785914072734
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.089, 0.139, 0.0, 0.0, 0.054, 0.51, 0.0, 0.0, 0.136]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0459 | Steps: 2 | Val loss: 1.8025 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:49:14 (running for 00:29:49.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.693 |      0.212 |                   70 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.021 |      0.301 |                   70 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.378 |                   46 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  5.105 |      0.093 |                   46 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27371)[0m top1: 0.2224813432835821
[2m[36m(func pid=27371)[0m top5: 0.7723880597014925
[2m[36m(func pid=27371)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=27371)[0m f1_macro: 0.2115845451422224
[2m[36m(func pid=27371)[0m f1_weighted: 0.23612545494162257
[2m[36m(func pid=27371)[0m f1_per_class: [0.192, 0.262, 0.468, 0.307, 0.065, 0.115, 0.226, 0.173, 0.186, 0.122]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m top1: 0.40111940298507465
[2m[36m(func pid=33107)[0m top5: 0.9291044776119403
[2m[36m(func pid=33107)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=33107)[0m f1_macro: 0.3756387083229839
[2m[36m(func pid=33107)[0m f1_weighted: 0.41235115827369284
[2m[36m(func pid=33107)[0m f1_per_class: [0.504, 0.483, 0.558, 0.489, 0.196, 0.214, 0.415, 0.294, 0.255, 0.348]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5024 | Steps: 2 | Val loss: 2.4992 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=27838)[0m top1: 0.3260261194029851
[2m[36m(func pid=27838)[0m top5: 0.8927238805970149
[2m[36m(func pid=27838)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=27838)[0m f1_macro: 0.3001581634688947
[2m[36m(func pid=27838)[0m f1_weighted: 0.3325397637376084
[2m[36m(func pid=27838)[0m f1_per_class: [0.358, 0.378, 0.545, 0.455, 0.122, 0.154, 0.291, 0.255, 0.216, 0.227]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6568 | Steps: 2 | Val loss: 2.0771 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=33680)[0m top1: 0.048507462686567165
[2m[36m(func pid=33680)[0m top5: 0.5419776119402985
[2m[36m(func pid=33680)[0m f1_micro: 0.048507462686567165
[2m[36m(func pid=33680)[0m f1_macro: 0.04278477998941902
[2m[36m(func pid=33680)[0m f1_weighted: 0.027760072336551794
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.107, 0.16, 0.0, 0.0, 0.008, 0.015, 0.0, 0.067, 0.071]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7200 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0423 | Steps: 2 | Val loss: 1.8119 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 03:49:20 (running for 00:29:54.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.657 |      0.214 |                   71 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.046 |      0.3   |                   71 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.376 |                   47 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.502 |      0.043 |                   47 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27371)[0m top1: 0.2224813432835821
[2m[36m(func pid=27371)[0m top5: 0.7770522388059702
[2m[36m(func pid=27371)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=27371)[0m f1_macro: 0.2140530649060775
[2m[36m(func pid=27371)[0m f1_weighted: 0.23722272036997483
[2m[36m(func pid=27371)[0m f1_per_class: [0.201, 0.265, 0.5, 0.305, 0.064, 0.115, 0.23, 0.173, 0.173, 0.115]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33107)[0m top1: 0.39972014925373134
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=33107)[0m f1_macro: 0.3734845239977723
[2m[36m(func pid=33107)[0m f1_weighted: 0.41173629592464034
[2m[36m(func pid=33107)[0m f1_per_class: [0.492, 0.481, 0.571, 0.481, 0.19, 0.208, 0.426, 0.292, 0.255, 0.339]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.4387 | Steps: 2 | Val loss: 2.4054 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=27838)[0m top1: 0.3260261194029851
[2m[36m(func pid=27838)[0m top5: 0.8885261194029851
[2m[36m(func pid=27838)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=27838)[0m f1_macro: 0.3033757298080011
[2m[36m(func pid=27838)[0m f1_weighted: 0.33077191628903224
[2m[36m(func pid=27838)[0m f1_per_class: [0.35, 0.385, 0.545, 0.446, 0.122, 0.155, 0.287, 0.269, 0.211, 0.264]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.27798507462686567
[2m[36m(func pid=33680)[0m top5: 0.5405783582089553
[2m[36m(func pid=33680)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=33680)[0m f1_macro: 0.1286606249093205
[2m[36m(func pid=33680)[0m f1_weighted: 0.18645957899298182
[2m[36m(func pid=33680)[0m f1_per_class: [0.098, 0.067, 0.224, 0.0, 0.0, 0.008, 0.553, 0.0, 0.089, 0.247]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6232 | Steps: 2 | Val loss: 2.0709 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7195 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0283 | Steps: 2 | Val loss: 1.8017 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:49:25 (running for 00:30:00.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.623 |      0.218 |                   72 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.042 |      0.303 |                   72 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.373 |                   48 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.439 |      0.129 |                   48 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=27371)[0m top1: 0.22667910447761194
[2m[36m(func pid=27371)[0m top5: 0.7784514925373134
[2m[36m(func pid=27371)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=27371)[0m f1_macro: 0.21753039513606304
[2m[36m(func pid=27371)[0m f1_weighted: 0.24073004416562926
[2m[36m(func pid=27371)[0m f1_per_class: [0.203, 0.27, 0.5, 0.31, 0.063, 0.118, 0.231, 0.173, 0.188, 0.118]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6423 | Steps: 2 | Val loss: 2.3491 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=33107)[0m top1: 0.39925373134328357
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=33107)[0m f1_macro: 0.37587656523078344
[2m[36m(func pid=33107)[0m f1_weighted: 0.4118169720383907
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.482, 0.585, 0.482, 0.183, 0.199, 0.427, 0.284, 0.257, 0.351]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27838)[0m top1: 0.33115671641791045
[2m[36m(func pid=27838)[0m top5: 0.8903917910447762
[2m[36m(func pid=27838)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=27838)[0m f1_macro: 0.3043803634961584
[2m[36m(func pid=27838)[0m f1_weighted: 0.3360559797173188
[2m[36m(func pid=27838)[0m f1_per_class: [0.347, 0.387, 0.533, 0.449, 0.122, 0.155, 0.299, 0.277, 0.209, 0.264]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.29011194029850745
[2m[36m(func pid=33680)[0m top5: 0.5387126865671642
[2m[36m(func pid=33680)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=33680)[0m f1_macro: 0.11694804590236915
[2m[36m(func pid=33680)[0m f1_weighted: 0.18185671578434967
[2m[36m(func pid=33680)[0m f1_per_class: [0.038, 0.081, 0.198, 0.0, 0.0, 0.008, 0.536, 0.0, 0.082, 0.226]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7197 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6048 | Steps: 2 | Val loss: 2.0667 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0300 | Steps: 2 | Val loss: 1.7991 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7089 | Steps: 2 | Val loss: 2.3693 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:49:30 (running for 00:30:05.54)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.623 |      0.218 |                   72 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.028 |      0.304 |                   73 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.374 |                   50 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.642 |      0.117 |                   49 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3987873134328358
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=33107)[0m f1_macro: 0.3739987428059025
[2m[36m(func pid=33107)[0m f1_weighted: 0.4119758310703895
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.479, 0.585, 0.492, 0.185, 0.201, 0.42, 0.285, 0.26, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=27371)[0m top1: 0.22621268656716417
[2m[36m(func pid=27371)[0m top5: 0.7845149253731343
[2m[36m(func pid=27371)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=27371)[0m f1_macro: 0.21724754642435107
[2m[36m(func pid=27371)[0m f1_weighted: 0.2397830330368901
[2m[36m(func pid=27371)[0m f1_per_class: [0.203, 0.273, 0.5, 0.31, 0.065, 0.121, 0.227, 0.17, 0.182, 0.123]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.32975746268656714
[2m[36m(func pid=27838)[0m top5: 0.8927238805970149
[2m[36m(func pid=27838)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=27838)[0m f1_macro: 0.3040986258597889
[2m[36m(func pid=27838)[0m f1_weighted: 0.3366967251267633
[2m[36m(func pid=27838)[0m f1_per_class: [0.355, 0.384, 0.522, 0.442, 0.125, 0.159, 0.31, 0.272, 0.199, 0.274]
[2m[36m(func pid=27838)[0m 
[2m[36m(func pid=33680)[0m top1: 0.29011194029850745
[2m[36m(func pid=33680)[0m top5: 0.539179104477612
[2m[36m(func pid=33680)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=33680)[0m f1_macro: 0.12180802749616859
[2m[36m(func pid=33680)[0m f1_weighted: 0.17248812188184187
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.037, 0.289, 0.003, 0.0, 0.0, 0.53, 0.0, 0.06, 0.299]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7245 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7806 | Steps: 2 | Val loss: 2.0699 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=27838)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0312 | Steps: 2 | Val loss: 1.7951 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:49:35 (running for 00:30:10.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.377
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.605 |      0.217 |                   73 |
| train_2d480_00013 | RUNNING    | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |  0.03  |      0.304 |                   74 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.37  |                   51 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.709 |      0.122 |                   50 |
| train_2d480_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.394589552238806
[2m[36m(func pid=33107)[0m top5: 0.9272388059701493
[2m[36m(func pid=33107)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=33107)[0m f1_macro: 0.3699824207484404
[2m[36m(func pid=33107)[0m f1_weighted: 0.407496537080288
[2m[36m(func pid=33107)[0m f1_per_class: [0.481, 0.469, 0.585, 0.49, 0.188, 0.198, 0.415, 0.286, 0.259, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6574 | Steps: 2 | Val loss: 2.6680 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=27371)[0m top1: 0.22574626865671643
[2m[36m(func pid=27371)[0m top5: 0.7793843283582089
[2m[36m(func pid=27371)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=27371)[0m f1_macro: 0.21651211968616493
[2m[36m(func pid=27371)[0m f1_weighted: 0.23919139506906834
[2m[36m(func pid=27371)[0m f1_per_class: [0.204, 0.276, 0.5, 0.305, 0.065, 0.117, 0.229, 0.169, 0.181, 0.119]
[2m[36m(func pid=27371)[0m 
[2m[36m(func pid=27838)[0m top1: 0.32975746268656714
[2m[36m(func pid=27838)[0m top5: 0.8931902985074627
[2m[36m(func pid=27838)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=27838)[0m f1_macro: 0.3064832518193816
[2m[36m(func pid=27838)[0m f1_weighted: 0.33591437304541594
[2m[36m(func pid=27838)[0m f1_per_class: [0.363, 0.39, 0.533, 0.44, 0.129, 0.164, 0.303, 0.27, 0.205, 0.269]
[2m[36m(func pid=33680)[0m top1: 0.29384328358208955
[2m[36m(func pid=33680)[0m top5: 0.5387126865671642
[2m[36m(func pid=33680)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=33680)[0m f1_macro: 0.1177924606588148
[2m[36m(func pid=33680)[0m f1_weighted: 0.16261742872234025
[2m[36m(func pid=33680)[0m f1_per_class: [0.0, 0.0, 0.361, 0.0, 0.0, 0.0, 0.523, 0.0, 0.054, 0.24]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0013 | Steps: 2 | Val loss: 1.7363 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=27371)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6600 | Steps: 2 | Val loss: 2.0643 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=33107)[0m top1: 0.3941231343283582
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=33107)[0m f1_macro: 0.3711620365457867
[2m[36m(func pid=33107)[0m f1_weighted: 0.4047722382157209
[2m[36m(func pid=33107)[0m f1_per_class: [0.481, 0.474, 0.585, 0.482, 0.195, 0.206, 0.406, 0.297, 0.259, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3169 | Steps: 2 | Val loss: 2.7159 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=27371)[0m top1: 0.22574626865671643
[2m[36m(func pid=27371)[0m top5: 0.7845149253731343
[2m[36m(func pid=27371)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=27371)[0m f1_macro: 0.21696516793573922
[2m[36m(func pid=27371)[0m f1_weighted: 0.23921378704355573
[2m[36m(func pid=27371)[0m f1_per_class: [0.204, 0.273, 0.512, 0.314, 0.067, 0.115, 0.224, 0.166, 0.176, 0.119]
[2m[36m(func pid=33680)[0m top1: 0.29617537313432835
[2m[36m(func pid=33680)[0m top5: 0.5359141791044776
[2m[36m(func pid=33680)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=33680)[0m f1_macro: 0.11302726274340699
[2m[36m(func pid=33680)[0m f1_weighted: 0.1657444318506986
[2m[36m(func pid=33680)[0m f1_per_class: [0.053, 0.0, 0.242, 0.007, 0.0, 0.0, 0.525, 0.0, 0.067, 0.238]
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7350 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 03:49:41 (running for 00:30:15.89)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00012 | RUNNING    | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |  0.781 |      0.217 |                   74 |
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   52 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.657 |      0.118 |                   51 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 03:49:46 (running for 00:30:20.99)
Memory usage on this node: 23.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 3 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   52 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.657 |      0.118 |                   51 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45033)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45033)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=45033)[0m Configuration completed!
[2m[36m(func pid=45033)[0m New optimizer parameters:
[2m[36m(func pid=45033)[0m SGD (
[2m[36m(func pid=45033)[0m Parameter Group 0
[2m[36m(func pid=45033)[0m     dampening: 0
[2m[36m(func pid=45033)[0m     differentiable: False
[2m[36m(func pid=45033)[0m     foreach: None
[2m[36m(func pid=45033)[0m     lr: 0.0001
[2m[36m(func pid=45033)[0m     maximize: False
[2m[36m(func pid=45033)[0m     momentum: 0.99
[2m[36m(func pid=45033)[0m     nesterov: False
[2m[36m(func pid=45033)[0m     weight_decay: 1e-05
[2m[36m(func pid=45033)[0m )
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3931902985074627
[2m[36m(func pid=33107)[0m top5: 0.9286380597014925
[2m[36m(func pid=33107)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=33107)[0m f1_macro: 0.3725206608675293
[2m[36m(func pid=33107)[0m f1_weighted: 0.4013966012073265
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.483, 0.571, 0.484, 0.198, 0.207, 0.384, 0.299, 0.263, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4784 | Steps: 2 | Val loss: 2.7033 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.7475 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0992 | Steps: 2 | Val loss: 2.5113 | Batch size: 32 | lr: 0.0001 | Duration: 4.92s
[2m[36m(func pid=33680)[0m top1: 0.29524253731343286
[2m[36m(func pid=33680)[0m top5: 0.5555037313432836
[2m[36m(func pid=33680)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=33680)[0m f1_macro: 0.10941920862483359
[2m[36m(func pid=33680)[0m f1_weighted: 0.16815234744311952
[2m[36m(func pid=33680)[0m f1_per_class: [0.058, 0.0, 0.152, 0.013, 0.0, 0.0, 0.525, 0.0, 0.091, 0.255]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m top1: 0.38992537313432835
[2m[36m(func pid=33107)[0m top5: 0.929570895522388
[2m[36m(func pid=33107)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=33107)[0m f1_macro: 0.3677800955388484
[2m[36m(func pid=33107)[0m f1_weighted: 0.3979883270709568
[2m[36m(func pid=33107)[0m f1_per_class: [0.504, 0.478, 0.545, 0.495, 0.198, 0.196, 0.371, 0.296, 0.261, 0.333]
[2m[36m(func pid=45033)[0m top1: 0.06809701492537314
[2m[36m(func pid=45033)[0m top5: 0.4864738805970149
[2m[36m(func pid=45033)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=45033)[0m f1_macro: 0.04216910840482598
[2m[36m(func pid=45033)[0m f1_weighted: 0.04056539067954805
[2m[36m(func pid=45033)[0m f1_per_class: [0.135, 0.01, 0.0, 0.096, 0.0, 0.019, 0.0, 0.104, 0.022, 0.036]
== Status ==
Current time: 2024-01-07 03:49:51 (running for 00:30:26.43)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.373 |                   53 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.478 |      0.109 |                   53 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 4.8334 | Steps: 2 | Val loss: 2.6673 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45559)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=45559)[0m Configuration completed!
[2m[36m(func pid=45559)[0m New optimizer parameters:
[2m[36m(func pid=45559)[0m SGD (
[2m[36m(func pid=45559)[0m Parameter Group 0
[2m[36m(func pid=45559)[0m     dampening: 0
[2m[36m(func pid=45559)[0m     differentiable: False
[2m[36m(func pid=45559)[0m     foreach: None
[2m[36m(func pid=45559)[0m     lr: 0.001
[2m[36m(func pid=45559)[0m     maximize: False
[2m[36m(func pid=45559)[0m     momentum: 0.99
[2m[36m(func pid=45559)[0m     nesterov: False
[2m[36m(func pid=45559)[0m     weight_decay: 1e-05
[2m[36m(func pid=45559)[0m )
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:49:56 (running for 00:30:31.77)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.368 |                   54 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  4.833 |      0.124 |                   54 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  3.099 |      0.042 |                    1 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.29617537313432835
[2m[36m(func pid=33680)[0m top5: 0.5676305970149254
[2m[36m(func pid=33680)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=33680)[0m f1_macro: 0.12432034341949327
[2m[36m(func pid=33680)[0m f1_weighted: 0.175573389138432
[2m[36m(func pid=33680)[0m f1_per_class: [0.136, 0.0, 0.122, 0.027, 0.024, 0.0, 0.531, 0.0, 0.065, 0.338]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7309 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1443 | Steps: 2 | Val loss: 2.5347 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1173 | Steps: 2 | Val loss: 2.4903 | Batch size: 32 | lr: 0.001 | Duration: 4.64s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.0336 | Steps: 2 | Val loss: 2.5248 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=33107)[0m top1: 0.39598880597014924
[2m[36m(func pid=33107)[0m top5: 0.9291044776119403
[2m[36m(func pid=33107)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=33107)[0m f1_macro: 0.3763480163993323
[2m[36m(func pid=33107)[0m f1_weighted: 0.4054451557327345
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.479, 0.6, 0.5, 0.204, 0.221, 0.381, 0.295, 0.263, 0.325]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.06296641791044776
[2m[36m(func pid=45033)[0m top5: 0.47901119402985076
[2m[36m(func pid=45033)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=45033)[0m f1_macro: 0.034098853528940806
[2m[36m(func pid=45033)[0m f1_weighted: 0.034547328673232215
[2m[36m(func pid=45033)[0m f1_per_class: [0.076, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.102, 0.023, 0.032]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.06949626865671642
[2m[36m(func pid=45559)[0m top5: 0.4832089552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=45559)[0m f1_macro: 0.04415957478329167
[2m[36m(func pid=45559)[0m f1_weighted: 0.04395387185906096
[2m[36m(func pid=45559)[0m f1_per_class: [0.149, 0.01, 0.0, 0.108, 0.0, 0.018, 0.0, 0.104, 0.021, 0.032]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:02 (running for 00:30:37.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.376 |                   55 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.034 |      0.099 |                   55 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  3.144 |      0.034 |                    2 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  3.117 |      0.044 |                    1 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.14272388059701493
[2m[36m(func pid=33680)[0m top5: 0.6884328358208955
[2m[36m(func pid=33680)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=33680)[0m f1_macro: 0.09904502023764959
[2m[36m(func pid=33680)[0m f1_weighted: 0.09920375261167616
[2m[36m(func pid=33680)[0m f1_per_class: [0.101, 0.0, 0.133, 0.023, 0.0, 0.0, 0.246, 0.219, 0.041, 0.229]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7266 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0392 | Steps: 2 | Val loss: 2.5480 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9201 | Steps: 2 | Val loss: 2.4519 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=33107)[0m top1: 0.39552238805970147
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=33107)[0m f1_macro: 0.3706840513579321
[2m[36m(func pid=33107)[0m f1_weighted: 0.4060215458390138
[2m[36m(func pid=33107)[0m f1_per_class: [0.489, 0.47, 0.558, 0.503, 0.195, 0.216, 0.388, 0.298, 0.257, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4503 | Steps: 2 | Val loss: 2.2808 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=45033)[0m top1: 0.0625
[2m[36m(func pid=45033)[0m top5: 0.47201492537313433
[2m[36m(func pid=45033)[0m f1_micro: 0.0625
[2m[36m(func pid=45033)[0m f1_macro: 0.03155137776712379
[2m[36m(func pid=45033)[0m f1_weighted: 0.03486139139978057
[2m[36m(func pid=45033)[0m f1_per_class: [0.055, 0.014, 0.0, 0.081, 0.0, 0.02, 0.0, 0.101, 0.0, 0.044]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.07369402985074627
[2m[36m(func pid=45559)[0m top5: 0.4808768656716418
[2m[36m(func pid=45559)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=45559)[0m f1_macro: 0.05227546726311538
[2m[36m(func pid=45559)[0m f1_weighted: 0.05436062437226148
[2m[36m(func pid=45559)[0m f1_per_class: [0.139, 0.056, 0.0, 0.111, 0.0, 0.024, 0.0, 0.108, 0.041, 0.044]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:07 (running for 00:30:42.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   56 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.45  |      0.103 |                   56 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  3.039 |      0.032 |                    3 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  2.92  |      0.052 |                    2 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.10307835820895522
[2m[36m(func pid=33680)[0m top5: 0.7378731343283582
[2m[36m(func pid=33680)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=33680)[0m f1_macro: 0.10250070871021717
[2m[36m(func pid=33680)[0m f1_weighted: 0.05329150393700228
[2m[36m(func pid=33680)[0m f1_per_class: [0.11, 0.175, 0.202, 0.018, 0.0, 0.0, 0.0, 0.167, 0.043, 0.311]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.7157 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1209 | Steps: 2 | Val loss: 2.5556 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5935 | Steps: 2 | Val loss: 2.4064 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=33107)[0m top1: 0.4001865671641791
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=33107)[0m f1_macro: 0.3744248560116013
[2m[36m(func pid=33107)[0m f1_weighted: 0.41195668333208524
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.476, 0.558, 0.497, 0.206, 0.221, 0.409, 0.295, 0.257, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4124 | Steps: 2 | Val loss: 2.1428 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=45033)[0m top1: 0.0625
[2m[36m(func pid=45033)[0m top5: 0.4748134328358209
[2m[36m(func pid=45033)[0m f1_micro: 0.0625
[2m[36m(func pid=45033)[0m f1_macro: 0.036114754110186165
[2m[36m(func pid=45033)[0m f1_weighted: 0.03988254047098002
[2m[36m(func pid=45033)[0m f1_per_class: [0.049, 0.036, 0.0, 0.084, 0.0, 0.02, 0.0, 0.096, 0.024, 0.051]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.08162313432835822
[2m[36m(func pid=45559)[0m top5: 0.47761194029850745
[2m[36m(func pid=45559)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=45559)[0m f1_macro: 0.0714472365989634
[2m[36m(func pid=45559)[0m f1_weighted: 0.07723659264183848
[2m[36m(func pid=45559)[0m f1_per_class: [0.104, 0.114, 0.056, 0.133, 0.0, 0.029, 0.012, 0.12, 0.112, 0.034]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:12 (running for 00:30:47.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.374 |                   57 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.412 |      0.114 |                   57 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  3.121 |      0.036 |                    4 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  2.593 |      0.071 |                    3 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.12546641791044777
[2m[36m(func pid=33680)[0m top5: 0.7789179104477612
[2m[36m(func pid=33680)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=33680)[0m f1_macro: 0.1141639086964669
[2m[36m(func pid=33680)[0m f1_weighted: 0.06816554337451232
[2m[36m(func pid=33680)[0m f1_per_class: [0.057, 0.27, 0.235, 0.012, 0.0, 0.0, 0.0, 0.178, 0.04, 0.349]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7177 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9145 | Steps: 2 | Val loss: 2.5453 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.2404 | Steps: 2 | Val loss: 2.3602 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=33107)[0m top1: 0.39738805970149255
[2m[36m(func pid=33107)[0m top5: 0.9267723880597015
[2m[36m(func pid=33107)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=33107)[0m f1_macro: 0.3714638186589213
[2m[36m(func pid=33107)[0m f1_weighted: 0.4096228075828149
[2m[36m(func pid=33107)[0m f1_per_class: [0.481, 0.467, 0.558, 0.494, 0.206, 0.209, 0.414, 0.291, 0.261, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3575 | Steps: 2 | Val loss: 2.0847 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45033)[0m top1: 0.06763059701492537
[2m[36m(func pid=45033)[0m top5: 0.47388059701492535
[2m[36m(func pid=45033)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=45033)[0m f1_macro: 0.0399923634920827
[2m[36m(func pid=45033)[0m f1_weighted: 0.048955203685065236
[2m[36m(func pid=45033)[0m f1_per_class: [0.043, 0.068, 0.0, 0.097, 0.0, 0.019, 0.0, 0.099, 0.024, 0.05]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.08395522388059702
[2m[36m(func pid=45559)[0m top5: 0.5130597014925373
[2m[36m(func pid=45559)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=45559)[0m f1_macro: 0.09328821423975472
[2m[36m(func pid=45559)[0m f1_weighted: 0.08635957352745703
[2m[36m(func pid=45559)[0m f1_per_class: [0.101, 0.088, 0.189, 0.129, 0.024, 0.057, 0.044, 0.131, 0.129, 0.041]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:18 (running for 00:30:52.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.371 |                   58 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.358 |      0.11  |                   58 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.914 |      0.04  |                    5 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  2.24  |      0.093 |                    4 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.15298507462686567
[2m[36m(func pid=33680)[0m top5: 0.7728544776119403
[2m[36m(func pid=33680)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=33680)[0m f1_macro: 0.10954739705313214
[2m[36m(func pid=33680)[0m f1_weighted: 0.07932290987046936
[2m[36m(func pid=33680)[0m f1_per_class: [0.065, 0.354, 0.214, 0.0, 0.0, 0.0, 0.0, 0.193, 0.07, 0.2]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.7256 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8628 | Steps: 2 | Val loss: 2.5319 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8859 | Steps: 2 | Val loss: 2.3243 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=33107)[0m top1: 0.3927238805970149
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=33107)[0m f1_macro: 0.3681508148578878
[2m[36m(func pid=33107)[0m f1_weighted: 0.4032453886888164
[2m[36m(func pid=33107)[0m f1_per_class: [0.478, 0.469, 0.558, 0.493, 0.204, 0.206, 0.395, 0.289, 0.254, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3413 | Steps: 2 | Val loss: 2.1297 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=45033)[0m top1: 0.07229477611940298
[2m[36m(func pid=45033)[0m top5: 0.4664179104477612
[2m[36m(func pid=45033)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=45033)[0m f1_macro: 0.045738637813539276
[2m[36m(func pid=45033)[0m f1_weighted: 0.056569072697406526
[2m[36m(func pid=45033)[0m f1_per_class: [0.053, 0.094, 0.0, 0.107, 0.0, 0.013, 0.0, 0.103, 0.045, 0.042]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.09561567164179105
[2m[36m(func pid=45559)[0m top5: 0.5606343283582089
[2m[36m(func pid=45559)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=45559)[0m f1_macro: 0.09750819025669322
[2m[36m(func pid=45559)[0m f1_weighted: 0.10474068080448327
[2m[36m(func pid=45559)[0m f1_per_class: [0.113, 0.09, 0.148, 0.086, 0.025, 0.085, 0.137, 0.106, 0.13, 0.054]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:23 (running for 00:30:58.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.368 |                   59 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.341 |      0.102 |                   59 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.863 |      0.046 |                    6 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  1.886 |      0.098 |                    5 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.12733208955223882
[2m[36m(func pid=33680)[0m top5: 0.7821828358208955
[2m[36m(func pid=33680)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=33680)[0m f1_macro: 0.10204114303601308
[2m[36m(func pid=33680)[0m f1_weighted: 0.06410646937555982
[2m[36m(func pid=33680)[0m f1_per_class: [0.038, 0.263, 0.226, 0.0, 0.0, 0.0, 0.0, 0.192, 0.094, 0.206]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7182 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7846 | Steps: 2 | Val loss: 2.5107 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3888 | Steps: 2 | Val loss: 2.2860 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4317 | Steps: 2 | Val loss: 2.2744 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=33107)[0m top1: 0.39738805970149255
[2m[36m(func pid=33107)[0m top5: 0.929570895522388
[2m[36m(func pid=33107)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=33107)[0m f1_macro: 0.37184850691665905
[2m[36m(func pid=33107)[0m f1_weighted: 0.40794706561741445
[2m[36m(func pid=33107)[0m f1_per_class: [0.492, 0.471, 0.558, 0.5, 0.202, 0.211, 0.399, 0.296, 0.259, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.07835820895522388
[2m[36m(func pid=45033)[0m top5: 0.4701492537313433
[2m[36m(func pid=45033)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=45033)[0m f1_macro: 0.05012134514441924
[2m[36m(func pid=45033)[0m f1_weighted: 0.06557965527414285
[2m[36m(func pid=45033)[0m f1_per_class: [0.057, 0.111, 0.0, 0.128, 0.0, 0.012, 0.0, 0.109, 0.043, 0.041]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.12686567164179105
[2m[36m(func pid=45559)[0m top5: 0.6035447761194029
[2m[36m(func pid=45559)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=45559)[0m f1_macro: 0.106647552375625
[2m[36m(func pid=45559)[0m f1_weighted: 0.139942591787713
[2m[36m(func pid=45559)[0m f1_per_class: [0.111, 0.104, 0.123, 0.085, 0.03, 0.104, 0.255, 0.029, 0.133, 0.093]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:28 (running for 00:31:03.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.372 |                   60 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.432 |      0.096 |                   60 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.785 |      0.05  |                    7 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  1.389 |      0.107 |                    6 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.1021455223880597
[2m[36m(func pid=33680)[0m top5: 0.7686567164179104
[2m[36m(func pid=33680)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=33680)[0m f1_macro: 0.09583728953050544
[2m[36m(func pid=33680)[0m f1_weighted: 0.04414482353516627
[2m[36m(func pid=33680)[0m f1_per_class: [0.056, 0.139, 0.301, 0.0, 0.0, 0.0, 0.0, 0.212, 0.093, 0.156]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7378 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6453 | Steps: 2 | Val loss: 2.4859 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1203 | Steps: 2 | Val loss: 2.2410 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.0750 | Steps: 2 | Val loss: 2.4198 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=33107)[0m top1: 0.3908582089552239
[2m[36m(func pid=33107)[0m top5: 0.9272388059701493
[2m[36m(func pid=33107)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=33107)[0m f1_macro: 0.36873329166103713
[2m[36m(func pid=33107)[0m f1_weighted: 0.40250458455852195
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.46, 0.558, 0.494, 0.195, 0.204, 0.396, 0.29, 0.254, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.08815298507462686
[2m[36m(func pid=45033)[0m top5: 0.47341417910447764
[2m[36m(func pid=45033)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=45033)[0m f1_macro: 0.06362394197815183
[2m[36m(func pid=45033)[0m f1_weighted: 0.08022006210086209
[2m[36m(func pid=45033)[0m f1_per_class: [0.09, 0.135, 0.0, 0.155, 0.0, 0.018, 0.0, 0.11, 0.088, 0.039]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.1646455223880597
[2m[36m(func pid=45559)[0m top5: 0.6487873134328358
[2m[36m(func pid=45559)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=45559)[0m f1_macro: 0.12207300560789869
[2m[36m(func pid=45559)[0m f1_weighted: 0.1778231183865734
[2m[36m(func pid=45559)[0m f1_per_class: [0.12, 0.123, 0.115, 0.086, 0.038, 0.133, 0.364, 0.0, 0.118, 0.123]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:33 (running for 00:31:08.55)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.369 |                   61 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.075 |      0.094 |                   61 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.645 |      0.064 |                    8 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  1.12  |      0.122 |                    7 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.08955223880597014
[2m[36m(func pid=33680)[0m top5: 0.7868470149253731
[2m[36m(func pid=33680)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=33680)[0m f1_macro: 0.09409714606866075
[2m[36m(func pid=33680)[0m f1_weighted: 0.02974998939743523
[2m[36m(func pid=33680)[0m f1_per_class: [0.061, 0.054, 0.381, 0.0, 0.0, 0.0, 0.0, 0.214, 0.086, 0.145]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7353 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5687 | Steps: 2 | Val loss: 2.4617 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8076 | Steps: 2 | Val loss: 2.1690 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2088 | Steps: 2 | Val loss: 2.4312 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=33107)[0m top1: 0.3941231343283582
[2m[36m(func pid=33107)[0m top5: 0.9272388059701493
[2m[36m(func pid=33107)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=33107)[0m f1_macro: 0.37073709295055196
[2m[36m(func pid=33107)[0m f1_weighted: 0.406229183533124
[2m[36m(func pid=33107)[0m f1_per_class: [0.504, 0.465, 0.558, 0.499, 0.191, 0.205, 0.4, 0.292, 0.254, 0.339]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.0914179104477612
[2m[36m(func pid=45033)[0m top5: 0.470615671641791
[2m[36m(func pid=45033)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=45033)[0m f1_macro: 0.06679213126167022
[2m[36m(func pid=45033)[0m f1_weighted: 0.08511538448619854
[2m[36m(func pid=45033)[0m f1_per_class: [0.1, 0.148, 0.0, 0.162, 0.0, 0.022, 0.0, 0.114, 0.089, 0.033]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.21361940298507462
[2m[36m(func pid=45559)[0m top5: 0.7010261194029851
[2m[36m(func pid=45559)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=45559)[0m f1_macro: 0.14793936402862187
[2m[36m(func pid=45559)[0m f1_weighted: 0.22228761758970758
[2m[36m(func pid=45559)[0m f1_per_class: [0.133, 0.171, 0.14, 0.118, 0.069, 0.131, 0.455, 0.0, 0.11, 0.154]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:38 (running for 00:31:13.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.371 |                   62 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.209 |      0.098 |                   62 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.569 |      0.067 |                    9 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.808 |      0.148 |                    8 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.08861940298507463
[2m[36m(func pid=33680)[0m top5: 0.7826492537313433
[2m[36m(func pid=33680)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=33680)[0m f1_macro: 0.09821573232346328
[2m[36m(func pid=33680)[0m f1_weighted: 0.02559119267909068
[2m[36m(func pid=33680)[0m f1_per_class: [0.088, 0.02, 0.4, 0.0, 0.0, 0.0, 0.0, 0.227, 0.085, 0.161]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7472 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4615 | Steps: 2 | Val loss: 2.4428 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6815 | Steps: 2 | Val loss: 2.0853 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.9442 | Steps: 2 | Val loss: 2.3606 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=33107)[0m top1: 0.3903917910447761
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=33107)[0m f1_macro: 0.3683212012729246
[2m[36m(func pid=33107)[0m f1_weighted: 0.4021374751869657
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.463, 0.558, 0.491, 0.18, 0.207, 0.395, 0.291, 0.253, 0.345]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.09375
[2m[36m(func pid=45033)[0m top5: 0.4650186567164179
[2m[36m(func pid=45033)[0m f1_micro: 0.09375
[2m[36m(func pid=45033)[0m f1_macro: 0.08184988162708942
[2m[36m(func pid=45033)[0m f1_weighted: 0.09021059953049512
[2m[36m(func pid=45033)[0m f1_per_class: [0.078, 0.15, 0.143, 0.163, 0.0, 0.037, 0.006, 0.12, 0.094, 0.027]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.2635261194029851
[2m[36m(func pid=45559)[0m top5: 0.7700559701492538
[2m[36m(func pid=45559)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=45559)[0m f1_macro: 0.17654937993941594
[2m[36m(func pid=45559)[0m f1_weighted: 0.2723081947146579
[2m[36m(func pid=45559)[0m f1_per_class: [0.171, 0.228, 0.158, 0.19, 0.095, 0.133, 0.516, 0.0, 0.112, 0.161]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:44 (running for 00:31:18.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.368 |                   63 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.944 |      0.101 |                   63 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.462 |      0.082 |                   10 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.681 |      0.177 |                    9 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.09095149253731344
[2m[36m(func pid=33680)[0m top5: 0.7747201492537313
[2m[36m(func pid=33680)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=33680)[0m f1_macro: 0.10136924093625777
[2m[36m(func pid=33680)[0m f1_weighted: 0.024563279645090245
[2m[36m(func pid=33680)[0m f1_per_class: [0.096, 0.005, 0.381, 0.0, 0.0, 0.0, 0.0, 0.242, 0.093, 0.196]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7488 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3153 | Steps: 2 | Val loss: 2.4243 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4473 | Steps: 2 | Val loss: 2.0078 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6131 | Steps: 2 | Val loss: 2.2950 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=33107)[0m top1: 0.3931902985074627
[2m[36m(func pid=33107)[0m top5: 0.9281716417910447
[2m[36m(func pid=33107)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=33107)[0m f1_macro: 0.3691912279268614
[2m[36m(func pid=33107)[0m f1_weighted: 0.40498942752437556
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.465, 0.558, 0.5, 0.175, 0.207, 0.394, 0.293, 0.255, 0.345]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.09421641791044776
[2m[36m(func pid=45033)[0m top5: 0.4673507462686567
[2m[36m(func pid=45033)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=45033)[0m f1_macro: 0.08512679531114622
[2m[36m(func pid=45033)[0m f1_weighted: 0.09137602144195173
[2m[36m(func pid=45033)[0m f1_per_class: [0.082, 0.15, 0.161, 0.162, 0.0, 0.049, 0.006, 0.124, 0.083, 0.034]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.29197761194029853
[2m[36m(func pid=45559)[0m top5: 0.8227611940298507
[2m[36m(func pid=45559)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=45559)[0m f1_macro: 0.21202204740705963
[2m[36m(func pid=45559)[0m f1_weighted: 0.30659933591497335
[2m[36m(func pid=45559)[0m f1_per_class: [0.233, 0.271, 0.194, 0.278, 0.105, 0.137, 0.513, 0.0, 0.124, 0.267]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:49 (running for 00:31:24.21)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.369 |                   64 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.613 |      0.106 |                   64 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.315 |      0.085 |                   11 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.447 |      0.212 |                   10 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.08162313432835822
[2m[36m(func pid=33680)[0m top5: 0.7126865671641791
[2m[36m(func pid=33680)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=33680)[0m f1_macro: 0.10646053284940822
[2m[36m(func pid=33680)[0m f1_weighted: 0.027137642230423382
[2m[36m(func pid=33680)[0m f1_per_class: [0.09, 0.016, 0.386, 0.0, 0.0, 0.0, 0.0, 0.262, 0.068, 0.243]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0036 | Steps: 2 | Val loss: 1.7383 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2577 | Steps: 2 | Val loss: 2.4085 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3307 | Steps: 2 | Val loss: 1.9231 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3802 | Steps: 2 | Val loss: 2.2304 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=33107)[0m top1: 0.39552238805970147
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=33107)[0m f1_macro: 0.3744957006208879
[2m[36m(func pid=33107)[0m f1_weighted: 0.4097570478340746
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.468, 0.585, 0.505, 0.192, 0.2, 0.407, 0.283, 0.254, 0.339]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.09934701492537314
[2m[36m(func pid=45033)[0m top5: 0.47294776119402987
[2m[36m(func pid=45033)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=45033)[0m f1_macro: 0.0894263670665171
[2m[36m(func pid=45033)[0m f1_weighted: 0.09916499172787933
[2m[36m(func pid=45033)[0m f1_per_class: [0.091, 0.16, 0.133, 0.15, 0.0, 0.062, 0.03, 0.136, 0.086, 0.047]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.31949626865671643
[2m[36m(func pid=45559)[0m top5: 0.855410447761194
[2m[36m(func pid=45559)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=45559)[0m f1_macro: 0.23653794711220097
[2m[36m(func pid=45559)[0m f1_weighted: 0.33298317698637414
[2m[36m(func pid=45559)[0m f1_per_class: [0.265, 0.302, 0.27, 0.351, 0.106, 0.125, 0.509, 0.015, 0.15, 0.271]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:54 (running for 00:31:29.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.004 |      0.374 |                   65 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.38  |      0.12  |                   65 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.258 |      0.089 |                   12 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.331 |      0.237 |                   11 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.08488805970149253
[2m[36m(func pid=33680)[0m top5: 0.6660447761194029
[2m[36m(func pid=33680)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=33680)[0m f1_macro: 0.11991801760866924
[2m[36m(func pid=33680)[0m f1_weighted: 0.036818760189851356
[2m[36m(func pid=33680)[0m f1_per_class: [0.093, 0.066, 0.31, 0.0, 0.006, 0.0, 0.0, 0.252, 0.073, 0.4]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7546 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0807 | Steps: 2 | Val loss: 2.3901 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2239 | Steps: 2 | Val loss: 1.8693 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.4029 | Steps: 2 | Val loss: 2.2290 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=33107)[0m top1: 0.38899253731343286
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=33107)[0m f1_macro: 0.37041583575168774
[2m[36m(func pid=33107)[0m f1_weighted: 0.40016364209495386
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.47, 0.585, 0.489, 0.164, 0.209, 0.382, 0.301, 0.264, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.09794776119402986
[2m[36m(func pid=45033)[0m top5: 0.4878731343283582
[2m[36m(func pid=45033)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=45033)[0m f1_macro: 0.09080289394340539
[2m[36m(func pid=45033)[0m f1_weighted: 0.10281381908428447
[2m[36m(func pid=45033)[0m f1_per_class: [0.084, 0.148, 0.14, 0.142, 0.0, 0.078, 0.05, 0.138, 0.088, 0.04]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.32369402985074625
[2m[36m(func pid=45559)[0m top5: 0.8796641791044776
[2m[36m(func pid=45559)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=45559)[0m f1_macro: 0.25887438312692895
[2m[36m(func pid=45559)[0m f1_weighted: 0.34298782471860045
[2m[36m(func pid=45559)[0m f1_per_class: [0.308, 0.326, 0.308, 0.377, 0.107, 0.162, 0.463, 0.138, 0.153, 0.246]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:50:59 (running for 00:31:34.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.37  |                   66 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.403 |      0.119 |                   66 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.081 |      0.091 |                   13 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.224 |      0.259 |                   12 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33680)[0m top1: 0.07789179104477612
[2m[36m(func pid=33680)[0m top5: 0.6721082089552238
[2m[36m(func pid=33680)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=33680)[0m f1_macro: 0.11914633392587452
[2m[36m(func pid=33680)[0m f1_weighted: 0.02618956659407625
[2m[36m(func pid=33680)[0m f1_per_class: [0.101, 0.01, 0.379, 0.0, 0.005, 0.0, 0.0, 0.225, 0.071, 0.4]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7448 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.0320 | Steps: 2 | Val loss: 2.3704 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1341 | Steps: 2 | Val loss: 1.8096 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.7480 | Steps: 2 | Val loss: 2.2152 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=33107)[0m top1: 0.3871268656716418
[2m[36m(func pid=33107)[0m top5: 0.925839552238806
[2m[36m(func pid=33107)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=33107)[0m f1_macro: 0.3709864675650759
[2m[36m(func pid=33107)[0m f1_weighted: 0.39783370128616596
[2m[36m(func pid=33107)[0m f1_per_class: [0.516, 0.466, 0.6, 0.486, 0.164, 0.213, 0.377, 0.297, 0.271, 0.32]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45559)[0m top1: 0.34095149253731344
[2m[36m(func pid=45559)[0m top5: 0.8950559701492538
[2m[36m(func pid=45559)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=45559)[0m f1_macro: 0.2921986833908278
[2m[36m(func pid=45559)[0m f1_weighted: 0.3564766487083091
[2m[36m(func pid=45559)[0m f1_per_class: [0.405, 0.344, 0.375, 0.433, 0.114, 0.157, 0.423, 0.216, 0.147, 0.308]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.09888059701492537
[2m[36m(func pid=45033)[0m top5: 0.5018656716417911
[2m[36m(func pid=45033)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=45033)[0m f1_macro: 0.08929352925276894
[2m[36m(func pid=45033)[0m f1_weighted: 0.10725450441966249
[2m[36m(func pid=45033)[0m f1_per_class: [0.089, 0.148, 0.116, 0.134, 0.0, 0.08, 0.077, 0.11, 0.094, 0.047]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=33680)[0m top1: 0.07602611940298508
[2m[36m(func pid=33680)[0m top5: 0.6506529850746269
[2m[36m(func pid=33680)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=33680)[0m f1_macro: 0.11296734277429735
[2m[36m(func pid=33680)[0m f1_weighted: 0.02513115043481119
[2m[36m(func pid=33680)[0m f1_per_class: [0.092, 0.02, 0.429, 0.0, 0.0, 0.0, 0.0, 0.183, 0.09, 0.316]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7479 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1082 | Steps: 2 | Val loss: 1.7722 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8440 | Steps: 2 | Val loss: 2.3497 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1836 | Steps: 2 | Val loss: 2.1680 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 03:51:08 (running for 00:31:43.72)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.37  |                   68 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.748 |      0.113 |                   67 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  2.032 |      0.089 |                   14 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.134 |      0.292 |                   13 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.38899253731343286
[2m[36m(func pid=33107)[0m top5: 0.9244402985074627
[2m[36m(func pid=33107)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=33107)[0m f1_macro: 0.36987103222933354
[2m[36m(func pid=33107)[0m f1_weighted: 0.40096232401867327
[2m[36m(func pid=33107)[0m f1_per_class: [0.492, 0.471, 0.585, 0.49, 0.162, 0.231, 0.377, 0.295, 0.268, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45559)[0m top1: 0.34654850746268656
[2m[36m(func pid=45559)[0m top5: 0.902518656716418
[2m[36m(func pid=45559)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=45559)[0m f1_macro: 0.30892628621303586
[2m[36m(func pid=45559)[0m f1_weighted: 0.35334937480115536
[2m[36m(func pid=45559)[0m f1_per_class: [0.455, 0.362, 0.429, 0.465, 0.122, 0.163, 0.351, 0.282, 0.169, 0.29]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.10307835820895522
[2m[36m(func pid=45033)[0m top5: 0.5284514925373134
[2m[36m(func pid=45033)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=45033)[0m f1_macro: 0.0889429988144759
[2m[36m(func pid=45033)[0m f1_weighted: 0.1167182826029974
[2m[36m(func pid=45033)[0m f1_per_class: [0.086, 0.143, 0.107, 0.128, 0.0, 0.09, 0.122, 0.067, 0.091, 0.055]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=33680)[0m top1: 0.08675373134328358
[2m[36m(func pid=33680)[0m top5: 0.7709888059701493
[2m[36m(func pid=33680)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=33680)[0m f1_macro: 0.10908397844193
[2m[36m(func pid=33680)[0m f1_weighted: 0.04887765538032227
[2m[36m(func pid=33680)[0m f1_per_class: [0.092, 0.047, 0.478, 0.084, 0.0, 0.0, 0.0, 0.161, 0.025, 0.204]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0683 | Steps: 2 | Val loss: 1.7599 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.7530 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7010 | Steps: 2 | Val loss: 2.3303 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2834 | Steps: 2 | Val loss: 2.1074 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:51:14 (running for 00:31:49.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.37  |                   68 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.184 |      0.109 |                   68 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.844 |      0.089 |                   15 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.068 |      0.318 |                   15 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3871268656716418
[2m[36m(func pid=33107)[0m top5: 0.9244402985074627
[2m[36m(func pid=33107)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=33107)[0m f1_macro: 0.3689802676337024
[2m[36m(func pid=33107)[0m f1_weighted: 0.4001372548218711
[2m[36m(func pid=33107)[0m f1_per_class: [0.492, 0.471, 0.585, 0.489, 0.173, 0.212, 0.382, 0.29, 0.264, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45559)[0m top1: 0.3423507462686567
[2m[36m(func pid=45559)[0m top5: 0.9067164179104478
[2m[36m(func pid=45559)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=45559)[0m f1_macro: 0.3176944131660164
[2m[36m(func pid=45559)[0m f1_weighted: 0.33618468763428516
[2m[36m(func pid=45559)[0m f1_per_class: [0.478, 0.381, 0.453, 0.476, 0.135, 0.143, 0.265, 0.334, 0.174, 0.338]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.11287313432835822
[2m[36m(func pid=45033)[0m top5: 0.5475746268656716
[2m[36m(func pid=45033)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=45033)[0m f1_macro: 0.09434546580299358
[2m[36m(func pid=45033)[0m f1_weighted: 0.12882267512091558
[2m[36m(func pid=45033)[0m f1_per_class: [0.092, 0.151, 0.11, 0.119, 0.008, 0.096, 0.165, 0.051, 0.097, 0.054]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=33680)[0m top1: 0.09841417910447761
[2m[36m(func pid=33680)[0m top5: 0.8069029850746269
[2m[36m(func pid=33680)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=33680)[0m f1_macro: 0.11217074658971242
[2m[36m(func pid=33680)[0m f1_weighted: 0.06638176830941096
[2m[36m(func pid=33680)[0m f1_per_class: [0.083, 0.085, 0.44, 0.084, 0.0, 0.0, 0.04, 0.164, 0.0, 0.226]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0485 | Steps: 2 | Val loss: 1.7580 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0014 | Steps: 2 | Val loss: 1.7448 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.6272 | Steps: 2 | Val loss: 2.3185 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.1328 | Steps: 2 | Val loss: 2.0263 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 03:51:19 (running for 00:31:54.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.369 |                   69 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.283 |      0.112 |                   69 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.701 |      0.094 |                   16 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.049 |      0.33  |                   16 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.34095149253731344
[2m[36m(func pid=45559)[0m top5: 0.9071828358208955
[2m[36m(func pid=45559)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=45559)[0m f1_macro: 0.3296985347208368
[2m[36m(func pid=45559)[0m f1_weighted: 0.3209472432579115
[2m[36m(func pid=45559)[0m f1_per_class: [0.519, 0.407, 0.49, 0.471, 0.149, 0.138, 0.191, 0.374, 0.184, 0.375]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.38899253731343286
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=33107)[0m f1_macro: 0.3714940847390729
[2m[36m(func pid=33107)[0m f1_weighted: 0.4028698446183212
[2m[36m(func pid=33107)[0m f1_per_class: [0.488, 0.467, 0.6, 0.499, 0.175, 0.235, 0.377, 0.288, 0.258, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.15438432835820895
[2m[36m(func pid=33680)[0m top5: 0.8232276119402985
[2m[36m(func pid=33680)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=33680)[0m f1_macro: 0.13212345191069147
[2m[36m(func pid=33680)[0m f1_weighted: 0.13665428374686167
[2m[36m(func pid=33680)[0m f1_per_class: [0.091, 0.127, 0.4, 0.113, 0.0, 0.0, 0.218, 0.209, 0.0, 0.163]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45033)[0m top1: 0.1142723880597015
[2m[36m(func pid=45033)[0m top5: 0.5666977611940298
[2m[36m(func pid=45033)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=45033)[0m f1_macro: 0.09078323641705235
[2m[36m(func pid=45033)[0m f1_weighted: 0.13140888797287087
[2m[36m(func pid=45033)[0m f1_per_class: [0.096, 0.143, 0.095, 0.113, 0.007, 0.104, 0.189, 0.014, 0.092, 0.055]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0502 | Steps: 2 | Val loss: 1.7602 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7436 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3404 | Steps: 2 | Val loss: 1.9788 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4901 | Steps: 2 | Val loss: 2.3045 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:51:24 (running for 00:31:59.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   70 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  3.133 |      0.132 |                   70 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.627 |      0.091 |                   17 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.05  |      0.333 |                   17 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.33861940298507465
[2m[36m(func pid=45559)[0m top5: 0.9123134328358209
[2m[36m(func pid=45559)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=45559)[0m f1_macro: 0.33348866720648296
[2m[36m(func pid=45559)[0m f1_weighted: 0.30484205139986925
[2m[36m(func pid=45559)[0m f1_per_class: [0.544, 0.424, 0.558, 0.483, 0.187, 0.12, 0.123, 0.362, 0.175, 0.361]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3885261194029851
[2m[36m(func pid=33107)[0m top5: 0.9267723880597015
[2m[36m(func pid=33107)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=33107)[0m f1_macro: 0.37380132690016976
[2m[36m(func pid=33107)[0m f1_weighted: 0.40147728070309996
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.469, 0.6, 0.5, 0.186, 0.229, 0.371, 0.291, 0.263, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.24580223880597016
[2m[36m(func pid=33680)[0m top5: 0.8376865671641791
[2m[36m(func pid=33680)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=33680)[0m f1_macro: 0.16987038521694037
[2m[36m(func pid=33680)[0m f1_weighted: 0.22861477284933326
[2m[36m(func pid=33680)[0m f1_per_class: [0.081, 0.065, 0.259, 0.249, 0.0, 0.0, 0.404, 0.362, 0.0, 0.278]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45033)[0m top1: 0.12360074626865672
[2m[36m(func pid=45033)[0m top5: 0.5802238805970149
[2m[36m(func pid=45033)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=45033)[0m f1_macro: 0.09467180087707547
[2m[36m(func pid=45033)[0m f1_weighted: 0.14082921697875103
[2m[36m(func pid=45033)[0m f1_per_class: [0.103, 0.139, 0.09, 0.095, 0.015, 0.11, 0.239, 0.0, 0.093, 0.062]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0490 | Steps: 2 | Val loss: 1.7907 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7574 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2729 | Steps: 2 | Val loss: 2.0467 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.3674 | Steps: 2 | Val loss: 2.2872 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 03:51:29 (running for 00:32:04.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.374 |                   71 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.34  |      0.17  |                   71 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.49  |      0.095 |                   18 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.049 |      0.339 |                   18 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.33908582089552236
[2m[36m(func pid=45559)[0m top5: 0.9137126865671642
[2m[36m(func pid=45559)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=45559)[0m f1_macro: 0.3386595405471709
[2m[36m(func pid=45559)[0m f1_weighted: 0.30005719948773296
[2m[36m(func pid=45559)[0m f1_per_class: [0.549, 0.427, 0.558, 0.487, 0.192, 0.113, 0.101, 0.36, 0.18, 0.419]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3824626865671642
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=33107)[0m f1_macro: 0.3706770249501688
[2m[36m(func pid=33107)[0m f1_weighted: 0.39443157916102206
[2m[36m(func pid=33107)[0m f1_per_class: [0.512, 0.472, 0.6, 0.485, 0.169, 0.236, 0.358, 0.285, 0.259, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.2681902985074627
[2m[36m(func pid=33680)[0m top5: 0.8428171641791045
[2m[36m(func pid=33680)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=33680)[0m f1_macro: 0.14253042475090422
[2m[36m(func pid=33680)[0m f1_weighted: 0.2481003255081005
[2m[36m(func pid=33680)[0m f1_per_class: [0.088, 0.029, 0.238, 0.315, 0.0, 0.024, 0.491, 0.0, 0.0, 0.241]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45033)[0m top1: 0.1357276119402985
[2m[36m(func pid=45033)[0m top5: 0.6012126865671642
[2m[36m(func pid=45033)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=45033)[0m f1_macro: 0.10086841090181622
[2m[36m(func pid=45033)[0m f1_weighted: 0.15512899288302623
[2m[36m(func pid=45033)[0m f1_per_class: [0.113, 0.142, 0.085, 0.1, 0.022, 0.105, 0.282, 0.0, 0.088, 0.07]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0349 | Steps: 2 | Val loss: 1.8136 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7592 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1847 | Steps: 2 | Val loss: 2.0396 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.2643 | Steps: 2 | Val loss: 2.2701 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 03:51:34 (running for 00:32:09.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   72 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.273 |      0.143 |                   72 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.367 |      0.101 |                   19 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.035 |      0.338 |                   19 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3400186567164179
[2m[36m(func pid=45559)[0m top5: 0.9127798507462687
[2m[36m(func pid=45559)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=45559)[0m f1_macro: 0.33847371816140515
[2m[36m(func pid=45559)[0m f1_weighted: 0.29610110180270555
[2m[36m(func pid=45559)[0m f1_per_class: [0.549, 0.437, 0.571, 0.493, 0.194, 0.109, 0.08, 0.35, 0.183, 0.419]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3805970149253731
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=33107)[0m f1_macro: 0.3715114209594063
[2m[36m(func pid=33107)[0m f1_weighted: 0.38999509444544256
[2m[36m(func pid=33107)[0m f1_per_class: [0.53, 0.47, 0.6, 0.48, 0.171, 0.236, 0.346, 0.293, 0.263, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.25466417910447764
[2m[36m(func pid=33680)[0m top5: 0.8456156716417911
[2m[36m(func pid=33680)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=33680)[0m f1_macro: 0.17273313489099815
[2m[36m(func pid=33680)[0m f1_weighted: 0.24902940971875387
[2m[36m(func pid=33680)[0m f1_per_class: [0.096, 0.005, 0.407, 0.311, 0.0, 0.283, 0.409, 0.0, 0.0, 0.215]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45033)[0m top1: 0.14972014925373134
[2m[36m(func pid=45033)[0m top5: 0.6189365671641791
[2m[36m(func pid=45033)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=45033)[0m f1_macro: 0.10616670243082862
[2m[36m(func pid=45033)[0m f1_weighted: 0.16839229244645262
[2m[36m(func pid=45033)[0m f1_per_class: [0.117, 0.126, 0.09, 0.092, 0.029, 0.115, 0.339, 0.0, 0.092, 0.062]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0235 | Steps: 2 | Val loss: 1.8233 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7687 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3936 | Steps: 2 | Val loss: 2.0723 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2311 | Steps: 2 | Val loss: 2.2532 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=45559)[0m top1: 0.341884328358209
[2m[36m(func pid=45559)[0m top5: 0.9174440298507462
[2m[36m(func pid=45559)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=45559)[0m f1_macro: 0.34308390623045676
[2m[36m(func pid=45559)[0m f1_weighted: 0.2978578332927173
[2m[36m(func pid=45559)[0m f1_per_class: [0.557, 0.434, 0.585, 0.497, 0.198, 0.12, 0.077, 0.351, 0.185, 0.426]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:51:40 (running for 00:32:15.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.36824999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   74 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.185 |      0.173 |                   73 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.264 |      0.106 |                   20 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.024 |      0.343 |                   20 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.38199626865671643
[2m[36m(func pid=33107)[0m top5: 0.9249067164179104
[2m[36m(func pid=33107)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=33107)[0m f1_macro: 0.3709462136396564
[2m[36m(func pid=33107)[0m f1_weighted: 0.39106898997452405
[2m[36m(func pid=33107)[0m f1_per_class: [0.521, 0.472, 0.585, 0.489, 0.176, 0.234, 0.34, 0.297, 0.261, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.20382462686567165
[2m[36m(func pid=33680)[0m top5: 0.8376865671641791
[2m[36m(func pid=33680)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=33680)[0m f1_macro: 0.1485552677338583
[2m[36m(func pid=33680)[0m f1_weighted: 0.17816408174389275
[2m[36m(func pid=33680)[0m f1_per_class: [0.09, 0.0, 0.4, 0.24, 0.0, 0.309, 0.232, 0.0, 0.0, 0.215]
[2m[36m(func pid=33680)[0m 
[2m[36m(func pid=45033)[0m top1: 0.16651119402985073
[2m[36m(func pid=45033)[0m top5: 0.6301305970149254
[2m[36m(func pid=45033)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=45033)[0m f1_macro: 0.1190722829501953
[2m[36m(func pid=45033)[0m f1_weighted: 0.18259418404755418
[2m[36m(func pid=45033)[0m f1_per_class: [0.119, 0.136, 0.091, 0.091, 0.03, 0.116, 0.379, 0.0, 0.095, 0.135]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0204 | Steps: 2 | Val loss: 1.8283 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7569 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=33680)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0351 | Steps: 2 | Val loss: 2.0383 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=45559)[0m top1: 0.3516791044776119
[2m[36m(func pid=45559)[0m top5: 0.9239738805970149
[2m[36m(func pid=45559)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=45559)[0m f1_macro: 0.35135230482265095
[2m[36m(func pid=45559)[0m f1_weighted: 0.30852239239018897
[2m[36m(func pid=45559)[0m f1_per_class: [0.562, 0.445, 0.6, 0.506, 0.206, 0.134, 0.09, 0.361, 0.182, 0.426]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.0849 | Steps: 2 | Val loss: 2.2292 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:51:45 (running for 00:32:20.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.371 |                   75 |
| train_2d480_00015 | RUNNING    | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |  2.394 |      0.149 |                   74 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.231 |      0.119 |                   21 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.02  |      0.351 |                   21 |
| train_2d480_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.38152985074626866
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=33107)[0m f1_macro: 0.370552709590554
[2m[36m(func pid=33107)[0m f1_weighted: 0.38988215306484597
[2m[36m(func pid=33107)[0m f1_per_class: [0.53, 0.473, 0.585, 0.489, 0.179, 0.218, 0.341, 0.297, 0.259, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=33680)[0m top1: 0.2439365671641791
[2m[36m(func pid=33680)[0m top5: 0.8512126865671642
[2m[36m(func pid=33680)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=33680)[0m f1_macro: 0.1476901719328878
[2m[36m(func pid=33680)[0m f1_weighted: 0.2207165706083986
[2m[36m(func pid=33680)[0m f1_per_class: [0.092, 0.0, 0.25, 0.142, 0.0, 0.274, 0.481, 0.0, 0.0, 0.237]
[2m[36m(func pid=45033)[0m top1: 0.18050373134328357
[2m[36m(func pid=45033)[0m top5: 0.6497201492537313
[2m[36m(func pid=45033)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=45033)[0m f1_macro: 0.12540483111761458
[2m[36m(func pid=45033)[0m f1_weighted: 0.19440248182405243
[2m[36m(func pid=45033)[0m f1_per_class: [0.128, 0.141, 0.096, 0.099, 0.031, 0.112, 0.407, 0.0, 0.095, 0.145]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0240 | Steps: 2 | Val loss: 1.8552 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7684 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=45559)[0m top1: 0.34841417910447764
[2m[36m(func pid=45559)[0m top5: 0.9216417910447762
[2m[36m(func pid=45559)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=45559)[0m f1_macro: 0.3485417156587993
[2m[36m(func pid=45559)[0m f1_weighted: 0.30110539210320725
[2m[36m(func pid=45559)[0m f1_per_class: [0.559, 0.444, 0.615, 0.506, 0.206, 0.097, 0.079, 0.359, 0.186, 0.433]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.0283 | Steps: 2 | Val loss: 2.2075 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=33107)[0m top1: 0.37966417910447764
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=33107)[0m f1_macro: 0.36677785447143435
[2m[36m(func pid=33107)[0m f1_weighted: 0.3899014389872919
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.471, 0.571, 0.488, 0.171, 0.216, 0.346, 0.298, 0.256, 0.333]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.19309701492537312
[2m[36m(func pid=45033)[0m top5: 0.6716417910447762
[2m[36m(func pid=45033)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=45033)[0m f1_macro: 0.13040967626663258
[2m[36m(func pid=45033)[0m f1_weighted: 0.20390960274545009
[2m[36m(func pid=45033)[0m f1_per_class: [0.132, 0.142, 0.105, 0.105, 0.04, 0.119, 0.429, 0.0, 0.104, 0.128]
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0294 | Steps: 2 | Val loss: 1.8604 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7856 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:51:50 (running for 00:32:25.48)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.367 |                   76 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.085 |      0.125 |                   22 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.024 |      0.349 |                   22 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=50583)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=50583)[0m Configuration completed!
[2m[36m(func pid=50583)[0m New optimizer parameters:
[2m[36m(func pid=50583)[0m SGD (
[2m[36m(func pid=50583)[0m Parameter Group 0
[2m[36m(func pid=50583)[0m     dampening: 0
[2m[36m(func pid=50583)[0m     differentiable: False
[2m[36m(func pid=50583)[0m     foreach: None
[2m[36m(func pid=50583)[0m     lr: 0.01
[2m[36m(func pid=50583)[0m     maximize: False
[2m[36m(func pid=50583)[0m     momentum: 0.99
[2m[36m(func pid=50583)[0m     nesterov: False
[2m[36m(func pid=50583)[0m     weight_decay: 1e-05
[2m[36m(func pid=50583)[0m )
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.35401119402985076
[2m[36m(func pid=45559)[0m top5: 0.9272388059701493
[2m[36m(func pid=45559)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=45559)[0m f1_macro: 0.3504289930393322
[2m[36m(func pid=45559)[0m f1_weighted: 0.3089104025522452
[2m[36m(func pid=45559)[0m f1_per_class: [0.543, 0.449, 0.6, 0.509, 0.218, 0.115, 0.093, 0.366, 0.192, 0.419]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:51:56 (running for 00:32:30.98)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.363 |                   77 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  1.028 |      0.13  |                   23 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.029 |      0.35  |                   23 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.37546641791044777
[2m[36m(func pid=33107)[0m top5: 0.9235074626865671
[2m[36m(func pid=33107)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=33107)[0m f1_macro: 0.3634401617880533
[2m[36m(func pid=33107)[0m f1_weighted: 0.3842949926170488
[2m[36m(func pid=33107)[0m f1_per_class: [0.504, 0.472, 0.571, 0.482, 0.167, 0.217, 0.333, 0.3, 0.258, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.9086 | Steps: 2 | Val loss: 2.1836 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0300 | Steps: 2 | Val loss: 1.8595 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1472 | Steps: 2 | Val loss: 2.3414 | Batch size: 32 | lr: 0.01 | Duration: 4.48s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7911 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=45033)[0m top1: 0.20988805970149255
[2m[36m(func pid=45033)[0m top5: 0.6898320895522388
[2m[36m(func pid=45033)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=45033)[0m f1_macro: 0.13816950892454183
[2m[36m(func pid=45033)[0m f1_weighted: 0.2173235093155961
[2m[36m(func pid=45033)[0m f1_per_class: [0.143, 0.139, 0.112, 0.116, 0.04, 0.129, 0.46, 0.0, 0.108, 0.133]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.3582089552238806
[2m[36m(func pid=45559)[0m top5: 0.9281716417910447
[2m[36m(func pid=45559)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=45559)[0m f1_macro: 0.3535376499607233
[2m[36m(func pid=45559)[0m f1_weighted: 0.3156319208882636
[2m[36m(func pid=45559)[0m f1_per_class: [0.547, 0.459, 0.6, 0.507, 0.206, 0.144, 0.101, 0.362, 0.196, 0.413]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:01 (running for 00:32:36.13)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.368 |                   78 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.909 |      0.138 |                   24 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.03  |      0.354 |                   24 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3763992537313433
[2m[36m(func pid=33107)[0m top5: 0.9221082089552238
[2m[36m(func pid=33107)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=33107)[0m f1_macro: 0.36799102728186023
[2m[36m(func pid=33107)[0m f1_weighted: 0.3851995554420105
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.472, 0.571, 0.488, 0.183, 0.214, 0.328, 0.304, 0.263, 0.339]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.08115671641791045
[2m[36m(func pid=50583)[0m top5: 0.534981343283582
[2m[36m(func pid=50583)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=50583)[0m f1_macro: 0.07900846469675088
[2m[36m(func pid=50583)[0m f1_weighted: 0.08596222228086185
[2m[36m(func pid=50583)[0m f1_per_class: [0.11, 0.058, 0.089, 0.161, 0.018, 0.042, 0.044, 0.101, 0.12, 0.047]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8317 | Steps: 2 | Val loss: 2.1611 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0091 | Steps: 2 | Val loss: 1.8676 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7842 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9040 | Steps: 2 | Val loss: 2.3203 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=45033)[0m top1: 0.22201492537313433
[2m[36m(func pid=45033)[0m top5: 0.7010261194029851
[2m[36m(func pid=45033)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=45033)[0m f1_macro: 0.14569533683750083
[2m[36m(func pid=45033)[0m f1_weighted: 0.22760796578536738
[2m[36m(func pid=45033)[0m f1_per_class: [0.141, 0.137, 0.125, 0.136, 0.043, 0.13, 0.475, 0.0, 0.111, 0.158]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.3568097014925373
[2m[36m(func pid=45559)[0m top5: 0.9323694029850746
[2m[36m(func pid=45559)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=45559)[0m f1_macro: 0.3530935996411005
[2m[36m(func pid=45559)[0m f1_weighted: 0.315214740168831
[2m[36m(func pid=45559)[0m f1_per_class: [0.547, 0.452, 0.6, 0.515, 0.232, 0.147, 0.098, 0.352, 0.189, 0.4]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:06 (running for 00:32:41.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.365 |                   79 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.832 |      0.146 |                   25 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.009 |      0.353 |                   25 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  3.147 |      0.079 |                    1 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.37593283582089554
[2m[36m(func pid=33107)[0m top5: 0.9239738805970149
[2m[36m(func pid=33107)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=33107)[0m f1_macro: 0.3652583574155312
[2m[36m(func pid=33107)[0m f1_weighted: 0.3832332009284338
[2m[36m(func pid=33107)[0m f1_per_class: [0.525, 0.473, 0.571, 0.486, 0.167, 0.206, 0.326, 0.302, 0.259, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.1259328358208955
[2m[36m(func pid=50583)[0m top5: 0.6222014925373134
[2m[36m(func pid=50583)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=50583)[0m f1_macro: 0.08619991099465481
[2m[36m(func pid=50583)[0m f1_weighted: 0.13877268612721672
[2m[36m(func pid=50583)[0m f1_per_class: [0.187, 0.0, 0.038, 0.043, 0.033, 0.041, 0.384, 0.0, 0.087, 0.049]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7514 | Steps: 2 | Val loss: 2.1331 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0126 | Steps: 2 | Val loss: 1.8852 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7827 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0325 | Steps: 2 | Val loss: 2.1599 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=45033)[0m top1: 0.23647388059701493
[2m[36m(func pid=45033)[0m top5: 0.7164179104477612
[2m[36m(func pid=45033)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=45033)[0m f1_macro: 0.15475850297576735
[2m[36m(func pid=45033)[0m f1_weighted: 0.2381456888358608
[2m[36m(func pid=45033)[0m f1_per_class: [0.142, 0.14, 0.153, 0.151, 0.044, 0.131, 0.493, 0.0, 0.117, 0.177]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.363339552238806
[2m[36m(func pid=45559)[0m top5: 0.9351679104477612
[2m[36m(func pid=45559)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=45559)[0m f1_macro: 0.3615064370004228
[2m[36m(func pid=45559)[0m f1_weighted: 0.3233514283856375
[2m[36m(func pid=45559)[0m f1_per_class: [0.547, 0.452, 0.632, 0.521, 0.234, 0.168, 0.109, 0.359, 0.194, 0.4]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:11 (running for 00:32:46.52)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.366 |                   80 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.751 |      0.155 |                   26 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.013 |      0.362 |                   26 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  1.904 |      0.086 |                    2 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3787313432835821
[2m[36m(func pid=33107)[0m top5: 0.9235074626865671
[2m[36m(func pid=33107)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=33107)[0m f1_macro: 0.365925629342784
[2m[36m(func pid=33107)[0m f1_weighted: 0.3862224739128276
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.473, 0.558, 0.493, 0.183, 0.214, 0.327, 0.305, 0.255, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.21361940298507462
[2m[36m(func pid=50583)[0m top5: 0.7402052238805971
[2m[36m(func pid=50583)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=50583)[0m f1_macro: 0.15663606167854086
[2m[36m(func pid=50583)[0m f1_weighted: 0.23978930003715726
[2m[36m(func pid=50583)[0m f1_per_class: [0.292, 0.101, 0.049, 0.192, 0.063, 0.077, 0.494, 0.0, 0.109, 0.188]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6926 | Steps: 2 | Val loss: 2.1066 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0171 | Steps: 2 | Val loss: 1.9064 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.8004 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.3412 | Steps: 2 | Val loss: 2.1457 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=45033)[0m top1: 0.2555970149253731
[2m[36m(func pid=45033)[0m top5: 0.7318097014925373
[2m[36m(func pid=45033)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=45033)[0m f1_macro: 0.16343991388963602
[2m[36m(func pid=45033)[0m f1_weighted: 0.2537167675496729
[2m[36m(func pid=45033)[0m f1_per_class: [0.144, 0.138, 0.161, 0.176, 0.054, 0.134, 0.521, 0.0, 0.124, 0.184]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.36613805970149255
[2m[36m(func pid=45559)[0m top5: 0.9379664179104478
[2m[36m(func pid=45559)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=45559)[0m f1_macro: 0.3704484009899844
[2m[36m(func pid=45559)[0m f1_weighted: 0.324956976348287
[2m[36m(func pid=45559)[0m f1_per_class: [0.574, 0.452, 0.649, 0.526, 0.247, 0.168, 0.106, 0.358, 0.196, 0.429]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:16 (running for 00:32:51.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.362 |                   81 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.693 |      0.163 |                   27 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.017 |      0.37  |                   27 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  1.033 |      0.157 |                    3 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3736007462686567
[2m[36m(func pid=33107)[0m top5: 0.9221082089552238
[2m[36m(func pid=33107)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=33107)[0m f1_macro: 0.3616374952391279
[2m[36m(func pid=33107)[0m f1_weighted: 0.3826103828077091
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.47, 0.545, 0.485, 0.176, 0.216, 0.325, 0.303, 0.254, 0.345]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.2392723880597015
[2m[36m(func pid=50583)[0m top5: 0.7826492537313433
[2m[36m(func pid=50583)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=50583)[0m f1_macro: 0.23816040125870672
[2m[36m(func pid=50583)[0m f1_weighted: 0.21058783264153969
[2m[36m(func pid=50583)[0m f1_per_class: [0.387, 0.318, 0.253, 0.332, 0.17, 0.099, 0.045, 0.336, 0.149, 0.293]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7275 | Steps: 2 | Val loss: 2.0839 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0078 | Steps: 2 | Val loss: 1.8998 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7909 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.1392 | Steps: 2 | Val loss: 2.4460 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=45033)[0m top1: 0.27238805970149255
[2m[36m(func pid=45033)[0m top5: 0.7504664179104478
[2m[36m(func pid=45033)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=45033)[0m f1_macro: 0.17456780200240674
[2m[36m(func pid=45033)[0m f1_weighted: 0.27443581590831434
[2m[36m(func pid=45033)[0m f1_per_class: [0.149, 0.166, 0.167, 0.215, 0.062, 0.135, 0.535, 0.0, 0.128, 0.189]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.37080223880597013
[2m[36m(func pid=45559)[0m top5: 0.9398320895522388
[2m[36m(func pid=45559)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=45559)[0m f1_macro: 0.3679507423325786
[2m[36m(func pid=45559)[0m f1_weighted: 0.3342340438394301
[2m[36m(func pid=45559)[0m f1_per_class: [0.568, 0.452, 0.615, 0.527, 0.253, 0.168, 0.138, 0.357, 0.194, 0.407]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:22 (running for 00:32:56.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.362 |                   82 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.727 |      0.175 |                   28 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.008 |      0.368 |                   28 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.341 |      0.238 |                    4 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.376865671641791
[2m[36m(func pid=33107)[0m top5: 0.9239738805970149
[2m[36m(func pid=33107)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=33107)[0m f1_macro: 0.36159022375581784
[2m[36m(func pid=33107)[0m f1_weighted: 0.38871055262799625
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.471, 0.545, 0.484, 0.175, 0.221, 0.348, 0.29, 0.251, 0.33]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.26399253731343286
[2m[36m(func pid=50583)[0m top5: 0.7056902985074627
[2m[36m(func pid=50583)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=50583)[0m f1_macro: 0.31152132354403816
[2m[36m(func pid=50583)[0m f1_weighted: 0.2086669820310356
[2m[36m(func pid=50583)[0m f1_per_class: [0.55, 0.368, 0.649, 0.308, 0.197, 0.076, 0.003, 0.381, 0.197, 0.386]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0047 | Steps: 2 | Val loss: 1.8846 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5356 | Steps: 2 | Val loss: 2.0683 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7889 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.1326 | Steps: 2 | Val loss: 2.8166 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=45559)[0m top1: 0.3810634328358209
[2m[36m(func pid=45559)[0m top5: 0.941231343283582
[2m[36m(func pid=45559)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=45559)[0m f1_macro: 0.3758516771771827
[2m[36m(func pid=45559)[0m f1_weighted: 0.35127238966955787
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.47, 0.615, 0.532, 0.255, 0.173, 0.177, 0.355, 0.199, 0.4]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.279384328358209
[2m[36m(func pid=45033)[0m top5: 0.7625932835820896
[2m[36m(func pid=45033)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=45033)[0m f1_macro: 0.18114517239143907
[2m[36m(func pid=45033)[0m f1_weighted: 0.2801928686950726
[2m[36m(func pid=45033)[0m f1_per_class: [0.166, 0.177, 0.185, 0.218, 0.064, 0.144, 0.54, 0.0, 0.137, 0.182]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:52:27 (running for 00:33:02.01)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.357 |                   83 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.536 |      0.181 |                   29 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.005 |      0.376 |                   29 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.139 |      0.312 |                    5 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.37406716417910446
[2m[36m(func pid=33107)[0m top5: 0.9249067164179104
[2m[36m(func pid=33107)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=33107)[0m f1_macro: 0.3565790312727933
[2m[36m(func pid=33107)[0m f1_weighted: 0.3867328665077075
[2m[36m(func pid=33107)[0m f1_per_class: [0.488, 0.45, 0.545, 0.49, 0.165, 0.207, 0.354, 0.29, 0.247, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.2733208955223881
[2m[36m(func pid=50583)[0m top5: 0.6875
[2m[36m(func pid=50583)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=50583)[0m f1_macro: 0.3224251276410971
[2m[36m(func pid=50583)[0m f1_weighted: 0.21265805796001697
[2m[36m(func pid=50583)[0m f1_per_class: [0.413, 0.409, 0.828, 0.282, 0.245, 0.126, 0.0, 0.407, 0.201, 0.314]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0091 | Steps: 2 | Val loss: 1.8886 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7807 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5720 | Steps: 2 | Val loss: 2.0318 | Batch size: 32 | lr: 0.0001 | Duration: 3.25s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.0575 | Steps: 2 | Val loss: 3.1141 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=45559)[0m top1: 0.3903917910447761
[2m[36m(func pid=45559)[0m top5: 0.9449626865671642
[2m[36m(func pid=45559)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=45559)[0m f1_macro: 0.38059565063430245
[2m[36m(func pid=45559)[0m f1_weighted: 0.36498615049010386
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.479, 0.615, 0.539, 0.258, 0.17, 0.213, 0.355, 0.202, 0.393]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:32 (running for 00:33:07.14)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.362 |                   84 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.536 |      0.181 |                   29 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.009 |      0.381 |                   30 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.133 |      0.322 |                    6 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.3763992537313433
[2m[36m(func pid=33107)[0m top5: 0.9239738805970149
[2m[36m(func pid=33107)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=33107)[0m f1_macro: 0.3616123922557883
[2m[36m(func pid=33107)[0m f1_weighted: 0.38545023056617883
[2m[36m(func pid=33107)[0m f1_per_class: [0.484, 0.47, 0.558, 0.491, 0.176, 0.208, 0.333, 0.299, 0.254, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.2933768656716418
[2m[36m(func pid=45033)[0m top5: 0.7821828358208955
[2m[36m(func pid=45033)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=45033)[0m f1_macro: 0.19459050397627112
[2m[36m(func pid=45033)[0m f1_weighted: 0.2968128760432368
[2m[36m(func pid=45033)[0m f1_per_class: [0.173, 0.196, 0.222, 0.264, 0.065, 0.147, 0.537, 0.0, 0.141, 0.2]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.25466417910447764
[2m[36m(func pid=50583)[0m top5: 0.699160447761194
[2m[36m(func pid=50583)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=50583)[0m f1_macro: 0.3118666867834813
[2m[36m(func pid=50583)[0m f1_weighted: 0.20249058662937952
[2m[36m(func pid=50583)[0m f1_per_class: [0.326, 0.375, 0.923, 0.231, 0.267, 0.266, 0.003, 0.336, 0.185, 0.207]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0054 | Steps: 2 | Val loss: 1.8884 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7701 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4438 | Steps: 2 | Val loss: 2.0113 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.0434 | Steps: 2 | Val loss: 3.3900 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=45559)[0m top1: 0.3969216417910448
[2m[36m(func pid=45559)[0m top5: 0.9468283582089553
[2m[36m(func pid=45559)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=45559)[0m f1_macro: 0.38694264371153836
[2m[36m(func pid=45559)[0m f1_weighted: 0.37628323380147094
[2m[36m(func pid=45559)[0m f1_per_class: [0.574, 0.496, 0.615, 0.54, 0.264, 0.202, 0.228, 0.352, 0.204, 0.393]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:37 (running for 00:33:12.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.364 |                   85 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.572 |      0.195 |                   30 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.005 |      0.387 |                   31 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.058 |      0.312 |                    7 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.37919776119402987
[2m[36m(func pid=33107)[0m top5: 0.925839552238806
[2m[36m(func pid=33107)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=33107)[0m f1_macro: 0.3642019416565017
[2m[36m(func pid=33107)[0m f1_weighted: 0.3913770361255185
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.473, 0.558, 0.489, 0.173, 0.221, 0.351, 0.286, 0.253, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.30363805970149255
[2m[36m(func pid=45033)[0m top5: 0.7924440298507462
[2m[36m(func pid=45033)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=45033)[0m f1_macro: 0.2029620413598301
[2m[36m(func pid=45033)[0m f1_weighted: 0.3062891806514303
[2m[36m(func pid=45033)[0m f1_per_class: [0.196, 0.212, 0.238, 0.275, 0.067, 0.15, 0.546, 0.0, 0.148, 0.198]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.24766791044776118
[2m[36m(func pid=50583)[0m top5: 0.7112873134328358
[2m[36m(func pid=50583)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=50583)[0m f1_macro: 0.2980962495663698
[2m[36m(func pid=50583)[0m f1_weighted: 0.19574434597010884
[2m[36m(func pid=50583)[0m f1_per_class: [0.33, 0.341, 0.857, 0.233, 0.292, 0.293, 0.003, 0.265, 0.179, 0.188]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0030 | Steps: 2 | Val loss: 1.8996 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.7602 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3857 | Steps: 2 | Val loss: 1.9910 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0235 | Steps: 2 | Val loss: 3.6445 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=45559)[0m top1: 0.40345149253731344
[2m[36m(func pid=45559)[0m top5: 0.9468283582089553
[2m[36m(func pid=45559)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=45559)[0m f1_macro: 0.3931871809846662
[2m[36m(func pid=45559)[0m f1_weighted: 0.38609795074224534
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.5, 0.615, 0.546, 0.264, 0.235, 0.24, 0.353, 0.205, 0.393]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:52:42 (running for 00:33:17.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.364 |                   85 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.444 |      0.203 |                   31 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.393 |                   32 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.043 |      0.298 |                    8 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.38152985074626866
[2m[36m(func pid=33107)[0m top5: 0.9277052238805971
[2m[36m(func pid=33107)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=33107)[0m f1_macro: 0.36868695952242636
[2m[36m(func pid=33107)[0m f1_weighted: 0.3931484756102675
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.465, 0.571, 0.494, 0.191, 0.236, 0.35, 0.289, 0.252, 0.339]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.31343283582089554
[2m[36m(func pid=45033)[0m top5: 0.8059701492537313
[2m[36m(func pid=45033)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=45033)[0m f1_macro: 0.21316013316196986
[2m[36m(func pid=45033)[0m f1_weighted: 0.31452100598801574
[2m[36m(func pid=45033)[0m f1_per_class: [0.215, 0.21, 0.258, 0.293, 0.075, 0.158, 0.552, 0.0, 0.159, 0.212]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.23227611940298507
[2m[36m(func pid=50583)[0m top5: 0.7257462686567164
[2m[36m(func pid=50583)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=50583)[0m f1_macro: 0.26505950968233555
[2m[36m(func pid=50583)[0m f1_weighted: 0.1795775244212685
[2m[36m(func pid=50583)[0m f1_per_class: [0.333, 0.311, 0.706, 0.222, 0.245, 0.277, 0.006, 0.15, 0.2, 0.2]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7754 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0033 | Steps: 2 | Val loss: 1.9211 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3980 | Steps: 2 | Val loss: 1.9645 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0130 | Steps: 2 | Val loss: 3.7499 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 03:52:47 (running for 00:33:22.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.364 |                   87 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.386 |      0.213 |                   32 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.393 |                   32 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.024 |      0.265 |                    9 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=33107)[0m top1: 0.37453358208955223
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=33107)[0m f1_macro: 0.36369932806385286
[2m[36m(func pid=33107)[0m f1_weighted: 0.38703210176520003
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.458, 0.571, 0.48, 0.18, 0.224, 0.352, 0.285, 0.247, 0.331]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45559)[0m top1: 0.40298507462686567
[2m[36m(func pid=45559)[0m top5: 0.9463619402985075
[2m[36m(func pid=45559)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=45559)[0m f1_macro: 0.39051228432706037
[2m[36m(func pid=45559)[0m f1_weighted: 0.38706531123395743
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.502, 0.615, 0.551, 0.263, 0.205, 0.252, 0.341, 0.195, 0.4]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.3185634328358209
[2m[36m(func pid=45033)[0m top5: 0.8255597014925373
[2m[36m(func pid=45033)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=45033)[0m f1_macro: 0.22171824418178293
[2m[36m(func pid=45033)[0m f1_weighted: 0.32315984406431053
[2m[36m(func pid=45033)[0m f1_per_class: [0.229, 0.226, 0.264, 0.312, 0.078, 0.17, 0.544, 0.015, 0.168, 0.212]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.23973880597014927
[2m[36m(func pid=50583)[0m top5: 0.7481343283582089
[2m[36m(func pid=50583)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=50583)[0m f1_macro: 0.261021013292254
[2m[36m(func pid=50583)[0m f1_weighted: 0.19331762322598037
[2m[36m(func pid=50583)[0m f1_per_class: [0.342, 0.307, 0.545, 0.232, 0.267, 0.323, 0.03, 0.145, 0.193, 0.226]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7918 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0041 | Steps: 2 | Val loss: 1.9239 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4212 | Steps: 2 | Val loss: 1.9487 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0041 | Steps: 2 | Val loss: 3.8232 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 03:52:53 (running for 00:33:27.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.364 |                   87 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.398 |      0.222 |                   33 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.004 |      0.399 |                   34 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.013 |      0.261 |                   10 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.40718283582089554
[2m[36m(func pid=45559)[0m top5: 0.9496268656716418
[2m[36m(func pid=45559)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=45559)[0m f1_macro: 0.3988518770127417
[2m[36m(func pid=45559)[0m f1_weighted: 0.3950113002689708
[2m[36m(func pid=45559)[0m f1_per_class: [0.587, 0.489, 0.632, 0.559, 0.289, 0.22, 0.272, 0.335, 0.192, 0.414]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.37546641791044777
[2m[36m(func pid=33107)[0m top5: 0.9244402985074627
[2m[36m(func pid=33107)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=33107)[0m f1_macro: 0.3640389075086374
[2m[36m(func pid=33107)[0m f1_weighted: 0.38649483034516985
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.47, 0.571, 0.483, 0.164, 0.22, 0.339, 0.302, 0.253, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.3162313432835821
[2m[36m(func pid=45033)[0m top5: 0.8404850746268657
[2m[36m(func pid=45033)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=45033)[0m f1_macro: 0.22677972955722603
[2m[36m(func pid=45033)[0m f1_weighted: 0.32543559268024913
[2m[36m(func pid=45033)[0m f1_per_class: [0.224, 0.239, 0.289, 0.329, 0.09, 0.177, 0.524, 0.015, 0.175, 0.205]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.24486940298507462
[2m[36m(func pid=50583)[0m top5: 0.784981343283582
[2m[36m(func pid=50583)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=50583)[0m f1_macro: 0.2700384029427013
[2m[36m(func pid=50583)[0m f1_weighted: 0.20508142083532985
[2m[36m(func pid=50583)[0m f1_per_class: [0.328, 0.308, 0.49, 0.234, 0.264, 0.325, 0.061, 0.158, 0.206, 0.327]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0027 | Steps: 2 | Val loss: 1.9334 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.7769 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0089 | Steps: 2 | Val loss: 3.8769 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3200 | Steps: 2 | Val loss: 1.9265 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 03:52:58 (running for 00:33:33.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.364 |                   88 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.421 |      0.227 |                   34 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.397 |                   35 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.004 |      0.27  |                   11 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.408115671641791
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=45559)[0m f1_macro: 0.3974741777449055
[2m[36m(func pid=45559)[0m f1_weighted: 0.3989527736666451
[2m[36m(func pid=45559)[0m f1_per_class: [0.584, 0.49, 0.632, 0.562, 0.28, 0.235, 0.28, 0.328, 0.192, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.37779850746268656
[2m[36m(func pid=33107)[0m top5: 0.9267723880597015
[2m[36m(func pid=33107)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=33107)[0m f1_macro: 0.36622634448759994
[2m[36m(func pid=33107)[0m f1_weighted: 0.3907447257885796
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.467, 0.585, 0.481, 0.177, 0.218, 0.36, 0.288, 0.25, 0.328]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=45033)[0m top1: 0.32322761194029853
[2m[36m(func pid=45033)[0m top5: 0.8512126865671642
[2m[36m(func pid=45033)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=45033)[0m f1_macro: 0.2408795481933089
[2m[36m(func pid=45033)[0m f1_weighted: 0.33499585059537346
[2m[36m(func pid=45033)[0m f1_per_class: [0.239, 0.254, 0.304, 0.351, 0.091, 0.176, 0.518, 0.044, 0.184, 0.247]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.26259328358208955
[2m[36m(func pid=50583)[0m top5: 0.8190298507462687
[2m[36m(func pid=50583)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=50583)[0m f1_macro: 0.2769318672256513
[2m[36m(func pid=50583)[0m f1_weighted: 0.23133763948721592
[2m[36m(func pid=50583)[0m f1_per_class: [0.333, 0.329, 0.407, 0.234, 0.246, 0.358, 0.123, 0.165, 0.202, 0.372]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0025 | Steps: 2 | Val loss: 1.9408 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7956 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0028 | Steps: 2 | Val loss: 3.9468 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3171 | Steps: 2 | Val loss: 1.8956 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 03:53:03 (running for 00:33:38.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.366 |                   89 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.32  |      0.241 |                   35 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.4   |                   36 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.009 |      0.277 |                   12 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.41138059701492535
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=45559)[0m f1_macro: 0.3997334503327969
[2m[36m(func pid=45559)[0m f1_weighted: 0.40511900300555964
[2m[36m(func pid=45559)[0m f1_per_class: [0.578, 0.498, 0.632, 0.562, 0.298, 0.242, 0.294, 0.32, 0.194, 0.379]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.37453358208955223
[2m[36m(func pid=33107)[0m top5: 0.925839552238806
[2m[36m(func pid=33107)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=33107)[0m f1_macro: 0.3641562784379026
[2m[36m(func pid=33107)[0m f1_weighted: 0.38516018510485556
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.466, 0.585, 0.473, 0.164, 0.216, 0.347, 0.301, 0.256, 0.325]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.27658582089552236
[2m[36m(func pid=50583)[0m top5: 0.8362873134328358
[2m[36m(func pid=50583)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=50583)[0m f1_macro: 0.2812578694750406
[2m[36m(func pid=50583)[0m f1_weighted: 0.25467762896462415
[2m[36m(func pid=50583)[0m f1_per_class: [0.316, 0.345, 0.32, 0.233, 0.222, 0.377, 0.18, 0.209, 0.2, 0.41]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.33255597014925375
[2m[36m(func pid=45033)[0m top5: 0.8638059701492538
[2m[36m(func pid=45033)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=45033)[0m f1_macro: 0.25411069027568123
[2m[36m(func pid=45033)[0m f1_weighted: 0.3459930739069519
[2m[36m(func pid=45033)[0m f1_per_class: [0.246, 0.272, 0.364, 0.379, 0.092, 0.169, 0.513, 0.071, 0.193, 0.242]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0054 | Steps: 2 | Val loss: 1.9532 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7947 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0031 | Steps: 2 | Val loss: 4.0114 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2422 | Steps: 2 | Val loss: 1.8727 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:53:08 (running for 00:33:43.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.364 |                   90 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.317 |      0.254 |                   36 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.005 |      0.402 |                   37 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.003 |      0.281 |                   13 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.41651119402985076
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=45559)[0m f1_macro: 0.40219790150857493
[2m[36m(func pid=45559)[0m f1_weighted: 0.4106303904868363
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.508, 0.649, 0.561, 0.289, 0.261, 0.301, 0.32, 0.204, 0.349]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3763992537313433
[2m[36m(func pid=33107)[0m top5: 0.9239738805970149
[2m[36m(func pid=33107)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=33107)[0m f1_macro: 0.3637602374256578
[2m[36m(func pid=33107)[0m f1_weighted: 0.38810201565974295
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.469, 0.585, 0.479, 0.17, 0.201, 0.358, 0.292, 0.257, 0.33]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.292910447761194
[2m[36m(func pid=50583)[0m top5: 0.8512126865671642
[2m[36m(func pid=50583)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=50583)[0m f1_macro: 0.2819300086055526
[2m[36m(func pid=50583)[0m f1_weighted: 0.27671134845060735
[2m[36m(func pid=50583)[0m f1_per_class: [0.324, 0.369, 0.276, 0.244, 0.271, 0.386, 0.232, 0.193, 0.201, 0.324]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.34375
[2m[36m(func pid=45033)[0m top5: 0.8708022388059702
[2m[36m(func pid=45033)[0m f1_micro: 0.34375
[2m[36m(func pid=45033)[0m f1_macro: 0.26692441243572435
[2m[36m(func pid=45033)[0m f1_weighted: 0.35810987349020174
[2m[36m(func pid=45033)[0m f1_per_class: [0.259, 0.282, 0.381, 0.396, 0.101, 0.188, 0.514, 0.122, 0.192, 0.235]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0044 | Steps: 2 | Val loss: 1.9831 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.7784 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0078 | Steps: 2 | Val loss: 4.0413 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2739 | Steps: 2 | Val loss: 1.8600 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 03:53:13 (running for 00:33:48.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.364 |                   91 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.242 |      0.267 |                   37 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.004 |      0.409 |                   38 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.003 |      0.282 |                   14 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4197761194029851
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=45559)[0m f1_macro: 0.408732088841005
[2m[36m(func pid=45559)[0m f1_weighted: 0.41904846221995073
[2m[36m(func pid=45559)[0m f1_per_class: [0.593, 0.501, 0.667, 0.567, 0.277, 0.241, 0.334, 0.318, 0.196, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3787313432835821
[2m[36m(func pid=33107)[0m top5: 0.9235074626865671
[2m[36m(func pid=33107)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=33107)[0m f1_macro: 0.3668049324368605
[2m[36m(func pid=33107)[0m f1_weighted: 0.3874678521631306
[2m[36m(func pid=33107)[0m f1_per_class: [0.508, 0.47, 0.571, 0.483, 0.18, 0.209, 0.345, 0.301, 0.258, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.302705223880597
[2m[36m(func pid=50583)[0m top5: 0.8638059701492538
[2m[36m(func pid=50583)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=50583)[0m f1_macro: 0.27850044063071777
[2m[36m(func pid=50583)[0m f1_weighted: 0.292754439765958
[2m[36m(func pid=50583)[0m f1_per_class: [0.248, 0.382, 0.276, 0.247, 0.212, 0.392, 0.272, 0.228, 0.203, 0.324]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.34654850746268656
[2m[36m(func pid=45033)[0m top5: 0.8740671641791045
[2m[36m(func pid=45033)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=45033)[0m f1_macro: 0.2686313952260543
[2m[36m(func pid=45033)[0m f1_weighted: 0.36221920407890273
[2m[36m(func pid=45033)[0m f1_per_class: [0.268, 0.294, 0.381, 0.398, 0.095, 0.196, 0.515, 0.12, 0.2, 0.218]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0043 | Steps: 2 | Val loss: 2.0121 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7695 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0053 | Steps: 2 | Val loss: 4.2113 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2142 | Steps: 2 | Val loss: 1.8396 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 03:53:18 (running for 00:33:53.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.367 |                   92 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.274 |      0.269 |                   38 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.004 |      0.402 |                   39 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.008 |      0.279 |                   15 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.416044776119403
[2m[36m(func pid=45559)[0m top5: 0.9514925373134329
[2m[36m(func pid=45559)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=45559)[0m f1_macro: 0.40186640058758805
[2m[36m(func pid=45559)[0m f1_weighted: 0.41410423459150675
[2m[36m(func pid=45559)[0m f1_per_class: [0.587, 0.492, 0.667, 0.571, 0.283, 0.219, 0.33, 0.305, 0.198, 0.367]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.37919776119402987
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=33107)[0m f1_macro: 0.36446451342733105
[2m[36m(func pid=33107)[0m f1_weighted: 0.38973613677310187
[2m[36m(func pid=33107)[0m f1_per_class: [0.492, 0.468, 0.558, 0.484, 0.18, 0.227, 0.349, 0.298, 0.253, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.3069029850746269
[2m[36m(func pid=50583)[0m top5: 0.871268656716418
[2m[36m(func pid=50583)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=50583)[0m f1_macro: 0.29534950110962177
[2m[36m(func pid=50583)[0m f1_weighted: 0.29449018738856975
[2m[36m(func pid=50583)[0m f1_per_class: [0.327, 0.398, 0.414, 0.242, 0.176, 0.37, 0.275, 0.234, 0.194, 0.324]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.3512126865671642
[2m[36m(func pid=45033)[0m top5: 0.8782649253731343
[2m[36m(func pid=45033)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=45033)[0m f1_macro: 0.27713059367722864
[2m[36m(func pid=45033)[0m f1_weighted: 0.36784473191255607
[2m[36m(func pid=45033)[0m f1_per_class: [0.272, 0.294, 0.407, 0.419, 0.102, 0.184, 0.511, 0.152, 0.206, 0.224]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0033 | Steps: 2 | Val loss: 2.0445 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7707 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0084 | Steps: 2 | Val loss: 4.3612 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 03:53:23 (running for 00:33:58.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.364 |                   93 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.214 |      0.277 |                   39 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.399 |                   40 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.005 |      0.295 |                   16 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1903 | Steps: 2 | Val loss: 1.8330 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=45559)[0m top1: 0.4123134328358209
[2m[36m(func pid=45559)[0m top5: 0.9528917910447762
[2m[36m(func pid=45559)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=45559)[0m f1_macro: 0.39883140405936435
[2m[36m(func pid=45559)[0m f1_weighted: 0.41116782820340714
[2m[36m(func pid=45559)[0m f1_per_class: [0.584, 0.479, 0.667, 0.574, 0.275, 0.211, 0.33, 0.303, 0.194, 0.373]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=33107)[0m top1: 0.3805970149253731
[2m[36m(func pid=33107)[0m top5: 0.9267723880597015
[2m[36m(func pid=33107)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=33107)[0m f1_macro: 0.3679458774504593
[2m[36m(func pid=33107)[0m f1_weighted: 0.3887946470878051
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.478, 0.558, 0.489, 0.171, 0.232, 0.33, 0.298, 0.258, 0.348]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.3138992537313433
[2m[36m(func pid=50583)[0m top5: 0.8759328358208955
[2m[36m(func pid=50583)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=50583)[0m f1_macro: 0.3104863586902037
[2m[36m(func pid=50583)[0m f1_weighted: 0.30260982497005445
[2m[36m(func pid=50583)[0m f1_per_class: [0.362, 0.404, 0.436, 0.231, 0.194, 0.353, 0.297, 0.307, 0.195, 0.324]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.355410447761194
[2m[36m(func pid=45033)[0m top5: 0.8810634328358209
[2m[36m(func pid=45033)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=45033)[0m f1_macro: 0.2853055464969991
[2m[36m(func pid=45033)[0m f1_weighted: 0.3741102833315928
[2m[36m(func pid=45033)[0m f1_per_class: [0.28, 0.303, 0.407, 0.434, 0.099, 0.19, 0.501, 0.197, 0.204, 0.239]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.0709 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7784 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0023 | Steps: 2 | Val loss: 4.5416 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:53:29 (running for 00:34:03.91)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.368 |                   94 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.19  |      0.285 |                   40 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.401 |                   41 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.008 |      0.31  |                   17 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4141791044776119
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=45559)[0m f1_macro: 0.4005317833930075
[2m[36m(func pid=45559)[0m f1_weighted: 0.4133762092016671
[2m[36m(func pid=45559)[0m f1_per_class: [0.578, 0.475, 0.667, 0.579, 0.289, 0.202, 0.337, 0.304, 0.196, 0.379]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1711 | Steps: 2 | Val loss: 1.8227 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=33107)[0m top1: 0.3787313432835821
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=33107)[0m f1_macro: 0.36294807170620225
[2m[36m(func pid=33107)[0m f1_weighted: 0.38853792343691473
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.473, 0.545, 0.481, 0.174, 0.223, 0.346, 0.297, 0.254, 0.336]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.3138992537313433
[2m[36m(func pid=50583)[0m top5: 0.8810634328358209
[2m[36m(func pid=50583)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=50583)[0m f1_macro: 0.3155944256714453
[2m[36m(func pid=50583)[0m f1_weighted: 0.29925085418283265
[2m[36m(func pid=50583)[0m f1_per_class: [0.376, 0.408, 0.522, 0.206, 0.182, 0.331, 0.307, 0.348, 0.206, 0.27]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.35774253731343286
[2m[36m(func pid=45033)[0m top5: 0.8875932835820896
[2m[36m(func pid=45033)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=45033)[0m f1_macro: 0.290121771626498
[2m[36m(func pid=45033)[0m f1_weighted: 0.3781127543952455
[2m[36m(func pid=45033)[0m f1_per_class: [0.282, 0.314, 0.421, 0.438, 0.098, 0.189, 0.5, 0.227, 0.2, 0.234]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.0735 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7724 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0889 | Steps: 2 | Val loss: 4.7030 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=45559)[0m top1: 0.41744402985074625
[2m[36m(func pid=45559)[0m top5: 0.9538246268656716
[2m[36m(func pid=45559)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=45559)[0m f1_macro: 0.4050401851176456
[2m[36m(func pid=45559)[0m f1_weighted: 0.41855551008151987
[2m[36m(func pid=45559)[0m f1_per_class: [0.591, 0.488, 0.667, 0.575, 0.304, 0.241, 0.337, 0.295, 0.198, 0.355]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:53:34 (running for 00:34:09.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.363 |                   95 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.171 |      0.29  |                   41 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.405 |                   42 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.002 |      0.316 |                   18 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1496 | Steps: 2 | Val loss: 1.8140 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=33107)[0m top1: 0.37919776119402987
[2m[36m(func pid=33107)[0m top5: 0.925839552238806
[2m[36m(func pid=33107)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=33107)[0m f1_macro: 0.3622330159112443
[2m[36m(func pid=33107)[0m f1_weighted: 0.3898615387396651
[2m[36m(func pid=33107)[0m f1_per_class: [0.5, 0.468, 0.533, 0.48, 0.171, 0.211, 0.358, 0.302, 0.257, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.32322761194029853
[2m[36m(func pid=50583)[0m top5: 0.871268656716418
[2m[36m(func pid=50583)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=50583)[0m f1_macro: 0.33348593502852947
[2m[36m(func pid=50583)[0m f1_weighted: 0.30028084506963654
[2m[36m(func pid=50583)[0m f1_per_class: [0.364, 0.424, 0.522, 0.253, 0.136, 0.375, 0.232, 0.354, 0.223, 0.453]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.353544776119403
[2m[36m(func pid=45033)[0m top5: 0.8885261194029851
[2m[36m(func pid=45033)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=45033)[0m f1_macro: 0.29281790843687366
[2m[36m(func pid=45033)[0m f1_weighted: 0.3735913974204548
[2m[36m(func pid=45033)[0m f1_per_class: [0.292, 0.328, 0.421, 0.442, 0.103, 0.188, 0.465, 0.26, 0.204, 0.224]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.0825 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.7742 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0076 | Steps: 2 | Val loss: 5.0022 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:53:39 (running for 00:34:14.49)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.362 |                   96 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.15  |      0.293 |                   42 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.408 |                   43 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.089 |      0.333 |                   19 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4197761194029851
[2m[36m(func pid=45559)[0m top5: 0.9533582089552238
[2m[36m(func pid=45559)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=45559)[0m f1_macro: 0.4081178387991219
[2m[36m(func pid=45559)[0m f1_weighted: 0.42212249874877256
[2m[36m(func pid=45559)[0m f1_per_class: [0.591, 0.488, 0.667, 0.574, 0.318, 0.269, 0.34, 0.291, 0.194, 0.349]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1248 | Steps: 2 | Val loss: 1.8095 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=33107)[0m top1: 0.37919776119402987
[2m[36m(func pid=33107)[0m top5: 0.9263059701492538
[2m[36m(func pid=33107)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=33107)[0m f1_macro: 0.36326017093869856
[2m[36m(func pid=33107)[0m f1_weighted: 0.38870476654919384
[2m[36m(func pid=33107)[0m f1_per_class: [0.496, 0.471, 0.533, 0.486, 0.184, 0.216, 0.344, 0.302, 0.258, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.27705223880597013
[2m[36m(func pid=50583)[0m top5: 0.7868470149253731
[2m[36m(func pid=50583)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=50583)[0m f1_macro: 0.2525982304212242
[2m[36m(func pid=50583)[0m f1_weighted: 0.2392286082308808
[2m[36m(func pid=50583)[0m f1_per_class: [0.318, 0.401, 0.316, 0.21, 0.146, 0.429, 0.109, 0.209, 0.189, 0.2]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.34794776119402987
[2m[36m(func pid=45033)[0m top5: 0.8913246268656716
[2m[36m(func pid=45033)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=45033)[0m f1_macro: 0.29307007367925114
[2m[36m(func pid=45033)[0m f1_weighted: 0.3684558199852491
[2m[36m(func pid=45033)[0m f1_per_class: [0.298, 0.34, 0.429, 0.443, 0.102, 0.197, 0.438, 0.253, 0.207, 0.225]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.0672 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.7745 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0037 | Steps: 2 | Val loss: 5.7783 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=45559)[0m top1: 0.43050373134328357
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=45559)[0m f1_macro: 0.4152629911180477
[2m[36m(func pid=45559)[0m f1_weighted: 0.43631178283961763
[2m[36m(func pid=45559)[0m f1_per_class: [0.591, 0.491, 0.649, 0.574, 0.298, 0.305, 0.37, 0.297, 0.21, 0.369]
[2m[36m(func pid=45559)[0m 
== Status ==
Current time: 2024-01-07 03:53:45 (running for 00:34:19.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.363 |                   97 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.125 |      0.293 |                   43 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.415 |                   44 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.008 |      0.253 |                   20 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1235 | Steps: 2 | Val loss: 1.8018 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=33107)[0m top1: 0.3763992537313433
[2m[36m(func pid=33107)[0m top5: 0.9253731343283582
[2m[36m(func pid=33107)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=33107)[0m f1_macro: 0.3686245194135005
[2m[36m(func pid=33107)[0m f1_weighted: 0.38575189323448017
[2m[36m(func pid=33107)[0m f1_per_class: [0.517, 0.466, 0.585, 0.484, 0.175, 0.223, 0.334, 0.3, 0.259, 0.342]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.23460820895522388
[2m[36m(func pid=50583)[0m top5: 0.7220149253731343
[2m[36m(func pid=50583)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=50583)[0m f1_macro: 0.2096233138239926
[2m[36m(func pid=50583)[0m f1_weighted: 0.20022880262855283
[2m[36m(func pid=50583)[0m f1_per_class: [0.211, 0.381, 0.316, 0.156, 0.133, 0.464, 0.068, 0.059, 0.163, 0.144]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.341884328358209
[2m[36m(func pid=45033)[0m top5: 0.8913246268656716
[2m[36m(func pid=45033)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=45033)[0m f1_macro: 0.2911727223837544
[2m[36m(func pid=45033)[0m f1_weighted: 0.3595935801595296
[2m[36m(func pid=45033)[0m f1_per_class: [0.298, 0.341, 0.436, 0.447, 0.108, 0.189, 0.405, 0.268, 0.198, 0.222]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.0758 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.7917 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1130 | Steps: 2 | Val loss: 5.8688 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:53:50 (running for 00:34:25.00)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0     |      0.369 |                   98 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.124 |      0.291 |                   44 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.416 |                   45 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.004 |      0.21  |                   21 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4319029850746269
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=45559)[0m f1_macro: 0.41583654191802594
[2m[36m(func pid=45559)[0m f1_weighted: 0.4392648334279171
[2m[36m(func pid=45559)[0m f1_per_class: [0.584, 0.492, 0.667, 0.573, 0.308, 0.305, 0.381, 0.294, 0.211, 0.344]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.1720 | Steps: 2 | Val loss: 1.7910 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=33107)[0m top1: 0.37546641791044777
[2m[36m(func pid=33107)[0m top5: 0.9249067164179104
[2m[36m(func pid=33107)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=33107)[0m f1_macro: 0.3664608444642053
[2m[36m(func pid=33107)[0m f1_weighted: 0.3851459156255274
[2m[36m(func pid=33107)[0m f1_per_class: [0.484, 0.47, 0.585, 0.488, 0.177, 0.205, 0.334, 0.3, 0.263, 0.357]
[2m[36m(func pid=33107)[0m 
[2m[36m(func pid=50583)[0m top1: 0.25419776119402987
[2m[36m(func pid=50583)[0m top5: 0.78125
[2m[36m(func pid=50583)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=50583)[0m f1_macro: 0.23434894423005198
[2m[36m(func pid=50583)[0m f1_weighted: 0.22414524876085773
[2m[36m(func pid=50583)[0m f1_per_class: [0.338, 0.345, 0.296, 0.156, 0.0, 0.424, 0.119, 0.35, 0.205, 0.109]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0019 | Steps: 2 | Val loss: 2.0859 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=45033)[0m top1: 0.3358208955223881
[2m[36m(func pid=45033)[0m top5: 0.9015858208955224
[2m[36m(func pid=45033)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=45033)[0m f1_macro: 0.29273333915963395
[2m[36m(func pid=45033)[0m f1_weighted: 0.3509788713106554
[2m[36m(func pid=45033)[0m f1_per_class: [0.312, 0.346, 0.436, 0.438, 0.108, 0.188, 0.374, 0.3, 0.204, 0.221]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=33107)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.7971 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0027 | Steps: 2 | Val loss: 7.5793 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=45559)[0m top1: 0.43330223880597013
[2m[36m(func pid=45559)[0m top5: 0.9538246268656716
[2m[36m(func pid=45559)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=45559)[0m f1_macro: 0.4161353637449975
[2m[36m(func pid=45559)[0m f1_weighted: 0.4414579767937739
[2m[36m(func pid=45559)[0m f1_per_class: [0.571, 0.486, 0.667, 0.579, 0.308, 0.312, 0.386, 0.283, 0.206, 0.364]
== Status ==
Current time: 2024-01-07 03:53:55 (running for 00:34:30.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_2d480_00014 | RUNNING    | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |  0.001 |      0.366 |                   99 |
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |  0.172 |      0.293 |                   45 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.416 |                   46 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |  0.113 |      0.234 |                   22 |
| train_2d480_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |  0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |  0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |  0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |  6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |  0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |  0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |  0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |  1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |  0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |  0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |  1.37  |      0.048 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1037 | Steps: 2 | Val loss: 1.7820 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=33107)[0m top1: 0.37593283582089554
[2m[36m(func pid=33107)[0m top5: 0.9239738805970149
[2m[36m(func pid=33107)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=33107)[0m f1_macro: 0.3647023840831679
[2m[36m(func pid=33107)[0m f1_weighted: 0.38505551386361797
[2m[36m(func pid=33107)[0m f1_per_class: [0.474, 0.468, 0.571, 0.49, 0.184, 0.215, 0.33, 0.305, 0.26, 0.349]
[2m[36m(func pid=50583)[0m top1: 0.19636194029850745
[2m[36m(func pid=50583)[0m top5: 0.7201492537313433
[2m[36m(func pid=50583)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=50583)[0m f1_macro: 0.19180908321860057
[2m[36m(func pid=50583)[0m f1_weighted: 0.16032843654536255
[2m[36m(func pid=50583)[0m f1_per_class: [0.283, 0.277, 0.296, 0.123, 0.0, 0.261, 0.039, 0.373, 0.215, 0.051]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.0986 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=45033)[0m top1: 0.3353544776119403
[2m[36m(func pid=45033)[0m top5: 0.9029850746268657
[2m[36m(func pid=45033)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=45033)[0m f1_macro: 0.29722114770152785
[2m[36m(func pid=45033)[0m f1_weighted: 0.34902026772066796
[2m[36m(func pid=45033)[0m f1_per_class: [0.316, 0.354, 0.48, 0.451, 0.107, 0.183, 0.35, 0.299, 0.216, 0.216]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0808 | Steps: 2 | Val loss: 8.1677 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=45559)[0m top1: 0.435634328358209
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=45559)[0m f1_macro: 0.41881599956763893
[2m[36m(func pid=45559)[0m f1_weighted: 0.445404851255464
[2m[36m(func pid=45559)[0m f1_per_class: [0.598, 0.495, 0.667, 0.573, 0.308, 0.317, 0.397, 0.285, 0.211, 0.338]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1262 | Steps: 2 | Val loss: 1.7734 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=50583)[0m top1: 0.18889925373134328
[2m[36m(func pid=50583)[0m top5: 0.710820895522388
[2m[36m(func pid=50583)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=50583)[0m f1_macro: 0.1784796180775406
[2m[36m(func pid=50583)[0m f1_weighted: 0.13823716977344014
[2m[36m(func pid=50583)[0m f1_per_class: [0.245, 0.319, 0.364, 0.108, 0.0, 0.107, 0.018, 0.359, 0.207, 0.057]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.0975 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=45033)[0m top1: 0.34095149253731344
[2m[36m(func pid=45033)[0m top5: 0.9043843283582089
[2m[36m(func pid=45033)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=45033)[0m f1_macro: 0.31285913774756324
[2m[36m(func pid=45033)[0m f1_weighted: 0.3517201821200629
[2m[36m(func pid=45033)[0m f1_per_class: [0.335, 0.38, 0.558, 0.459, 0.107, 0.191, 0.326, 0.313, 0.225, 0.235]
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0038 | Steps: 2 | Val loss: 8.7930 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:54:02 (running for 00:34:37.15)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.104 |      0.297 |                   46 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.419 |                   47 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.081 |      0.178 |                   24 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=56084)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=56084)[0m Configuration completed!
[2m[36m(func pid=56084)[0m New optimizer parameters:
[2m[36m(func pid=56084)[0m SGD (
[2m[36m(func pid=56084)[0m Parameter Group 0
[2m[36m(func pid=56084)[0m     dampening: 0
[2m[36m(func pid=56084)[0m     differentiable: False
[2m[36m(func pid=56084)[0m     foreach: None
[2m[36m(func pid=56084)[0m     lr: 0.1
[2m[36m(func pid=56084)[0m     maximize: False
[2m[36m(func pid=56084)[0m     momentum: 0.99
[2m[36m(func pid=56084)[0m     nesterov: False
[2m[36m(func pid=56084)[0m     weight_decay: 1e-05
[2m[36m(func pid=56084)[0m )
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m top1: 0.4337686567164179
[2m[36m(func pid=45559)[0m top5: 0.9552238805970149
[2m[36m(func pid=45559)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=45559)[0m f1_macro: 0.4133758815549977
[2m[36m(func pid=45559)[0m f1_weighted: 0.4448461476007673
[2m[36m(func pid=45559)[0m f1_per_class: [0.553, 0.483, 0.667, 0.574, 0.304, 0.322, 0.404, 0.273, 0.21, 0.344]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m top1: 0.18936567164179105
[2m[36m(func pid=50583)[0m top5: 0.6879664179104478
[2m[36m(func pid=50583)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=50583)[0m f1_macro: 0.16545284508470887
[2m[36m(func pid=50583)[0m f1_weighted: 0.12882617766424545
[2m[36m(func pid=50583)[0m f1_per_class: [0.239, 0.338, 0.338, 0.11, 0.0, 0.016, 0.012, 0.356, 0.188, 0.057]
== Status ==
Current time: 2024-01-07 03:54:07 (running for 00:34:42.47)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.126 |      0.313 |                   47 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.002 |      0.413 |                   48 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.004 |      0.165 |                   25 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.1606 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0750 | Steps: 2 | Val loss: 1.7680 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.9304 | Steps: 2 | Val loss: 3.8824 | Batch size: 32 | lr: 0.1 | Duration: 4.80s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0099 | Steps: 2 | Val loss: 8.8691 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=45559)[0m top1: 0.4295708955223881
[2m[36m(func pid=45559)[0m top5: 0.9500932835820896
[2m[36m(func pid=45559)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=45559)[0m f1_macro: 0.4104621918885897
[2m[36m(func pid=45559)[0m f1_weighted: 0.4402304025202208
[2m[36m(func pid=45559)[0m f1_per_class: [0.568, 0.478, 0.667, 0.579, 0.286, 0.298, 0.396, 0.27, 0.211, 0.353]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.3400186567164179
[2m[36m(func pid=45033)[0m top5: 0.9053171641791045
[2m[36m(func pid=45033)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=45033)[0m f1_macro: 0.3125479815439978
[2m[36m(func pid=45033)[0m f1_weighted: 0.3483572488334567
[2m[36m(func pid=45033)[0m f1_per_class: [0.342, 0.381, 0.558, 0.465, 0.107, 0.197, 0.307, 0.304, 0.229, 0.235]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:12 (running for 00:34:47.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.075 |      0.313 |                   48 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.41  |                   49 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.004 |      0.165 |                   25 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.93  |      0.075 |                    1 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m top1: 0.2080223880597015
[2m[36m(func pid=56084)[0m top5: 0.7327425373134329
[2m[36m(func pid=56084)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=56084)[0m f1_macro: 0.07545912683573865
[2m[36m(func pid=56084)[0m f1_weighted: 0.09675918020991098
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.365, 0.0, 0.0, 0.104, 0.282, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.20102611940298507
[2m[36m(func pid=50583)[0m top5: 0.683768656716418
[2m[36m(func pid=50583)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=50583)[0m f1_macro: 0.17425154530877043
[2m[36m(func pid=50583)[0m f1_weighted: 0.13054815319558086
[2m[36m(func pid=50583)[0m f1_per_class: [0.244, 0.366, 0.364, 0.109, 0.0, 0.0, 0.006, 0.35, 0.199, 0.105]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.1608 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0878 | Steps: 2 | Val loss: 1.7648 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1747 | Steps: 2 | Val loss: 7.9345 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1073 | Steps: 2 | Val loss: 28.6754 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=45559)[0m top1: 0.4398320895522388
[2m[36m(func pid=45559)[0m top5: 0.9510261194029851
[2m[36m(func pid=45559)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=45559)[0m f1_macro: 0.42113761082182133
[2m[36m(func pid=45559)[0m f1_weighted: 0.45237480568649724
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.49, 0.667, 0.583, 0.289, 0.338, 0.408, 0.265, 0.219, 0.371]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.34095149253731344
[2m[36m(func pid=45033)[0m top5: 0.9043843283582089
[2m[36m(func pid=45033)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=45033)[0m f1_macro: 0.3126308866405053
[2m[36m(func pid=45033)[0m f1_weighted: 0.3462109555689864
[2m[36m(func pid=45033)[0m f1_per_class: [0.368, 0.376, 0.533, 0.467, 0.11, 0.197, 0.295, 0.332, 0.223, 0.225]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.24113805970149255
[2m[36m(func pid=50583)[0m top5: 0.7952425373134329
[2m[36m(func pid=50583)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=50583)[0m f1_macro: 0.1699404739722386
[2m[36m(func pid=50583)[0m f1_weighted: 0.2076649003511158
[2m[36m(func pid=50583)[0m f1_per_class: [0.169, 0.392, 0.156, 0.295, 0.0, 0.016, 0.095, 0.278, 0.181, 0.118]
== Status ==
Current time: 2024-01-07 03:54:18 (running for 00:34:53.09)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.088 |      0.313 |                   49 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.421 |                   50 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.175 |      0.17  |                   27 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.93  |      0.075 |                    1 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.022388059701492536
[2m[36m(func pid=56084)[0m top5: 0.21455223880597016
[2m[36m(func pid=56084)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=56084)[0m f1_macro: 0.04208733071452077
[2m[36m(func pid=56084)[0m f1_weighted: 0.01413685029493043
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.188, 0.031, 0.183]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.1896 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1062 | Steps: 2 | Val loss: 1.7661 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7839 | Steps: 2 | Val loss: 12.7422 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.5864 | Steps: 2 | Val loss: 786.3685 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=45559)[0m top1: 0.43423507462686567
[2m[36m(func pid=45559)[0m top5: 0.9510261194029851
[2m[36m(func pid=45559)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=45559)[0m f1_macro: 0.41543267489694397
[2m[36m(func pid=45559)[0m f1_weighted: 0.4474866234491634
[2m[36m(func pid=45559)[0m f1_per_class: [0.581, 0.478, 0.667, 0.584, 0.292, 0.303, 0.414, 0.258, 0.217, 0.361]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.34048507462686567
[2m[36m(func pid=45033)[0m top5: 0.9067164179104478
[2m[36m(func pid=45033)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=45033)[0m f1_macro: 0.31523635083581947
[2m[36m(func pid=45033)[0m f1_weighted: 0.34005745132702114
[2m[36m(func pid=45033)[0m f1_per_class: [0.373, 0.385, 0.533, 0.472, 0.112, 0.196, 0.261, 0.34, 0.232, 0.248]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:23 (running for 00:34:58.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.106 |      0.315 |                   50 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.415 |                   51 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.784 |      0.113 |                   28 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.107 |      0.042 |                    2 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.16791044776119404
[2m[36m(func pid=50583)[0m top5: 0.7681902985074627
[2m[36m(func pid=50583)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=50583)[0m f1_macro: 0.11295892549124709
[2m[36m(func pid=50583)[0m f1_weighted: 0.09934119756330059
[2m[36m(func pid=50583)[0m f1_per_class: [0.1, 0.369, 0.066, 0.007, 0.0, 0.0, 0.022, 0.292, 0.233, 0.041]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.02005597014925373
[2m[36m(func pid=56084)[0m top5: 0.44869402985074625
[2m[36m(func pid=56084)[0m f1_micro: 0.02005597014925373
[2m[36m(func pid=56084)[0m f1_macro: 0.005566343042071197
[2m[36m(func pid=56084)[0m f1_weighted: 0.0011423465198280441
[2m[36m(func pid=56084)[0m f1_per_class: [0.056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.1901 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0687 | Steps: 2 | Val loss: 1.7541 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9053 | Steps: 2 | Val loss: 15.0430 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=45559)[0m top1: 0.4398320895522388
[2m[36m(func pid=45559)[0m top5: 0.9510261194029851
[2m[36m(func pid=45559)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=45559)[0m f1_macro: 0.4205732890240589
[2m[36m(func pid=45559)[0m f1_weighted: 0.4535872107000261
[2m[36m(func pid=45559)[0m f1_per_class: [0.577, 0.479, 0.667, 0.577, 0.292, 0.34, 0.425, 0.257, 0.219, 0.371]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 11.0513 | Steps: 2 | Val loss: 4647475.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=45033)[0m top1: 0.3423507462686567
[2m[36m(func pid=45033)[0m top5: 0.9067164179104478
[2m[36m(func pid=45033)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=45033)[0m f1_macro: 0.3195147293347028
[2m[36m(func pid=45033)[0m f1_weighted: 0.33563111449968525
[2m[36m(func pid=45033)[0m f1_per_class: [0.384, 0.393, 0.558, 0.481, 0.122, 0.196, 0.229, 0.345, 0.24, 0.246]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:28 (running for 00:35:03.64)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.069 |      0.32  |                   51 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.003 |      0.421 |                   52 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.905 |      0.044 |                   29 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.586 |      0.006 |                    3 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.045242537313432835
[2m[36m(func pid=50583)[0m top5: 0.3656716417910448
[2m[36m(func pid=50583)[0m f1_micro: 0.045242537313432835
[2m[36m(func pid=50583)[0m f1_macro: 0.044323854110417664
[2m[36m(func pid=50583)[0m f1_weighted: 0.02103891165947421
[2m[36m(func pid=50583)[0m f1_per_class: [0.062, 0.091, 0.186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.079, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0045 | Steps: 2 | Val loss: 2.1981 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1006 | Steps: 2 | Val loss: 1.7565 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0532 | Steps: 2 | Val loss: 16.2750 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=45559)[0m top1: 0.43330223880597013
[2m[36m(func pid=45559)[0m top5: 0.9533582089552238
[2m[36m(func pid=45559)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=45559)[0m f1_macro: 0.4137398092498755
[2m[36m(func pid=45559)[0m f1_weighted: 0.4489144950274988
[2m[36m(func pid=45559)[0m f1_per_class: [0.577, 0.462, 0.667, 0.578, 0.28, 0.309, 0.433, 0.251, 0.209, 0.371]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 14.1291 | Steps: 2 | Val loss: 9058050048.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=45033)[0m top1: 0.3400186567164179
[2m[36m(func pid=45033)[0m top5: 0.9067164179104478
[2m[36m(func pid=45033)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=45033)[0m f1_macro: 0.3178245037497942
[2m[36m(func pid=45033)[0m f1_weighted: 0.32527825610094113
[2m[36m(func pid=45033)[0m f1_per_class: [0.4, 0.409, 0.533, 0.484, 0.134, 0.204, 0.18, 0.341, 0.237, 0.256]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:34 (running for 00:35:08.88)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.101 |      0.318 |                   52 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.004 |      0.414 |                   53 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.053 |      0.078 |                   30 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  11.051 |      0.011 |                    4 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.10634328358208955
[2m[36m(func pid=50583)[0m top5: 0.4244402985074627
[2m[36m(func pid=50583)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=50583)[0m f1_macro: 0.0780018269036798
[2m[36m(func pid=50583)[0m f1_weighted: 0.0708601246652142
[2m[36m(func pid=50583)[0m f1_per_class: [0.086, 0.369, 0.143, 0.0, 0.018, 0.0, 0.0, 0.0, 0.121, 0.043]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.01166044776119403
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=56084)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=56084)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.2120 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0654 | Steps: 2 | Val loss: 1.7606 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7217 | Steps: 2 | Val loss: 19.0548 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=45559)[0m top1: 0.4337686567164179
[2m[36m(func pid=45559)[0m top5: 0.9542910447761194
[2m[36m(func pid=45559)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=45559)[0m f1_macro: 0.41394226206605644
[2m[36m(func pid=45559)[0m f1_weighted: 0.44872997966701966
[2m[36m(func pid=45559)[0m f1_per_class: [0.562, 0.462, 0.667, 0.583, 0.292, 0.31, 0.43, 0.241, 0.211, 0.382]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 16.0732 | Steps: 2 | Val loss: 341261824.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45033)[0m top1: 0.33722014925373134
[2m[36m(func pid=45033)[0m top5: 0.9039179104477612
[2m[36m(func pid=45033)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=45033)[0m f1_macro: 0.31604268418409387
[2m[36m(func pid=45033)[0m f1_weighted: 0.32095449019496874
[2m[36m(func pid=45033)[0m f1_per_class: [0.388, 0.401, 0.558, 0.486, 0.145, 0.2, 0.172, 0.335, 0.239, 0.237]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:39 (running for 00:35:14.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.065 |      0.316 |                   53 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.414 |                   54 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.722 |      0.088 |                   31 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  14.129 |      0.002 |                    5 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.15391791044776118
[2m[36m(func pid=50583)[0m top5: 0.6315298507462687
[2m[36m(func pid=50583)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=50583)[0m f1_macro: 0.08785248307762131
[2m[36m(func pid=50583)[0m f1_weighted: 0.11277805010437976
[2m[36m(func pid=50583)[0m f1_per_class: [0.123, 0.471, 0.0, 0.0, 0.0, 0.0, 0.068, 0.136, 0.0, 0.08]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.2300 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=56084)[0m top1: 0.2887126865671642
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=56084)[0m f1_macro: 0.054501264576056585
[2m[36m(func pid=56084)[0m f1_weighted: 0.14852929462459064
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.493, 0.0, 0.052, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0832 | Steps: 2 | Val loss: 1.7610 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.1794 | Steps: 2 | Val loss: 16.6888 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=45559)[0m top1: 0.4375
[2m[36m(func pid=45559)[0m top5: 0.9566231343283582
[2m[36m(func pid=45559)[0m f1_micro: 0.4375
[2m[36m(func pid=45559)[0m f1_macro: 0.41370468554595324
[2m[36m(func pid=45559)[0m f1_weighted: 0.45171547463776207
[2m[36m(func pid=45559)[0m f1_per_class: [0.559, 0.458, 0.667, 0.585, 0.283, 0.335, 0.431, 0.239, 0.216, 0.364]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 15.4421 | Steps: 2 | Val loss: 92913936.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=45033)[0m top1: 0.3400186567164179
[2m[36m(func pid=45033)[0m top5: 0.9048507462686567
[2m[36m(func pid=45033)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=45033)[0m f1_macro: 0.3206938754250435
[2m[36m(func pid=45033)[0m f1_weighted: 0.3211732964654003
[2m[36m(func pid=45033)[0m f1_per_class: [0.413, 0.415, 0.585, 0.488, 0.15, 0.189, 0.164, 0.329, 0.251, 0.222]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:44 (running for 00:35:19.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.083 |      0.321 |                   54 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.414 |                   55 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   3.179 |      0.056 |                   32 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  16.073 |      0.055 |                    6 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.1166044776119403
[2m[36m(func pid=50583)[0m top5: 0.7896455223880597
[2m[36m(func pid=50583)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.05568986215584265
[2m[36m(func pid=50583)[0m f1_weighted: 0.09065498234018168
[2m[36m(func pid=50583)[0m f1_per_class: [0.011, 0.282, 0.0, 0.0, 0.0, 0.0, 0.111, 0.153, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.2503 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0549 | Steps: 2 | Val loss: 1.7594 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=45559)[0m top1: 0.44029850746268656
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=45559)[0m f1_macro: 0.4133842296202846
[2m[36m(func pid=45559)[0m f1_weighted: 0.4556265635046876
[2m[36m(func pid=45559)[0m f1_per_class: [0.553, 0.453, 0.667, 0.589, 0.28, 0.326, 0.449, 0.232, 0.21, 0.375]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.3347 | Steps: 2 | Val loss: 24.1355 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 6.9951 | Steps: 2 | Val loss: 23548904.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45033)[0m top1: 0.33722014925373134
[2m[36m(func pid=45033)[0m top5: 0.9039179104477612
[2m[36m(func pid=45033)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=45033)[0m f1_macro: 0.3199908816818697
[2m[36m(func pid=45033)[0m f1_weighted: 0.3142981941210573
[2m[36m(func pid=45033)[0m f1_per_class: [0.42, 0.413, 0.585, 0.489, 0.153, 0.192, 0.139, 0.326, 0.261, 0.221]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:49 (running for 00:35:24.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.055 |      0.32  |                   55 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.413 |                   56 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.335 |      0.073 |                   33 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  15.442 |      0.011 |                    7 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.17257462686567165
[2m[36m(func pid=50583)[0m top5: 0.7658582089552238
[2m[36m(func pid=50583)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=50583)[0m f1_macro: 0.07295442465307136
[2m[36m(func pid=50583)[0m f1_weighted: 0.11843654450434349
[2m[36m(func pid=50583)[0m f1_per_class: [0.05, 0.005, 0.039, 0.0, 0.0, 0.0, 0.331, 0.304, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.2619 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=56084)[0m top1: 0.03311567164179104
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=56084)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=56084)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0507 | Steps: 2 | Val loss: 1.7548 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=45559)[0m top1: 0.4430970149253731
[2m[36m(func pid=45559)[0m top5: 0.9556902985074627
[2m[36m(func pid=45559)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=45559)[0m f1_macro: 0.4151896214901923
[2m[36m(func pid=45559)[0m f1_weighted: 0.45820781491960333
[2m[36m(func pid=45559)[0m f1_per_class: [0.553, 0.452, 0.667, 0.584, 0.283, 0.345, 0.456, 0.236, 0.214, 0.364]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.0948 | Steps: 2 | Val loss: 48.1108 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 13.9712 | Steps: 2 | Val loss: 1857682432.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=45033)[0m top1: 0.33955223880597013
[2m[36m(func pid=45033)[0m top5: 0.9057835820895522
[2m[36m(func pid=45033)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=45033)[0m f1_macro: 0.32555307592297206
[2m[36m(func pid=45033)[0m f1_weighted: 0.31464843944085935
[2m[36m(func pid=45033)[0m f1_per_class: [0.429, 0.413, 0.632, 0.496, 0.155, 0.189, 0.134, 0.327, 0.254, 0.227]
[2m[36m(func pid=45033)[0m 
== Status ==
Current time: 2024-01-07 03:54:55 (running for 00:35:29.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.051 |      0.326 |                   56 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.415 |                   57 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.095 |      0.045 |                   34 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   6.995 |      0.006 |                    8 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.07882462686567164
[2m[36m(func pid=50583)[0m top5: 0.8232276119402985
[2m[36m(func pid=50583)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=50583)[0m f1_macro: 0.0453263583583023
[2m[36m(func pid=50583)[0m f1_weighted: 0.03669779993049161
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.149, 0.073, 0.007, 0.083, 0.0, 0.0, 0.141, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.2829 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0526 | Steps: 2 | Val loss: 1.7542 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=45559)[0m top1: 0.44029850746268656
[2m[36m(func pid=45559)[0m top5: 0.9552238805970149
[2m[36m(func pid=45559)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=45559)[0m f1_macro: 0.4130082386604621
[2m[36m(func pid=45559)[0m f1_weighted: 0.4571582362653828
[2m[36m(func pid=45559)[0m f1_per_class: [0.549, 0.458, 0.667, 0.587, 0.28, 0.333, 0.452, 0.229, 0.211, 0.364]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6063 | Steps: 2 | Val loss: 63.6926 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 4.9131 | Steps: 2 | Val loss: 286320160.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:55:00 (running for 00:35:34.97)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.053 |      0.324 |                   57 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.413 |                   58 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.095 |      0.045 |                   34 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  13.971 |      0.011 |                    9 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.33908582089552236
[2m[36m(func pid=45033)[0m top5: 0.90625
[2m[36m(func pid=45033)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=45033)[0m f1_macro: 0.3242523386218999
[2m[36m(func pid=45033)[0m f1_weighted: 0.3128381953708041
[2m[36m(func pid=45033)[0m f1_per_class: [0.448, 0.411, 0.615, 0.498, 0.145, 0.175, 0.131, 0.328, 0.257, 0.234]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.013526119402985074
[2m[36m(func pid=50583)[0m top5: 0.5494402985074627
[2m[36m(func pid=50583)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=50583)[0m f1_macro: 0.005397896035454953
[2m[36m(func pid=50583)[0m f1_weighted: 0.0033671428438476346
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.005, 0.007, 0.0, 0.0, 0.0, 0.0, 0.041, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.3000 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m top1: 0.44076492537313433
[2m[36m(func pid=45559)[0m top5: 0.9542910447761194
[2m[36m(func pid=45559)[0m f1_micro: 0.44076492537313433
[2m[36m(func pid=45559)[0m f1_macro: 0.41042739394979966
[2m[36m(func pid=45559)[0m f1_weighted: 0.45834128315354467
[2m[36m(func pid=45559)[0m f1_per_class: [0.549, 0.454, 0.667, 0.586, 0.262, 0.327, 0.462, 0.228, 0.211, 0.358]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0531 | Steps: 2 | Val loss: 1.7566 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.8100 | Steps: 2 | Val loss: 68.4833 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 14.7563 | Steps: 2 | Val loss: 82301520.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 03:55:05 (running for 00:35:40.33)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.053 |      0.325 |                   58 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.41  |                   59 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.606 |      0.005 |                   35 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.913 |      0.011 |                   10 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.3381529850746269
[2m[36m(func pid=45033)[0m top5: 0.9076492537313433
[2m[36m(func pid=45033)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=45033)[0m f1_macro: 0.32467967244699303
[2m[36m(func pid=45033)[0m f1_weighted: 0.31104454764670947
[2m[36m(func pid=45033)[0m f1_per_class: [0.438, 0.417, 0.615, 0.49, 0.153, 0.182, 0.126, 0.329, 0.256, 0.241]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m top1: 0.06949626865671642
[2m[36m(func pid=50583)[0m top5: 0.7639925373134329
[2m[36m(func pid=50583)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=50583)[0m f1_macro: 0.027688587425956906
[2m[36m(func pid=50583)[0m f1_weighted: 0.039968804975037575
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.18, 0.044, 0.026, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0061 | Steps: 2 | Val loss: 2.3558 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m top1: 0.4361007462686567
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=45559)[0m f1_macro: 0.4116528187079559
[2m[36m(func pid=45559)[0m f1_weighted: 0.4542631487828813
[2m[36m(func pid=45559)[0m f1_per_class: [0.588, 0.458, 0.667, 0.588, 0.25, 0.314, 0.446, 0.229, 0.213, 0.364]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.1081 | Steps: 2 | Val loss: 147.8824 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0527 | Steps: 2 | Val loss: 1.7573 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.7185 | Steps: 2 | Val loss: 28316306.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 03:55:10 (running for 00:35:45.46)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.053 |      0.325 |                   58 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.006 |      0.412 |                   60 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   3.108 |      0.059 |                   37 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  14.756 |      0.011 |                   11 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.13759328358208955
[2m[36m(func pid=50583)[0m top5: 0.5872201492537313
[2m[36m(func pid=50583)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=50583)[0m f1_macro: 0.05866404948365568
[2m[36m(func pid=50583)[0m f1_weighted: 0.15890769006360841
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.078, 0.019, 0.031, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m top1: 0.3362873134328358
[2m[36m(func pid=45033)[0m top5: 0.9067164179104478
[2m[36m(func pid=45033)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=45033)[0m f1_macro: 0.3227008947065272
[2m[36m(func pid=45033)[0m f1_weighted: 0.3069805864520605
[2m[36m(func pid=45033)[0m f1_per_class: [0.435, 0.416, 0.632, 0.497, 0.161, 0.19, 0.106, 0.325, 0.25, 0.216]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.3775 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m top1: 0.43236940298507465
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=45559)[0m f1_macro: 0.4099929607298522
[2m[36m(func pid=45559)[0m f1_weighted: 0.45005760748701246
[2m[36m(func pid=45559)[0m f1_per_class: [0.562, 0.455, 0.686, 0.585, 0.259, 0.313, 0.44, 0.223, 0.208, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4721 | Steps: 2 | Val loss: 549.1520 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0437 | Steps: 2 | Val loss: 1.7550 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 12.9458 | Steps: 2 | Val loss: 76190312.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:55:15 (running for 00:35:50.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.053 |      0.323 |                   59 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.41  |                   61 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.472 |      0.004 |                   38 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.718 |      0.011 |                   12 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.010261194029850746
[2m[36m(func pid=50583)[0m top5: 0.527518656716418
[2m[36m(func pid=50583)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=50583)[0m f1_macro: 0.004148633779570879
[2m[36m(func pid=50583)[0m f1_weighted: 0.008237534333421835
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.012, 0.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0039 | Steps: 2 | Val loss: 2.3835 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=45033)[0m top1: 0.33675373134328357
[2m[36m(func pid=45033)[0m top5: 0.9081156716417911
[2m[36m(func pid=45033)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=45033)[0m f1_macro: 0.3262777659120973
[2m[36m(func pid=45033)[0m f1_weighted: 0.3014562119775624
[2m[36m(func pid=45033)[0m f1_per_class: [0.444, 0.419, 0.649, 0.499, 0.171, 0.194, 0.079, 0.332, 0.253, 0.222]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.2980410447761194
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=56084)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=56084)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.5478 | Steps: 2 | Val loss: 588.8088 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=45559)[0m top1: 0.4319029850746269
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=45559)[0m f1_macro: 0.40748875130319123
[2m[36m(func pid=45559)[0m f1_weighted: 0.44850608306330114
[2m[36m(func pid=45559)[0m f1_per_class: [0.549, 0.451, 0.667, 0.577, 0.277, 0.334, 0.437, 0.227, 0.208, 0.348]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0402 | Steps: 2 | Val loss: 1.7565 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 11.9162 | Steps: 2 | Val loss: 1712043264.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 03:55:20 (running for 00:35:55.74)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.044 |      0.326 |                   60 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.004 |      0.407 |                   62 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.548 |      0.005 |                   39 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  12.946 |      0.046 |                   13 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.009328358208955223
[2m[36m(func pid=50583)[0m top5: 0.5475746268656716
[2m[36m(func pid=50583)[0m f1_micro: 0.009328358208955223
[2m[36m(func pid=50583)[0m f1_macro: 0.004753029696127173
[2m[36m(func pid=50583)[0m f1_weighted: 0.006493991409906946
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.032, 0.012, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.4176 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=45033)[0m top1: 0.3358208955223881
[2m[36m(func pid=45033)[0m top5: 0.9076492537313433
[2m[36m(func pid=45033)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=45033)[0m f1_macro: 0.3252376619459789
[2m[36m(func pid=45033)[0m f1_weighted: 0.3007258817305244
[2m[36m(func pid=45033)[0m f1_per_class: [0.45, 0.418, 0.632, 0.496, 0.173, 0.196, 0.079, 0.332, 0.255, 0.222]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.2980410447761194
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=56084)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=56084)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4099 | Steps: 2 | Val loss: 540.0175 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=45559)[0m top1: 0.43283582089552236
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=45559)[0m f1_macro: 0.40924990951981954
[2m[36m(func pid=45559)[0m f1_weighted: 0.45144118434451935
[2m[36m(func pid=45559)[0m f1_per_class: [0.53, 0.458, 0.706, 0.586, 0.262, 0.322, 0.441, 0.227, 0.204, 0.358]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0552 | Steps: 2 | Val loss: 1.7626 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.7184 | Steps: 2 | Val loss: 315961664.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 03:55:25 (running for 00:36:00.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.04  |      0.325 |                   61 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.409 |                   63 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.41  |      0.061 |                   40 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  11.916 |      0.046 |                   14 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.12313432835820895
[2m[36m(func pid=50583)[0m top5: 0.49906716417910446
[2m[36m(func pid=50583)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=50583)[0m f1_macro: 0.060876300306563214
[2m[36m(func pid=50583)[0m f1_weighted: 0.08159866596801886
[2m[36m(func pid=50583)[0m f1_per_class: [0.059, 0.032, 0.231, 0.262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.4171 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=45033)[0m top1: 0.33955223880597013
[2m[36m(func pid=45033)[0m top5: 0.9043843283582089
[2m[36m(func pid=45033)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=45033)[0m f1_macro: 0.3288355855971393
[2m[36m(func pid=45033)[0m f1_weighted: 0.30407903660391944
[2m[36m(func pid=45033)[0m f1_per_class: [0.462, 0.428, 0.632, 0.495, 0.17, 0.191, 0.085, 0.336, 0.263, 0.228]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9502 | Steps: 2 | Val loss: 774.6881 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=45559)[0m top1: 0.43796641791044777
[2m[36m(func pid=45559)[0m top5: 0.9533582089552238
[2m[36m(func pid=45559)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=45559)[0m f1_macro: 0.41137360944425233
[2m[36m(func pid=45559)[0m f1_weighted: 0.4556544482420096
[2m[36m(func pid=45559)[0m f1_per_class: [0.543, 0.452, 0.686, 0.591, 0.28, 0.332, 0.448, 0.225, 0.208, 0.348]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0371 | Steps: 2 | Val loss: 1.7584 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 03:55:31 (running for 00:36:05.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.055 |      0.329 |                   62 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.411 |                   64 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.95  |      0.004 |                   41 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.718 |      0.011 |                   15 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=50583)[0m top1: 0.020522388059701493
[2m[36m(func pid=50583)[0m top5: 0.5018656716417911
[2m[36m(func pid=50583)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=50583)[0m f1_macro: 0.00402930402930403
[2m[36m(func pid=50583)[0m f1_weighted: 0.0008269094089989613
[2m[36m(func pid=50583)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.3655 | Steps: 2 | Val loss: 85105080.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0035 | Steps: 2 | Val loss: 2.4399 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=45033)[0m top1: 0.3423507462686567
[2m[36m(func pid=45033)[0m top5: 0.9071828358208955
[2m[36m(func pid=45033)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=45033)[0m f1_macro: 0.3302573441303785
[2m[36m(func pid=45033)[0m f1_weighted: 0.30617535155515513
[2m[36m(func pid=45033)[0m f1_per_class: [0.46, 0.435, 0.649, 0.501, 0.173, 0.194, 0.082, 0.332, 0.255, 0.222]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7178 | Steps: 2 | Val loss: 727.7968 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=45559)[0m top1: 0.43796641791044777
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=45559)[0m f1_macro: 0.411773556054175
[2m[36m(func pid=45559)[0m f1_weighted: 0.4552019713208719
[2m[36m(func pid=45559)[0m f1_per_class: [0.543, 0.452, 0.667, 0.591, 0.289, 0.346, 0.442, 0.223, 0.207, 0.358]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0443 | Steps: 2 | Val loss: 1.7429 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=50583)[0m top1: 0.020988805970149255
[2m[36m(func pid=50583)[0m top5: 0.5419776119402985
[2m[36m(func pid=50583)[0m f1_micro: 0.020988805970149255
[2m[36m(func pid=50583)[0m f1_macro: 0.004577237788246963
[2m[36m(func pid=50583)[0m f1_weighted: 0.0017587413169708119
[2m[36m(func pid=50583)[0m f1_per_class: [0.04, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 20.2826 | Steps: 2 | Val loss: 41896488.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0017 | Steps: 2 | Val loss: 2.4851 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 03:55:37 (running for 00:36:12.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.044 |      0.338 |                   64 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.004 |      0.412 |                   65 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.718 |      0.005 |                   42 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.366 |      0.011 |                   16 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.3451492537313433
[2m[36m(func pid=45033)[0m top5: 0.9090485074626866
[2m[36m(func pid=45033)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=45033)[0m f1_macro: 0.33754540420174983
[2m[36m(func pid=45033)[0m f1_weighted: 0.31291073343865367
[2m[36m(func pid=45033)[0m f1_per_class: [0.471, 0.435, 0.686, 0.504, 0.176, 0.2, 0.098, 0.33, 0.257, 0.218]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9762 | Steps: 2 | Val loss: 400.5367 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45559)[0m top1: 0.43097014925373134
[2m[36m(func pid=45559)[0m top5: 0.9533582089552238
[2m[36m(func pid=45559)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=45559)[0m f1_macro: 0.4055869291427475
[2m[36m(func pid=45559)[0m f1_weighted: 0.4481984298123869
[2m[36m(func pid=45559)[0m f1_per_class: [0.53, 0.445, 0.667, 0.585, 0.272, 0.326, 0.435, 0.231, 0.209, 0.355]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0300 | Steps: 2 | Val loss: 1.7397 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=50583)[0m top1: 0.04057835820895522
[2m[36m(func pid=50583)[0m top5: 0.6091417910447762
[2m[36m(func pid=50583)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=50583)[0m f1_macro: 0.022297792786923225
[2m[36m(func pid=50583)[0m f1_weighted: 0.033000527146982366
[2m[36m(func pid=50583)[0m f1_per_class: [0.046, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.026, 0.041]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 10.7816 | Steps: 2 | Val loss: 23526810.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.4846 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=45033)[0m top1: 0.34654850746268656
[2m[36m(func pid=45033)[0m top5: 0.9109141791044776
[2m[36m(func pid=45033)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=45033)[0m f1_macro: 0.33940688672030933
[2m[36m(func pid=45033)[0m f1_weighted: 0.3145631821039114
[2m[36m(func pid=45033)[0m f1_per_class: [0.487, 0.431, 0.686, 0.509, 0.18, 0.201, 0.101, 0.328, 0.249, 0.222]
== Status ==
Current time: 2024-01-07 03:55:43 (running for 00:36:18.13)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.03  |      0.339 |                   65 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.002 |      0.406 |                   66 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.976 |      0.022 |                   43 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  20.283 |      0.011 |                   17 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.0607 | Steps: 2 | Val loss: 636.7319 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=45559)[0m top1: 0.4319029850746269
[2m[36m(func pid=45559)[0m top5: 0.9538246268656716
[2m[36m(func pid=45559)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=45559)[0m f1_macro: 0.4062236638297893
[2m[36m(func pid=45559)[0m f1_weighted: 0.4484357598063918
[2m[36m(func pid=45559)[0m f1_per_class: [0.53, 0.447, 0.667, 0.583, 0.272, 0.329, 0.435, 0.233, 0.211, 0.355]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0398 | Steps: 2 | Val loss: 1.7495 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=50583)[0m top1: 0.010727611940298507
[2m[36m(func pid=50583)[0m top5: 0.4939365671641791
[2m[36m(func pid=50583)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=50583)[0m f1_macro: 0.0023150478107700047
[2m[36m(func pid=50583)[0m f1_weighted: 0.00026994494062150244
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.3486 | Steps: 2 | Val loss: 9648959.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0264 | Steps: 2 | Val loss: 2.5388 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 03:55:48 (running for 00:36:23.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.04  |      0.337 |                   66 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.406 |                   67 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.061 |      0.002 |                   44 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  10.782 |      0.011 |                   18 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.34421641791044777
[2m[36m(func pid=45033)[0m top5: 0.9085820895522388
[2m[36m(func pid=45033)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=45033)[0m f1_macro: 0.3367655312456671
[2m[36m(func pid=45033)[0m f1_weighted: 0.31136622184680246
[2m[36m(func pid=45033)[0m f1_per_class: [0.471, 0.437, 0.686, 0.511, 0.176, 0.202, 0.087, 0.32, 0.251, 0.227]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8721 | Steps: 2 | Val loss: 1083.2635 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m top1: 0.42723880597014924
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=45559)[0m f1_macro: 0.40340032597614384
[2m[36m(func pid=45559)[0m f1_weighted: 0.44426535273265993
[2m[36m(func pid=45559)[0m f1_per_class: [0.519, 0.448, 0.667, 0.583, 0.257, 0.329, 0.422, 0.23, 0.214, 0.367]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0368 | Steps: 2 | Val loss: 1.7542 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=50583)[0m top1: 0.010727611940298507
[2m[36m(func pid=50583)[0m top5: 0.49673507462686567
[2m[36m(func pid=50583)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=50583)[0m f1_macro: 0.002184235517568851
[2m[36m(func pid=50583)[0m f1_weighted: 0.00025469164150756195
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 4.0673 | Steps: 2 | Val loss: 4097012.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0039 | Steps: 2 | Val loss: 2.5654 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 03:55:54 (running for 00:36:28.94)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.037 |      0.338 |                   67 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.026 |      0.403 |                   68 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.872 |      0.002 |                   45 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.349 |      0.011 |                   19 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.34654850746268656
[2m[36m(func pid=45033)[0m top5: 0.9076492537313433
[2m[36m(func pid=45033)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=45033)[0m f1_macro: 0.3376874830710891
[2m[36m(func pid=45033)[0m f1_weighted: 0.3130093346522887
[2m[36m(func pid=45033)[0m f1_per_class: [0.45, 0.431, 0.706, 0.518, 0.175, 0.2, 0.09, 0.326, 0.252, 0.229]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6351 | Steps: 2 | Val loss: 846.0704 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=45559)[0m top1: 0.4281716417910448
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=45559)[0m f1_macro: 0.4107342064836734
[2m[36m(func pid=45559)[0m f1_weighted: 0.44648143282515584
[2m[36m(func pid=45559)[0m f1_per_class: [0.545, 0.454, 0.686, 0.595, 0.264, 0.311, 0.419, 0.233, 0.2, 0.4]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05783582089552239
[2m[36m(func pid=56084)[0m top5: 0.5149253731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=56084)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=56084)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.016791044776119403
[2m[36m(func pid=50583)[0m top5: 0.42117537313432835
[2m[36m(func pid=50583)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.011248299777204453
[2m[36m(func pid=50583)[0m f1_weighted: 0.014615774040016972
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.02]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0253 | Steps: 2 | Val loss: 1.7570 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.5755 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 4.0279 | Steps: 2 | Val loss: 2745932.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 03:55:59 (running for 00:36:34.44)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.025 |      0.339 |                   68 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.004 |      0.411 |                   69 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.635 |      0.011 |                   46 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.067 |      0.011 |                   20 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.34654850746268656
[2m[36m(func pid=45033)[0m top5: 0.9071828358208955
[2m[36m(func pid=45033)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=45033)[0m f1_macro: 0.3385166710205214
[2m[36m(func pid=45033)[0m f1_weighted: 0.3099541203068825
[2m[36m(func pid=45033)[0m f1_per_class: [0.456, 0.429, 0.706, 0.521, 0.176, 0.2, 0.076, 0.326, 0.261, 0.234]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.6069 | Steps: 2 | Val loss: 723.7027 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=45559)[0m top1: 0.4230410447761194
[2m[36m(func pid=45559)[0m top5: 0.9556902985074627
[2m[36m(func pid=45559)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=45559)[0m f1_macro: 0.40793723955684114
[2m[36m(func pid=45559)[0m f1_weighted: 0.44164048075539025
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.456, 0.667, 0.584, 0.264, 0.305, 0.412, 0.247, 0.2, 0.407]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05830223880597015
[2m[36m(func pid=56084)[0m top5: 0.5153917910447762
[2m[36m(func pid=56084)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=56084)[0m f1_macro: 0.011480108251171331
[2m[36m(func pid=56084)[0m f1_weighted: 0.007257303430866033
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0323 | Steps: 2 | Val loss: 1.7562 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=50583)[0m top1: 0.020988805970149255
[2m[36m(func pid=50583)[0m top5: 0.38759328358208955
[2m[36m(func pid=50583)[0m f1_micro: 0.020988805970149255
[2m[36m(func pid=50583)[0m f1_macro: 0.027074727731428928
[2m[36m(func pid=50583)[0m f1_weighted: 0.02348190539160335
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.13, 0.0, 0.0, 0.118, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.5669 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.1384 | Steps: 2 | Val loss: 1764711.7500 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=45033)[0m top1: 0.34841417910447764== Status ==
Current time: 2024-01-07 03:56:04 (running for 00:36:39.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.032 |      0.341 |                   69 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.408 |                   70 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.607 |      0.027 |                   47 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.028 |      0.011 |                   21 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=45033)[0m top5: 0.9081156716417911
[2m[36m(func pid=45033)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=45033)[0m f1_macro: 0.34114452195143175
[2m[36m(func pid=45033)[0m f1_weighted: 0.31021572988393326
[2m[36m(func pid=45033)[0m f1_per_class: [0.473, 0.448, 0.706, 0.516, 0.182, 0.201, 0.068, 0.333, 0.26, 0.224]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.4857 | Steps: 2 | Val loss: 809.8359 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=45559)[0m top1: 0.4239738805970149
[2m[36m(func pid=45559)[0m top5: 0.9538246268656716
[2m[36m(func pid=45559)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=45559)[0m f1_macro: 0.4113411657793469
[2m[36m(func pid=45559)[0m f1_weighted: 0.4435163678273519
[2m[36m(func pid=45559)[0m f1_per_class: [0.545, 0.467, 0.686, 0.578, 0.272, 0.303, 0.417, 0.252, 0.201, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.058768656716417914
[2m[36m(func pid=56084)[0m top5: 0.5167910447761194
[2m[36m(func pid=56084)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=56084)[0m f1_macro: 0.012032230720143247
[2m[36m(func pid=56084)[0m f1_weighted: 0.0081909867196548
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.013059701492537313
[2m[36m(func pid=50583)[0m top5: 0.46222014925373134
[2m[36m(func pid=50583)[0m f1_micro: 0.013059701492537313
[2m[36m(func pid=50583)[0m f1_macro: 0.020860880854888644
[2m[36m(func pid=50583)[0m f1_weighted: 0.013264342176066635
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.071, 0.0, 0.0, 0.118, 0.0, 0.0, 0.0, 0.0, 0.02]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0349 | Steps: 2 | Val loss: 1.7605 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.5919 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.9416 | Steps: 2 | Val loss: 1255935.1250 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 03:56:10 (running for 00:36:45.18)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.035 |      0.337 |                   70 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.411 |                   71 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.486 |      0.021 |                   48 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.138 |      0.012 |                   22 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.345615671641791
[2m[36m(func pid=45033)[0m top5: 0.9067164179104478
[2m[36m(func pid=45033)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=45033)[0m f1_macro: 0.3371426065502427
[2m[36m(func pid=45033)[0m f1_weighted: 0.30821216040076665
[2m[36m(func pid=45033)[0m f1_per_class: [0.486, 0.448, 0.667, 0.51, 0.176, 0.201, 0.068, 0.33, 0.258, 0.227]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0835 | Steps: 2 | Val loss: 1025.3276 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=45559)[0m top1: 0.42117537313432835
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=45559)[0m f1_macro: 0.4096696066223709
[2m[36m(func pid=45559)[0m f1_weighted: 0.44057517753314895
[2m[36m(func pid=45559)[0m f1_per_class: [0.553, 0.469, 0.667, 0.573, 0.262, 0.29, 0.411, 0.276, 0.205, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.060167910447761194
[2m[36m(func pid=56084)[0m top5: 0.5209888059701493
[2m[36m(func pid=56084)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=56084)[0m f1_macro: 0.01367421216156236
[2m[36m(func pid=56084)[0m f1_weighted: 0.010955856536226988
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.008861940298507462
[2m[36m(func pid=50583)[0m top5: 0.49673507462686567
[2m[36m(func pid=50583)[0m f1_micro: 0.008861940298507462
[2m[36m(func pid=50583)[0m f1_macro: 0.02730986887508626
[2m[36m(func pid=50583)[0m f1_weighted: 0.007345196893379893
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.032, 0.0, 0.0, 0.222, 0.0, 0.0, 0.0, 0.0, 0.019]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0258 | Steps: 2 | Val loss: 1.7548 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0005 | Steps: 2 | Val loss: 2.6166 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6928 | Steps: 2 | Val loss: 882246.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.0532 | Steps: 2 | Val loss: 1283.9142 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:56:15 (running for 00:36:50.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.026 |      0.343 |                   71 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.41  |                   72 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.084 |      0.027 |                   49 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.942 |      0.014 |                   23 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45033)[0m top1: 0.34841417910447764
[2m[36m(func pid=45033)[0m top5: 0.9085820895522388
[2m[36m(func pid=45033)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=45033)[0m f1_macro: 0.342973614261742
[2m[36m(func pid=45033)[0m f1_weighted: 0.31179752058924315
[2m[36m(func pid=45033)[0m f1_per_class: [0.486, 0.455, 0.706, 0.519, 0.19, 0.203, 0.068, 0.323, 0.243, 0.236]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=45559)[0m top1: 0.41697761194029853
[2m[36m(func pid=45559)[0m top5: 0.9552238805970149
[2m[36m(func pid=45559)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=45559)[0m f1_macro: 0.4058545458933368
[2m[36m(func pid=45559)[0m f1_weighted: 0.4381705019305579
[2m[36m(func pid=45559)[0m f1_per_class: [0.553, 0.478, 0.667, 0.565, 0.246, 0.275, 0.413, 0.269, 0.202, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.060167910447761194
[2m[36m(func pid=56084)[0m top5: 0.5270522388059702
[2m[36m(func pid=56084)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=56084)[0m f1_macro: 0.013502373020595579
[2m[36m(func pid=56084)[0m f1_weighted: 0.010971340939974697
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.021, 0.0, 0.0, 0.0, 0.0, 0.003, 0.111, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.011194029850746268
[2m[36m(func pid=50583)[0m top5: 0.503731343283582
[2m[36m(func pid=50583)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=50583)[0m f1_macro: 0.024726490861949026
[2m[36m(func pid=50583)[0m f1_weighted: 0.0019503837292384593
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.222, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0317 | Steps: 2 | Val loss: 1.7475 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.6451 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 10.4817 | Steps: 2 | Val loss: 630975.8125 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0926 | Steps: 2 | Val loss: 2203.3228 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 03:56:21 (running for 00:36:56.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.026 |      0.343 |                   71 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.405 |                   74 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   4.053 |      0.025 |                   50 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.693 |      0.014 |                   24 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.41511194029850745
[2m[36m(func pid=45559)[0m top5: 0.9561567164179104
[2m[36m(func pid=45559)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=45559)[0m f1_macro: 0.40464918296747887
[2m[36m(func pid=45559)[0m f1_weighted: 0.435509454314834
[2m[36m(func pid=45559)[0m f1_per_class: [0.545, 0.492, 0.667, 0.562, 0.231, 0.275, 0.397, 0.273, 0.21, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.34888059701492535
[2m[36m(func pid=45033)[0m top5: 0.9104477611940298
[2m[36m(func pid=45033)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=45033)[0m f1_macro: 0.34417616062579437
[2m[36m(func pid=45033)[0m f1_weighted: 0.3138403778910359
[2m[36m(func pid=45033)[0m f1_per_class: [0.496, 0.456, 0.706, 0.519, 0.186, 0.205, 0.073, 0.318, 0.246, 0.236]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.06110074626865672
[2m[36m(func pid=56084)[0m top5: 0.5373134328358209
[2m[36m(func pid=56084)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=56084)[0m f1_macro: 0.014858083093377212
[2m[36m(func pid=56084)[0m f1_weighted: 0.012748653892200864
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.01166044776119403
[2m[36m(func pid=50583)[0m top5: 0.5144589552238806
[2m[36m(func pid=50583)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=50583)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.6617 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0299 | Steps: 2 | Val loss: 1.7479 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3710 | Steps: 2 | Val loss: 386433.7812 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6160 | Steps: 2 | Val loss: 2440.3760 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 03:56:26 (running for 00:37:01.35)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.032 |      0.344 |                   72 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.402 |                   75 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.093 |      0.002 |                   51 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |  10.482 |      0.015 |                   25 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4118470149253731
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=45559)[0m f1_macro: 0.4023362604971695
[2m[36m(func pid=45559)[0m f1_weighted: 0.4320394660423289
[2m[36m(func pid=45559)[0m f1_per_class: [0.532, 0.499, 0.667, 0.552, 0.23, 0.268, 0.393, 0.278, 0.211, 0.393]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.3521455223880597
[2m[36m(func pid=45033)[0m top5: 0.9104477611940298
[2m[36m(func pid=45033)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=45033)[0m f1_macro: 0.34785933358981663
[2m[36m(func pid=45033)[0m f1_weighted: 0.31534044955577356
[2m[36m(func pid=45033)[0m f1_per_class: [0.515, 0.461, 0.706, 0.522, 0.196, 0.207, 0.071, 0.321, 0.246, 0.234]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.07042910447761194
[2m[36m(func pid=56084)[0m top5: 0.5601679104477612
[2m[36m(func pid=56084)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=56084)[0m f1_macro: 0.023718939446339543
[2m[36m(func pid=56084)[0m f1_weighted: 0.028302002538774653
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.115, 0.0, 0.0, 0.0, 0.0, 0.006, 0.116, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.01166044776119403
[2m[36m(func pid=50583)[0m top5: 0.5139925373134329
[2m[36m(func pid=50583)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=50583)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.6822 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0300 | Steps: 2 | Val loss: 1.7384 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 6.8818 | Steps: 2 | Val loss: 278433.8438 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0167 | Steps: 2 | Val loss: 1870.4413 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 03:56:31 (running for 00:37:06.52)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.03  |      0.348 |                   73 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.002 |      0.394 |                   76 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.616 |      0.002 |                   52 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.371 |      0.024 |                   26 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4085820895522388
[2m[36m(func pid=45559)[0m top5: 0.9552238805970149
[2m[36m(func pid=45559)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=45559)[0m f1_macro: 0.39417915029706885
[2m[36m(func pid=45559)[0m f1_weighted: 0.42826341409467034
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.501, 0.667, 0.547, 0.235, 0.268, 0.388, 0.274, 0.208, 0.316]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.353544776119403
[2m[36m(func pid=45033)[0m top5: 0.9151119402985075
[2m[36m(func pid=45033)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=45033)[0m f1_macro: 0.34910690287527035
[2m[36m(func pid=45033)[0m f1_weighted: 0.31672446853270014
[2m[36m(func pid=45033)[0m f1_per_class: [0.519, 0.464, 0.706, 0.526, 0.202, 0.204, 0.071, 0.321, 0.24, 0.238]
[2m[36m(func pid=45033)[0m 
[2m[36m(func pid=56084)[0m top1: 0.08022388059701492
[2m[36m(func pid=56084)[0m top5: 0.5867537313432836
[2m[36m(func pid=56084)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=56084)[0m f1_macro: 0.03117616526523409
[2m[36m(func pid=56084)[0m f1_weighted: 0.04115550013867132
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.178, 0.0, 0.0, 0.0, 0.0, 0.012, 0.122, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.01166044776119403
[2m[36m(func pid=50583)[0m top5: 0.5111940298507462
[2m[36m(func pid=50583)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.002315886984715146
[2m[36m(func pid=50583)[0m f1_weighted: 0.0002700427920610011
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.7103 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=45033)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0283 | Steps: 2 | Val loss: 1.7303 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6043 | Steps: 2 | Val loss: 205205.4062 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5900 | Steps: 2 | Val loss: 1485.7657 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 03:56:36 (running for 00:37:11.64)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.372
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00016 | RUNNING    | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.03  |      0.349 |                   74 |
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.397 |                   77 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.017 |      0.002 |                   53 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   6.882 |      0.031 |                   27 |
| train_2d480_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.40671641791044777
[2m[36m(func pid=45559)[0m top5: 0.9547574626865671
[2m[36m(func pid=45559)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=45559)[0m f1_macro: 0.3973366855568697
[2m[36m(func pid=45559)[0m f1_weighted: 0.42489551634823464
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.496, 0.667, 0.54, 0.237, 0.268, 0.381, 0.294, 0.208, 0.345]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=45033)[0m top1: 0.35447761194029853
[2m[36m(func pid=45033)[0m top5: 0.9165111940298507
[2m[36m(func pid=45033)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=45033)[0m f1_macro: 0.34899949993463314
[2m[36m(func pid=45033)[0m f1_weighted: 0.31821305885125223
[2m[36m(func pid=45033)[0m f1_per_class: [0.519, 0.468, 0.706, 0.527, 0.194, 0.205, 0.073, 0.316, 0.237, 0.245]
[2m[36m(func pid=56084)[0m top1: 0.06856343283582089
[2m[36m(func pid=56084)[0m top5: 0.5890858208955224
[2m[36m(func pid=56084)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=56084)[0m f1_macro: 0.021434749015062815
[2m[36m(func pid=56084)[0m f1_weighted: 0.026288710786661274
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.025, 0.12, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m top1: 0.01585820895522388
[2m[36m(func pid=50583)[0m top5: 0.5261194029850746
[2m[36m(func pid=50583)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=50583)[0m f1_macro: 0.00697671616469783
[2m[36m(func pid=50583)[0m f1_weighted: 0.008256969182729662
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.7317 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 5.1265 | Steps: 2 | Val loss: 126532.7109 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.1404 | Steps: 2 | Val loss: 1142.8080 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=45559)[0m top1: 0.4048507462686567
[2m[36m(func pid=45559)[0m top5: 0.9542910447761194
[2m[36m(func pid=45559)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=45559)[0m f1_macro: 0.397751011045629
[2m[36m(func pid=45559)[0m f1_weighted: 0.4240388591578341
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.5, 0.667, 0.542, 0.243, 0.256, 0.38, 0.282, 0.208, 0.361]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.06576492537313433
[2m[36m(func pid=56084)[0m top5: 0.5881529850746269
[2m[36m(func pid=56084)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=56084)[0m f1_macro: 0.016712638049295483
[2m[36m(func pid=56084)[0m f1_weighted: 0.02006968454533714
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043, 0.124, 0.0, 0.0]
[2m[36m(func pid=50583)[0m top1: 0.020522388059701493
[2m[36m(func pid=50583)[0m top5: 0.539179104477612
[2m[36m(func pid=50583)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=50583)[0m f1_macro: 0.011646979647722263
[2m[36m(func pid=50583)[0m f1_weighted: 0.01622885808824574
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.7435 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 03:56:42 (running for 00:37:16.93)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   78 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.59  |      0.007 |                   54 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.604 |      0.021 |                   28 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=62698)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=62698)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=62698)[0m Configuration completed!
[2m[36m(func pid=62698)[0m New optimizer parameters:
[2m[36m(func pid=62698)[0m SGD (
[2m[36m(func pid=62698)[0m Parameter Group 0
[2m[36m(func pid=62698)[0m     dampening: 0
[2m[36m(func pid=62698)[0m     differentiable: False
[2m[36m(func pid=62698)[0m     foreach: None
[2m[36m(func pid=62698)[0m     lr: 0.0001
[2m[36m(func pid=62698)[0m     maximize: False
[2m[36m(func pid=62698)[0m     momentum: 0.9
[2m[36m(func pid=62698)[0m     nesterov: False
[2m[36m(func pid=62698)[0m     weight_decay: 1e-05
[2m[36m(func pid=62698)[0m )
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m 
== Status ==
Current time: 2024-01-07 03:56:47 (running for 00:37:22.19)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   79 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.14  |      0.012 |                   55 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   5.127 |      0.017 |                   29 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.404384328358209
[2m[36m(func pid=45559)[0m top5: 0.9533582089552238
[2m[36m(func pid=45559)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=45559)[0m f1_macro: 0.3980014244209901
[2m[36m(func pid=45559)[0m f1_weighted: 0.42093860196787425
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.504, 0.667, 0.538, 0.243, 0.256, 0.368, 0.29, 0.213, 0.361]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6458 | Steps: 2 | Val loss: 994.9833 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 4.6628 | Steps: 2 | Val loss: 66804.6562 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1515 | Steps: 2 | Val loss: 2.5159 | Batch size: 32 | lr: 0.0001 | Duration: 4.53s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.7587 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=50583)[0m top1: 0.01912313432835821
[2m[36m(func pid=50583)[0m top5: 0.5387126865671642
[2m[36m(func pid=50583)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=50583)[0m f1_macro: 0.010060053524136036
[2m[36m(func pid=50583)[0m f1_weighted: 0.013834177176964115
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.073, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.07369402985074627
[2m[36m(func pid=56084)[0m top5: 0.5872201492537313
[2m[36m(func pid=56084)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=56084)[0m f1_macro: 0.02097398754385575
[2m[36m(func pid=56084)[0m f1_weighted: 0.02949916079845271
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.069, 0.137, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:56:52 (running for 00:37:27.21)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   79 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.646 |      0.01  |                   56 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.663 |      0.021 |                   30 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   3.152 |      0.04  |                    1 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.06716417910447761
[2m[36m(func pid=62698)[0m top5: 0.4864738805970149
[2m[36m(func pid=62698)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=62698)[0m f1_macro: 0.03993526500043466
[2m[36m(func pid=62698)[0m f1_weighted: 0.037507156622430565
[2m[36m(func pid=62698)[0m f1_per_class: [0.12, 0.01, 0.0, 0.085, 0.0, 0.019, 0.0, 0.106, 0.022, 0.037]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=45559)[0m top1: 0.40298507462686567
[2m[36m(func pid=45559)[0m top5: 0.9552238805970149
[2m[36m(func pid=45559)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=45559)[0m f1_macro: 0.3983635803981194
[2m[36m(func pid=45559)[0m f1_weighted: 0.4194354213897054
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.503, 0.667, 0.528, 0.25, 0.263, 0.372, 0.291, 0.213, 0.361]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.7588 | Steps: 2 | Val loss: 46036.8164 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4997 | Steps: 2 | Val loss: 812.3123 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1144 | Steps: 2 | Val loss: 2.5344 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.7822 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=50583)[0m top1: 0.016791044776119403
[2m[36m(func pid=50583)[0m top5: 0.5139925373134329
[2m[36m(func pid=50583)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.007946564117848753
[2m[36m(func pid=50583)[0m f1_weighted: 0.00981791666121904
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.0830223880597015
[2m[36m(func pid=56084)[0m top5: 0.5970149253731343
[2m[36m(func pid=56084)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=56084)[0m f1_macro: 0.02600837417087314
[2m[36m(func pid=56084)[0m f1_weighted: 0.04052120429148211
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.091, 0.153, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:56:57 (running for 00:37:32.64)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   80 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.5   |      0.008 |                   57 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.759 |      0.026 |                   31 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   3.114 |      0.032 |                    2 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.06343283582089553
[2m[36m(func pid=62698)[0m top5: 0.4864738805970149
[2m[36m(func pid=62698)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=62698)[0m f1_macro: 0.03234081386614499
[2m[36m(func pid=62698)[0m f1_weighted: 0.03483329746769242
[2m[36m(func pid=62698)[0m f1_per_class: [0.054, 0.01, 0.0, 0.081, 0.0, 0.019, 0.0, 0.103, 0.023, 0.033]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=45559)[0m top1: 0.40158582089552236
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=45559)[0m f1_macro: 0.39770245939950466
[2m[36m(func pid=45559)[0m f1_weighted: 0.41902799527049767
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.503, 0.686, 0.518, 0.248, 0.254, 0.383, 0.292, 0.212, 0.344]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3866 | Steps: 2 | Val loss: 591.9203 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.3301 | Steps: 2 | Val loss: 40022.0781 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0963 | Steps: 2 | Val loss: 2.5510 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.7912 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=50583)[0m top1: 0.01632462686567164
[2m[36m(func pid=50583)[0m top5: 0.44216417910447764
[2m[36m(func pid=50583)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=50583)[0m f1_macro: 0.01364623691522315
[2m[36m(func pid=50583)[0m f1_weighted: 0.008979146231874967
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.037, 0.0, 0.007, 0.069, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.0625
[2m[36m(func pid=56084)[0m top5: 0.6105410447761194
[2m[36m(func pid=56084)[0m f1_micro: 0.0625
[2m[36m(func pid=56084)[0m f1_macro: 0.01598709732235275
[2m[36m(func pid=56084)[0m f1_weighted: 0.01643581926612063
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.016, 0.0, 0.0, 0.015, 0.129, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:02 (running for 00:37:37.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   81 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.387 |      0.014 |                   58 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.33  |      0.016 |                   32 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   3.096 |      0.032 |                    3 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.06389925373134328
[2m[36m(func pid=62698)[0m top5: 0.47901119402985076
[2m[36m(func pid=62698)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=62698)[0m f1_macro: 0.03191959473492055
[2m[36m(func pid=62698)[0m f1_weighted: 0.035634187998620996
[2m[36m(func pid=62698)[0m f1_per_class: [0.056, 0.015, 0.0, 0.083, 0.0, 0.02, 0.0, 0.103, 0.0, 0.043]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=45559)[0m top1: 0.40298507462686567
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=45559)[0m f1_macro: 0.3989064511008434
[2m[36m(func pid=45559)[0m f1_weighted: 0.42226332330591804
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.505, 0.686, 0.514, 0.257, 0.258, 0.396, 0.286, 0.205, 0.344]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 5.3973 | Steps: 2 | Val loss: 426.7173 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.4867 | Steps: 2 | Val loss: 35506.4141 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0407 | Steps: 2 | Val loss: 2.5621 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0008 | Steps: 2 | Val loss: 2.8160 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=50583)[0m top1: 0.02332089552238806
[2m[36m(func pid=50583)[0m top5: 0.36847014925373134
[2m[36m(func pid=50583)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=50583)[0m f1_macro: 0.047211848295073286
[2m[36m(func pid=50583)[0m f1_weighted: 0.016776610015051586
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.046, 0.084, 0.016, 0.278, 0.0, 0.0, 0.023, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05830223880597015
[2m[36m(func pid=56084)[0m top5: 0.6520522388059702
[2m[36m(func pid=56084)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=56084)[0m f1_macro: 0.014873031635693007
[2m[36m(func pid=56084)[0m f1_weighted: 0.007907573425255622
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.134, 0.0, 0.015]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:08 (running for 00:37:43.07)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.399 |                   82 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   5.397 |      0.047 |                   59 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.487 |      0.015 |                   33 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   3.041 |      0.034 |                    4 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.408115671641791
[2m[36m(func pid=45559)[0m top5: 0.9500932835820896
[2m[36m(func pid=45559)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=45559)[0m f1_macro: 0.40317791509790285
[2m[36m(func pid=45559)[0m f1_weighted: 0.4257904195153942
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.504, 0.686, 0.512, 0.255, 0.246, 0.41, 0.302, 0.215, 0.364]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.06343283582089553
[2m[36m(func pid=62698)[0m top5: 0.4762126865671642
[2m[36m(func pid=62698)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=62698)[0m f1_macro: 0.03389382854271261
[2m[36m(func pid=62698)[0m f1_weighted: 0.03931671944428999
[2m[36m(func pid=62698)[0m f1_per_class: [0.049, 0.033, 0.0, 0.086, 0.0, 0.02, 0.0, 0.098, 0.0, 0.052]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.1795 | Steps: 2 | Val loss: 723.4736 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.3425 | Steps: 2 | Val loss: 29041.7637 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0001 | Steps: 2 | Val loss: 2.8192 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0001 | Steps: 2 | Val loss: 2.5589 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=50583)[0m top1: 0.018190298507462687
[2m[36m(func pid=50583)[0m top5: 0.3983208955223881
[2m[36m(func pid=50583)[0m f1_micro: 0.018190298507462687
[2m[36m(func pid=50583)[0m f1_macro: 0.03880585635135717
[2m[36m(func pid=50583)[0m f1_weighted: 0.010673816141403423
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.032, 0.0, 0.007, 0.312, 0.0, 0.0, 0.014, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.058768656716417914
[2m[36m(func pid=56084)[0m top5: 0.6711753731343284
[2m[36m(func pid=56084)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=56084)[0m f1_macro: 0.016339288049590665
[2m[36m(func pid=56084)[0m f1_weighted: 0.008382322686777969
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.023]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:13 (running for 00:37:48.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.403 |                   84 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.179 |      0.039 |                   60 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.342 |      0.016 |                   34 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   3.041 |      0.034 |                    4 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.40625
[2m[36m(func pid=45559)[0m top5: 0.9514925373134329
[2m[36m(func pid=45559)[0m f1_micro: 0.40625
[2m[36m(func pid=45559)[0m f1_macro: 0.4028454325624498
[2m[36m(func pid=45559)[0m f1_weighted: 0.4227510623408385
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.507, 0.686, 0.502, 0.257, 0.246, 0.408, 0.302, 0.214, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.06436567164179105
[2m[36m(func pid=62698)[0m top5: 0.478544776119403
[2m[36m(func pid=62698)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=62698)[0m f1_macro: 0.0354585904904072
[2m[36m(func pid=62698)[0m f1_weighted: 0.04434834507794771
[2m[36m(func pid=62698)[0m f1_per_class: [0.048, 0.053, 0.0, 0.095, 0.0, 0.013, 0.0, 0.096, 0.0, 0.049]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8717 | Steps: 2 | Val loss: 968.2170 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6013 | Steps: 2 | Val loss: 24699.9062 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0004 | Steps: 2 | Val loss: 2.8008 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9195 | Steps: 2 | Val loss: 2.5476 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=50583)[0m top1: 0.012126865671641791
[2m[36m(func pid=50583)[0m top5: 0.40951492537313433
[2m[36m(func pid=50583)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=50583)[0m f1_macro: 0.0037746529900595524
[2m[36m(func pid=50583)[0m f1_weighted: 0.0029869313002949395
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.06063432835820896
[2m[36m(func pid=56084)[0m top5: 0.6884328358208955
[2m[36m(func pid=56084)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=56084)[0m f1_macro: 0.019810448451225152
[2m[36m(func pid=56084)[0m f1_weighted: 0.009258708192739494
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.048]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:18 (running for 00:37:53.64)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.405 |                   85 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.872 |      0.004 |                   61 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.601 |      0.02  |                   35 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   3     |      0.035 |                    5 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.41138059701492535
[2m[36m(func pid=45559)[0m top5: 0.9519589552238806
[2m[36m(func pid=45559)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=45559)[0m f1_macro: 0.4048722110967474
[2m[36m(func pid=45559)[0m f1_weighted: 0.4276403577173067
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.514, 0.686, 0.51, 0.26, 0.249, 0.413, 0.289, 0.221, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.06763059701492537
[2m[36m(func pid=62698)[0m top5: 0.4766791044776119
[2m[36m(func pid=62698)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=62698)[0m f1_macro: 0.03946697752207176
[2m[36m(func pid=62698)[0m f1_weighted: 0.049791291511853866
[2m[36m(func pid=62698)[0m f1_per_class: [0.043, 0.073, 0.0, 0.1, 0.0, 0.013, 0.0, 0.1, 0.026, 0.04]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1641 | Steps: 2 | Val loss: 970.0497 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.9035 | Steps: 2 | Val loss: 21175.9453 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.8282 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8950 | Steps: 2 | Val loss: 2.5351 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=50583)[0m top1: 0.013526119402985074
[2m[36m(func pid=50583)[0m top5: 0.41277985074626866
[2m[36m(func pid=50583)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=50583)[0m f1_macro: 0.021108837970540098
[2m[36m(func pid=50583)[0m f1_weighted: 0.0025398592654566773
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.005, 0.0, 0.0, 0.185, 0.0, 0.0, 0.0, 0.0, 0.021]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.05970149253731343
[2m[36m(func pid=56084)[0m top5: 0.7042910447761194
[2m[36m(func pid=56084)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=56084)[0m f1_macro: 0.019131200867443753
[2m[36m(func pid=56084)[0m f1_weighted: 0.00941556659977424
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.156, 0.0, 0.036]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:24 (running for 00:37:58.86)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.406 |                   86 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.164 |      0.021 |                   62 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.903 |      0.019 |                   36 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.919 |      0.039 |                    6 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.40904850746268656
[2m[36m(func pid=45559)[0m top5: 0.9524253731343284
[2m[36m(func pid=45559)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=45559)[0m f1_macro: 0.4064520117080688
[2m[36m(func pid=45559)[0m f1_weighted: 0.4229966276135651
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.513, 0.686, 0.509, 0.268, 0.238, 0.399, 0.303, 0.222, 0.388]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.07229477611940298
[2m[36m(func pid=62698)[0m top5: 0.4766791044776119
[2m[36m(func pid=62698)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=62698)[0m f1_macro: 0.045046693544815476
[2m[36m(func pid=62698)[0m f1_weighted: 0.057117114227360315
[2m[36m(func pid=62698)[0m f1_per_class: [0.057, 0.078, 0.0, 0.118, 0.0, 0.013, 0.0, 0.106, 0.051, 0.028]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.8829 | Steps: 2 | Val loss: 815.6874 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.4360 | Steps: 2 | Val loss: 22253.3887 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.8304 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7524 | Steps: 2 | Val loss: 2.5246 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=50583)[0m top1: 0.013526119402985074
[2m[36m(func pid=50583)[0m top5: 0.4533582089552239
[2m[36m(func pid=50583)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=50583)[0m f1_macro: 0.013523333987411509
[2m[36m(func pid=50583)[0m f1_weighted: 0.00285556519526111
[2m[36m(func pid=50583)[0m f1_per_class: [0.0, 0.011, 0.0, 0.0, 0.103, 0.0, 0.0, 0.0, 0.0, 0.021]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.060167910447761194
[2m[36m(func pid=56084)[0m top5: 0.7168843283582089
[2m[36m(func pid=56084)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=56084)[0m f1_macro: 0.02038519072550486
[2m[36m(func pid=56084)[0m f1_weighted: 0.009810997834313847
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.161, 0.0, 0.043]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:29 (running for 00:38:04.05)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.406 |                   87 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.883 |      0.014 |                   63 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.436 |      0.02  |                   37 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.895 |      0.045 |                    7 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4076492537313433
[2m[36m(func pid=45559)[0m top5: 0.9528917910447762
[2m[36m(func pid=45559)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=45559)[0m f1_macro: 0.4061119314735976
[2m[36m(func pid=45559)[0m f1_weighted: 0.4219314406598168
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.517, 0.686, 0.491, 0.274, 0.239, 0.41, 0.299, 0.219, 0.388]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.07695895522388059
[2m[36m(func pid=62698)[0m top5: 0.4766791044776119
[2m[36m(func pid=62698)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=62698)[0m f1_macro: 0.04958351680696891
[2m[36m(func pid=62698)[0m f1_weighted: 0.06150755616494706
[2m[36m(func pid=62698)[0m f1_per_class: [0.065, 0.088, 0.0, 0.126, 0.0, 0.012, 0.0, 0.111, 0.047, 0.048]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 3.7621 | Steps: 2 | Val loss: 593.5446 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.4157 | Steps: 2 | Val loss: 21664.3711 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0003 | Steps: 2 | Val loss: 2.8532 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8023 | Steps: 2 | Val loss: 2.5124 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=50583)[0m top1: 0.01912313432835821
[2m[36m(func pid=50583)[0m top5: 0.498134328358209
[2m[36m(func pid=50583)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=50583)[0m f1_macro: 0.020054524377657795
[2m[36m(func pid=50583)[0m f1_weighted: 0.007821181197430306
[2m[36m(func pid=50583)[0m f1_per_class: [0.012, 0.026, 0.0, 0.007, 0.132, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m top1: 0.06110074626865672
[2m[36m(func pid=56084)[0m top5: 0.7290111940298507
[2m[36m(func pid=56084)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=56084)[0m f1_macro: 0.021457745274336436
[2m[36m(func pid=56084)[0m f1_weighted: 0.011792963780133755
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006, 0.164, 0.0, 0.045]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:34 (running for 00:38:09.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.402 |                   88 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   3.762 |      0.02  |                   64 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.416 |      0.021 |                   38 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.752 |      0.05  |                    8 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.40531716417910446
[2m[36m(func pid=45559)[0m top5: 0.9500932835820896
[2m[36m(func pid=45559)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=45559)[0m f1_macro: 0.4021542066137692
[2m[36m(func pid=45559)[0m f1_weighted: 0.4194867373022766
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.512, 0.686, 0.492, 0.268, 0.239, 0.407, 0.294, 0.215, 0.371]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.07835820895522388
[2m[36m(func pid=62698)[0m top5: 0.478544776119403
[2m[36m(func pid=62698)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=62698)[0m f1_macro: 0.05145633394672329
[2m[36m(func pid=62698)[0m f1_weighted: 0.06661232658744375
[2m[36m(func pid=62698)[0m f1_per_class: [0.069, 0.093, 0.0, 0.139, 0.0, 0.019, 0.0, 0.109, 0.043, 0.042]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.9194 | Steps: 2 | Val loss: 405.0115 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.7737 | Steps: 2 | Val loss: 16244.2080 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.8796 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=50583)[0m top1: 0.03591417910447761
[2m[36m(func pid=50583)[0m top5: 0.5433768656716418
[2m[36m(func pid=50583)[0m f1_micro: 0.03591417910447761
[2m[36m(func pid=50583)[0m f1_macro: 0.036019872099062974
[2m[36m(func pid=50583)[0m f1_weighted: 0.029128424047627822
[2m[36m(func pid=50583)[0m f1_per_class: [0.077, 0.154, 0.0, 0.0, 0.104, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7018 | Steps: 2 | Val loss: 2.4959 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=56084)[0m top1: 0.0648320895522388
[2m[36m(func pid=56084)[0m top5: 0.7472014925373134
[2m[36m(func pid=56084)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=56084)[0m f1_macro: 0.023469323521103157
[2m[36m(func pid=56084)[0m f1_weighted: 0.021761860619152005
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.004, 0.022, 0.0, 0.008, 0.015, 0.172, 0.0, 0.013]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:39 (running for 00:38:14.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.403 |                   89 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.919 |      0.036 |                   65 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.774 |      0.023 |                   39 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.802 |      0.051 |                    9 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.40718283582089554
[2m[36m(func pid=45559)[0m top5: 0.9482276119402985
[2m[36m(func pid=45559)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=45559)[0m f1_macro: 0.40305605034661457
[2m[36m(func pid=45559)[0m f1_weighted: 0.4206468849123475
[2m[36m(func pid=45559)[0m f1_per_class: [0.532, 0.521, 0.686, 0.478, 0.274, 0.237, 0.419, 0.292, 0.221, 0.371]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.0830223880597015
[2m[36m(func pid=62698)[0m top5: 0.478544776119403
[2m[36m(func pid=62698)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=62698)[0m f1_macro: 0.05734934376088406
[2m[36m(func pid=62698)[0m f1_weighted: 0.07303681921828961
[2m[36m(func pid=62698)[0m f1_per_class: [0.061, 0.105, 0.0, 0.15, 0.0, 0.018, 0.0, 0.114, 0.082, 0.043]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8513 | Steps: 2 | Val loss: 382.3043 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.4430 | Steps: 2 | Val loss: 13560.5762 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=50583)[0m top1: 0.052705223880597014
[2m[36m(func pid=50583)[0m top5: 0.5816231343283582
[2m[36m(func pid=50583)[0m f1_micro: 0.05270522388059702
[2m[36m(func pid=50583)[0m f1_macro: 0.04817956883717775
[2m[36m(func pid=50583)[0m f1_weighted: 0.042009925689112465
[2m[36m(func pid=50583)[0m f1_per_class: [0.146, 0.221, 0.0, 0.0, 0.086, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.9266 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6156 | Steps: 2 | Val loss: 2.4814 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=56084)[0m top1: 0.09001865671641791
[2m[36m(func pid=56084)[0m top5: 0.7560634328358209
[2m[36m(func pid=56084)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=56084)[0m f1_macro: 0.03786448124071165
[2m[36m(func pid=56084)[0m f1_weighted: 0.05994341127988778
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.01, 0.132, 0.0, 0.016, 0.037, 0.176, 0.0, 0.009]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:45 (running for 00:38:20.08)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.403 |                   90 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.851 |      0.048 |                   66 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.443 |      0.038 |                   40 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.702 |      0.057 |                   10 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.4039179104477612
[2m[36m(func pid=45559)[0m top5: 0.9500932835820896
[2m[36m(func pid=45559)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=45559)[0m f1_macro: 0.403442349170942
[2m[36m(func pid=45559)[0m f1_weighted: 0.4149185606106206
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.516, 0.686, 0.478, 0.283, 0.24, 0.397, 0.309, 0.22, 0.366]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.08582089552238806
[2m[36m(func pid=62698)[0m top5: 0.4762126865671642
[2m[36m(func pid=62698)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=62698)[0m f1_macro: 0.059496826677559914
[2m[36m(func pid=62698)[0m f1_weighted: 0.07879409658731494
[2m[36m(func pid=62698)[0m f1_per_class: [0.063, 0.105, 0.0, 0.167, 0.0, 0.028, 0.0, 0.113, 0.081, 0.037]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.0949 | Steps: 2 | Val loss: 471.3332 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 4.5686 | Steps: 2 | Val loss: 11497.5537 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0002 | Steps: 2 | Val loss: 2.9980 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=50583)[0m top1: 0.05177238805970149
[2m[36m(func pid=50583)[0m top5: 0.5806902985074627
[2m[36m(func pid=50583)[0m f1_micro: 0.05177238805970149
[2m[36m(func pid=50583)[0m f1_macro: 0.045701952948612276
[2m[36m(func pid=50583)[0m f1_weighted: 0.04046859241715064
[2m[36m(func pid=50583)[0m f1_per_class: [0.12, 0.215, 0.0, 0.0, 0.093, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6027 | Steps: 2 | Val loss: 2.4693 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=56084)[0m top1: 0.1226679104477612
[2m[36m(func pid=56084)[0m top5: 0.7630597014925373
[2m[36m(func pid=56084)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=56084)[0m f1_macro: 0.05023865038406851
[2m[36m(func pid=56084)[0m f1_weighted: 0.09356858093575741
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.013, 0.264, 0.0, 0.008, 0.029, 0.179, 0.0, 0.01]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:50 (running for 00:38:25.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.399 |                   91 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.095 |      0.046 |                   67 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.569 |      0.05  |                   41 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.616 |      0.059 |                   11 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3931902985074627
[2m[36m(func pid=45559)[0m top5: 0.9486940298507462
[2m[36m(func pid=45559)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=45559)[0m f1_macro: 0.3993330237301825
[2m[36m(func pid=45559)[0m f1_weighted: 0.4008522478661455
[2m[36m(func pid=45559)[0m f1_per_class: [0.538, 0.518, 0.686, 0.467, 0.274, 0.233, 0.362, 0.305, 0.228, 0.382]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.08861940298507463
[2m[36m(func pid=62698)[0m top5: 0.4832089552238806
[2m[36m(func pid=62698)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=62698)[0m f1_macro: 0.06210302162811312
[2m[36m(func pid=62698)[0m f1_weighted: 0.08307450452345493
[2m[36m(func pid=62698)[0m f1_per_class: [0.069, 0.12, 0.0, 0.169, 0.0, 0.038, 0.0, 0.115, 0.077, 0.033]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4488 | Steps: 2 | Val loss: 578.8809 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.4617 | Steps: 2 | Val loss: 9743.7988 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=50583)[0m top1: 0.05970149253731343
[2m[36m(func pid=50583)[0m top5: 0.5881529850746269
[2m[36m(func pid=50583)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=50583)[0m f1_macro: 0.050097021481548085
[2m[36m(func pid=50583)[0m f1_weighted: 0.04662357062586431
[2m[36m(func pid=50583)[0m f1_per_class: [0.117, 0.25, 0.0, 0.0, 0.104, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0020 | Steps: 2 | Val loss: 3.0504 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5059 | Steps: 2 | Val loss: 2.4596 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=56084)[0m top1: 0.14225746268656717
[2m[36m(func pid=56084)[0m top5: 0.7639925373134329
[2m[36m(func pid=56084)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=56084)[0m f1_macro: 0.05686738672291317
[2m[36m(func pid=56084)[0m f1_weighted: 0.10669801875898
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.0, 0.024, 0.318, 0.0, 0.016, 0.018, 0.182, 0.0, 0.011]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:57:55 (running for 00:38:30.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.002 |      0.39  |                   92 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.449 |      0.05  |                   68 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   5.462 |      0.057 |                   42 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.603 |      0.062 |                   12 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.38619402985074625
[2m[36m(func pid=45559)[0m top5: 0.949160447761194
[2m[36m(func pid=45559)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=45559)[0m f1_macro: 0.39025829233143433
[2m[36m(func pid=45559)[0m f1_weighted: 0.3930867150853102
[2m[36m(func pid=45559)[0m f1_per_class: [0.493, 0.511, 0.686, 0.468, 0.274, 0.232, 0.345, 0.3, 0.225, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.09514925373134328
[2m[36m(func pid=62698)[0m top5: 0.47947761194029853
[2m[36m(func pid=62698)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=62698)[0m f1_macro: 0.07742386546332222
[2m[36m(func pid=62698)[0m f1_weighted: 0.08960148404384922
[2m[36m(func pid=62698)[0m f1_per_class: [0.065, 0.125, 0.111, 0.182, 0.0, 0.043, 0.0, 0.123, 0.087, 0.038]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7381 | Steps: 2 | Val loss: 608.5596 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7694 | Steps: 2 | Val loss: 6953.1216 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=50583)[0m top1: 0.07416044776119403
[2m[36m(func pid=50583)[0m top5: 0.5914179104477612
[2m[36m(func pid=50583)[0m f1_micro: 0.07416044776119403
[2m[36m(func pid=50583)[0m f1_macro: 0.060554788801971834
[2m[36m(func pid=50583)[0m f1_weighted: 0.05829176293099088
[2m[36m(func pid=50583)[0m f1_per_class: [0.125, 0.305, 0.0, 0.0, 0.13, 0.016, 0.0, 0.0, 0.0, 0.029]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.0642 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4228 | Steps: 2 | Val loss: 2.4458 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=56084)[0m top1: 0.11147388059701492
[2m[36m(func pid=56084)[0m top5: 0.7737873134328358
[2m[36m(func pid=56084)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=56084)[0m f1_macro: 0.05637948858471006
[2m[36m(func pid=56084)[0m f1_weighted: 0.07871294824808679
[2m[36m(func pid=56084)[0m f1_per_class: [0.063, 0.0, 0.044, 0.206, 0.0, 0.0, 0.028, 0.188, 0.0, 0.034]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:58:01 (running for 00:38:35.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.4   |                   93 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.738 |      0.061 |                   69 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.769 |      0.056 |                   43 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.506 |      0.077 |                   13 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3931902985074627
[2m[36m(func pid=45559)[0m top5: 0.9496268656716418
[2m[36m(func pid=45559)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=45559)[0m f1_macro: 0.3995061312048248
[2m[36m(func pid=45559)[0m f1_weighted: 0.3991994250645406
[2m[36m(func pid=45559)[0m f1_per_class: [0.521, 0.523, 0.706, 0.466, 0.289, 0.236, 0.355, 0.305, 0.226, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7198 | Steps: 2 | Val loss: 584.6764 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=62698)[0m top1: 0.09888059701492537
[2m[36m(func pid=62698)[0m top5: 0.4822761194029851
[2m[36m(func pid=62698)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=62698)[0m f1_macro: 0.08837925084606467
[2m[36m(func pid=62698)[0m f1_weighted: 0.094098135570012
[2m[36m(func pid=62698)[0m f1_per_class: [0.061, 0.137, 0.208, 0.184, 0.0, 0.047, 0.003, 0.125, 0.079, 0.039]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4642 | Steps: 2 | Val loss: 4987.1455 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=50583)[0m top1: 0.08768656716417911
[2m[36m(func pid=50583)[0m top5: 0.5634328358208955
[2m[36m(func pid=50583)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=50583)[0m f1_macro: 0.0729757577581
[2m[36m(func pid=50583)[0m f1_weighted: 0.07290648296634095
[2m[36m(func pid=50583)[0m f1_per_class: [0.103, 0.365, 0.0, 0.01, 0.189, 0.031, 0.0, 0.0, 0.0, 0.033]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0001 | Steps: 2 | Val loss: 3.0953 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3926 | Steps: 2 | Val loss: 2.4318 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=56084)[0m top1: 0.10261194029850747
[2m[36m(func pid=56084)[0m top5: 0.7803171641791045
[2m[36m(func pid=56084)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=56084)[0m f1_macro: 0.057275636786887
[2m[36m(func pid=56084)[0m f1_weighted: 0.07130584645441371
[2m[36m(func pid=56084)[0m f1_per_class: [0.051, 0.0, 0.047, 0.157, 0.0, 0.031, 0.037, 0.193, 0.0, 0.058]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:58:06 (running for 00:38:41.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.395 |                   94 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.72  |      0.073 |                   70 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.464 |      0.057 |                   44 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.423 |      0.088 |                   14 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3885261194029851
[2m[36m(func pid=45559)[0m top5: 0.9472947761194029
[2m[36m(func pid=45559)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=45559)[0m f1_macro: 0.395296929447147
[2m[36m(func pid=45559)[0m f1_weighted: 0.3952085185774994
[2m[36m(func pid=45559)[0m f1_per_class: [0.521, 0.513, 0.706, 0.46, 0.265, 0.219, 0.358, 0.31, 0.232, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6452 | Steps: 2 | Val loss: 526.3099 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=62698)[0m top1: 0.10307835820895522
[2m[36m(func pid=62698)[0m top5: 0.4846082089552239
[2m[36m(func pid=62698)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=62698)[0m f1_macro: 0.09066380804717414
[2m[36m(func pid=62698)[0m f1_weighted: 0.09872008429867468
[2m[36m(func pid=62698)[0m f1_per_class: [0.059, 0.148, 0.196, 0.189, 0.0, 0.047, 0.006, 0.131, 0.087, 0.044]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 4.3912 | Steps: 2 | Val loss: 3397.7339 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=50583)[0m top1: 0.09048507462686567
[2m[36m(func pid=50583)[0m top5: 0.5284514925373134
[2m[36m(func pid=50583)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=50583)[0m f1_macro: 0.06776020542576719
[2m[36m(func pid=50583)[0m f1_weighted: 0.07947298162765914
[2m[36m(func pid=50583)[0m f1_per_class: [0.071, 0.401, 0.0, 0.01, 0.118, 0.045, 0.0, 0.0, 0.0, 0.034]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0001 | Steps: 2 | Val loss: 3.1539 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2663 | Steps: 2 | Val loss: 2.4193 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=56084)[0m top1: 0.09561567164179105
[2m[36m(func pid=56084)[0m top5: 0.7919776119402985
[2m[36m(func pid=56084)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=56084)[0m f1_macro: 0.05313298014478769
[2m[36m(func pid=56084)[0m f1_weighted: 0.058820620129645666
[2m[36m(func pid=56084)[0m f1_per_class: [0.056, 0.0, 0.049, 0.126, 0.0, 0.023, 0.026, 0.194, 0.0, 0.057]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:58:11 (running for 00:38:46.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.395 |                   95 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   0.645 |      0.068 |                   71 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.391 |      0.053 |                   45 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.393 |      0.091 |                   15 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3833955223880597
[2m[36m(func pid=45559)[0m top5: 0.9444962686567164
[2m[36m(func pid=45559)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=45559)[0m f1_macro: 0.39481514537148515
[2m[36m(func pid=45559)[0m f1_weighted: 0.3875076998913467
[2m[36m(func pid=45559)[0m f1_per_class: [0.521, 0.511, 0.727, 0.449, 0.268, 0.218, 0.344, 0.307, 0.234, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 5.6619 | Steps: 2 | Val loss: 459.9658 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=62698)[0m top1: 0.10261194029850747
[2m[36m(func pid=62698)[0m top5: 0.48740671641791045
[2m[36m(func pid=62698)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=62698)[0m f1_macro: 0.08745780385721078
[2m[36m(func pid=62698)[0m f1_weighted: 0.09924353302311305
[2m[36m(func pid=62698)[0m f1_per_class: [0.058, 0.152, 0.169, 0.189, 0.0, 0.051, 0.006, 0.13, 0.079, 0.041]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m top1: 0.09561567164179105
[2m[36m(func pid=50583)[0m top5: 0.5205223880597015
[2m[36m(func pid=50583)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=50583)[0m f1_macro: 0.05893140599022495
[2m[36m(func pid=50583)[0m f1_weighted: 0.08499717200630535
[2m[36m(func pid=50583)[0m f1_per_class: [0.061, 0.426, 0.0, 0.01, 0.0, 0.065, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 4.1800 | Steps: 2 | Val loss: 2562.8196 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0001 | Steps: 2 | Val loss: 3.1582 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2589 | Steps: 2 | Val loss: 2.4027 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=56084)[0m top1: 0.10820895522388059
[2m[36m(func pid=56084)[0m top5: 0.8003731343283582
[2m[36m(func pid=56084)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=56084)[0m f1_macro: 0.058602468903324025
[2m[36m(func pid=56084)[0m f1_weighted: 0.07354599198787655
[2m[36m(func pid=56084)[0m f1_per_class: [0.07, 0.0, 0.093, 0.145, 0.0, 0.016, 0.059, 0.203, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1138 | Steps: 2 | Val loss: 353.9484 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 03:58:16 (running for 00:38:51.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   96 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   5.662 |      0.059 |                   72 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.18  |      0.059 |                   46 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.266 |      0.087 |                   16 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3885261194029851
[2m[36m(func pid=45559)[0m top5: 0.9440298507462687
[2m[36m(func pid=45559)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=45559)[0m f1_macro: 0.3978350527845947
[2m[36m(func pid=45559)[0m f1_weighted: 0.3923056928024738
[2m[36m(func pid=45559)[0m f1_per_class: [0.521, 0.515, 0.727, 0.449, 0.274, 0.22, 0.356, 0.311, 0.237, 0.369]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.11054104477611941
[2m[36m(func pid=62698)[0m top5: 0.498134328358209
[2m[36m(func pid=62698)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=62698)[0m f1_macro: 0.09680420021422592
[2m[36m(func pid=62698)[0m f1_weighted: 0.10630813912991709
[2m[36m(func pid=62698)[0m f1_per_class: [0.065, 0.17, 0.212, 0.196, 0.0, 0.06, 0.006, 0.139, 0.078, 0.042]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m top1: 0.11847014925373134
[2m[36m(func pid=50583)[0m top5: 0.5153917910447762
[2m[36m(func pid=50583)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=50583)[0m f1_macro: 0.0828118975969804
[2m[36m(func pid=50583)[0m f1_weighted: 0.0939922340601356
[2m[36m(func pid=50583)[0m f1_per_class: [0.079, 0.449, 0.0, 0.013, 0.188, 0.078, 0.003, 0.0, 0.0, 0.018]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.5534 | Steps: 2 | Val loss: 1355.8436 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0001 | Steps: 2 | Val loss: 3.2130 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.1837 | Steps: 2 | Val loss: 2.3924 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2237 | Steps: 2 | Val loss: 255.6389 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 03:58:22 (running for 00:38:56.86)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.398 |                   96 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   2.114 |      0.083 |                   73 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.553 |      0.076 |                   47 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.259 |      0.097 |                   17 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m top1: 0.1226679104477612
[2m[36m(func pid=56084)[0m top5: 0.8115671641791045
[2m[36m(func pid=56084)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=56084)[0m f1_macro: 0.07625059609119085
[2m[36m(func pid=56084)[0m f1_weighted: 0.09636216975100215
[2m[36m(func pid=56084)[0m f1_per_class: [0.07, 0.04, 0.156, 0.157, 0.0, 0.023, 0.093, 0.223, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=45559)[0m top1: 0.38386194029850745
[2m[36m(func pid=45559)[0m top5: 0.9416977611940298
[2m[36m(func pid=45559)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=45559)[0m f1_macro: 0.39560038896079247
[2m[36m(func pid=45559)[0m f1_weighted: 0.3871553623332002
[2m[36m(func pid=45559)[0m f1_per_class: [0.521, 0.508, 0.727, 0.437, 0.265, 0.214, 0.356, 0.311, 0.235, 0.382]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=62698)[0m top1: 0.11473880597014925
[2m[36m(func pid=62698)[0m top5: 0.5065298507462687
[2m[36m(func pid=62698)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=62698)[0m f1_macro: 0.10048305426368469
[2m[36m(func pid=62698)[0m f1_weighted: 0.11098026042569217
[2m[36m(func pid=62698)[0m f1_per_class: [0.073, 0.182, 0.222, 0.196, 0.0, 0.063, 0.012, 0.144, 0.073, 0.039]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m top1: 0.14692164179104478
[2m[36m(func pid=50583)[0m top5: 0.5447761194029851
[2m[36m(func pid=50583)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=50583)[0m f1_macro: 0.0848562219402956
[2m[36m(func pid=50583)[0m f1_weighted: 0.09511037965239665
[2m[36m(func pid=50583)[0m f1_per_class: [0.128, 0.445, 0.0, 0.019, 0.158, 0.064, 0.006, 0.0, 0.0, 0.029]
[2m[36m(func pid=50583)[0m 
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.2424 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.8686 | Steps: 2 | Val loss: 1224.6228 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1342 | Steps: 2 | Val loss: 2.3815 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=50583)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 8.1982 | Steps: 2 | Val loss: 163.3510 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:58:27 (running for 00:39:02.31)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.37175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.391 |                   98 |
| train_2d480_00018 | RUNNING    | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   1.224 |      0.085 |                   74 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.553 |      0.076 |                   47 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.184 |      0.1   |                   18 |
| train_2d480_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45559)[0m top1: 0.3824626865671642
[2m[36m(func pid=45559)[0m top5: 0.9421641791044776
[2m[36m(func pid=45559)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=45559)[0m f1_macro: 0.39114327285828143
[2m[36m(func pid=45559)[0m f1_weighted: 0.38518251330840353
[2m[36m(func pid=45559)[0m f1_per_class: [0.521, 0.506, 0.727, 0.436, 0.26, 0.197, 0.359, 0.315, 0.233, 0.358]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.11100746268656717
[2m[36m(func pid=56084)[0m top5: 0.8157649253731343
[2m[36m(func pid=56084)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=56084)[0m f1_macro: 0.07596423255225829
[2m[36m(func pid=56084)[0m f1_weighted: 0.07921018442076229
[2m[36m(func pid=56084)[0m f1_per_class: [0.069, 0.049, 0.198, 0.129, 0.0, 0.068, 0.043, 0.206, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=62698)[0m top1: 0.11567164179104478
[2m[36m(func pid=62698)[0m top5: 0.5102611940298507
[2m[36m(func pid=62698)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=62698)[0m f1_macro: 0.10283732167831487
[2m[36m(func pid=62698)[0m f1_weighted: 0.11352127910166404
[2m[36m(func pid=62698)[0m f1_per_class: [0.078, 0.184, 0.231, 0.192, 0.0, 0.063, 0.024, 0.14, 0.071, 0.045]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=50583)[0m top1: 0.1609141791044776
[2m[36m(func pid=50583)[0m top5: 0.5293843283582089
[2m[36m(func pid=50583)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=50583)[0m f1_macro: 0.11172926140462931
[2m[36m(func pid=50583)[0m f1_weighted: 0.10888861283017014
[2m[36m(func pid=50583)[0m f1_per_class: [0.283, 0.43, 0.063, 0.049, 0.171, 0.069, 0.018, 0.0, 0.0, 0.033]
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.3069 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.8290 | Steps: 2 | Val loss: 1178.8665 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.0414 | Steps: 2 | Val loss: 2.3715 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=45559)[0m top1: 0.38013059701492535
[2m[36m(func pid=45559)[0m top5: 0.9421641791044776
[2m[36m(func pid=45559)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=45559)[0m f1_macro: 0.39532750847655895
[2m[36m(func pid=45559)[0m f1_weighted: 0.380275951046181
[2m[36m(func pid=45559)[0m f1_per_class: [0.587, 0.504, 0.706, 0.431, 0.255, 0.188, 0.346, 0.315, 0.24, 0.382]
[2m[36m(func pid=45559)[0m 
[2m[36m(func pid=56084)[0m top1: 0.09095149253731344
[2m[36m(func pid=56084)[0m top5: 0.8148320895522388
[2m[36m(func pid=56084)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=56084)[0m f1_macro: 0.05368456028467859
[2m[36m(func pid=56084)[0m f1_weighted: 0.04907242969810927
[2m[36m(func pid=56084)[0m f1_per_class: [0.054, 0.063, 0.107, 0.071, 0.0, 0.047, 0.0, 0.195, 0.0, 0.0]
[2m[36m(func pid=62698)[0m top1: 0.1166044776119403
[2m[36m(func pid=62698)[0m top5: 0.5172574626865671
[2m[36m(func pid=62698)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=62698)[0m f1_macro: 0.10458091373616256
[2m[36m(func pid=62698)[0m f1_weighted: 0.11469465017557998
[2m[36m(func pid=62698)[0m f1_per_class: [0.086, 0.186, 0.231, 0.193, 0.0, 0.063, 0.024, 0.141, 0.08, 0.042]
[2m[36m(func pid=45559)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0015 | Steps: 2 | Val loss: 3.3103 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 03:58:32 (running for 00:39:07.61)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00017 | RUNNING    | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0     |      0.395 |                   99 |
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.869 |      0.076 |                   48 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.134 |      0.103 |                   19 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=67665)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=67665)[0m Configuration completed!
[2m[36m(func pid=67665)[0m New optimizer parameters:
[2m[36m(func pid=67665)[0m SGD (
[2m[36m(func pid=67665)[0m Parameter Group 0
[2m[36m(func pid=67665)[0m     dampening: 0
[2m[36m(func pid=67665)[0m     differentiable: False
[2m[36m(func pid=67665)[0m     foreach: None
[2m[36m(func pid=67665)[0m     lr: 0.001
[2m[36m(func pid=67665)[0m     maximize: False
[2m[36m(func pid=67665)[0m     momentum: 0.9
[2m[36m(func pid=67665)[0m     nesterov: False
[2m[36m(func pid=67665)[0m     weight_decay: 1e-05
[2m[36m(func pid=67665)[0m )
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=45559)[0m top1: 0.3787313432835821
[2m[36m(func pid=45559)[0m top5: 0.9402985074626866
[2m[36m(func pid=45559)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=45559)[0m f1_macro: 0.39184654457104995
[2m[36m(func pid=45559)[0m f1_weighted: 0.3794440612649169
[2m[36m(func pid=45559)[0m f1_per_class: [0.528, 0.5, 0.727, 0.431, 0.237, 0.204, 0.344, 0.313, 0.236, 0.4]
== Status ==
Current time: 2024-01-07 03:58:38 (running for 00:39:13.01)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.829 |      0.054 |                   49 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   2.041 |      0.105 |                   20 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.9556 | Steps: 2 | Val loss: 2.3629 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.8780 | Steps: 2 | Val loss: 929.2396 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1353 | Steps: 2 | Val loss: 2.4916 | Batch size: 32 | lr: 0.001 | Duration: 4.94s
[2m[36m(func pid=62698)[0m top1: 0.11333955223880597
[2m[36m(func pid=62698)[0m top5: 0.5284514925373134
[2m[36m(func pid=62698)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=62698)[0m f1_macro: 0.10272666827616364
[2m[36m(func pid=62698)[0m f1_weighted: 0.1117580594728801
[2m[36m(func pid=62698)[0m f1_per_class: [0.1, 0.188, 0.212, 0.18, 0.0, 0.063, 0.027, 0.126, 0.087, 0.044]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.08069029850746269
[2m[36m(func pid=56084)[0m top5: 0.8092350746268657
[2m[36m(func pid=56084)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=56084)[0m f1_macro: 0.04313096952110964
[2m[36m(func pid=56084)[0m f1_weighted: 0.03240822675217531
[2m[36m(func pid=56084)[0m f1_per_class: [0.042, 0.031, 0.1, 0.044, 0.0, 0.016, 0.0, 0.198, 0.0, 0.0]
[2m[36m(func pid=67665)[0m top1: 0.06949626865671642
[2m[36m(func pid=67665)[0m top5: 0.48367537313432835
[2m[36m(func pid=67665)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=67665)[0m f1_macro: 0.045932405585179095
[2m[36m(func pid=67665)[0m f1_weighted: 0.043023587927907635
[2m[36m(func pid=67665)[0m f1_per_class: [0.168, 0.01, 0.0, 0.103, 0.0, 0.018, 0.0, 0.104, 0.021, 0.035]
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8993 | Steps: 2 | Val loss: 2.3547 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 03:58:43 (running for 00:39:18.04)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.829 |      0.054 |                   49 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.956 |      0.103 |                   21 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=68221)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=68221)[0m Configuration completed!
[2m[36m(func pid=68221)[0m New optimizer parameters:
[2m[36m(func pid=68221)[0m SGD (
[2m[36m(func pid=68221)[0m Parameter Group 0
[2m[36m(func pid=68221)[0m     dampening: 0
[2m[36m(func pid=68221)[0m     differentiable: False
[2m[36m(func pid=68221)[0m     foreach: None
[2m[36m(func pid=68221)[0m     lr: 0.01
[2m[36m(func pid=68221)[0m     maximize: False
[2m[36m(func pid=68221)[0m     momentum: 0.9
[2m[36m(func pid=68221)[0m     nesterov: False
[2m[36m(func pid=68221)[0m     weight_decay: 1e-05
[2m[36m(func pid=68221)[0m )
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:58:48 (running for 00:39:23.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.878 |      0.043 |                   50 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.899 |      0.104 |                   22 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   3.135 |      0.046 |                    1 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.11520522388059702
[2m[36m(func pid=62698)[0m top5: 0.5307835820895522
[2m[36m(func pid=62698)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=62698)[0m f1_macro: 0.10438302103592172
[2m[36m(func pid=62698)[0m f1_weighted: 0.11417736247440427
[2m[36m(func pid=62698)[0m f1_per_class: [0.103, 0.189, 0.207, 0.178, 0.0, 0.071, 0.032, 0.126, 0.086, 0.05]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 5.3758 | Steps: 2 | Val loss: 648.0043 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9738 | Steps: 2 | Val loss: 2.4696 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0588 | Steps: 2 | Val loss: 2.3386 | Batch size: 32 | lr: 0.01 | Duration: 4.58s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.8856 | Steps: 2 | Val loss: 2.3475 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=56084)[0m top1: 0.07882462686567164
[2m[36m(func pid=56084)[0m top5: 0.8120335820895522
[2m[36m(func pid=56084)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=56084)[0m f1_macro: 0.04103035030428405
[2m[36m(func pid=56084)[0m f1_weighted: 0.028089722945301532
[2m[36m(func pid=56084)[0m f1_per_class: [0.044, 0.011, 0.105, 0.044, 0.0, 0.008, 0.0, 0.199, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m top1: 0.07042910447761194
[2m[36m(func pid=67665)[0m top5: 0.4766791044776119
[2m[36m(func pid=67665)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=67665)[0m f1_macro: 0.04875444736382674
[2m[36m(func pid=67665)[0m f1_weighted: 0.049092040109043576
[2m[36m(func pid=67665)[0m f1_per_class: [0.14, 0.042, 0.0, 0.103, 0.0, 0.021, 0.0, 0.105, 0.04, 0.036]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.09421641791044776
[2m[36m(func pid=68221)[0m top5: 0.5354477611940298
[2m[36m(func pid=68221)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=68221)[0m f1_macro: 0.08956898857600623
[2m[36m(func pid=68221)[0m f1_weighted: 0.09395082302697201
[2m[36m(func pid=68221)[0m f1_per_class: [0.112, 0.148, 0.114, 0.144, 0.016, 0.039, 0.032, 0.121, 0.108, 0.062]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=62698)[0m top1: 0.11847014925373134
[2m[36m(func pid=62698)[0m top5: 0.5345149253731343
[2m[36m(func pid=62698)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=62698)[0m f1_macro: 0.1072876967312633
[2m[36m(func pid=62698)[0m f1_weighted: 0.11834380010911112
[2m[36m(func pid=62698)[0m f1_per_class: [0.106, 0.191, 0.202, 0.178, 0.0, 0.08, 0.041, 0.134, 0.086, 0.056]
== Status ==
Current time: 2024-01-07 03:58:53 (running for 00:39:28.59)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   5.376 |      0.041 |                   51 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.886 |      0.107 |                   23 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   2.974 |      0.049 |                    2 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   3.059 |      0.09  |                    1 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6773 | Steps: 2 | Val loss: 2.4204 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.8269 | Steps: 2 | Val loss: 277.5188 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.9882 | Steps: 2 | Val loss: 2.1653 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8035 | Steps: 2 | Val loss: 2.3355 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=56084)[0m top1: 0.11520522388059702
[2m[36m(func pid=56084)[0m top5: 0.8194962686567164
[2m[36m(func pid=56084)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=56084)[0m f1_macro: 0.07170634871067527
[2m[36m(func pid=56084)[0m f1_weighted: 0.08512598876586694
[2m[36m(func pid=56084)[0m f1_per_class: [0.052, 0.01, 0.124, 0.056, 0.0, 0.016, 0.162, 0.239, 0.057, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m top1: 0.08348880597014925
[2m[36m(func pid=67665)[0m top5: 0.4832089552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=67665)[0m f1_macro: 0.06637956583091102
[2m[36m(func pid=67665)[0m f1_weighted: 0.07563390656738214
[2m[36m(func pid=67665)[0m f1_per_class: [0.126, 0.118, 0.0, 0.132, 0.0, 0.029, 0.009, 0.105, 0.093, 0.052]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.16744402985074627
[2m[36m(func pid=68221)[0m top5: 0.738339552238806
[2m[36m(func pid=68221)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=68221)[0m f1_macro: 0.1312989128965531
[2m[36m(func pid=68221)[0m f1_weighted: 0.19204951844100857
[2m[36m(func pid=68221)[0m f1_per_class: [0.115, 0.126, 0.136, 0.264, 0.038, 0.106, 0.244, 0.072, 0.092, 0.121]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=62698)[0m top1: 0.12033582089552239
[2m[36m(func pid=62698)[0m top5: 0.5517723880597015
[2m[36m(func pid=62698)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=62698)[0m f1_macro: 0.10987048691269789
[2m[36m(func pid=62698)[0m f1_weighted: 0.12219883125232871
[2m[36m(func pid=62698)[0m f1_per_class: [0.105, 0.19, 0.205, 0.177, 0.0, 0.083, 0.052, 0.136, 0.095, 0.056]
[2m[36m(func pid=62698)[0m 
== Status ==
Current time: 2024-01-07 03:58:59 (running for 00:39:34.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.827 |      0.072 |                   52 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.804 |      0.11  |                   24 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   2.677 |      0.066 |                    3 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   1.988 |      0.131 |                    2 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 4.0262 | Steps: 2 | Val loss: 236.2173 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3230 | Steps: 2 | Val loss: 2.3614 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8008 | Steps: 2 | Val loss: 2.0467 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.8579 | Steps: 2 | Val loss: 2.3265 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=56084)[0m top1: 0.13246268656716417
[2m[36m(func pid=56084)[0m top5: 0.8339552238805971
[2m[36m(func pid=56084)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=56084)[0m f1_macro: 0.08919060229967773
[2m[36m(func pid=56084)[0m f1_weighted: 0.10858208251172516
[2m[36m(func pid=56084)[0m f1_per_class: [0.058, 0.062, 0.161, 0.038, 0.0, 0.038, 0.215, 0.255, 0.066, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m top1: 0.09654850746268656
[2m[36m(func pid=67665)[0m top5: 0.5167910447761194
[2m[36m(func pid=67665)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=67665)[0m f1_macro: 0.09869345696297596
[2m[36m(func pid=67665)[0m f1_weighted: 0.09751595477129328
[2m[36m(func pid=67665)[0m f1_per_class: [0.108, 0.156, 0.226, 0.161, 0.0, 0.044, 0.021, 0.114, 0.12, 0.037]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.25419776119402987
[2m[36m(func pid=68221)[0m top5: 0.7742537313432836
[2m[36m(func pid=68221)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=68221)[0m f1_macro: 0.17493779227878525
[2m[36m(func pid=68221)[0m f1_weighted: 0.22045140680518405
[2m[36m(func pid=68221)[0m f1_per_class: [0.223, 0.117, 0.189, 0.429, 0.088, 0.015, 0.195, 0.13, 0.135, 0.23]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:04 (running for 00:39:39.24)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.026 |      0.089 |                   53 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.858 |      0.112 |                   25 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   2.323 |      0.099 |                    4 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.801 |      0.175 |                    3 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.12220149253731344
[2m[36m(func pid=62698)[0m top5: 0.5550373134328358
[2m[36m(func pid=62698)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=62698)[0m f1_macro: 0.11156390627932049
[2m[36m(func pid=62698)[0m f1_weighted: 0.12580823792354512
[2m[36m(func pid=62698)[0m f1_per_class: [0.114, 0.188, 0.2, 0.176, 0.0, 0.082, 0.066, 0.138, 0.094, 0.057]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.0342 | Steps: 2 | Val loss: 2.3047 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.8281 | Steps: 2 | Val loss: 293.1682 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.3115 | Steps: 2 | Val loss: 2.0171 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.7218 | Steps: 2 | Val loss: 2.3202 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=67665)[0m top1: 0.1226679104477612
[2m[36m(func pid=67665)[0m top5: 0.5718283582089553
[2m[36m(func pid=67665)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=67665)[0m f1_macro: 0.11878659493774923
[2m[36m(func pid=67665)[0m f1_weighted: 0.12959648909588023
[2m[36m(func pid=67665)[0m f1_per_class: [0.121, 0.19, 0.22, 0.192, 0.02, 0.059, 0.067, 0.14, 0.117, 0.062]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=56084)[0m top1: 0.10261194029850747
[2m[36m(func pid=56084)[0m top5: 0.8390858208955224
[2m[36m(func pid=56084)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=56084)[0m f1_macro: 0.08013680047785736
[2m[36m(func pid=56084)[0m f1_weighted: 0.06645031748489333
[2m[36m(func pid=56084)[0m f1_per_class: [0.053, 0.087, 0.22, 0.035, 0.0, 0.087, 0.05, 0.22, 0.049, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m top1: 0.291044776119403
[2m[36m(func pid=68221)[0m top5: 0.7798507462686567
[2m[36m(func pid=68221)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=68221)[0m f1_macro: 0.21486366180366678
[2m[36m(func pid=68221)[0m f1_weighted: 0.19908728230005923
[2m[36m(func pid=68221)[0m f1_per_class: [0.347, 0.1, 0.282, 0.475, 0.136, 0.0, 0.036, 0.364, 0.143, 0.267]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=62698)[0m top1: 0.125
[2m[36m(func pid=62698)[0m top5: 0.5611007462686567
[2m[36m(func pid=62698)[0m f1_micro: 0.125
[2m[36m(func pid=62698)[0m f1_macro: 0.11500058213494231
[2m[36m(func pid=62698)[0m f1_weighted: 0.1287691882954794
[2m[36m(func pid=62698)[0m f1_per_class: [0.13, 0.185, 0.2, 0.174, 0.0, 0.086, 0.076, 0.141, 0.095, 0.063]
[2m[36m(func pid=62698)[0m 
== Status ==
Current time: 2024-01-07 03:59:09 (running for 00:39:44.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.828 |      0.08  |                   54 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.722 |      0.115 |                   26 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   2.034 |      0.119 |                    5 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.312 |      0.215 |                    4 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.7134 | Steps: 2 | Val loss: 2.2356 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1669 | Steps: 2 | Val loss: 349.9071 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.1642 | Steps: 2 | Val loss: 2.0353 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7389 | Steps: 2 | Val loss: 2.3119 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67665)[0m top1: 0.1501865671641791
[2m[36m(func pid=67665)[0m top5: 0.6455223880597015
[2m[36m(func pid=67665)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=67665)[0m f1_macro: 0.1337612298248433
[2m[36m(func pid=67665)[0m f1_weighted: 0.16719283642591373
[2m[36m(func pid=67665)[0m f1_per_class: [0.142, 0.217, 0.189, 0.217, 0.019, 0.081, 0.148, 0.113, 0.128, 0.084]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=56084)[0m top1: 0.09794776119402986
[2m[36m(func pid=56084)[0m top5: 0.8306902985074627
[2m[36m(func pid=56084)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=56084)[0m f1_macro: 0.07852239217424259
[2m[36m(func pid=56084)[0m f1_weighted: 0.05526610876722465
[2m[36m(func pid=56084)[0m f1_per_class: [0.054, 0.08, 0.209, 0.04, 0.0, 0.113, 0.0, 0.217, 0.073, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m top1: 0.30363805970149255
[2m[36m(func pid=68221)[0m top5: 0.7952425373134329
[2m[36m(func pid=68221)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=68221)[0m f1_macro: 0.25168941062011574
[2m[36m(func pid=68221)[0m f1_weighted: 0.21491638264555085
[2m[36m(func pid=68221)[0m f1_per_class: [0.44, 0.164, 0.348, 0.501, 0.159, 0.023, 0.006, 0.35, 0.175, 0.351]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:14 (running for 00:39:49.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.167 |      0.079 |                   55 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.739 |      0.116 |                   27 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   1.713 |      0.134 |                    6 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.164 |      0.252 |                    5 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.12733208955223882
[2m[36m(func pid=62698)[0m top5: 0.5690298507462687
[2m[36m(func pid=62698)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=62698)[0m f1_macro: 0.11590989896060602
[2m[36m(func pid=62698)[0m f1_weighted: 0.13232697290475773
[2m[36m(func pid=62698)[0m f1_per_class: [0.128, 0.186, 0.196, 0.178, 0.0, 0.086, 0.084, 0.143, 0.094, 0.065]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4270 | Steps: 2 | Val loss: 2.1754 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3730 | Steps: 2 | Val loss: 319.5571 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.0662 | Steps: 2 | Val loss: 2.0032 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6656 | Steps: 2 | Val loss: 2.3029 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67665)[0m top1: 0.18190298507462688
[2m[36m(func pid=67665)[0m top5: 0.7075559701492538
[2m[36m(func pid=67665)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=67665)[0m f1_macro: 0.1537496552257136
[2m[36m(func pid=67665)[0m f1_weighted: 0.20179157407822487
[2m[36m(func pid=67665)[0m f1_per_class: [0.152, 0.258, 0.214, 0.232, 0.021, 0.104, 0.221, 0.079, 0.14, 0.119]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=56084)[0m top1: 0.09468283582089553
[2m[36m(func pid=56084)[0m top5: 0.8213619402985075
[2m[36m(func pid=56084)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=56084)[0m f1_macro: 0.0733626230204619
[2m[36m(func pid=56084)[0m f1_weighted: 0.04890887046653975
[2m[36m(func pid=56084)[0m f1_per_class: [0.061, 0.03, 0.186, 0.059, 0.05, 0.089, 0.0, 0.228, 0.03, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3045708955223881
[2m[36m(func pid=68221)[0m top5: 0.8278917910447762
[2m[36m(func pid=68221)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=68221)[0m f1_macro: 0.28381618146807985
[2m[36m(func pid=68221)[0m f1_weighted: 0.24720336125880646
[2m[36m(func pid=68221)[0m f1_per_class: [0.504, 0.3, 0.369, 0.481, 0.172, 0.096, 0.018, 0.354, 0.19, 0.353]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:20 (running for 00:39:55.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.373 |      0.073 |                   56 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.666 |      0.121 |                   28 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   1.427 |      0.154 |                    7 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.066 |      0.284 |                    6 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.13059701492537312
[2m[36m(func pid=62698)[0m top5: 0.574160447761194
[2m[36m(func pid=62698)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=62698)[0m f1_macro: 0.12055678406889714
[2m[36m(func pid=62698)[0m f1_weighted: 0.1379471447110267
[2m[36m(func pid=62698)[0m f1_per_class: [0.13, 0.182, 0.212, 0.174, 0.0, 0.086, 0.107, 0.144, 0.103, 0.068]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.1997 | Steps: 2 | Val loss: 2.1354 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.2487 | Steps: 2 | Val loss: 311.6831 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.0668 | Steps: 2 | Val loss: 1.9653 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6765 | Steps: 2 | Val loss: 2.2917 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=67665)[0m top1: 0.20009328358208955
[2m[36m(func pid=67665)[0m top5: 0.7416044776119403
[2m[36m(func pid=67665)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=67665)[0m f1_macro: 0.16621560890543682
[2m[36m(func pid=67665)[0m f1_weighted: 0.22059758161209328
[2m[36m(func pid=67665)[0m f1_per_class: [0.163, 0.254, 0.244, 0.248, 0.038, 0.124, 0.265, 0.061, 0.134, 0.132]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=56084)[0m top1: 0.09375
[2m[36m(func pid=56084)[0m top5: 0.8152985074626866
[2m[36m(func pid=56084)[0m f1_micro: 0.09375
[2m[36m(func pid=56084)[0m f1_macro: 0.06899159921327955
[2m[36m(func pid=56084)[0m f1_weighted: 0.05097058503639285
[2m[36m(func pid=56084)[0m f1_per_class: [0.05, 0.0, 0.155, 0.078, 0.067, 0.098, 0.006, 0.236, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m top1: 0.29757462686567165
[2m[36m(func pid=68221)[0m top5: 0.8656716417910447
[2m[36m(func pid=68221)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=68221)[0m f1_macro: 0.29636756592052105
[2m[36m(func pid=68221)[0m f1_weighted: 0.2675645958944997
[2m[36m(func pid=68221)[0m f1_per_class: [0.529, 0.373, 0.364, 0.441, 0.151, 0.165, 0.057, 0.347, 0.196, 0.342]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:25 (running for 00:40:00.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.249 |      0.069 |                   57 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.677 |      0.122 |                   29 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   1.2   |      0.166 |                    8 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.067 |      0.296 |                    7 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.13292910447761194
[2m[36m(func pid=62698)[0m top5: 0.5909514925373134
[2m[36m(func pid=62698)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=62698)[0m f1_macro: 0.12190030014560109
[2m[36m(func pid=62698)[0m f1_weighted: 0.14144764303838828
[2m[36m(func pid=62698)[0m f1_per_class: [0.127, 0.179, 0.202, 0.176, 0.0, 0.085, 0.117, 0.148, 0.115, 0.07]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0480 | Steps: 2 | Val loss: 2.0910 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.0104 | Steps: 2 | Val loss: 243.6766 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.0256 | Steps: 2 | Val loss: 1.9421 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6461 | Steps: 2 | Val loss: 2.2813 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67665)[0m top1: 0.22761194029850745
[2m[36m(func pid=67665)[0m top5: 0.7784514925373134
[2m[36m(func pid=67665)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=67665)[0m f1_macro: 0.19259664078040478
[2m[36m(func pid=67665)[0m f1_weighted: 0.24799756293463163
[2m[36m(func pid=67665)[0m f1_per_class: [0.174, 0.277, 0.289, 0.282, 0.045, 0.132, 0.294, 0.119, 0.143, 0.173]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=56084)[0m top1: 0.10820895522388059
[2m[36m(func pid=56084)[0m top5: 0.8148320895522388
[2m[36m(func pid=56084)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=56084)[0m f1_macro: 0.07336095137790662
[2m[36m(func pid=56084)[0m f1_weighted: 0.07792954587261845
[2m[36m(func pid=56084)[0m f1_per_class: [0.053, 0.02, 0.146, 0.08, 0.043, 0.023, 0.108, 0.26, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3041044776119403
[2m[36m(func pid=68221)[0m top5: 0.8824626865671642
[2m[36m(func pid=68221)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=68221)[0m f1_macro: 0.3143002127502297
[2m[36m(func pid=68221)[0m f1_weighted: 0.29323503222050884
[2m[36m(func pid=68221)[0m f1_per_class: [0.549, 0.414, 0.407, 0.413, 0.117, 0.22, 0.122, 0.343, 0.212, 0.348]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:30 (running for 00:40:05.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.01  |      0.073 |                   58 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.646 |      0.127 |                   30 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   1.048 |      0.193 |                    9 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.026 |      0.314 |                    8 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.1394589552238806
[2m[36m(func pid=62698)[0m top5: 0.5960820895522388
[2m[36m(func pid=62698)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=62698)[0m f1_macro: 0.12725416195980088
[2m[36m(func pid=62698)[0m f1_weighted: 0.15239331128547512
[2m[36m(func pid=62698)[0m f1_per_class: [0.128, 0.184, 0.225, 0.19, 0.0, 0.092, 0.138, 0.131, 0.112, 0.072]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7522 | Steps: 2 | Val loss: 2.0615 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2309 | Steps: 2 | Val loss: 153.5866 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0090 | Steps: 2 | Val loss: 1.9471 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5928 | Steps: 2 | Val loss: 2.2713 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67665)[0m top1: 0.24253731343283583
[2m[36m(func pid=67665)[0m top5: 0.7952425373134329
[2m[36m(func pid=67665)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=67665)[0m f1_macro: 0.20607765618621815
[2m[36m(func pid=67665)[0m f1_weighted: 0.26284371590803196
[2m[36m(func pid=67665)[0m f1_per_class: [0.19, 0.271, 0.32, 0.311, 0.058, 0.132, 0.308, 0.155, 0.173, 0.144]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=56084)[0m top1: 0.13759328358208955
[2m[36m(func pid=56084)[0m top5: 0.8194962686567164
[2m[36m(func pid=56084)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=56084)[0m f1_macro: 0.09526725395293385
[2m[36m(func pid=56084)[0m f1_weighted: 0.12059731588765454
[2m[36m(func pid=56084)[0m f1_per_class: [0.048, 0.047, 0.156, 0.101, 0.049, 0.0, 0.214, 0.31, 0.0, 0.028]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m top1: 0.30550373134328357
[2m[36m(func pid=68221)[0m top5: 0.8857276119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=68221)[0m f1_macro: 0.32877317857460187
[2m[36m(func pid=68221)[0m f1_weighted: 0.3027656045680648
[2m[36m(func pid=68221)[0m f1_per_class: [0.549, 0.417, 0.453, 0.363, 0.11, 0.248, 0.181, 0.361, 0.218, 0.388]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:35 (running for 00:40:10.72)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.231 |      0.095 |                   59 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.593 |      0.128 |                   31 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.752 |      0.206 |                   10 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.009 |      0.329 |                    9 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.14272388059701493
[2m[36m(func pid=62698)[0m top5: 0.605410447761194
[2m[36m(func pid=62698)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=62698)[0m f1_macro: 0.12802055834307224
[2m[36m(func pid=62698)[0m f1_weighted: 0.15596020705953872
[2m[36m(func pid=62698)[0m f1_per_class: [0.129, 0.178, 0.212, 0.203, 0.0, 0.089, 0.142, 0.13, 0.118, 0.08]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6304 | Steps: 2 | Val loss: 2.0252 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.0043 | Steps: 2 | Val loss: 126.7123 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0080 | Steps: 2 | Val loss: 1.9618 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4716 | Steps: 2 | Val loss: 2.2662 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=67665)[0m top1: 0.26119402985074625
[2m[36m(func pid=67665)[0m top5: 0.8125
[2m[36m(func pid=67665)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=67665)[0m f1_macro: 0.2256321297099914
[2m[36m(func pid=67665)[0m f1_weighted: 0.2805066852556255
[2m[36m(func pid=67665)[0m f1_per_class: [0.218, 0.284, 0.421, 0.335, 0.063, 0.108, 0.338, 0.184, 0.16, 0.145]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3050373134328358
[2m[36m(func pid=68221)[0m top5: 0.8913246268656716
[2m[36m(func pid=68221)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=68221)[0m f1_macro: 0.3405363807700629
[2m[36m(func pid=68221)[0m f1_weighted: 0.304737185496959
[2m[36m(func pid=68221)[0m f1_per_class: [0.583, 0.425, 0.533, 0.336, 0.104, 0.267, 0.201, 0.346, 0.217, 0.394]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=56084)[0m top1: 0.1767723880597015
[2m[36m(func pid=56084)[0m top5: 0.8213619402985075
[2m[36m(func pid=56084)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=56084)[0m f1_macro: 0.12739492610102718
[2m[36m(func pid=56084)[0m f1_weighted: 0.17139186543106358
[2m[36m(func pid=56084)[0m f1_per_class: [0.06, 0.138, 0.255, 0.197, 0.027, 0.0, 0.237, 0.32, 0.0, 0.039]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:59:41 (running for 00:40:15.88)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.004 |      0.127 |                   60 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.472 |      0.129 |                   32 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.63  |      0.226 |                   11 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.008 |      0.341 |                   10 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.14319029850746268
[2m[36m(func pid=62698)[0m top5: 0.6082089552238806
[2m[36m(func pid=62698)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=62698)[0m f1_macro: 0.12870083523384254
[2m[36m(func pid=62698)[0m f1_weighted: 0.15616116284039955
[2m[36m(func pid=62698)[0m f1_per_class: [0.132, 0.181, 0.212, 0.195, 0.0, 0.088, 0.147, 0.137, 0.117, 0.079]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5016 | Steps: 2 | Val loss: 2.0105 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0070 | Steps: 2 | Val loss: 1.9568 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.8658 | Steps: 2 | Val loss: 169.3336 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.4957 | Steps: 2 | Val loss: 2.2518 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=67665)[0m top1: 0.26492537313432835
[2m[36m(func pid=67665)[0m top5: 0.8222947761194029
[2m[36m(func pid=67665)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=67665)[0m f1_macro: 0.23637090794097512
[2m[36m(func pid=67665)[0m f1_weighted: 0.28282447867260596
[2m[36m(func pid=67665)[0m f1_per_class: [0.243, 0.297, 0.462, 0.346, 0.079, 0.112, 0.317, 0.219, 0.167, 0.123]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.31156716417910446
[2m[36m(func pid=68221)[0m top5: 0.8955223880597015
[2m[36m(func pid=68221)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=68221)[0m f1_macro: 0.35029967808784346
[2m[36m(func pid=68221)[0m f1_weighted: 0.3157962270009494
[2m[36m(func pid=68221)[0m f1_per_class: [0.609, 0.43, 0.571, 0.31, 0.106, 0.274, 0.255, 0.34, 0.215, 0.394]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=56084)[0m top1: 0.15951492537313433
[2m[36m(func pid=56084)[0m top5: 0.8180970149253731
[2m[36m(func pid=56084)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=56084)[0m f1_macro: 0.11712328508227157
[2m[36m(func pid=56084)[0m f1_weighted: 0.15029684975816487
[2m[36m(func pid=56084)[0m f1_per_class: [0.067, 0.131, 0.256, 0.225, 0.034, 0.0, 0.149, 0.295, 0.0, 0.014]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:59:46 (running for 00:40:21.23)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.866 |      0.117 |                   61 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.496 |      0.137 |                   33 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.502 |      0.236 |                   12 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.007 |      0.35  |                   11 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.15111940298507462
[2m[36m(func pid=62698)[0m top5: 0.6226679104477612
[2m[36m(func pid=62698)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=62698)[0m f1_macro: 0.13739542879632888
[2m[36m(func pid=62698)[0m f1_weighted: 0.1657692984918342
[2m[36m(func pid=62698)[0m f1_per_class: [0.138, 0.175, 0.25, 0.213, 0.01, 0.1, 0.161, 0.135, 0.111, 0.081]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4012 | Steps: 2 | Val loss: 2.0014 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0051 | Steps: 2 | Val loss: 1.9699 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.8886 | Steps: 2 | Val loss: 164.1423 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.4137 | Steps: 2 | Val loss: 2.2438 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=67665)[0m top1: 0.26072761194029853
[2m[36m(func pid=67665)[0m top5: 0.8255597014925373
[2m[36m(func pid=67665)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=67665)[0m f1_macro: 0.23755686812930427
[2m[36m(func pid=67665)[0m f1_weighted: 0.27603774425122685
[2m[36m(func pid=67665)[0m f1_per_class: [0.262, 0.298, 0.468, 0.353, 0.074, 0.116, 0.282, 0.221, 0.178, 0.124]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.31343283582089554
[2m[36m(func pid=68221)[0m top5: 0.9015858208955224
[2m[36m(func pid=68221)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=68221)[0m f1_macro: 0.3519409697078318
[2m[36m(func pid=68221)[0m f1_weighted: 0.3220191648123313
[2m[36m(func pid=68221)[0m f1_per_class: [0.581, 0.433, 0.6, 0.297, 0.096, 0.276, 0.29, 0.323, 0.218, 0.406]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=56084)[0m top1: 0.10960820895522388
[2m[36m(func pid=56084)[0m top5: 0.8059701492537313
[2m[36m(func pid=56084)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=56084)[0m f1_macro: 0.07304589258854967
[2m[36m(func pid=56084)[0m f1_weighted: 0.0747599792365781
[2m[36m(func pid=56084)[0m f1_per_class: [0.058, 0.037, 0.16, 0.176, 0.0, 0.0, 0.003, 0.276, 0.0, 0.021]
[2m[36m(func pid=56084)[0m 
== Status ==
Current time: 2024-01-07 03:59:51 (running for 00:40:26.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.889 |      0.073 |                   62 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.414 |      0.142 |                   34 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.401 |      0.238 |                   13 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.005 |      0.352 |                   12 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.15764925373134328
[2m[36m(func pid=62698)[0m top5: 0.6371268656716418
[2m[36m(func pid=62698)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=62698)[0m f1_macro: 0.14234908271900049
[2m[36m(func pid=62698)[0m f1_weighted: 0.1735201086225464
[2m[36m(func pid=62698)[0m f1_per_class: [0.143, 0.181, 0.257, 0.223, 0.01, 0.099, 0.173, 0.14, 0.118, 0.08]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3191 | Steps: 2 | Val loss: 1.9939 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0064 | Steps: 2 | Val loss: 1.9581 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.4300 | Steps: 2 | Val loss: 119.1599 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.4057 | Steps: 2 | Val loss: 2.2397 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67665)[0m top1: 0.2635261194029851
[2m[36m(func pid=67665)[0m top5: 0.8213619402985075
[2m[36m(func pid=67665)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=67665)[0m f1_macro: 0.24846414083027063
[2m[36m(func pid=67665)[0m f1_weighted: 0.27369964924346885
[2m[36m(func pid=67665)[0m f1_per_class: [0.28, 0.301, 0.512, 0.367, 0.083, 0.114, 0.249, 0.253, 0.197, 0.127]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.31902985074626866
[2m[36m(func pid=68221)[0m top5: 0.9076492537313433
[2m[36m(func pid=68221)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=68221)[0m f1_macro: 0.3526181124912075
[2m[36m(func pid=68221)[0m f1_weighted: 0.3308899499520257
[2m[36m(func pid=68221)[0m f1_per_class: [0.553, 0.435, 0.615, 0.29, 0.093, 0.277, 0.326, 0.32, 0.223, 0.394]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 03:59:56 (running for 00:40:31.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.43  |      0.067 |                   63 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.414 |      0.142 |                   34 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.319 |      0.248 |                   14 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.006 |      0.353 |                   13 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56084)[0m top1: 0.10587686567164178
[2m[36m(func pid=56084)[0m top5: 0.7919776119402985
[2m[36m(func pid=56084)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=56084)[0m f1_macro: 0.06662664912683669
[2m[36m(func pid=56084)[0m f1_weighted: 0.07033498386400991
[2m[36m(func pid=56084)[0m f1_per_class: [0.05, 0.015, 0.12, 0.176, 0.0, 0.0, 0.0, 0.287, 0.0, 0.017]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=62698)[0m top1: 0.15858208955223882
[2m[36m(func pid=62698)[0m top5: 0.6399253731343284
[2m[36m(func pid=62698)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=62698)[0m f1_macro: 0.14457481072625802
[2m[36m(func pid=62698)[0m f1_weighted: 0.17491758124959358
[2m[36m(func pid=62698)[0m f1_per_class: [0.146, 0.179, 0.274, 0.226, 0.01, 0.092, 0.177, 0.142, 0.122, 0.079]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2680 | Steps: 2 | Val loss: 1.9743 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0044 | Steps: 2 | Val loss: 1.9627 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.3629 | Steps: 2 | Val loss: 2.2302 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6805 | Steps: 2 | Val loss: 78.8485 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=67665)[0m top1: 0.26865671641791045
[2m[36m(func pid=67665)[0m top5: 0.8269589552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=67665)[0m f1_macro: 0.25231008881689226
[2m[36m(func pid=67665)[0m f1_weighted: 0.27459249120675405
[2m[36m(func pid=67665)[0m f1_per_class: [0.286, 0.299, 0.524, 0.392, 0.087, 0.121, 0.226, 0.255, 0.204, 0.13]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.31949626865671643
[2m[36m(func pid=68221)[0m top5: 0.90625
[2m[36m(func pid=68221)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=68221)[0m f1_macro: 0.35617391927591
[2m[36m(func pid=68221)[0m f1_weighted: 0.33058920679489545
[2m[36m(func pid=68221)[0m f1_per_class: [0.559, 0.441, 0.649, 0.269, 0.091, 0.269, 0.342, 0.325, 0.229, 0.388]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:00:02 (running for 00:40:36.87)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   3.43  |      0.067 |                   63 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.363 |      0.146 |                   36 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.268 |      0.252 |                   15 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.004 |      0.356 |                   14 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.16044776119402984
[2m[36m(func pid=62698)[0m top5: 0.6436567164179104
[2m[36m(func pid=62698)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=62698)[0m f1_macro: 0.14592299973936013
[2m[36m(func pid=62698)[0m f1_weighted: 0.17614111987595005
[2m[36m(func pid=62698)[0m f1_per_class: [0.147, 0.191, 0.274, 0.217, 0.01, 0.096, 0.181, 0.144, 0.12, 0.08]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.16044776119402984
[2m[36m(func pid=56084)[0m top5: 0.7896455223880597
[2m[36m(func pid=56084)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=56084)[0m f1_macro: 0.0856419410328094
[2m[36m(func pid=56084)[0m f1_weighted: 0.11680707381175615
[2m[36m(func pid=56084)[0m f1_per_class: [0.064, 0.005, 0.135, 0.33, 0.0, 0.0, 0.015, 0.294, 0.0, 0.013]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3286 | Steps: 2 | Val loss: 1.9564 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0076 | Steps: 2 | Val loss: 1.9569 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3264 | Steps: 2 | Val loss: 2.2253 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6192 | Steps: 2 | Val loss: 48.4442 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67665)[0m top1: 0.27425373134328357
[2m[36m(func pid=67665)[0m top5: 0.8339552238805971
[2m[36m(func pid=67665)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=67665)[0m f1_macro: 0.2571915147779252
[2m[36m(func pid=67665)[0m f1_weighted: 0.2765847617298466
[2m[36m(func pid=67665)[0m f1_per_class: [0.289, 0.293, 0.524, 0.413, 0.108, 0.127, 0.212, 0.259, 0.207, 0.141]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.32136194029850745
[2m[36m(func pid=68221)[0m top5: 0.909981343283582
[2m[36m(func pid=68221)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=68221)[0m f1_macro: 0.3580245136101529
[2m[36m(func pid=68221)[0m f1_weighted: 0.3333066353498021
[2m[36m(func pid=68221)[0m f1_per_class: [0.568, 0.443, 0.667, 0.289, 0.093, 0.269, 0.331, 0.319, 0.235, 0.366]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:00:07 (running for 00:40:42.17)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.68  |      0.086 |                   64 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.326 |      0.148 |                   37 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.329 |      0.257 |                   16 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.008 |      0.358 |                   15 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.166044776119403
[2m[36m(func pid=62698)[0m top5: 0.6487873134328358
[2m[36m(func pid=62698)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=62698)[0m f1_macro: 0.14773870350408033
[2m[36m(func pid=62698)[0m f1_weighted: 0.18296596977451157
[2m[36m(func pid=62698)[0m f1_per_class: [0.144, 0.198, 0.256, 0.224, 0.011, 0.104, 0.188, 0.154, 0.119, 0.079]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.310634328358209
[2m[36m(func pid=56084)[0m top5: 0.7868470149253731
[2m[36m(func pid=56084)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=56084)[0m f1_macro: 0.13187948794607318
[2m[36m(func pid=56084)[0m f1_weighted: 0.23888854726026693
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.005, 0.198, 0.514, 0.0, 0.0, 0.246, 0.346, 0.0, 0.009]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2615 | Steps: 2 | Val loss: 1.9520 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0030 | Steps: 2 | Val loss: 1.9626 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.2953 | Steps: 2 | Val loss: 2.2192 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6858 | Steps: 2 | Val loss: 36.7334 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67665)[0m top1: 0.27472014925373134
[2m[36m(func pid=67665)[0m top5: 0.8316231343283582
[2m[36m(func pid=67665)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=67665)[0m f1_macro: 0.25806556020821214
[2m[36m(func pid=67665)[0m f1_weighted: 0.27155147171647515
[2m[36m(func pid=67665)[0m f1_per_class: [0.295, 0.303, 0.537, 0.422, 0.107, 0.121, 0.181, 0.271, 0.192, 0.152]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.32322761194029853
[2m[36m(func pid=68221)[0m top5: 0.9081156716417911
[2m[36m(func pid=68221)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=68221)[0m f1_macro: 0.36122517838542534
[2m[36m(func pid=68221)[0m f1_weighted: 0.33312759336179637
[2m[36m(func pid=68221)[0m f1_per_class: [0.587, 0.447, 0.667, 0.301, 0.097, 0.262, 0.314, 0.338, 0.238, 0.361]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:00:12 (running for 00:40:47.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.619 |      0.132 |                   65 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.295 |      0.149 |                   38 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.262 |      0.258 |                   17 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.003 |      0.361 |                   16 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.16791044776119404
[2m[36m(func pid=62698)[0m top5: 0.6529850746268657
[2m[36m(func pid=62698)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=62698)[0m f1_macro: 0.14949727267383797
[2m[36m(func pid=62698)[0m f1_weighted: 0.1846022203477197
[2m[36m(func pid=62698)[0m f1_per_class: [0.148, 0.195, 0.256, 0.234, 0.01, 0.108, 0.183, 0.157, 0.126, 0.078]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.3101679104477612
[2m[36m(func pid=56084)[0m top5: 0.7826492537313433
[2m[36m(func pid=56084)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=56084)[0m f1_macro: 0.12646143480368244
[2m[36m(func pid=56084)[0m f1_weighted: 0.2281787122305327
[2m[36m(func pid=56084)[0m f1_per_class: [0.0, 0.005, 0.207, 0.515, 0.0, 0.0, 0.214, 0.324, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1905 | Steps: 2 | Val loss: 1.9557 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0026 | Steps: 2 | Val loss: 1.9637 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.3509 | Steps: 2 | Val loss: 2.2113 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3907 | Steps: 2 | Val loss: 50.9000 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=68221)[0m top1: 0.324160447761194
[2m[36m(func pid=68221)[0m top5: 0.9076492537313433
[2m[36m(func pid=68221)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=68221)[0m f1_macro: 0.36626390933949887
[2m[36m(func pid=68221)[0m f1_weighted: 0.33242722905505767
[2m[36m(func pid=68221)[0m f1_per_class: [0.593, 0.449, 0.667, 0.304, 0.099, 0.259, 0.308, 0.342, 0.238, 0.405]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.27472014925373134
[2m[36m(func pid=67665)[0m top5: 0.8311567164179104
[2m[36m(func pid=67665)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=67665)[0m f1_macro: 0.2649927713613181
[2m[36m(func pid=67665)[0m f1_weighted: 0.2703320526710142
[2m[36m(func pid=67665)[0m f1_per_class: [0.29, 0.302, 0.579, 0.415, 0.126, 0.139, 0.176, 0.263, 0.21, 0.149]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:17 (running for 00:40:52.64)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.686 |      0.126 |                   66 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.351 |      0.155 |                   39 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.191 |      0.265 |                   18 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.003 |      0.366 |                   17 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.1730410447761194
[2m[36m(func pid=62698)[0m top5: 0.6613805970149254
[2m[36m(func pid=62698)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=62698)[0m f1_macro: 0.15473609574693317
[2m[36m(func pid=62698)[0m f1_weighted: 0.18992887184158866
[2m[36m(func pid=62698)[0m f1_per_class: [0.154, 0.205, 0.27, 0.24, 0.011, 0.111, 0.187, 0.155, 0.131, 0.082]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.20475746268656717
[2m[36m(func pid=56084)[0m top5: 0.7742537313432836
[2m[36m(func pid=56084)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=56084)[0m f1_macro: 0.10298089038177238
[2m[36m(func pid=56084)[0m f1_weighted: 0.14944602979676824
[2m[36m(func pid=56084)[0m f1_per_class: [0.067, 0.028, 0.134, 0.383, 0.065, 0.024, 0.056, 0.273, 0.0, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1827 | Steps: 2 | Val loss: 1.9514 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0035 | Steps: 2 | Val loss: 1.9393 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.1844 | Steps: 2 | Val loss: 2.2098 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4127 | Steps: 2 | Val loss: 55.6136 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=68221)[0m top1: 0.3302238805970149
[2m[36m(func pid=68221)[0m top5: 0.9085820895522388
[2m[36m(func pid=68221)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=68221)[0m f1_macro: 0.36646705473847657
[2m[36m(func pid=68221)[0m f1_weighted: 0.34084784567406134
[2m[36m(func pid=68221)[0m f1_per_class: [0.609, 0.453, 0.667, 0.328, 0.107, 0.255, 0.316, 0.333, 0.218, 0.38]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.2789179104477612
[2m[36m(func pid=67665)[0m top5: 0.8362873134328358
[2m[36m(func pid=67665)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=67665)[0m f1_macro: 0.27455489004614064
[2m[36m(func pid=67665)[0m f1_weighted: 0.27547020189764354
[2m[36m(func pid=67665)[0m f1_per_class: [0.29, 0.31, 0.629, 0.415, 0.134, 0.146, 0.184, 0.264, 0.212, 0.162]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:22 (running for 00:40:57.71)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.391 |      0.103 |                   67 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.184 |      0.156 |                   40 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.183 |      0.275 |                   19 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.003 |      0.366 |                   18 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.17164179104477612
[2m[36m(func pid=62698)[0m top5: 0.659981343283582
[2m[36m(func pid=62698)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=62698)[0m f1_macro: 0.15620257480460342
[2m[36m(func pid=62698)[0m f1_weighted: 0.18886149968104013
[2m[36m(func pid=62698)[0m f1_per_class: [0.154, 0.203, 0.286, 0.234, 0.01, 0.112, 0.189, 0.157, 0.136, 0.081]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.15205223880597016
[2m[36m(func pid=56084)[0m top5: 0.7756529850746269
[2m[36m(func pid=56084)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=56084)[0m f1_macro: 0.0986047783137433
[2m[36m(func pid=56084)[0m f1_weighted: 0.12094462686938809
[2m[36m(func pid=56084)[0m f1_per_class: [0.058, 0.04, 0.118, 0.265, 0.077, 0.081, 0.039, 0.27, 0.039, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0057 | Steps: 2 | Val loss: 1.9327 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2009 | Steps: 2 | Val loss: 1.9284 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.2295 | Steps: 2 | Val loss: 2.2053 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.9643 | Steps: 2 | Val loss: 55.8219 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=68221)[0m top1: 0.3358208955223881
[2m[36m(func pid=68221)[0m top5: 0.9127798507462687
[2m[36m(func pid=68221)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=68221)[0m f1_macro: 0.37640453768291604
[2m[36m(func pid=68221)[0m f1_weighted: 0.34616657098774495
[2m[36m(func pid=68221)[0m f1_per_class: [0.609, 0.46, 0.706, 0.344, 0.108, 0.26, 0.306, 0.354, 0.217, 0.4]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.28171641791044777
[2m[36m(func pid=67665)[0m top5: 0.8423507462686567
[2m[36m(func pid=67665)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=67665)[0m f1_macro: 0.27586501898804133
[2m[36m(func pid=67665)[0m f1_weighted: 0.27955280790551523
[2m[36m(func pid=67665)[0m f1_per_class: [0.287, 0.306, 0.629, 0.421, 0.137, 0.144, 0.195, 0.265, 0.211, 0.164]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:28 (running for 00:41:02.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.413 |      0.099 |                   68 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.23  |      0.159 |                   41 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.201 |      0.276 |                   20 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.006 |      0.376 |                   19 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.1730410447761194
[2m[36m(func pid=62698)[0m top5: 0.6627798507462687
[2m[36m(func pid=62698)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=62698)[0m f1_macro: 0.15949605672506595
[2m[36m(func pid=62698)[0m f1_weighted: 0.19027940880596803
[2m[36m(func pid=62698)[0m f1_per_class: [0.163, 0.204, 0.313, 0.239, 0.019, 0.109, 0.191, 0.152, 0.123, 0.083]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.09095149253731344
[2m[36m(func pid=56084)[0m top5: 0.7555970149253731
[2m[36m(func pid=56084)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=56084)[0m f1_macro: 0.06544685332745559
[2m[36m(func pid=56084)[0m f1_weighted: 0.051211262970686657
[2m[36m(func pid=56084)[0m f1_per_class: [0.047, 0.044, 0.123, 0.066, 0.0, 0.053, 0.0, 0.272, 0.05, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0047 | Steps: 2 | Val loss: 1.9317 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1151 | Steps: 2 | Val loss: 1.9188 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2468 | Steps: 2 | Val loss: 2.1981 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1563 | Steps: 2 | Val loss: 49.2692 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=68221)[0m top1: 0.3358208955223881
[2m[36m(func pid=68221)[0m top5: 0.9151119402985075
[2m[36m(func pid=68221)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=68221)[0m f1_macro: 0.3765675768453348
[2m[36m(func pid=68221)[0m f1_weighted: 0.3470570321734453
[2m[36m(func pid=68221)[0m f1_per_class: [0.615, 0.458, 0.706, 0.362, 0.107, 0.254, 0.297, 0.346, 0.22, 0.4]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.283115671641791
[2m[36m(func pid=67665)[0m top5: 0.8530783582089553
[2m[36m(func pid=67665)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=67665)[0m f1_macro: 0.2783193868840309
[2m[36m(func pid=67665)[0m f1_weighted: 0.2808326178506272
[2m[36m(func pid=67665)[0m f1_per_class: [0.286, 0.306, 0.629, 0.423, 0.148, 0.155, 0.193, 0.265, 0.211, 0.169]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:33 (running for 00:41:08.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.964 |      0.065 |                   69 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.247 |      0.161 |                   42 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.115 |      0.278 |                   21 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.005 |      0.377 |                   20 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.17397388059701493
[2m[36m(func pid=62698)[0m top5: 0.6711753731343284
[2m[36m(func pid=62698)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=62698)[0m f1_macro: 0.16114187259654825
[2m[36m(func pid=62698)[0m f1_weighted: 0.1906409399107571
[2m[36m(func pid=62698)[0m f1_per_class: [0.166, 0.208, 0.323, 0.24, 0.02, 0.106, 0.19, 0.153, 0.123, 0.085]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.07742537313432836
[2m[36m(func pid=56084)[0m top5: 0.7318097014925373
[2m[36m(func pid=56084)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=56084)[0m f1_macro: 0.055828628120961524
[2m[36m(func pid=56084)[0m f1_weighted: 0.0272055618190985
[2m[36m(func pid=56084)[0m f1_per_class: [0.044, 0.027, 0.138, 0.003, 0.0, 0.016, 0.0, 0.289, 0.041, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0028 | Steps: 2 | Val loss: 1.9245 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1074 | Steps: 2 | Val loss: 1.9046 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.1546 | Steps: 2 | Val loss: 2.1915 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3060 | Steps: 2 | Val loss: 41.9709 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=67665)[0m top1: 0.291044776119403
[2m[36m(func pid=67665)[0m top5: 0.8600746268656716
[2m[36m(func pid=67665)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=67665)[0m f1_macro: 0.28614818690670063
[2m[36m(func pid=67665)[0m f1_weighted: 0.2912273893013663
[2m[36m(func pid=67665)[0m f1_per_class: [0.284, 0.32, 0.629, 0.423, 0.157, 0.18, 0.209, 0.272, 0.202, 0.186]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3414179104477612
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=68221)[0m f1_macro: 0.3773205061912378
[2m[36m(func pid=68221)[0m f1_weighted: 0.35196936076533925
[2m[36m(func pid=68221)[0m f1_per_class: [0.622, 0.46, 0.686, 0.383, 0.113, 0.242, 0.297, 0.346, 0.224, 0.4]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:00:38 (running for 00:41:13.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.156 |      0.056 |                   70 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.155 |      0.165 |                   43 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.107 |      0.286 |                   22 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.003 |      0.377 |                   21 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.17863805970149255
[2m[36m(func pid=62698)[0m top5: 0.6781716417910447
[2m[36m(func pid=62698)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=62698)[0m f1_macro: 0.16493517370210814
[2m[36m(func pid=62698)[0m f1_weighted: 0.19691257524947983
[2m[36m(func pid=62698)[0m f1_per_class: [0.17, 0.218, 0.345, 0.249, 0.019, 0.11, 0.196, 0.145, 0.118, 0.08]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.07929104477611941
[2m[36m(func pid=56084)[0m top5: 0.7374067164179104
[2m[36m(func pid=56084)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=56084)[0m f1_macro: 0.058003423711349755
[2m[36m(func pid=56084)[0m f1_weighted: 0.033709821845471916
[2m[36m(func pid=56084)[0m f1_per_class: [0.047, 0.032, 0.107, 0.003, 0.0, 0.066, 0.0, 0.295, 0.029, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1119 | Steps: 2 | Val loss: 1.8885 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0022 | Steps: 2 | Val loss: 1.9297 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1423 | Steps: 2 | Val loss: 2.1877 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 4.0806 | Steps: 2 | Val loss: 41.4045 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67665)[0m top1: 0.2966417910447761
[2m[36m(func pid=67665)[0m top5: 0.8656716417910447
[2m[36m(func pid=67665)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=67665)[0m f1_macro: 0.2877987974823883
[2m[36m(func pid=67665)[0m f1_weighted: 0.2965725184090295
[2m[36m(func pid=67665)[0m f1_per_class: [0.296, 0.31, 0.611, 0.44, 0.162, 0.192, 0.212, 0.27, 0.192, 0.192]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.34281716417910446
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=68221)[0m f1_macro: 0.3767069586506585
[2m[36m(func pid=68221)[0m f1_weighted: 0.35061973497641047
[2m[36m(func pid=68221)[0m f1_per_class: [0.629, 0.468, 0.686, 0.412, 0.111, 0.243, 0.259, 0.352, 0.227, 0.38]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:00:43 (running for 00:41:18.83)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.306 |      0.058 |                   71 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.142 |      0.169 |                   44 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.112 |      0.288 |                   23 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.377 |                   22 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.18097014925373134
[2m[36m(func pid=62698)[0m top5: 0.6833022388059702
[2m[36m(func pid=62698)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=62698)[0m f1_macro: 0.1688636044246584
[2m[36m(func pid=62698)[0m f1_weighted: 0.200484578184263
[2m[36m(func pid=62698)[0m f1_per_class: [0.161, 0.222, 0.364, 0.246, 0.028, 0.11, 0.207, 0.152, 0.12, 0.078]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.10774253731343283
[2m[36m(func pid=56084)[0m top5: 0.7779850746268657
[2m[36m(func pid=56084)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=56084)[0m f1_macro: 0.09669091317167945
[2m[36m(func pid=56084)[0m f1_weighted: 0.0697165260916104
[2m[36m(func pid=56084)[0m f1_per_class: [0.042, 0.157, 0.207, 0.0, 0.0, 0.181, 0.0, 0.296, 0.084, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1167 | Steps: 2 | Val loss: 1.8797 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0025 | Steps: 2 | Val loss: 1.9271 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0644 | Steps: 2 | Val loss: 2.1856 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.9048 | Steps: 2 | Val loss: 31.9945 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=68221)[0m top1: 0.3423507462686567
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=68221)[0m f1_macro: 0.37176980517161357
[2m[36m(func pid=68221)[0m f1_weighted: 0.3509732323627155
[2m[36m(func pid=68221)[0m f1_per_class: [0.609, 0.462, 0.686, 0.419, 0.112, 0.237, 0.261, 0.352, 0.231, 0.349]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.2994402985074627
[2m[36m(func pid=67665)[0m top5: 0.8694029850746269
[2m[36m(func pid=67665)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=67665)[0m f1_macro: 0.289956358223338
[2m[36m(func pid=67665)[0m f1_weighted: 0.29663468516970565
[2m[36m(func pid=67665)[0m f1_per_class: [0.306, 0.332, 0.595, 0.439, 0.155, 0.192, 0.199, 0.268, 0.2, 0.213]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:49 (running for 00:41:24.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   4.081 |      0.097 |                   72 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.064 |      0.168 |                   45 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.117 |      0.29  |                   24 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.003 |      0.372 |                   23 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.17957089552238806
[2m[36m(func pid=62698)[0m top5: 0.6870335820895522
[2m[36m(func pid=62698)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=62698)[0m f1_macro: 0.16846508698811716
[2m[36m(func pid=62698)[0m f1_weighted: 0.1980400316037098
[2m[36m(func pid=62698)[0m f1_per_class: [0.167, 0.224, 0.364, 0.242, 0.019, 0.106, 0.202, 0.155, 0.122, 0.084]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.14132462686567165
[2m[36m(func pid=56084)[0m top5: 0.7472014925373134
[2m[36m(func pid=56084)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=56084)[0m f1_macro: 0.11225701593552961
[2m[36m(func pid=56084)[0m f1_weighted: 0.09366439092884216
[2m[36m(func pid=56084)[0m f1_per_class: [0.058, 0.193, 0.158, 0.0, 0.0, 0.334, 0.0, 0.307, 0.073, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0013 | Steps: 2 | Val loss: 1.9239 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0838 | Steps: 2 | Val loss: 1.8757 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.0266 | Steps: 2 | Val loss: 2.1808 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.8980 | Steps: 2 | Val loss: 23.0863 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=68221)[0m top1: 0.34468283582089554
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=68221)[0m f1_macro: 0.3708213827567545
[2m[36m(func pid=68221)[0m f1_weighted: 0.3530835080406108
[2m[36m(func pid=68221)[0m f1_per_class: [0.586, 0.464, 0.686, 0.424, 0.114, 0.238, 0.263, 0.358, 0.227, 0.349]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.29990671641791045
[2m[36m(func pid=67665)[0m top5: 0.8717350746268657
[2m[36m(func pid=67665)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=67665)[0m f1_macro: 0.29283328794976704
[2m[36m(func pid=67665)[0m f1_weighted: 0.3011061098400335
[2m[36m(func pid=67665)[0m f1_per_class: [0.306, 0.334, 0.595, 0.435, 0.164, 0.2, 0.215, 0.266, 0.191, 0.222]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:54 (running for 00:41:29.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.905 |      0.112 |                   73 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.027 |      0.173 |                   46 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.084 |      0.293 |                   25 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.371 |                   24 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.18470149253731344
[2m[36m(func pid=62698)[0m top5: 0.6926305970149254
[2m[36m(func pid=62698)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=62698)[0m f1_macro: 0.17332196820856788
[2m[36m(func pid=62698)[0m f1_weighted: 0.2036390429379946
[2m[36m(func pid=62698)[0m f1_per_class: [0.168, 0.23, 0.364, 0.242, 0.03, 0.11, 0.214, 0.152, 0.138, 0.086]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.134794776119403
[2m[36m(func pid=56084)[0m top5: 0.7653917910447762
[2m[36m(func pid=56084)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=56084)[0m f1_macro: 0.11132840382487703
[2m[36m(func pid=56084)[0m f1_weighted: 0.10503935113702932
[2m[36m(func pid=56084)[0m f1_per_class: [0.049, 0.037, 0.145, 0.0, 0.0, 0.336, 0.12, 0.346, 0.082, 0.0]
[2m[36m(func pid=56084)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0022 | Steps: 2 | Val loss: 1.9120 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0762 | Steps: 2 | Val loss: 1.8686 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.1679 | Steps: 2 | Val loss: 2.1779 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=68221)[0m top1: 0.3451492537313433
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=68221)[0m f1_macro: 0.37080472968150474
[2m[36m(func pid=68221)[0m f1_weighted: 0.3534905115587928
[2m[36m(func pid=68221)[0m f1_per_class: [0.592, 0.462, 0.686, 0.426, 0.12, 0.222, 0.271, 0.35, 0.231, 0.349]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=56084)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.5944 | Steps: 2 | Val loss: 17.9699 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=67665)[0m top1: 0.30130597014925375
[2m[36m(func pid=67665)[0m top5: 0.8759328358208955
[2m[36m(func pid=67665)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=67665)[0m f1_macro: 0.2928820116888805
[2m[36m(func pid=67665)[0m f1_weighted: 0.30314082870664505
[2m[36m(func pid=67665)[0m f1_per_class: [0.32, 0.343, 0.579, 0.431, 0.149, 0.192, 0.222, 0.272, 0.196, 0.226]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:00:59 (running for 00:41:34.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3715
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00019 | RUNNING    | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.898 |      0.111 |                   74 |
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.168 |      0.174 |                   47 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.076 |      0.293 |                   26 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.371 |                   25 |
| train_2d480_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.18423507462686567
[2m[36m(func pid=62698)[0m top5: 0.6907649253731343
[2m[36m(func pid=62698)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=62698)[0m f1_macro: 0.17394336488421008
[2m[36m(func pid=62698)[0m f1_weighted: 0.2031977966936223
[2m[36m(func pid=62698)[0m f1_per_class: [0.172, 0.225, 0.373, 0.241, 0.028, 0.1, 0.218, 0.16, 0.138, 0.084]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=56084)[0m top1: 0.11240671641791045
[2m[36m(func pid=56084)[0m top5: 0.7565298507462687
[2m[36m(func pid=56084)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=56084)[0m f1_macro: 0.09748792813927952
[2m[36m(func pid=56084)[0m f1_weighted: 0.10097713757033783
[2m[36m(func pid=56084)[0m f1_per_class: [0.047, 0.011, 0.117, 0.0, 0.047, 0.097, 0.204, 0.419, 0.034, 0.0]
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0020 | Steps: 2 | Val loss: 1.9139 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1043 | Steps: 2 | Val loss: 1.8593 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0596 | Steps: 2 | Val loss: 2.1685 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=68221)[0m top1: 0.345615671641791
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=68221)[0m f1_macro: 0.3746484067969095
[2m[36m(func pid=68221)[0m f1_weighted: 0.35294836213473874
[2m[36m(func pid=68221)[0m f1_per_class: [0.611, 0.464, 0.686, 0.429, 0.117, 0.22, 0.262, 0.352, 0.24, 0.366]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.302705223880597
[2m[36m(func pid=67665)[0m top5: 0.8815298507462687
[2m[36m(func pid=67665)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=67665)[0m f1_macro: 0.29560168960734695
[2m[36m(func pid=67665)[0m f1_weighted: 0.3079967313099052
[2m[36m(func pid=67665)[0m f1_per_class: [0.33, 0.338, 0.579, 0.429, 0.142, 0.194, 0.24, 0.267, 0.204, 0.232]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=62698)[0m top1: 0.18936567164179105
[2m[36m(func pid=62698)[0m top5: 0.699160447761194
[2m[36m(func pid=62698)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=62698)[0m f1_macro: 0.17706925446772834
[2m[36m(func pid=62698)[0m f1_weighted: 0.20678266232058282
[2m[36m(func pid=62698)[0m f1_per_class: [0.172, 0.242, 0.379, 0.254, 0.028, 0.106, 0.206, 0.157, 0.13, 0.095]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.8976 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0701 | Steps: 2 | Val loss: 1.8450 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9789 | Steps: 2 | Val loss: 2.1596 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=68221)[0m top1: 0.3530783582089552
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=68221)[0m f1_macro: 0.3774005474574762
[2m[36m(func pid=68221)[0m f1_weighted: 0.36264901054108845
[2m[36m(func pid=68221)[0m f1_per_class: [0.598, 0.463, 0.686, 0.438, 0.121, 0.234, 0.282, 0.356, 0.244, 0.353]
[2m[36m(func pid=67665)[0m top1: 0.310634328358209
[2m[36m(func pid=67665)[0m top5: 0.8847947761194029
[2m[36m(func pid=67665)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=67665)[0m f1_macro: 0.2978699805547895
[2m[36m(func pid=67665)[0m f1_weighted: 0.3143745313295366
[2m[36m(func pid=67665)[0m f1_per_class: [0.338, 0.363, 0.564, 0.437, 0.146, 0.196, 0.24, 0.265, 0.203, 0.226]
== Status ==
Current time: 2024-01-07 04:01:05 (running for 00:41:40.01)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.06  |      0.177 |                   48 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.104 |      0.296 |                   27 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.375 |                   26 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=74157)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=74157)[0m Configuration completed!
[2m[36m(func pid=74157)[0m New optimizer parameters:
[2m[36m(func pid=74157)[0m SGD (
[2m[36m(func pid=74157)[0m Parameter Group 0
[2m[36m(func pid=74157)[0m     dampening: 0
[2m[36m(func pid=74157)[0m     differentiable: False
[2m[36m(func pid=74157)[0m     foreach: None
[2m[36m(func pid=74157)[0m     lr: 0.1
[2m[36m(func pid=74157)[0m     maximize: False
[2m[36m(func pid=74157)[0m     momentum: 0.9
[2m[36m(func pid=74157)[0m     nesterov: False
[2m[36m(func pid=74157)[0m     weight_decay: 1e-05
[2m[36m(func pid=74157)[0m )
[2m[36m(func pid=74157)[0m 
== Status ==
Current time: 2024-01-07 04:01:10 (running for 00:41:45.36)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.979 |      0.178 |                   49 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.07  |      0.298 |                   28 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.377 |                   27 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.19309701492537312
[2m[36m(func pid=62698)[0m top5: 0.707089552238806
[2m[36m(func pid=62698)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=62698)[0m f1_macro: 0.17784782927472337
[2m[36m(func pid=62698)[0m f1_weighted: 0.21046991084879121
[2m[36m(func pid=62698)[0m f1_per_class: [0.173, 0.244, 0.367, 0.254, 0.029, 0.103, 0.219, 0.161, 0.131, 0.099]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.1029 | Steps: 2 | Val loss: 1.8364 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0022 | Steps: 2 | Val loss: 1.8997 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0008 | Steps: 2 | Val loss: 2.1566 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.5415 | Steps: 2 | Val loss: 5.4962 | Batch size: 32 | lr: 0.1 | Duration: 4.64s
[2m[36m(func pid=67665)[0m top1: 0.31296641791044777
[2m[36m(func pid=67665)[0m top5: 0.8894589552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=67665)[0m f1_macro: 0.2994101866373823
[2m[36m(func pid=67665)[0m f1_weighted: 0.3167265139565295
[2m[36m(func pid=67665)[0m f1_per_class: [0.338, 0.365, 0.564, 0.44, 0.14, 0.19, 0.245, 0.268, 0.205, 0.239]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.34701492537313433
[2m[36m(func pid=68221)[0m top5: 0.9211753731343284
[2m[36m(func pid=68221)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=68221)[0m f1_macro: 0.3713011676721053
[2m[36m(func pid=68221)[0m f1_weighted: 0.3558379258534116
[2m[36m(func pid=68221)[0m f1_per_class: [0.594, 0.467, 0.686, 0.432, 0.121, 0.233, 0.267, 0.35, 0.222, 0.341]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:01:15 (running for 00:41:50.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.001 |      0.178 |                   50 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.103 |      0.299 |                   29 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.371 |                   28 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.19542910447761194
[2m[36m(func pid=62698)[0m top5: 0.7052238805970149
[2m[36m(func pid=62698)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=62698)[0m f1_macro: 0.17779476893756274
[2m[36m(func pid=62698)[0m f1_weighted: 0.21238495502384835
[2m[36m(func pid=62698)[0m f1_per_class: [0.171, 0.253, 0.349, 0.255, 0.029, 0.103, 0.218, 0.165, 0.132, 0.102]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.024720149253731342
[2m[36m(func pid=74157)[0m top5: 0.49486940298507465
[2m[36m(func pid=74157)[0m f1_micro: 0.024720149253731342
[2m[36m(func pid=74157)[0m f1_macro: 0.05125721379802213
[2m[36m(func pid=74157)[0m f1_weighted: 0.006725375453510671
[2m[36m(func pid=74157)[0m f1_per_class: [0.042, 0.005, 0.435, 0.0, 0.0, 0.016, 0.0, 0.0, 0.015, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.8973 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0662 | Steps: 2 | Val loss: 1.8181 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0042 | Steps: 2 | Val loss: 2.1477 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.2435 | Steps: 2 | Val loss: 23.5970 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=67665)[0m top1: 0.322294776119403
[2m[36m(func pid=67665)[0m top5: 0.8941231343283582
[2m[36m(func pid=67665)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=67665)[0m f1_macro: 0.3076784829540081
[2m[36m(func pid=67665)[0m f1_weighted: 0.3249678443021994
[2m[36m(func pid=67665)[0m f1_per_class: [0.348, 0.369, 0.595, 0.451, 0.137, 0.198, 0.255, 0.266, 0.215, 0.243]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.35261194029850745
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=68221)[0m f1_macro: 0.37580883474537713
[2m[36m(func pid=68221)[0m f1_weighted: 0.3618639886118116
[2m[36m(func pid=68221)[0m f1_per_class: [0.588, 0.469, 0.686, 0.438, 0.12, 0.23, 0.277, 0.359, 0.245, 0.345]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:01:21 (running for 00:41:55.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.004 |      0.184 |                   51 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.066 |      0.308 |                   30 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.376 |                   29 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.542 |      0.051 |                    1 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.19962686567164178
[2m[36m(func pid=62698)[0m top5: 0.715018656716418
[2m[36m(func pid=62698)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=62698)[0m f1_macro: 0.18376783626572152
[2m[36m(func pid=62698)[0m f1_weighted: 0.21593006199185133
[2m[36m(func pid=62698)[0m f1_per_class: [0.177, 0.26, 0.379, 0.257, 0.031, 0.105, 0.221, 0.17, 0.136, 0.101]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.16557835820895522
[2m[36m(func pid=74157)[0m top5: 0.5368470149253731
[2m[36m(func pid=74157)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=74157)[0m f1_macro: 0.10830204674741632
[2m[36m(func pid=74157)[0m f1_weighted: 0.07522772673460446
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.277, 0.167, 0.0, 0.0, 0.0, 0.0, 0.345, 0.146, 0.148]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0708 | Steps: 2 | Val loss: 1.8140 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8843 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0101 | Steps: 2 | Val loss: 2.1428 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5144 | Steps: 2 | Val loss: 283.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=67665)[0m top1: 0.32975746268656714
[2m[36m(func pid=67665)[0m top5: 0.8917910447761194
[2m[36m(func pid=67665)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=67665)[0m f1_macro: 0.313229267683389
[2m[36m(func pid=67665)[0m f1_weighted: 0.3340136314117803
[2m[36m(func pid=67665)[0m f1_per_class: [0.344, 0.376, 0.611, 0.454, 0.128, 0.195, 0.277, 0.273, 0.222, 0.252]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3516791044776119
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=68221)[0m f1_macro: 0.3716080412949684
[2m[36m(func pid=68221)[0m f1_weighted: 0.36126296624171983
[2m[36m(func pid=68221)[0m f1_per_class: [0.556, 0.464, 0.686, 0.447, 0.12, 0.228, 0.275, 0.352, 0.233, 0.356]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:01:26 (running for 00:42:01.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   1.01  |      0.184 |                   52 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.071 |      0.313 |                   31 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.372 |                   30 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.243 |      0.108 |                    2 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.197294776119403
[2m[36m(func pid=62698)[0m top5: 0.7234141791044776
[2m[36m(func pid=62698)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=62698)[0m f1_macro: 0.18372133943605815
[2m[36m(func pid=62698)[0m f1_weighted: 0.21463417168273632
[2m[36m(func pid=62698)[0m f1_per_class: [0.177, 0.245, 0.386, 0.256, 0.03, 0.102, 0.226, 0.17, 0.145, 0.1]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.00792910447761194
[2m[36m(func pid=74157)[0m top5: 0.48880597014925375
[2m[36m(func pid=74157)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=74157)[0m f1_macro: 0.0028818711624228536
[2m[36m(func pid=74157)[0m f1_weighted: 0.0002342039603337726
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0652 | Steps: 2 | Val loss: 1.8083 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.8798 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.9987 | Steps: 2 | Val loss: 2.1354 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7936 | Steps: 2 | Val loss: 716.1860 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=68221)[0m top1: 0.3558768656716418
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=68221)[0m f1_macro: 0.3743740700175392
[2m[36m(func pid=68221)[0m f1_weighted: 0.36645753396085173
[2m[36m(func pid=68221)[0m f1_per_class: [0.55, 0.465, 0.686, 0.444, 0.123, 0.235, 0.292, 0.345, 0.248, 0.356]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.32882462686567165
[2m[36m(func pid=67665)[0m top5: 0.8950559701492538
[2m[36m(func pid=67665)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=67665)[0m f1_macro: 0.310875148576088
[2m[36m(func pid=67665)[0m f1_weighted: 0.33549230346533476
[2m[36m(func pid=67665)[0m f1_per_class: [0.336, 0.379, 0.595, 0.446, 0.125, 0.19, 0.293, 0.261, 0.221, 0.263]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:01:31 (running for 00:42:06.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.999 |      0.189 |                   53 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.065 |      0.311 |                   32 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.374 |                   31 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.514 |      0.003 |                    3 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.20475746268656717
[2m[36m(func pid=62698)[0m top5: 0.7308768656716418
[2m[36m(func pid=62698)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=62698)[0m f1_macro: 0.189392371501426
[2m[36m(func pid=62698)[0m f1_weighted: 0.2225132287816092
[2m[36m(func pid=62698)[0m f1_per_class: [0.177, 0.256, 0.407, 0.276, 0.04, 0.098, 0.229, 0.175, 0.134, 0.102]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.006063432835820896
[2m[36m(func pid=74157)[0m top5: 0.49673507462686567
[2m[36m(func pid=74157)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=74157)[0m f1_macro: 0.001270772238514174
[2m[36m(func pid=74157)[0m f1_weighted: 7.705242117856465e-05
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0777 | Steps: 2 | Val loss: 1.8011 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8748 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8640 | Steps: 2 | Val loss: 2.1339 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.1660 | Steps: 2 | Val loss: 29049.2578 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=67665)[0m top1: 0.3269589552238806
[2m[36m(func pid=67665)[0m top5: 0.8969216417910447
[2m[36m(func pid=67665)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=67665)[0m f1_macro: 0.30754107310821566
[2m[36m(func pid=67665)[0m f1_weighted: 0.3335116891833377
[2m[36m(func pid=67665)[0m f1_per_class: [0.358, 0.373, 0.579, 0.448, 0.123, 0.187, 0.29, 0.255, 0.217, 0.245]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.35634328358208955
[2m[36m(func pid=68221)[0m top5: 0.9169776119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=68221)[0m f1_macro: 0.3742644656388957
[2m[36m(func pid=68221)[0m f1_weighted: 0.3659889077813424
[2m[36m(func pid=68221)[0m f1_per_class: [0.55, 0.466, 0.686, 0.44, 0.126, 0.231, 0.294, 0.348, 0.249, 0.352]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=62698)[0m top1: 0.20662313432835822
[2m[36m(func pid=62698)[0m top5: 0.7336753731343284
[2m[36m(func pid=62698)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=62698)[0m f1_macro: 0.19171300089983168
[2m[36m(func pid=62698)[0m f1_weighted: 0.2249391467218084
[2m[36m(func pid=62698)[0m f1_per_class: [0.174, 0.257, 0.415, 0.275, 0.04, 0.103, 0.234, 0.177, 0.136, 0.106]== Status ==
Current time: 2024-01-07 04:01:37 (running for 00:42:11.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.864 |      0.192 |                   54 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.078 |      0.308 |                   33 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.374 |                   32 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   0.794 |      0.001 |                    4 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.006063432835820896
[2m[36m(func pid=74157)[0m top5: 0.5093283582089553
[2m[36m(func pid=74157)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=74157)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=74157)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0495 | Steps: 2 | Val loss: 1.8019 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.8896 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9338 | Steps: 2 | Val loss: 2.1234 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.9630 | Steps: 2 | Val loss: 1159213.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=67665)[0m top1: 0.3260261194029851
[2m[36m(func pid=67665)[0m top5: 0.8936567164179104
[2m[36m(func pid=67665)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=67665)[0m f1_macro: 0.31259983341180936
[2m[36m(func pid=67665)[0m f1_weighted: 0.3321479304695393
[2m[36m(func pid=67665)[0m f1_per_class: [0.362, 0.374, 0.595, 0.439, 0.125, 0.196, 0.287, 0.256, 0.221, 0.27]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3568097014925373
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=68221)[0m f1_macro: 0.3738152118695666
[2m[36m(func pid=68221)[0m f1_weighted: 0.3658878420326446
[2m[36m(func pid=68221)[0m f1_per_class: [0.55, 0.467, 0.686, 0.454, 0.123, 0.223, 0.284, 0.347, 0.248, 0.356]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:01:42 (running for 00:42:17.18)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.934 |      0.193 |                   55 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.05  |      0.313 |                   34 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.374 |                   33 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.166 |      0.001 |                    5 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.20848880597014927
[2m[36m(func pid=62698)[0m top5: 0.7430037313432836
[2m[36m(func pid=62698)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=62698)[0m f1_macro: 0.19296682932728154
[2m[36m(func pid=62698)[0m f1_weighted: 0.22563611485711366
[2m[36m(func pid=62698)[0m f1_per_class: [0.184, 0.257, 0.415, 0.278, 0.03, 0.103, 0.232, 0.18, 0.138, 0.112]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.006063432835820896
[2m[36m(func pid=74157)[0m top5: 0.5093283582089553
[2m[36m(func pid=74157)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=74157)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=74157)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.8678 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0485 | Steps: 2 | Val loss: 1.7988 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9202 | Steps: 2 | Val loss: 2.1216 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 5.4100 | Steps: 2 | Val loss: 137453424.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=68221)[0m top1: 0.3596082089552239
[2m[36m(func pid=68221)[0m top5: 0.9202425373134329
[2m[36m(func pid=68221)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=68221)[0m f1_macro: 0.3763650372043855
[2m[36m(func pid=68221)[0m f1_weighted: 0.36741506533339946
[2m[36m(func pid=68221)[0m f1_per_class: [0.554, 0.466, 0.686, 0.454, 0.127, 0.225, 0.287, 0.349, 0.253, 0.364]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3283582089552239
[2m[36m(func pid=67665)[0m top5: 0.894589552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=67665)[0m f1_macro: 0.31519622862057634
[2m[36m(func pid=67665)[0m f1_weighted: 0.3370793339679117
[2m[36m(func pid=67665)[0m f1_per_class: [0.356, 0.373, 0.611, 0.437, 0.129, 0.204, 0.304, 0.258, 0.221, 0.259]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:01:47 (running for 00:42:22.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.92  |      0.193 |                   56 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.049 |      0.315 |                   35 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.376 |                   34 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   5.963 |      0.001 |                    6 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.20848880597014927
[2m[36m(func pid=62698)[0m top5: 0.7458022388059702
[2m[36m(func pid=62698)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=62698)[0m f1_macro: 0.19346460685242295
[2m[36m(func pid=62698)[0m f1_weighted: 0.22521301367692836
[2m[36m(func pid=62698)[0m f1_per_class: [0.18, 0.265, 0.423, 0.276, 0.031, 0.105, 0.229, 0.176, 0.136, 0.113]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.03311567164179104
[2m[36m(func pid=74157)[0m top5: 0.5149253731343284
[2m[36m(func pid=74157)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=74157)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=74157)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.8855 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0613 | Steps: 2 | Val loss: 1.7918 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7964 | Steps: 2 | Val loss: 2.1205 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0145 | Steps: 2 | Val loss: 10157180.0000 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=68221)[0m top1: 0.35447761194029853
[2m[36m(func pid=68221)[0m top5: 0.9146455223880597
[2m[36m(func pid=68221)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=68221)[0m f1_macro: 0.3725402398195684
[2m[36m(func pid=68221)[0m f1_weighted: 0.3620594914787856
[2m[36m(func pid=68221)[0m f1_per_class: [0.549, 0.462, 0.686, 0.439, 0.121, 0.22, 0.288, 0.349, 0.256, 0.356]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3306902985074627
[2m[36m(func pid=67665)[0m top5: 0.8992537313432836
[2m[36m(func pid=67665)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=67665)[0m f1_macro: 0.3174162117019915
[2m[36m(func pid=67665)[0m f1_weighted: 0.33863931493138244
[2m[36m(func pid=67665)[0m f1_per_class: [0.358, 0.393, 0.611, 0.433, 0.131, 0.201, 0.301, 0.258, 0.227, 0.261]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:01:53 (running for 00:42:27.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.796 |      0.194 |                   57 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.061 |      0.317 |                   36 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.373 |                   35 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   5.41  |      0.006 |                    7 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.20988805970149255
[2m[36m(func pid=62698)[0m top5: 0.7439365671641791
[2m[36m(func pid=62698)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=62698)[0m f1_macro: 0.19398682039243437
[2m[36m(func pid=62698)[0m f1_weighted: 0.2263983656596141
[2m[36m(func pid=62698)[0m f1_per_class: [0.193, 0.266, 0.423, 0.279, 0.031, 0.102, 0.231, 0.178, 0.124, 0.113]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.006063432835820896
[2m[36m(func pid=74157)[0m top5: 0.5093283582089553
[2m[36m(func pid=74157)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=74157)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=74157)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8945 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0589 | Steps: 2 | Val loss: 1.7854 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7895 | Steps: 2 | Val loss: 2.1176 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 9.9679 | Steps: 2 | Val loss: 1043149.8125 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=67665)[0m top1: 0.3306902985074627
[2m[36m(func pid=67665)[0m top5: 0.9020522388059702
[2m[36m(func pid=67665)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=67665)[0m f1_macro: 0.3150862516676667
[2m[36m(func pid=67665)[0m f1_weighted: 0.339713249471335
[2m[36m(func pid=67665)[0m f1_per_class: [0.351, 0.377, 0.611, 0.441, 0.136, 0.208, 0.306, 0.246, 0.231, 0.242]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3521455223880597
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=68221)[0m f1_macro: 0.3691392300309308
[2m[36m(func pid=68221)[0m f1_weighted: 0.357438276620725
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.459, 0.686, 0.442, 0.119, 0.209, 0.276, 0.345, 0.26, 0.352]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:01:58 (running for 00:42:33.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.79  |      0.196 |                   58 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.059 |      0.315 |                   37 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.369 |                   36 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.015 |      0.001 |                    8 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21222014925373134
[2m[36m(func pid=62698)[0m top5: 0.7444029850746269
[2m[36m(func pid=62698)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=62698)[0m f1_macro: 0.19645533929163075
[2m[36m(func pid=62698)[0m f1_weighted: 0.2280767035503186
[2m[36m(func pid=62698)[0m f1_per_class: [0.195, 0.267, 0.423, 0.28, 0.033, 0.106, 0.23, 0.182, 0.134, 0.113]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.03311567164179104
[2m[36m(func pid=74157)[0m top5: 0.5149253731343284
[2m[36m(func pid=74157)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=74157)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=74157)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0438 | Steps: 2 | Val loss: 1.7933 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.8753 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7820 | Steps: 2 | Val loss: 2.1158 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 7.9105 | Steps: 2 | Val loss: 546039.8750 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=67665)[0m top1: 0.32882462686567165
[2m[36m(func pid=67665)[0m top5: 0.8964552238805971
[2m[36m(func pid=67665)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=67665)[0m f1_macro: 0.31248867659330637
[2m[36m(func pid=67665)[0m f1_weighted: 0.3394988292439634
[2m[36m(func pid=67665)[0m f1_per_class: [0.345, 0.379, 0.595, 0.435, 0.129, 0.2, 0.311, 0.264, 0.23, 0.236]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3591417910447761
[2m[36m(func pid=68221)[0m top5: 0.9165111940298507
[2m[36m(func pid=68221)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=68221)[0m f1_macro: 0.3730034896838691
[2m[36m(func pid=68221)[0m f1_weighted: 0.36722075793794495
[2m[36m(func pid=68221)[0m f1_per_class: [0.554, 0.462, 0.686, 0.452, 0.124, 0.204, 0.3, 0.344, 0.253, 0.352]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:02:03 (running for 00:42:38.49)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.782 |      0.196 |                   59 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.044 |      0.312 |                   38 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.373 |                   37 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   9.968 |      0.006 |                    9 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21222014925373134
[2m[36m(func pid=62698)[0m top5: 0.7481343283582089
[2m[36m(func pid=62698)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=62698)[0m f1_macro: 0.1955247015037925
[2m[36m(func pid=62698)[0m f1_weighted: 0.22805746843526195
[2m[36m(func pid=62698)[0m f1_per_class: [0.19, 0.265, 0.415, 0.275, 0.035, 0.107, 0.236, 0.18, 0.143, 0.108]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.03311567164179104
[2m[36m(func pid=74157)[0m top5: 0.5149253731343284
[2m[36m(func pid=74157)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=74157)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=74157)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0619 | Steps: 2 | Val loss: 1.7892 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0016 | Steps: 2 | Val loss: 1.8674 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8605 | Steps: 2 | Val loss: 2.1151 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 7.2409 | Steps: 2 | Val loss: 244871.2812 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=67665)[0m top1: 0.3306902985074627
[2m[36m(func pid=67665)[0m top5: 0.898320895522388
[2m[36m(func pid=67665)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=67665)[0m f1_macro: 0.31545348899398323
[2m[36m(func pid=67665)[0m f1_weighted: 0.34182185115716773
[2m[36m(func pid=67665)[0m f1_per_class: [0.335, 0.383, 0.615, 0.438, 0.129, 0.198, 0.314, 0.268, 0.231, 0.242]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3591417910447761
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=68221)[0m f1_macro: 0.3734697382078799
[2m[36m(func pid=68221)[0m f1_weighted: 0.366062842771441
[2m[36m(func pid=68221)[0m f1_per_class: [0.557, 0.463, 0.667, 0.445, 0.123, 0.202, 0.301, 0.347, 0.256, 0.374]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:02:08 (running for 00:42:43.75)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.861 |      0.197 |                   60 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.062 |      0.315 |                   39 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.373 |                   38 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   7.911 |      0.006 |                   10 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21175373134328357
[2m[36m(func pid=62698)[0m top5: 0.75
[2m[36m(func pid=62698)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=62698)[0m f1_macro: 0.19656196952523874
[2m[36m(func pid=62698)[0m f1_weighted: 0.22843354864553406
[2m[36m(func pid=62698)[0m f1_per_class: [0.189, 0.259, 0.415, 0.282, 0.046, 0.11, 0.233, 0.177, 0.149, 0.104]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.006063432835820896
[2m[36m(func pid=74157)[0m top5: 0.49300373134328357
[2m[36m(func pid=74157)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=74157)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=74157)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0636 | Steps: 2 | Val loss: 1.7773 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8742 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7550 | Steps: 2 | Val loss: 2.1098 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 8.5084 | Steps: 2 | Val loss: 92701.2500 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=68221)[0m top1: 0.36100746268656714
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=68221)[0m f1_macro: 0.37631066336037966
[2m[36m(func pid=68221)[0m f1_weighted: 0.3685757932479327
[2m[36m(func pid=68221)[0m f1_per_class: [0.564, 0.467, 0.667, 0.447, 0.121, 0.224, 0.297, 0.348, 0.256, 0.374]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.33302238805970147
[2m[36m(func pid=67665)[0m top5: 0.9029850746268657
[2m[36m(func pid=67665)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=67665)[0m f1_macro: 0.31820739074944854
[2m[36m(func pid=67665)[0m f1_weighted: 0.34246545935155975
[2m[36m(func pid=67665)[0m f1_per_class: [0.35, 0.386, 0.615, 0.445, 0.135, 0.199, 0.307, 0.259, 0.235, 0.25]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:14 (running for 00:42:49.15)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.755 |      0.196 |                   61 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.064 |      0.318 |                   40 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.376 |                   39 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   7.241 |      0.001 |                   11 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21455223880597016
[2m[36m(func pid=62698)[0m top5: 0.7527985074626866
[2m[36m(func pid=62698)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=62698)[0m f1_macro: 0.1963978402790327
[2m[36m(func pid=62698)[0m f1_weighted: 0.23109655465457132
[2m[36m(func pid=62698)[0m f1_per_class: [0.192, 0.266, 0.423, 0.291, 0.036, 0.106, 0.233, 0.18, 0.133, 0.105]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.17024253731343283
[2m[36m(func pid=74157)[0m top5: 0.4864738805970149
[2m[36m(func pid=74157)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=74157)[0m f1_macro: 0.033424908424908424
[2m[36m(func pid=74157)[0m f1_weighted: 0.057527011235033614
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0018 | Steps: 2 | Val loss: 1.8512 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0515 | Steps: 2 | Val loss: 1.7856 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7229 | Steps: 2 | Val loss: 2.1092 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 6.0131 | Steps: 2 | Val loss: 7615.8247 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=68221)[0m top1: 0.36613805970149255
[2m[36m(func pid=68221)[0m top5: 0.9188432835820896
[2m[36m(func pid=68221)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=68221)[0m f1_macro: 0.3797503449362073
[2m[36m(func pid=68221)[0m f1_weighted: 0.37516550141614624
[2m[36m(func pid=68221)[0m f1_per_class: [0.549, 0.466, 0.686, 0.454, 0.129, 0.238, 0.309, 0.351, 0.242, 0.375]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3260261194029851
[2m[36m(func pid=67665)[0m top5: 0.8992537313432836
[2m[36m(func pid=67665)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=67665)[0m f1_macro: 0.3136518359934919
[2m[36m(func pid=67665)[0m f1_weighted: 0.33500623878013314
[2m[36m(func pid=67665)[0m f1_per_class: [0.337, 0.376, 0.615, 0.439, 0.142, 0.202, 0.293, 0.264, 0.232, 0.236]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:19 (running for 00:42:54.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.723 |      0.198 |                   62 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.051 |      0.314 |                   41 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.38  |                   40 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   8.508 |      0.033 |                   12 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21641791044776118
[2m[36m(func pid=62698)[0m top5: 0.75
[2m[36m(func pid=62698)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=62698)[0m f1_macro: 0.1984822240699291
[2m[36m(func pid=62698)[0m f1_weighted: 0.2330141474425443
[2m[36m(func pid=62698)[0m f1_per_class: [0.188, 0.271, 0.431, 0.291, 0.035, 0.105, 0.235, 0.186, 0.134, 0.108]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.034981343283582086
[2m[36m(func pid=74157)[0m top5: 0.37546641791044777
[2m[36m(func pid=74157)[0m f1_micro: 0.034981343283582086
[2m[36m(func pid=74157)[0m f1_macro: 0.04543267985993683
[2m[36m(func pid=74157)[0m f1_weighted: 0.025600958389685513
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.441, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0018 | Steps: 2 | Val loss: 1.8667 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0546 | Steps: 2 | Val loss: 1.7901 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7738 | Steps: 2 | Val loss: 2.1018 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=68221)[0m top1: 0.3619402985074627
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=68221)[0m f1_macro: 0.37705233640651653
[2m[36m(func pid=68221)[0m f1_weighted: 0.3726991514834055
[2m[36m(func pid=68221)[0m f1_per_class: [0.559, 0.47, 0.686, 0.465, 0.122, 0.218, 0.296, 0.344, 0.251, 0.36]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.1151 | Steps: 2 | Val loss: 5249.5527 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=67665)[0m top1: 0.32882462686567165
[2m[36m(func pid=67665)[0m top5: 0.8973880597014925
[2m[36m(func pid=67665)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=67665)[0m f1_macro: 0.31287363275603053
[2m[36m(func pid=67665)[0m f1_weighted: 0.33795197656756143
[2m[36m(func pid=67665)[0m f1_per_class: [0.338, 0.389, 0.6, 0.436, 0.134, 0.195, 0.301, 0.263, 0.237, 0.234]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:24 (running for 00:42:59.55)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.774 |      0.2   |                   63 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.055 |      0.313 |                   42 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.377 |                   41 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   6.013 |      0.045 |                   13 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21455223880597016
[2m[36m(func pid=62698)[0m top5: 0.7588619402985075
[2m[36m(func pid=62698)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=62698)[0m f1_macro: 0.19968598990223463
[2m[36m(func pid=62698)[0m f1_weighted: 0.22911022206545173
[2m[36m(func pid=62698)[0m f1_per_class: [0.195, 0.272, 0.431, 0.283, 0.046, 0.105, 0.228, 0.183, 0.144, 0.11]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.054104477611940295
[2m[36m(func pid=74157)[0m top5: 0.4864738805970149
[2m[36m(func pid=74157)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=74157)[0m f1_macro: 0.03652470380058544
[2m[36m(func pid=74157)[0m f1_weighted: 0.023915451074888613
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.005, 0.016, 0.0, 0.0, 0.054, 0.0, 0.29, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8658 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0392 | Steps: 2 | Val loss: 1.7939 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7338 | Steps: 2 | Val loss: 2.0982 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.3289 | Steps: 2 | Val loss: 2714.8743 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=68221)[0m top1: 0.363339552238806
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=68221)[0m f1_macro: 0.3775205772540706
[2m[36m(func pid=68221)[0m f1_weighted: 0.37330281617782907
[2m[36m(func pid=68221)[0m f1_per_class: [0.549, 0.469, 0.686, 0.455, 0.126, 0.216, 0.308, 0.347, 0.254, 0.366]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.32742537313432835
[2m[36m(func pid=67665)[0m top5: 0.8969216417910447
[2m[36m(func pid=67665)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=67665)[0m f1_macro: 0.3107930976697134
[2m[36m(func pid=67665)[0m f1_weighted: 0.33657135222263423
[2m[36m(func pid=67665)[0m f1_per_class: [0.342, 0.392, 0.585, 0.431, 0.13, 0.188, 0.3, 0.27, 0.239, 0.229]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:30 (running for 00:43:04.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.734 |      0.203 |                   64 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.039 |      0.311 |                   43 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.378 |                   42 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   4.115 |      0.037 |                   14 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21735074626865672
[2m[36m(func pid=62698)[0m top5: 0.7574626865671642
[2m[36m(func pid=62698)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=62698)[0m f1_macro: 0.2033330123903308
[2m[36m(func pid=62698)[0m f1_weighted: 0.2312143135410944
[2m[36m(func pid=62698)[0m f1_per_class: [0.192, 0.27, 0.449, 0.29, 0.036, 0.106, 0.224, 0.195, 0.159, 0.112]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.05317164179104478
[2m[36m(func pid=74157)[0m top5: 0.5223880597014925
[2m[36m(func pid=74157)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=74157)[0m f1_macro: 0.024284213375960397
[2m[36m(func pid=74157)[0m f1_weighted: 0.013066716821483517
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.0, 0.019, 0.0, 0.0, 0.0, 0.0, 0.224, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8502 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0454 | Steps: 2 | Val loss: 1.7971 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7186 | Steps: 2 | Val loss: 2.0976 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=68221)[0m top1: 0.3666044776119403
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=68221)[0m f1_macro: 0.3780205451243829
[2m[36m(func pid=68221)[0m f1_weighted: 0.3759476255942757
[2m[36m(func pid=68221)[0m f1_per_class: [0.547, 0.468, 0.686, 0.472, 0.131, 0.214, 0.304, 0.341, 0.257, 0.362]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 6.1101 | Steps: 2 | Val loss: 6853.0054 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=67665)[0m top1: 0.3269589552238806
[2m[36m(func pid=67665)[0m top5: 0.8969216417910447
[2m[36m(func pid=67665)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=67665)[0m f1_macro: 0.3141278499861667
[2m[36m(func pid=67665)[0m f1_weighted: 0.33518538008257553
[2m[36m(func pid=67665)[0m f1_per_class: [0.345, 0.401, 0.615, 0.425, 0.13, 0.189, 0.296, 0.268, 0.237, 0.234]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:35 (running for 00:43:10.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.719 |      0.205 |                   65 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.045 |      0.314 |                   44 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.378 |                   43 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   4.329 |      0.024 |                   15 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.21828358208955223
[2m[36m(func pid=62698)[0m top5: 0.7621268656716418
[2m[36m(func pid=62698)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=62698)[0m f1_macro: 0.2049310673310154
[2m[36m(func pid=62698)[0m f1_weighted: 0.23255857131105567
[2m[36m(func pid=62698)[0m f1_per_class: [0.2, 0.269, 0.458, 0.3, 0.045, 0.107, 0.221, 0.191, 0.149, 0.108]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.15205223880597016
[2m[36m(func pid=74157)[0m top5: 0.6450559701492538
[2m[36m(func pid=74157)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=74157)[0m f1_macro: 0.049289739767648375
[2m[36m(func pid=74157)[0m f1_weighted: 0.06396541245692755
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.183, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.8610 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0467 | Steps: 2 | Val loss: 1.7964 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7035 | Steps: 2 | Val loss: 2.0954 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=68221)[0m top1: 0.3628731343283582
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=68221)[0m f1_macro: 0.37303145616938577
[2m[36m(func pid=68221)[0m f1_weighted: 0.37218688424194146
[2m[36m(func pid=68221)[0m f1_per_class: [0.537, 0.468, 0.686, 0.464, 0.13, 0.197, 0.308, 0.334, 0.256, 0.352]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 4.5457 | Steps: 2 | Val loss: 6835.0977 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=67665)[0m top1: 0.32975746268656714
[2m[36m(func pid=67665)[0m top5: 0.8931902985074627
[2m[36m(func pid=67665)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=67665)[0m f1_macro: 0.31263617079024014
[2m[36m(func pid=67665)[0m f1_weighted: 0.33698754945983594
[2m[36m(func pid=67665)[0m f1_per_class: [0.347, 0.409, 0.6, 0.43, 0.131, 0.183, 0.297, 0.258, 0.241, 0.231]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:40 (running for 00:43:15.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.703 |      0.207 |                   66 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.047 |      0.313 |                   45 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.373 |                   44 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   6.11  |      0.049 |                   16 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.2178171641791045
[2m[36m(func pid=62698)[0m top5: 0.7602611940298507
[2m[36m(func pid=62698)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=62698)[0m f1_macro: 0.2074948132732209
[2m[36m(func pid=62698)[0m f1_weighted: 0.22991011884550372
[2m[36m(func pid=62698)[0m f1_per_class: [0.206, 0.268, 0.489, 0.303, 0.047, 0.107, 0.209, 0.193, 0.142, 0.111]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.13992537313432835
[2m[36m(func pid=74157)[0m top5: 0.5802238805970149
[2m[36m(func pid=74157)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=74157)[0m f1_macro: 0.054302178505526696
[2m[36m(func pid=74157)[0m f1_weighted: 0.06107187051669284
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.175, 0.0, 0.077]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.8851 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0418 | Steps: 2 | Val loss: 1.7956 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6842 | Steps: 2 | Val loss: 2.0869 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5704 | Steps: 2 | Val loss: 5305.4902 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=68221)[0m top1: 0.35494402985074625
[2m[36m(func pid=68221)[0m top5: 0.9137126865671642
[2m[36m(func pid=68221)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=68221)[0m f1_macro: 0.3724843569350954
[2m[36m(func pid=68221)[0m f1_weighted: 0.36176881563584123
[2m[36m(func pid=68221)[0m f1_per_class: [0.555, 0.459, 0.686, 0.438, 0.12, 0.198, 0.297, 0.343, 0.263, 0.366]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.332089552238806
[2m[36m(func pid=67665)[0m top5: 0.8927238805970149
[2m[36m(func pid=67665)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=67665)[0m f1_macro: 0.3108231461449621
[2m[36m(func pid=67665)[0m f1_weighted: 0.3409555194701237
[2m[36m(func pid=67665)[0m f1_per_class: [0.348, 0.4, 0.585, 0.435, 0.13, 0.178, 0.313, 0.258, 0.246, 0.214]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:02:45 (running for 00:43:20.68)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.684 |      0.21  |                   67 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.042 |      0.311 |                   46 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.372 |                   45 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   4.546 |      0.054 |                   17 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.22294776119402984
[2m[36m(func pid=62698)[0m top5: 0.7653917910447762
[2m[36m(func pid=62698)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=62698)[0m f1_macro: 0.2103784640183383
[2m[36m(func pid=62698)[0m f1_weighted: 0.23472549147762406
[2m[36m(func pid=62698)[0m f1_per_class: [0.202, 0.28, 0.468, 0.304, 0.057, 0.112, 0.214, 0.196, 0.156, 0.115]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.13199626865671643
[2m[36m(func pid=74157)[0m top5: 0.5713619402985075
[2m[36m(func pid=74157)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=74157)[0m f1_macro: 0.0454728370221328
[2m[36m(func pid=74157)[0m f1_weighted: 0.05905704198854232
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.168, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0020 | Steps: 2 | Val loss: 1.9078 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0424 | Steps: 2 | Val loss: 1.7913 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6521 | Steps: 2 | Val loss: 2.0851 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3970 | Steps: 2 | Val loss: 4320.1938 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=67665)[0m top1: 0.33302238805970147
[2m[36m(func pid=67665)[0m top5: 0.894589552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=67665)[0m f1_macro: 0.3107749519359858
[2m[36m(func pid=67665)[0m f1_weighted: 0.34271400639537686
[2m[36m(func pid=67665)[0m f1_per_class: [0.343, 0.391, 0.585, 0.433, 0.137, 0.184, 0.325, 0.253, 0.243, 0.213]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3516791044776119
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=68221)[0m f1_macro: 0.37287811432256845
[2m[36m(func pid=68221)[0m f1_weighted: 0.3564287564037925
[2m[36m(func pid=68221)[0m f1_per_class: [0.561, 0.458, 0.667, 0.44, 0.12, 0.215, 0.27, 0.356, 0.261, 0.382]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:02:51 (running for 00:43:26.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.652 |      0.21  |                   68 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.042 |      0.311 |                   47 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.373 |                   46 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.57  |      0.045 |                   18 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.22201492537313433
[2m[36m(func pid=62698)[0m top5: 0.7677238805970149
[2m[36m(func pid=62698)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=62698)[0m f1_macro: 0.20953248182826933
[2m[36m(func pid=62698)[0m f1_weighted: 0.23291748616550595
[2m[36m(func pid=62698)[0m f1_per_class: [0.205, 0.277, 0.458, 0.304, 0.06, 0.112, 0.209, 0.193, 0.163, 0.115]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.13152985074626866
[2m[36m(func pid=74157)[0m top5: 0.5802238805970149
[2m[36m(func pid=74157)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=74157)[0m f1_macro: 0.04666092037021942
[2m[36m(func pid=74157)[0m f1_weighted: 0.06119963920170786
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.28, 0.0, 0.0, 0.0, 0.014, 0.006, 0.167, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0405 | Steps: 2 | Val loss: 1.7920 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.9032 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6441 | Steps: 2 | Val loss: 2.0760 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3770 | Steps: 2 | Val loss: 2818.2625 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=67665)[0m top1: 0.33488805970149255
[2m[36m(func pid=67665)[0m top5: 0.894589552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=67665)[0m f1_macro: 0.312180455503862
[2m[36m(func pid=67665)[0m f1_weighted: 0.3432413702166989
[2m[36m(func pid=67665)[0m f1_per_class: [0.347, 0.39, 0.585, 0.447, 0.134, 0.192, 0.311, 0.256, 0.243, 0.217]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.35401119402985076
[2m[36m(func pid=68221)[0m top5: 0.9151119402985075
[2m[36m(func pid=68221)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=68221)[0m f1_macro: 0.3739507476806061
[2m[36m(func pid=68221)[0m f1_weighted: 0.35936934470025916
[2m[36m(func pid=68221)[0m f1_per_class: [0.561, 0.464, 0.667, 0.435, 0.121, 0.216, 0.28, 0.354, 0.264, 0.378]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:02:56 (running for 00:43:31.34)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.644 |      0.211 |                   69 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.041 |      0.312 |                   48 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.374 |                   47 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.397 |      0.047 |                   19 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.22574626865671643
[2m[36m(func pid=62698)[0m top5: 0.7728544776119403
[2m[36m(func pid=62698)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=62698)[0m f1_macro: 0.21064870271223146
[2m[36m(func pid=62698)[0m f1_weighted: 0.2386358744195985
[2m[36m(func pid=62698)[0m f1_per_class: [0.205, 0.283, 0.468, 0.302, 0.039, 0.111, 0.227, 0.195, 0.163, 0.113]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.12919776119402984
[2m[36m(func pid=74157)[0m top5: 0.5690298507462687
[2m[36m(func pid=74157)[0m f1_micro: 0.12919776119402984
[2m[36m(func pid=74157)[0m f1_macro: 0.04991868430002912
[2m[36m(func pid=74157)[0m f1_weighted: 0.07351766429582304
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.266, 0.0, 0.0, 0.0, 0.0, 0.059, 0.174, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.8944 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0345 | Steps: 2 | Val loss: 1.7957 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6699 | Steps: 2 | Val loss: 2.0703 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 20.9490 | Steps: 2 | Val loss: 1820.5195 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=68221)[0m top1: 0.3516791044776119
[2m[36m(func pid=68221)[0m top5: 0.9155783582089553
[2m[36m(func pid=68221)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=68221)[0m f1_macro: 0.3700833003638074
[2m[36m(func pid=68221)[0m f1_weighted: 0.3565728953539897
[2m[36m(func pid=68221)[0m f1_per_class: [0.549, 0.464, 0.667, 0.435, 0.12, 0.211, 0.275, 0.348, 0.259, 0.374]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3316231343283582
[2m[36m(func pid=67665)[0m top5: 0.8950559701492538
[2m[36m(func pid=67665)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=67665)[0m f1_macro: 0.30980481011263905
[2m[36m(func pid=67665)[0m f1_weighted: 0.3391502459066399
[2m[36m(func pid=67665)[0m f1_per_class: [0.343, 0.383, 0.585, 0.451, 0.137, 0.196, 0.296, 0.253, 0.241, 0.211]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:02 (running for 00:43:36.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.67  |      0.214 |                   70 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.034 |      0.31  |                   49 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.37  |                   48 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.377 |      0.05  |                   20 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.22901119402985073
[2m[36m(func pid=62698)[0m top5: 0.7803171641791045
[2m[36m(func pid=62698)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=62698)[0m f1_macro: 0.21375631034241768
[2m[36m(func pid=62698)[0m f1_weighted: 0.24131175831852875
[2m[36m(func pid=62698)[0m f1_per_class: [0.205, 0.275, 0.468, 0.314, 0.041, 0.112, 0.227, 0.197, 0.169, 0.129]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.11800373134328358
[2m[36m(func pid=74157)[0m top5: 0.5708955223880597
[2m[36m(func pid=74157)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=74157)[0m f1_macro: 0.046701821992360476
[2m[36m(func pid=74157)[0m f1_weighted: 0.05505273614182409
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.148, 0.0, 0.053]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0013 | Steps: 2 | Val loss: 1.8935 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0650 | Steps: 2 | Val loss: 1.8035 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6010 | Steps: 2 | Val loss: 2.0723 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.4821 | Steps: 2 | Val loss: 205.8966 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=67665)[0m top1: 0.32882462686567165
[2m[36m(func pid=67665)[0m top5: 0.8899253731343284
[2m[36m(func pid=67665)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=67665)[0m f1_macro: 0.30807680418558414
[2m[36m(func pid=67665)[0m f1_weighted: 0.3381204027221434
[2m[36m(func pid=67665)[0m f1_per_class: [0.345, 0.385, 0.585, 0.431, 0.131, 0.188, 0.315, 0.252, 0.231, 0.217]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.35494402985074625
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=68221)[0m f1_macro: 0.373848450076917
[2m[36m(func pid=68221)[0m f1_weighted: 0.3604068143729143
[2m[36m(func pid=68221)[0m f1_per_class: [0.552, 0.464, 0.667, 0.437, 0.12, 0.228, 0.279, 0.35, 0.264, 0.379]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:03:07 (running for 00:43:42.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.601 |      0.213 |                   71 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.065 |      0.308 |                   50 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.374 |                   49 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |  20.949 |      0.047 |                   21 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.2271455223880597
[2m[36m(func pid=62698)[0m top5: 0.7807835820895522
[2m[36m(func pid=62698)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=62698)[0m f1_macro: 0.2128984766567213
[2m[36m(func pid=62698)[0m f1_weighted: 0.23955291628382247
[2m[36m(func pid=62698)[0m f1_per_class: [0.208, 0.273, 0.468, 0.306, 0.042, 0.114, 0.23, 0.197, 0.168, 0.124]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.09328358208955224
[2m[36m(func pid=74157)[0m top5: 0.5956156716417911
[2m[36m(func pid=74157)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=74157)[0m f1_macro: 0.04058593690983945
[2m[36m(func pid=74157)[0m f1_weighted: 0.04655290055742968
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.151, 0.0, 0.037]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0426 | Steps: 2 | Val loss: 1.7993 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8969 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6126 | Steps: 2 | Val loss: 2.0694 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2749 | Steps: 2 | Val loss: 45.6560 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=67665)[0m top1: 0.32509328358208955
[2m[36m(func pid=67665)[0m top5: 0.8927238805970149
[2m[36m(func pid=67665)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=67665)[0m f1_macro: 0.30801665454702615
[2m[36m(func pid=67665)[0m f1_weighted: 0.3333558605175113
[2m[36m(func pid=67665)[0m f1_per_class: [0.357, 0.387, 0.585, 0.428, 0.129, 0.184, 0.302, 0.249, 0.227, 0.233]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3530783582089552
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=68221)[0m f1_macro: 0.37132782447100393
[2m[36m(func pid=68221)[0m f1_weighted: 0.3580498161969659
[2m[36m(func pid=68221)[0m f1_per_class: [0.552, 0.458, 0.667, 0.434, 0.123, 0.216, 0.281, 0.352, 0.263, 0.367]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=62698)[0m top1: 0.2294776119402985
[2m[36m(func pid=62698)[0m top5: 0.7821828358208955
[2m[36m(func pid=62698)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=62698)[0m f1_macro: 0.21427607518248734
[2m[36m(func pid=62698)[0m f1_weighted: 0.24180657736961772
[2m[36m(func pid=62698)[0m f1_per_class: [0.212, 0.273, 0.468, 0.316, 0.041, 0.114, 0.227, 0.198, 0.172, 0.122]
[2m[36m(func pid=62698)[0m 
== Status ==
Current time: 2024-01-07 04:03:12 (running for 00:43:47.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.613 |      0.214 |                   72 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.043 |      0.308 |                   51 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.371 |                   50 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.482 |      0.041 |                   22 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.26492537313432835
[2m[36m(func pid=74157)[0m top5: 0.6301305970149254
[2m[36m(func pid=74157)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=74157)[0m f1_macro: 0.09918189521174044
[2m[36m(func pid=74157)[0m f1_weighted: 0.2022556875884622
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.113, 0.041, 0.0, 0.0, 0.056, 0.551, 0.2, 0.0, 0.03]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0311 | Steps: 2 | Val loss: 1.8037 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8834 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6492 | Steps: 2 | Val loss: 2.0677 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=68221)[0m top1: 0.35634328358208955
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=68221)[0m f1_macro: 0.37096487322531657
[2m[36m(func pid=68221)[0m f1_weighted: 0.36032175204940936
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.46, 0.667, 0.435, 0.129, 0.218, 0.286, 0.357, 0.268, 0.353]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.6086 | Steps: 2 | Val loss: 35.6497 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=67665)[0m top1: 0.32276119402985076
[2m[36m(func pid=67665)[0m top5: 0.8913246268656716
[2m[36m(func pid=67665)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=67665)[0m f1_macro: 0.3043875825206367
[2m[36m(func pid=67665)[0m f1_weighted: 0.3321642522315305
[2m[36m(func pid=67665)[0m f1_per_class: [0.383, 0.38, 0.558, 0.436, 0.122, 0.175, 0.299, 0.242, 0.221, 0.227]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:18 (running for 00:43:52.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.649 |      0.214 |                   73 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.031 |      0.304 |                   52 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.371 |                   51 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.275 |      0.099 |                   23 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.228544776119403
[2m[36m(func pid=62698)[0m top5: 0.7826492537313433
[2m[36m(func pid=62698)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=62698)[0m f1_macro: 0.21414722537989134
[2m[36m(func pid=62698)[0m f1_weighted: 0.24186795517756465
[2m[36m(func pid=62698)[0m f1_per_class: [0.203, 0.271, 0.468, 0.307, 0.043, 0.118, 0.236, 0.201, 0.169, 0.127]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.2574626865671642
[2m[36m(func pid=74157)[0m top5: 0.6436567164179104
[2m[36m(func pid=74157)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=74157)[0m f1_macro: 0.1465310141802434
[2m[36m(func pid=74157)[0m f1_weighted: 0.22057305889197884
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.15, 0.158, 0.01, 0.049, 0.0, 0.538, 0.508, 0.022, 0.031]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8893 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0407 | Steps: 2 | Val loss: 1.8046 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5457 | Steps: 2 | Val loss: 2.0640 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8203 | Steps: 2 | Val loss: 42.2847 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=68221)[0m top1: 0.3530783582089552
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=68221)[0m f1_macro: 0.3687446571700529
[2m[36m(func pid=68221)[0m f1_weighted: 0.3605447369811932
[2m[36m(func pid=68221)[0m f1_per_class: [0.552, 0.46, 0.667, 0.431, 0.12, 0.207, 0.298, 0.346, 0.251, 0.356]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.322294776119403
[2m[36m(func pid=67665)[0m top5: 0.8899253731343284
[2m[36m(func pid=67665)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=67665)[0m f1_macro: 0.3032850900834868
[2m[36m(func pid=67665)[0m f1_weighted: 0.3321052011818909
[2m[36m(func pid=67665)[0m f1_per_class: [0.368, 0.376, 0.558, 0.432, 0.12, 0.179, 0.304, 0.24, 0.227, 0.229]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:23 (running for 00:43:58.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00020 | RUNNING    | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.546 |      0.219 |                   74 |
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.041 |      0.303 |                   53 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.369 |                   52 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.609 |      0.147 |                   24 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.23180970149253732
[2m[36m(func pid=62698)[0m top5: 0.7840485074626866
[2m[36m(func pid=62698)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=62698)[0m f1_macro: 0.21889524707400895
[2m[36m(func pid=62698)[0m f1_weighted: 0.24433848503958858
[2m[36m(func pid=62698)[0m f1_per_class: [0.211, 0.28, 0.468, 0.309, 0.068, 0.118, 0.235, 0.195, 0.178, 0.127]
[2m[36m(func pid=62698)[0m 
[2m[36m(func pid=74157)[0m top1: 0.10074626865671642
[2m[36m(func pid=74157)[0m top5: 0.6254664179104478
[2m[36m(func pid=74157)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=74157)[0m f1_macro: 0.05579320664248616
[2m[36m(func pid=74157)[0m f1_weighted: 0.07835698007968411
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.14, 0.0, 0.063, 0.022, 0.0, 0.076, 0.216, 0.041, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8869 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0314 | Steps: 2 | Val loss: 1.8127 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=62698)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5838 | Steps: 2 | Val loss: 2.0602 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.9885 | Steps: 2 | Val loss: 14.6589 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=68221)[0m top1: 0.3512126865671642
[2m[36m(func pid=68221)[0m top5: 0.9137126865671642
[2m[36m(func pid=68221)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=68221)[0m f1_macro: 0.3660183107908003
[2m[36m(func pid=68221)[0m f1_weighted: 0.35927850997994126
[2m[36m(func pid=68221)[0m f1_per_class: [0.542, 0.459, 0.667, 0.428, 0.12, 0.203, 0.302, 0.336, 0.247, 0.356]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3208955223880597
[2m[36m(func pid=67665)[0m top5: 0.886660447761194
[2m[36m(func pid=67665)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=67665)[0m f1_macro: 0.3046210122478212
[2m[36m(func pid=67665)[0m f1_weighted: 0.3305327047353063
[2m[36m(func pid=67665)[0m f1_per_class: [0.376, 0.382, 0.558, 0.423, 0.117, 0.18, 0.301, 0.248, 0.225, 0.236]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:29 (running for 00:44:03.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.031 |      0.305 |                   54 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.366 |                   53 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.82  |      0.056 |                   25 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62698)[0m top1: 0.2332089552238806
[2m[36m(func pid=62698)[0m top5: 0.7887126865671642
[2m[36m(func pid=62698)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=62698)[0m f1_macro: 0.21676915091680451
[2m[36m(func pid=62698)[0m f1_weighted: 0.2451774243816089
[2m[36m(func pid=62698)[0m f1_per_class: [0.211, 0.284, 0.458, 0.318, 0.055, 0.116, 0.23, 0.194, 0.166, 0.136]
[2m[36m(func pid=74157)[0m top1: 0.08488805970149253
[2m[36m(func pid=74157)[0m top5: 0.6203358208955224
[2m[36m(func pid=74157)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=74157)[0m f1_macro: 0.05453749983888696
[2m[36m(func pid=74157)[0m f1_weighted: 0.05317824809042669
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.102, 0.05, 0.0, 0.02, 0.043, 0.052, 0.22, 0.058, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0314 | Steps: 2 | Val loss: 1.8135 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.8765 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.3652 | Steps: 2 | Val loss: 19.6514 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=68221)[0m top1: 0.3568097014925373
[2m[36m(func pid=68221)[0m top5: 0.9137126865671642
[2m[36m(func pid=68221)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=68221)[0m f1_macro: 0.3685104246919145
[2m[36m(func pid=68221)[0m f1_weighted: 0.3655045999739359
[2m[36m(func pid=68221)[0m f1_per_class: [0.532, 0.459, 0.667, 0.432, 0.129, 0.21, 0.317, 0.333, 0.247, 0.36]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3204291044776119
[2m[36m(func pid=67665)[0m top5: 0.8871268656716418
[2m[36m(func pid=67665)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=67665)[0m f1_macro: 0.30625582062265355
[2m[36m(func pid=67665)[0m f1_weighted: 0.3276169969332988
[2m[36m(func pid=67665)[0m f1_per_class: [0.391, 0.399, 0.571, 0.419, 0.119, 0.18, 0.285, 0.246, 0.216, 0.236]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:34 (running for 00:44:09.36)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.031 |      0.306 |                   55 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.369 |                   54 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.365 |      0.113 |                   27 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.19449626865671643
[2m[36m(func pid=74157)[0m top5: 0.6567164179104478
[2m[36m(func pid=74157)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=74157)[0m f1_macro: 0.11282599255838206
[2m[36m(func pid=74157)[0m f1_weighted: 0.1838392871979506
[2m[36m(func pid=74157)[0m f1_per_class: [0.039, 0.0, 0.05, 0.122, 0.023, 0.121, 0.375, 0.398, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.8583 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0294 | Steps: 2 | Val loss: 1.8154 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6327 | Steps: 2 | Val loss: 11.8043 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=68221)[0m top1: 0.36240671641791045
[2m[36m(func pid=68221)[0m top5: 0.9169776119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=68221)[0m f1_macro: 0.3720972300834143
[2m[36m(func pid=68221)[0m f1_weighted: 0.37289240065006074
[2m[36m(func pid=68221)[0m f1_per_class: [0.541, 0.46, 0.667, 0.447, 0.124, 0.206, 0.327, 0.33, 0.251, 0.367]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3204291044776119
[2m[36m(func pid=67665)[0m top5: 0.8861940298507462
[2m[36m(func pid=67665)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=67665)[0m f1_macro: 0.3062983410782606
[2m[36m(func pid=67665)[0m f1_weighted: 0.32683526157830695
[2m[36m(func pid=67665)[0m f1_per_class: [0.395, 0.401, 0.571, 0.422, 0.116, 0.175, 0.279, 0.252, 0.214, 0.236]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:39 (running for 00:44:14.56)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.029 |      0.306 |                   56 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.372 |                   55 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.633 |      0.056 |                   28 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.07929104477611941
[2m[36m(func pid=74157)[0m top5: 0.7126865671641791
[2m[36m(func pid=74157)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=74157)[0m f1_macro: 0.055620063171208246
[2m[36m(func pid=74157)[0m f1_weighted: 0.036807740337230006
[2m[36m(func pid=74157)[0m f1_per_class: [0.056, 0.0, 0.045, 0.057, 0.084, 0.0, 0.009, 0.272, 0.0, 0.034]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.8628 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0468 | Steps: 2 | Val loss: 1.7948 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4040 | Steps: 2 | Val loss: 9.5324 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=68221)[0m top1: 0.3656716417910448
[2m[36m(func pid=68221)[0m top5: 0.9165111940298507
[2m[36m(func pid=68221)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=68221)[0m f1_macro: 0.37374840119229197
[2m[36m(func pid=68221)[0m f1_weighted: 0.37637621761759865
[2m[36m(func pid=68221)[0m f1_per_class: [0.534, 0.467, 0.667, 0.463, 0.135, 0.212, 0.319, 0.331, 0.241, 0.367]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3278917910447761
[2m[36m(func pid=67665)[0m top5: 0.8950559701492538
[2m[36m(func pid=67665)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=67665)[0m f1_macro: 0.31339014341446236
[2m[36m(func pid=67665)[0m f1_weighted: 0.33407716995824255
[2m[36m(func pid=67665)[0m f1_per_class: [0.4, 0.399, 0.571, 0.43, 0.129, 0.19, 0.288, 0.261, 0.221, 0.244]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:03:44 (running for 00:44:19.80)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.047 |      0.313 |                   57 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.374 |                   56 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.404 |      0.099 |                   29 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.14272388059701493
[2m[36m(func pid=74157)[0m top5: 0.5909514925373134
[2m[36m(func pid=74157)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=74157)[0m f1_macro: 0.09915416235633542
[2m[36m(func pid=74157)[0m f1_weighted: 0.08966399761490834
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.355, 0.065, 0.0, 0.059, 0.0, 0.0, 0.471, 0.0, 0.042]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8659 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0272 | Steps: 2 | Val loss: 1.8013 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 7.6388 | Steps: 2 | Val loss: 104.9240 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=68221)[0m top1: 0.3666044776119403
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=68221)[0m f1_macro: 0.37578583630224655
[2m[36m(func pid=68221)[0m f1_weighted: 0.3752899177289526
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.471, 0.667, 0.473, 0.135, 0.205, 0.303, 0.341, 0.255, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.32742537313432835
[2m[36m(func pid=67665)[0m top5: 0.8927238805970149
[2m[36m(func pid=67665)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=67665)[0m f1_macro: 0.3125172562908861
[2m[36m(func pid=67665)[0m f1_weighted: 0.33576469944394804
[2m[36m(func pid=67665)[0m f1_per_class: [0.391, 0.389, 0.571, 0.436, 0.129, 0.185, 0.294, 0.275, 0.221, 0.234]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.15625
[2m[36m(func pid=74157)[0m top5: 0.7262126865671642
[2m[36m(func pid=74157)[0m f1_micro: 0.15625
[2m[36m(func pid=74157)[0m f1_macro: 0.07626491708905517
[2m[36m(func pid=74157)[0m f1_weighted: 0.08720716703343122
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.353, 0.0, 0.023, 0.044, 0.0, 0.0, 0.343, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.8712 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0374 | Steps: 2 | Val loss: 1.8131 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 4.9768 | Steps: 2 | Val loss: 27.7776 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=68221)[0m top1: 0.3591417910447761
[2m[36m(func pid=68221)[0m top5: 0.9169776119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=68221)[0m f1_macro: 0.37198939484731175
[2m[36m(func pid=68221)[0m f1_weighted: 0.36653562168347714
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.462, 0.667, 0.452, 0.125, 0.205, 0.298, 0.343, 0.251, 0.379]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:03:54 (running for 00:44:28.98)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.027 |      0.313 |                   58 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.372 |                   58 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   7.639 |      0.076 |                   30 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.32136194029850745
[2m[36m(func pid=67665)[0m top5: 0.8894589552238806
[2m[36m(func pid=67665)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=67665)[0m f1_macro: 0.3104316466674146
[2m[36m(func pid=67665)[0m f1_weighted: 0.32799551847895797
[2m[36m(func pid=67665)[0m f1_per_class: [0.385, 0.394, 0.585, 0.424, 0.136, 0.189, 0.277, 0.266, 0.225, 0.224]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.25513059701492535
[2m[36m(func pid=74157)[0m top5: 0.7658582089552238
[2m[36m(func pid=74157)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=74157)[0m f1_macro: 0.12947448067219589
[2m[36m(func pid=74157)[0m f1_weighted: 0.24446277733340158
[2m[36m(func pid=74157)[0m f1_per_class: [0.048, 0.12, 0.018, 0.081, 0.04, 0.0, 0.597, 0.36, 0.03, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0015 | Steps: 2 | Val loss: 1.8816 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0290 | Steps: 2 | Val loss: 1.8146 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.3155 | Steps: 2 | Val loss: 21.9309 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:03:59 (running for 00:44:34.53)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.037 |      0.31  |                   59 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.373 |                   59 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   4.977 |      0.129 |                   31 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36473880597014924
[2m[36m(func pid=68221)[0m top5: 0.9169776119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=68221)[0m f1_macro: 0.37349207800367823
[2m[36m(func pid=68221)[0m f1_weighted: 0.3742612625648045
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.47, 0.667, 0.463, 0.129, 0.204, 0.311, 0.339, 0.251, 0.364]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.32276119402985076
[2m[36m(func pid=67665)[0m top5: 0.8899253731343284
[2m[36m(func pid=67665)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=67665)[0m f1_macro: 0.3096272513383858
[2m[36m(func pid=67665)[0m f1_weighted: 0.3311236725559996
[2m[36m(func pid=67665)[0m f1_per_class: [0.378, 0.39, 0.585, 0.43, 0.136, 0.184, 0.286, 0.267, 0.216, 0.222]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.16044776119402984
[2m[36m(func pid=74157)[0m top5: 0.7280783582089553
[2m[36m(func pid=74157)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=74157)[0m f1_macro: 0.07615267875530621
[2m[36m(func pid=74157)[0m f1_weighted: 0.1280472292745738
[2m[36m(func pid=74157)[0m f1_per_class: [0.039, 0.135, 0.032, 0.068, 0.0, 0.329, 0.159, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8706 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0319 | Steps: 2 | Val loss: 1.8191 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.1284 | Steps: 2 | Val loss: 33.2337 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 04:04:05 (running for 00:44:39.89)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.029 |      0.31  |                   60 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.371 |                   60 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.315 |      0.076 |                   32 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36473880597014924
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=68221)[0m f1_macro: 0.3710105060883865
[2m[36m(func pid=68221)[0m f1_weighted: 0.3748242575944865
[2m[36m(func pid=68221)[0m f1_per_class: [0.525, 0.477, 0.667, 0.469, 0.128, 0.201, 0.308, 0.329, 0.25, 0.356]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3199626865671642
[2m[36m(func pid=67665)[0m top5: 0.8857276119402985
[2m[36m(func pid=67665)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=67665)[0m f1_macro: 0.30782845560765404
[2m[36m(func pid=67665)[0m f1_weighted: 0.32695178730108604
[2m[36m(func pid=67665)[0m f1_per_class: [0.369, 0.39, 0.585, 0.43, 0.124, 0.179, 0.274, 0.271, 0.222, 0.234]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.14832089552238806
[2m[36m(func pid=74157)[0m top5: 0.7481343283582089
[2m[36m(func pid=74157)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=74157)[0m f1_macro: 0.08676512465372235
[2m[36m(func pid=74157)[0m f1_weighted: 0.08610189386082784
[2m[36m(func pid=74157)[0m f1_per_class: [0.071, 0.154, 0.0, 0.035, 0.0, 0.253, 0.006, 0.291, 0.0, 0.057]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0304 | Steps: 2 | Val loss: 1.8237 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0009 | Steps: 2 | Val loss: 1.8721 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 13.5707 | Steps: 2 | Val loss: 34.7904 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 04:04:10 (running for 00:44:45.45)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.032 |      0.308 |                   61 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.371 |                   61 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.128 |      0.087 |                   33 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.3218283582089552
[2m[36m(func pid=67665)[0m top5: 0.886660447761194
[2m[36m(func pid=67665)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=67665)[0m f1_macro: 0.31048187725673115
[2m[36m(func pid=67665)[0m f1_weighted: 0.33190638699336283
[2m[36m(func pid=67665)[0m f1_per_class: [0.365, 0.39, 0.6, 0.43, 0.123, 0.174, 0.291, 0.271, 0.231, 0.229]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.36380597014925375
[2m[36m(func pid=68221)[0m top5: 0.9160447761194029
[2m[36m(func pid=68221)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=68221)[0m f1_macro: 0.37141933504635405
[2m[36m(func pid=68221)[0m f1_weighted: 0.37186105362554306
[2m[36m(func pid=68221)[0m f1_per_class: [0.531, 0.476, 0.667, 0.474, 0.124, 0.199, 0.292, 0.342, 0.25, 0.36]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.12313432835820895
[2m[36m(func pid=74157)[0m top5: 0.7453358208955224
[2m[36m(func pid=74157)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=74157)[0m f1_macro: 0.0924968101333948
[2m[36m(func pid=74157)[0m f1_weighted: 0.08715225122981626
[2m[36m(func pid=74157)[0m f1_per_class: [0.0, 0.172, 0.0, 0.039, 0.148, 0.261, 0.0, 0.268, 0.0, 0.038]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0328 | Steps: 2 | Val loss: 1.8134 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8680 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.1563 | Steps: 2 | Val loss: 21.5918 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 04:04:16 (running for 00:44:50.90)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.033 |      0.308 |                   63 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.371 |                   61 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |  13.571 |      0.092 |                   34 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.32649253731343286
[2m[36m(func pid=67665)[0m top5: 0.8885261194029851
[2m[36m(func pid=67665)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=67665)[0m f1_macro: 0.30793365186468813
[2m[36m(func pid=67665)[0m f1_weighted: 0.33812620533236964
[2m[36m(func pid=67665)[0m f1_per_class: [0.36, 0.389, 0.564, 0.444, 0.128, 0.186, 0.299, 0.265, 0.231, 0.216]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.36800373134328357
[2m[36m(func pid=68221)[0m top5: 0.9165111940298507
[2m[36m(func pid=68221)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=68221)[0m f1_macro: 0.3745666379836564
[2m[36m(func pid=68221)[0m f1_weighted: 0.3775984139486826
[2m[36m(func pid=68221)[0m f1_per_class: [0.528, 0.476, 0.667, 0.476, 0.126, 0.199, 0.309, 0.335, 0.263, 0.367]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.22434701492537312
[2m[36m(func pid=74157)[0m top5: 0.8479477611940298
[2m[36m(func pid=74157)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=74157)[0m f1_macro: 0.1369584907505562
[2m[36m(func pid=74157)[0m f1_weighted: 0.20263226966928727
[2m[36m(func pid=74157)[0m f1_per_class: [0.069, 0.425, 0.0, 0.185, 0.079, 0.0, 0.185, 0.29, 0.098, 0.038]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0359 | Steps: 2 | Val loss: 1.8122 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0011 | Steps: 2 | Val loss: 1.8680 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6896 | Steps: 2 | Val loss: 7.8475 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:04:21 (running for 00:44:56.22)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.033 |      0.308 |                   63 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.375 |                   62 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.156 |      0.137 |                   35 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36800373134328357
[2m[36m(func pid=68221)[0m top5: 0.9165111940298507
[2m[36m(func pid=68221)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=68221)[0m f1_macro: 0.37616648556883514
[2m[36m(func pid=68221)[0m f1_weighted: 0.37943421407764955
[2m[36m(func pid=68221)[0m f1_per_class: [0.55, 0.472, 0.667, 0.478, 0.122, 0.195, 0.316, 0.338, 0.254, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3255597014925373
[2m[36m(func pid=67665)[0m top5: 0.8880597014925373
[2m[36m(func pid=67665)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=67665)[0m f1_macro: 0.31027244027814693
[2m[36m(func pid=67665)[0m f1_weighted: 0.33831203262108217
[2m[36m(func pid=67665)[0m f1_per_class: [0.363, 0.381, 0.6, 0.443, 0.114, 0.176, 0.306, 0.272, 0.228, 0.221]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.2271455223880597
[2m[36m(func pid=74157)[0m top5: 0.8330223880597015
[2m[36m(func pid=74157)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=74157)[0m f1_macro: 0.11725617706895738
[2m[36m(func pid=74157)[0m f1_weighted: 0.1425247956449052
[2m[36m(func pid=74157)[0m f1_per_class: [0.082, 0.441, 0.148, 0.042, 0.057, 0.0, 0.119, 0.285, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0245 | Steps: 2 | Val loss: 1.8189 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.8639 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.8210 | Steps: 2 | Val loss: 5.0148 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 04:04:26 (running for 00:45:01.70)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.036 |      0.31  |                   64 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.376 |                   63 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.69  |      0.117 |                   36 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.322294776119403
[2m[36m(func pid=67665)[0m top5: 0.8847947761194029
[2m[36m(func pid=67665)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=67665)[0m f1_macro: 0.308331567749914
[2m[36m(func pid=67665)[0m f1_weighted: 0.3342585941423966
[2m[36m(func pid=67665)[0m f1_per_class: [0.36, 0.38, 0.6, 0.439, 0.111, 0.173, 0.297, 0.272, 0.235, 0.214]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.36613805970149255
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=68221)[0m f1_macro: 0.37592628861540417
[2m[36m(func pid=68221)[0m f1_weighted: 0.37720976065693695
[2m[36m(func pid=68221)[0m f1_per_class: [0.557, 0.475, 0.667, 0.474, 0.126, 0.204, 0.306, 0.336, 0.254, 0.36]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.18003731343283583
[2m[36m(func pid=74157)[0m top5: 0.8274253731343284
[2m[36m(func pid=74157)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=74157)[0m f1_macro: 0.10503127721091801
[2m[36m(func pid=74157)[0m f1_weighted: 0.09592062823971562
[2m[36m(func pid=74157)[0m f1_per_class: [0.13, 0.449, 0.154, 0.0, 0.055, 0.0, 0.0, 0.242, 0.021, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0262 | Steps: 2 | Val loss: 1.8209 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.8409 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.7205 | Steps: 2 | Val loss: 4.1549 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 04:04:32 (running for 00:45:07.28)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.025 |      0.308 |                   65 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.376 |                   64 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.821 |      0.105 |                   37 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.32322761194029853
[2m[36m(func pid=67665)[0m top5: 0.8824626865671642
[2m[36m(func pid=67665)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=67665)[0m f1_macro: 0.30621268478837654
[2m[36m(func pid=67665)[0m f1_weighted: 0.33564111613874714
[2m[36m(func pid=67665)[0m f1_per_class: [0.346, 0.369, 0.585, 0.442, 0.113, 0.175, 0.306, 0.271, 0.232, 0.221]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37080223880597013
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=68221)[0m f1_macro: 0.3767840780145797
[2m[36m(func pid=68221)[0m f1_weighted: 0.3837318592323999
[2m[36m(func pid=68221)[0m f1_per_class: [0.542, 0.471, 0.667, 0.476, 0.129, 0.212, 0.328, 0.335, 0.252, 0.356]
[2m[36m(func pid=74157)[0m top1: 0.1394589552238806
[2m[36m(func pid=74157)[0m top5: 0.840018656716418
[2m[36m(func pid=74157)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=74157)[0m f1_macro: 0.11306514462021962
[2m[36m(func pid=74157)[0m f1_weighted: 0.09032633404154747
[2m[36m(func pid=74157)[0m f1_per_class: [0.097, 0.4, 0.227, 0.003, 0.07, 0.0, 0.0, 0.266, 0.022, 0.044]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0264 | Steps: 2 | Val loss: 1.8066 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.8097 | Steps: 2 | Val loss: 3.3857 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8501 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 04:04:37 (running for 00:45:12.71)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.026 |      0.306 |                   66 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.377 |                   65 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   3.72  |      0.113 |                   38 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.32742537313432835
[2m[36m(func pid=67665)[0m top5: 0.8852611940298507
[2m[36m(func pid=67665)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=67665)[0m f1_macro: 0.3078045678256634
[2m[36m(func pid=67665)[0m f1_weighted: 0.33800990722902763
[2m[36m(func pid=67665)[0m f1_per_class: [0.376, 0.374, 0.585, 0.45, 0.115, 0.167, 0.309, 0.26, 0.22, 0.222]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.14458955223880596
[2m[36m(func pid=74157)[0m top5: 0.867070895522388
[2m[36m(func pid=74157)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=74157)[0m f1_macro: 0.14940245230937302
[2m[36m(func pid=74157)[0m f1_weighted: 0.1164002801141243
[2m[36m(func pid=74157)[0m f1_per_class: [0.098, 0.364, 0.311, 0.039, 0.189, 0.203, 0.0, 0.236, 0.0, 0.053]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.3656716417910448
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=68221)[0m f1_macro: 0.3758925505034071
[2m[36m(func pid=68221)[0m f1_weighted: 0.3741212854106566
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.471, 0.686, 0.466, 0.127, 0.209, 0.304, 0.336, 0.259, 0.356]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0276 | Steps: 2 | Val loss: 1.8123 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1367 | Steps: 2 | Val loss: 2.6410 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8601 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:04:43 (running for 00:45:18.33)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.028 |      0.306 |                   68 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.376 |                   66 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.81  |      0.149 |                   39 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.32322761194029853
[2m[36m(func pid=67665)[0m top5: 0.8847947761194029
[2m[36m(func pid=67665)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=67665)[0m f1_macro: 0.306278187974191
[2m[36m(func pid=67665)[0m f1_weighted: 0.3342167219314019
[2m[36m(func pid=67665)[0m f1_per_class: [0.363, 0.37, 0.585, 0.445, 0.112, 0.167, 0.301, 0.272, 0.222, 0.225]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.36613805970149255
[2m[36m(func pid=68221)[0m top5: 0.9151119402985075
[2m[36m(func pid=68221)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=68221)[0m f1_macro: 0.37516731451164154
[2m[36m(func pid=68221)[0m f1_weighted: 0.3755524478949597
[2m[36m(func pid=68221)[0m f1_per_class: [0.52, 0.473, 0.686, 0.467, 0.134, 0.215, 0.306, 0.337, 0.265, 0.35]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.2681902985074627
[2m[36m(func pid=74157)[0m top5: 0.8903917910447762
[2m[36m(func pid=74157)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=74157)[0m f1_macro: 0.2094922169677656
[2m[36m(func pid=74157)[0m f1_weighted: 0.25362414025163
[2m[36m(func pid=74157)[0m f1_per_class: [0.107, 0.422, 0.35, 0.171, 0.235, 0.0, 0.348, 0.39, 0.0, 0.071]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0323 | Steps: 2 | Val loss: 1.8103 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9394 | Steps: 2 | Val loss: 3.0963 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8408 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 04:04:48 (running for 00:45:23.69)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.028 |      0.306 |                   68 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.375 |                   67 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.137 |      0.209 |                   40 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.26399253731343286
[2m[36m(func pid=74157)[0m top5: 0.8610074626865671
[2m[36m(func pid=74157)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=74157)[0m f1_macro: 0.13758191872413258
[2m[36m(func pid=74157)[0m f1_weighted: 0.23707443590973948
[2m[36m(func pid=74157)[0m f1_per_class: [0.099, 0.385, 0.328, 0.063, 0.0, 0.0, 0.501, 0.0, 0.0, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3260261194029851
[2m[36m(func pid=67665)[0m top5: 0.8861940298507462
[2m[36m(func pid=67665)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=67665)[0m f1_macro: 0.309484792743448
[2m[36m(func pid=67665)[0m f1_weighted: 0.335260549952813
[2m[36m(func pid=67665)[0m f1_per_class: [0.369, 0.377, 0.6, 0.45, 0.115, 0.171, 0.294, 0.275, 0.22, 0.224]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.375
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.375
[2m[36m(func pid=68221)[0m f1_macro: 0.38273704599183755
[2m[36m(func pid=68221)[0m f1_weighted: 0.3870872093405074
[2m[36m(func pid=68221)[0m f1_per_class: [0.552, 0.477, 0.686, 0.476, 0.135, 0.228, 0.327, 0.339, 0.248, 0.36]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7087 | Steps: 2 | Val loss: 3.2056 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0286 | Steps: 2 | Val loss: 1.8005 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8457 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 04:04:54 (running for 00:45:29.06)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.032 |      0.309 |                   69 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.383 |                   68 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.939 |      0.138 |                   41 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.2560634328358209
[2m[36m(func pid=74157)[0m top5: 0.875
[2m[36m(func pid=74157)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=74157)[0m f1_macro: 0.14869857370290904
[2m[36m(func pid=74157)[0m f1_weighted: 0.2607485394867267
[2m[36m(func pid=74157)[0m f1_per_class: [0.099, 0.444, 0.175, 0.121, 0.012, 0.163, 0.427, 0.0, 0.045, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m top1: 0.3292910447761194
[2m[36m(func pid=67665)[0m top5: 0.8903917910447762
[2m[36m(func pid=67665)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=67665)[0m f1_macro: 0.3094461464699446
[2m[36m(func pid=67665)[0m f1_weighted: 0.34018252274092703
[2m[36m(func pid=67665)[0m f1_per_class: [0.365, 0.374, 0.6, 0.451, 0.118, 0.158, 0.316, 0.273, 0.22, 0.221]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37173507462686567
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=68221)[0m f1_macro: 0.3814001558547264
[2m[36m(func pid=68221)[0m f1_weighted: 0.3827479173030474
[2m[36m(func pid=68221)[0m f1_per_class: [0.547, 0.473, 0.686, 0.46, 0.137, 0.229, 0.329, 0.339, 0.253, 0.36]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5290 | Steps: 2 | Val loss: 2.7815 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0226 | Steps: 2 | Val loss: 1.7948 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0013 | Steps: 2 | Val loss: 1.8545 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=67665)[0m top1: 0.333955223880597
[2m[36m(func pid=67665)[0m top5: 0.8903917910447762
[2m[36m(func pid=67665)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=67665)[0m f1_macro: 0.3168128079998477
[2m[36m(func pid=67665)[0m f1_weighted: 0.3441048100081594
[2m[36m(func pid=67665)[0m f1_per_class: [0.376, 0.375, 0.632, 0.458, 0.115, 0.166, 0.316, 0.275, 0.229, 0.226]
[2m[36m(func pid=67665)[0m 
== Status ==
Current time: 2024-01-07 04:04:59 (running for 00:45:34.67)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.023 |      0.317 |                   71 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.381 |                   69 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.709 |      0.149 |                   42 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.1912313432835821
[2m[36m(func pid=74157)[0m top5: 0.9043843283582089
[2m[36m(func pid=74157)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=74157)[0m f1_macro: 0.16735704987647537
[2m[36m(func pid=74157)[0m f1_weighted: 0.1897107324530578
[2m[36m(func pid=74157)[0m f1_per_class: [0.104, 0.212, 0.203, 0.087, 0.048, 0.322, 0.203, 0.471, 0.024, 0.0]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37033582089552236
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=68221)[0m f1_macro: 0.38381455504614526
[2m[36m(func pid=68221)[0m f1_weighted: 0.3791343015412601
[2m[36m(func pid=68221)[0m f1_per_class: [0.545, 0.473, 0.686, 0.457, 0.132, 0.234, 0.315, 0.348, 0.265, 0.383]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0203 | Steps: 2 | Val loss: 1.7914 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.1632 | Steps: 2 | Val loss: 2.4482 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8506 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:05:05 (running for 00:45:40.09)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.023 |      0.317 |                   71 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.384 |                   70 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.163 |      0.229 |                   44 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.3087686567164179
[2m[36m(func pid=74157)[0m top5: 0.9067164179104478
[2m[36m(func pid=74157)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=74157)[0m f1_macro: 0.2290126988672601
[2m[36m(func pid=74157)[0m f1_weighted: 0.2944139326621424
[2m[36m(func pid=74157)[0m f1_per_class: [0.095, 0.459, 0.343, 0.048, 0.053, 0.245, 0.468, 0.505, 0.0, 0.074]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m top1: 0.33255597014925375
[2m[36m(func pid=67665)[0m top5: 0.8917910447761194
[2m[36m(func pid=67665)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=67665)[0m f1_macro: 0.3135153565519187
[2m[36m(func pid=67665)[0m f1_weighted: 0.34254445769724223
[2m[36m(func pid=67665)[0m f1_per_class: [0.372, 0.371, 0.615, 0.46, 0.115, 0.174, 0.311, 0.27, 0.226, 0.221]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.375
[2m[36m(func pid=68221)[0m top5: 0.9188432835820896
[2m[36m(func pid=68221)[0m f1_micro: 0.375
[2m[36m(func pid=68221)[0m f1_macro: 0.38742621047109044
[2m[36m(func pid=68221)[0m f1_weighted: 0.3827455840285923
[2m[36m(func pid=68221)[0m f1_per_class: [0.565, 0.477, 0.686, 0.474, 0.134, 0.236, 0.307, 0.345, 0.269, 0.383]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0362 | Steps: 2 | Val loss: 1.8055 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7893 | Steps: 2 | Val loss: 2.5799 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0038 | Steps: 2 | Val loss: 1.8455 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 04:05:10 (running for 00:45:45.55)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.02  |      0.314 |                   72 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.387 |                   71 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.163 |      0.229 |                   44 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.333955223880597
[2m[36m(func pid=67665)[0m top5: 0.8875932835820896
[2m[36m(func pid=67665)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=67665)[0m f1_macro: 0.31013198293831834
[2m[36m(func pid=67665)[0m f1_weighted: 0.3441816481790549
[2m[36m(func pid=67665)[0m f1_per_class: [0.367, 0.369, 0.571, 0.466, 0.112, 0.17, 0.313, 0.276, 0.226, 0.231]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=74157)[0m top1: 0.29151119402985076
[2m[36m(func pid=74157)[0m top5: 0.8978544776119403
[2m[36m(func pid=74157)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=74157)[0m f1_macro: 0.19631290606073093
[2m[36m(func pid=74157)[0m f1_weighted: 0.273511608092276
[2m[36m(func pid=74157)[0m f1_per_class: [0.09, 0.431, 0.417, 0.003, 0.033, 0.178, 0.547, 0.162, 0.0, 0.103]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37173507462686567
[2m[36m(func pid=68221)[0m top5: 0.9216417910447762
[2m[36m(func pid=68221)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=68221)[0m f1_macro: 0.38437881425954734
[2m[36m(func pid=68221)[0m f1_weighted: 0.3784432464120449
[2m[36m(func pid=68221)[0m f1_per_class: [0.535, 0.476, 0.706, 0.472, 0.141, 0.235, 0.298, 0.347, 0.255, 0.379]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0252 | Steps: 2 | Val loss: 1.8119 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.0806 | Steps: 2 | Val loss: 3.6454 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8459 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 04:05:15 (running for 00:45:50.80)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.371
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00021 | RUNNING    | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.036 |      0.31  |                   73 |
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.004 |      0.384 |                   72 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.789 |      0.196 |                   45 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.09701492537313433
[2m[36m(func pid=74157)[0m top5: 0.8708022388059702
[2m[36m(func pid=74157)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=74157)[0m f1_macro: 0.10499834877203162
[2m[36m(func pid=74157)[0m f1_weighted: 0.11011629057287384
[2m[36m(func pid=74157)[0m f1_per_class: [0.101, 0.21, 0.361, 0.013, 0.027, 0.074, 0.189, 0.0, 0.0, 0.074]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=67665)[0m top1: 0.33115671641791045
[2m[36m(func pid=67665)[0m top5: 0.8852611940298507
[2m[36m(func pid=67665)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=67665)[0m f1_macro: 0.3088979573786264
[2m[36m(func pid=67665)[0m f1_weighted: 0.341652310469458
[2m[36m(func pid=67665)[0m f1_per_class: [0.353, 0.372, 0.571, 0.462, 0.11, 0.172, 0.307, 0.279, 0.221, 0.242]
[2m[36m(func pid=67665)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37593283582089554
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=68221)[0m f1_macro: 0.38329491097745494
[2m[36m(func pid=68221)[0m f1_weighted: 0.3831319073320634
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.482, 0.686, 0.483, 0.144, 0.222, 0.305, 0.345, 0.257, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 5.5519 | Steps: 2 | Val loss: 3.8228 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=67665)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0219 | Steps: 2 | Val loss: 1.8170 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0021 | Steps: 2 | Val loss: 1.8344 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:05:21 (running for 00:45:56.15)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3675
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.383 |                   73 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.081 |      0.105 |                   46 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67665)[0m top1: 0.32649253731343286
[2m[36m(func pid=67665)[0m top5: 0.8833955223880597
[2m[36m(func pid=67665)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=67665)[0m f1_macro: 0.30612272389739104
[2m[36m(func pid=67665)[0m f1_weighted: 0.336590154736439
[2m[36m(func pid=67665)[0m f1_per_class: [0.351, 0.378, 0.571, 0.452, 0.11, 0.167, 0.298, 0.278, 0.22, 0.236]
[2m[36m(func pid=74157)[0m top1: 0.14085820895522388
[2m[36m(func pid=74157)[0m top5: 0.8894589552238806
[2m[36m(func pid=74157)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=74157)[0m f1_macro: 0.16196489510539963
[2m[36m(func pid=74157)[0m f1_weighted: 0.16242381083161844
[2m[36m(func pid=74157)[0m f1_per_class: [0.096, 0.077, 0.319, 0.07, 0.032, 0.207, 0.239, 0.521, 0.0, 0.06]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37826492537313433
[2m[36m(func pid=68221)[0m top5: 0.9221082089552238
[2m[36m(func pid=68221)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=68221)[0m f1_macro: 0.3850161961853053
[2m[36m(func pid=68221)[0m f1_weighted: 0.3855853113292968
[2m[36m(func pid=68221)[0m f1_per_class: [0.527, 0.482, 0.686, 0.495, 0.148, 0.226, 0.301, 0.345, 0.248, 0.391]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7801 | Steps: 2 | Val loss: 4.8132 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0008 | Steps: 2 | Val loss: 1.8432 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:05:26 (running for 00:46:01.62)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3675
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.002 |      0.385 |                   74 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.78  |      0.164 |                   48 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.15858208955223882
[2m[36m(func pid=74157)[0m top5: 0.9057835820895522
[2m[36m(func pid=74157)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=74157)[0m f1_macro: 0.1640177304175567
[2m[36m(func pid=74157)[0m f1_weighted: 0.13534733485937428
[2m[36m(func pid=74157)[0m f1_per_class: [0.106, 0.035, 0.513, 0.13, 0.05, 0.25, 0.131, 0.313, 0.019, 0.094]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.37406716417910446
[2m[36m(func pid=68221)[0m top5: 0.9202425373134329
[2m[36m(func pid=68221)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=68221)[0m f1_macro: 0.3788441796339118
[2m[36m(func pid=68221)[0m f1_weighted: 0.3837096120166629
[2m[36m(func pid=68221)[0m f1_per_class: [0.525, 0.478, 0.667, 0.492, 0.134, 0.22, 0.304, 0.343, 0.246, 0.379]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.8708 | Steps: 2 | Val loss: 11.5846 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.8382 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:05:32 (running for 00:46:07.02)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.379 |                   75 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.78  |      0.164 |                   48 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.14598880597014927
[2m[36m(func pid=74157)[0m top5: 0.8903917910447762
[2m[36m(func pid=74157)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=74157)[0m f1_macro: 0.13058160467294416
[2m[36m(func pid=74157)[0m f1_weighted: 0.12694879083000893
[2m[36m(func pid=74157)[0m f1_per_class: [0.093, 0.091, 0.231, 0.066, 0.06, 0.146, 0.176, 0.276, 0.093, 0.074]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.373134328358209
[2m[36m(func pid=68221)[0m top5: 0.9221082089552238
[2m[36m(func pid=68221)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=68221)[0m f1_macro: 0.3808239170251758
[2m[36m(func pid=68221)[0m f1_weighted: 0.3817587380931201
[2m[36m(func pid=68221)[0m f1_per_class: [0.547, 0.477, 0.667, 0.481, 0.137, 0.219, 0.307, 0.344, 0.25, 0.379]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 13.1532 | Steps: 2 | Val loss: 26.8928 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8377 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 04:05:37 (running for 00:46:12.48)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.381 |                   76 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.871 |      0.131 |                   49 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.16044776119402984
[2m[36m(func pid=74157)[0m top5: 0.875
[2m[36m(func pid=74157)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=74157)[0m f1_macro: 0.11356795954405659
[2m[36m(func pid=74157)[0m f1_weighted: 0.1409431181815377
[2m[36m(func pid=74157)[0m f1_per_class: [0.095, 0.112, 0.0, 0.029, 0.062, 0.096, 0.262, 0.313, 0.089, 0.077]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.375
[2m[36m(func pid=68221)[0m top5: 0.9216417910447762
[2m[36m(func pid=68221)[0m f1_micro: 0.375
[2m[36m(func pid=68221)[0m f1_macro: 0.38150078098621926
[2m[36m(func pid=68221)[0m f1_weighted: 0.3852928815990789
[2m[36m(func pid=68221)[0m f1_per_class: [0.547, 0.474, 0.667, 0.486, 0.139, 0.226, 0.314, 0.342, 0.245, 0.375]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8459 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8254 | Steps: 2 | Val loss: 27.3558 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 04:05:43 (running for 00:46:17.84)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.382 |                   77 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |  13.153 |      0.114 |                   50 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.3726679104477612
[2m[36m(func pid=68221)[0m top5: 0.9202425373134329
[2m[36m(func pid=68221)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=68221)[0m f1_macro: 0.38036107141303155
[2m[36m(func pid=68221)[0m f1_weighted: 0.38192557424024604
[2m[36m(func pid=68221)[0m f1_per_class: [0.557, 0.477, 0.667, 0.485, 0.135, 0.218, 0.305, 0.344, 0.245, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.22901119402985073
[2m[36m(func pid=74157)[0m top5: 0.8763992537313433
[2m[36m(func pid=74157)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=74157)[0m f1_macro: 0.16159931780407039
[2m[36m(func pid=74157)[0m f1_weighted: 0.23901515265576648
[2m[36m(func pid=74157)[0m f1_per_class: [0.088, 0.036, 0.0, 0.242, 0.053, 0.135, 0.4, 0.425, 0.055, 0.182]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8617 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.9795 | Steps: 2 | Val loss: 18.1306 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 04:05:48 (running for 00:46:23.13)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.38  |                   78 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.825 |      0.162 |                   51 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.37080223880597013
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=68221)[0m f1_macro: 0.3798242784473971
[2m[36m(func pid=68221)[0m f1_weighted: 0.38054485611638444
[2m[36m(func pid=68221)[0m f1_per_class: [0.554, 0.477, 0.667, 0.483, 0.131, 0.212, 0.304, 0.342, 0.258, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.279384328358209
[2m[36m(func pid=74157)[0m top5: 0.8782649253731343
[2m[36m(func pid=74157)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=74157)[0m f1_macro: 0.19308733325777547
[2m[36m(func pid=74157)[0m f1_weighted: 0.28697273213809105
[2m[36m(func pid=74157)[0m f1_per_class: [0.082, 0.31, 0.0, 0.048, 0.057, 0.256, 0.524, 0.536, 0.02, 0.098]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8664 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4417 | Steps: 2 | Val loss: 15.0168 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 04:05:53 (running for 00:46:28.34)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.38  |                   79 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.979 |      0.193 |                   52 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36847014925373134
[2m[36m(func pid=68221)[0m top5: 0.9160447761194029
[2m[36m(func pid=68221)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=68221)[0m f1_macro: 0.37909842069085997
[2m[36m(func pid=68221)[0m f1_weighted: 0.37745017204631454
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.472, 0.667, 0.474, 0.133, 0.228, 0.298, 0.35, 0.254, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.31763059701492535
[2m[36m(func pid=74157)[0m top5: 0.8610074626865671
[2m[36m(func pid=74157)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=74157)[0m f1_macro: 0.19998314020536156
[2m[36m(func pid=74157)[0m f1_weighted: 0.28836460418167015
[2m[36m(func pid=74157)[0m f1_per_class: [0.079, 0.452, 0.0, 0.007, 0.069, 0.296, 0.475, 0.508, 0.024, 0.091]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8620 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4445 | Steps: 2 | Val loss: 10.9994 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:05:59 (running for 00:46:33.83)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.379 |                   80 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.442 |      0.2   |                   53 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.3726679104477612
[2m[36m(func pid=68221)[0m top5: 0.9165111940298507
[2m[36m(func pid=68221)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=68221)[0m f1_macro: 0.38044723010339887
[2m[36m(func pid=68221)[0m f1_weighted: 0.3839741372195322
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.478, 0.667, 0.475, 0.132, 0.227, 0.317, 0.341, 0.258, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.2891791044776119
[2m[36m(func pid=74157)[0m top5: 0.8493470149253731
[2m[36m(func pid=74157)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=74157)[0m f1_macro: 0.19453945949408255
[2m[36m(func pid=74157)[0m f1_weighted: 0.28633301986435394
[2m[36m(func pid=74157)[0m f1_per_class: [0.086, 0.347, 0.0, 0.039, 0.061, 0.248, 0.513, 0.515, 0.049, 0.088]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0010 | Steps: 2 | Val loss: 1.8514 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.6385 | Steps: 2 | Val loss: 8.1095 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:06:04 (running for 00:46:39.40)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.38  |                   81 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.445 |      0.195 |                   54 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36986940298507465
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=68221)[0m f1_macro: 0.378251608712836
[2m[36m(func pid=68221)[0m f1_weighted: 0.3814026803477203
[2m[36m(func pid=68221)[0m f1_per_class: [0.547, 0.476, 0.667, 0.479, 0.132, 0.224, 0.31, 0.335, 0.242, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.2677238805970149
[2m[36m(func pid=74157)[0m top5: 0.8409514925373134
[2m[36m(func pid=74157)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=74157)[0m f1_macro: 0.22282727534637464
[2m[36m(func pid=74157)[0m f1_weighted: 0.2957285124838807
[2m[36m(func pid=74157)[0m f1_per_class: [0.088, 0.198, 0.15, 0.129, 0.07, 0.283, 0.512, 0.533, 0.163, 0.102]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8603 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5724 | Steps: 2 | Val loss: 8.2102 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 04:06:09 (running for 00:46:44.61)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.378 |                   82 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.639 |      0.223 |                   55 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.3689365671641791
[2m[36m(func pid=68221)[0m top5: 0.9155783582089553
[2m[36m(func pid=68221)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=68221)[0m f1_macro: 0.37305647125225205
[2m[36m(func pid=68221)[0m f1_weighted: 0.3815914533436114
[2m[36m(func pid=68221)[0m f1_per_class: [0.533, 0.472, 0.632, 0.468, 0.129, 0.216, 0.327, 0.333, 0.249, 0.371]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.30970149253731344
[2m[36m(func pid=74157)[0m top5: 0.8316231343283582
[2m[36m(func pid=74157)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=74157)[0m f1_macro: 0.23434551996033734
[2m[36m(func pid=74157)[0m f1_weighted: 0.3400825809869572
[2m[36m(func pid=74157)[0m f1_per_class: [0.108, 0.175, 0.083, 0.302, 0.104, 0.295, 0.51, 0.543, 0.112, 0.109]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0007 | Steps: 2 | Val loss: 1.8679 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.8462 | Steps: 2 | Val loss: 10.9164 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 04:06:15 (running for 00:46:49.97)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.373 |                   83 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.572 |      0.234 |                   56 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36986940298507465
[2m[36m(func pid=68221)[0m top5: 0.9169776119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=68221)[0m f1_macro: 0.37768877481877355
[2m[36m(func pid=68221)[0m f1_weighted: 0.3808889331615908
[2m[36m(func pid=68221)[0m f1_per_class: [0.533, 0.478, 0.649, 0.476, 0.127, 0.212, 0.313, 0.335, 0.259, 0.396]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.17957089552238806
[2m[36m(func pid=74157)[0m top5: 0.8283582089552238
[2m[36m(func pid=74157)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=74157)[0m f1_macro: 0.15305409379871301
[2m[36m(func pid=74157)[0m f1_weighted: 0.17457979644459728
[2m[36m(func pid=74157)[0m f1_per_class: [0.106, 0.221, 0.087, 0.064, 0.145, 0.283, 0.208, 0.329, 0.03, 0.058]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8689 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.7120 | Steps: 2 | Val loss: 13.5996 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 04:06:20 (running for 00:46:55.25)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.378 |                   84 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.846 |      0.153 |                   57 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.3675373134328358
[2m[36m(func pid=68221)[0m top5: 0.9146455223880597
[2m[36m(func pid=68221)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=68221)[0m f1_macro: 0.3743910624183074
[2m[36m(func pid=68221)[0m f1_weighted: 0.37943289307176725
[2m[36m(func pid=68221)[0m f1_per_class: [0.542, 0.475, 0.615, 0.467, 0.127, 0.208, 0.321, 0.332, 0.253, 0.404]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.1982276119402985
[2m[36m(func pid=74157)[0m top5: 0.8306902985074627
[2m[36m(func pid=74157)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=74157)[0m f1_macro: 0.16040039126868602
[2m[36m(func pid=74157)[0m f1_weighted: 0.18898655290612423
[2m[36m(func pid=74157)[0m f1_per_class: [0.091, 0.349, 0.034, 0.074, 0.196, 0.281, 0.175, 0.34, 0.0, 0.064]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3562 | Steps: 2 | Val loss: 6.2467 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8695 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 04:06:25 (running for 00:47:00.65)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.374 |                   85 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.712 |      0.16  |                   58 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.22621268656716417
[2m[36m(func pid=74157)[0m top5: 0.8451492537313433
[2m[36m(func pid=74157)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=74157)[0m f1_macro: 0.1725923909240949
[2m[36m(func pid=74157)[0m f1_weighted: 0.24918534799817726
[2m[36m(func pid=74157)[0m f1_per_class: [0.087, 0.307, 0.167, 0.1, 0.095, 0.334, 0.396, 0.118, 0.022, 0.101]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m top1: 0.36986940298507465
[2m[36m(func pid=68221)[0m top5: 0.914179104477612
[2m[36m(func pid=68221)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=68221)[0m f1_macro: 0.37206538817608864
[2m[36m(func pid=68221)[0m f1_weighted: 0.38354078910733497
[2m[36m(func pid=68221)[0m f1_per_class: [0.533, 0.479, 0.615, 0.474, 0.126, 0.212, 0.325, 0.337, 0.248, 0.37]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.7545 | Steps: 2 | Val loss: 7.1806 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.8659 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 04:06:31 (running for 00:47:06.09)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.372 |                   86 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.356 |      0.173 |                   59 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36986940298507465
[2m[36m(func pid=68221)[0m top5: 0.9151119402985075
[2m[36m(func pid=68221)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=68221)[0m f1_macro: 0.374750314287991
[2m[36m(func pid=68221)[0m f1_weighted: 0.38042109666865304
[2m[36m(func pid=68221)[0m f1_per_class: [0.532, 0.474, 0.615, 0.46, 0.129, 0.216, 0.327, 0.339, 0.25, 0.404]
[2m[36m(func pid=74157)[0m top1: 0.17490671641791045
[2m[36m(func pid=74157)[0m top5: 0.8250932835820896
[2m[36m(func pid=74157)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=74157)[0m f1_macro: 0.15980377119712946
[2m[36m(func pid=74157)[0m f1_weighted: 0.18203829690377052
[2m[36m(func pid=74157)[0m f1_per_class: [0.088, 0.09, 0.157, 0.098, 0.066, 0.324, 0.263, 0.286, 0.074, 0.154]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8831 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.9471 | Steps: 2 | Val loss: 6.4453 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 04:06:36 (running for 00:47:11.51)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.375 |                   87 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.755 |      0.16  |                   60 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36100746268656714
[2m[36m(func pid=68221)[0m top5: 0.9146455223880597
[2m[36m(func pid=68221)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=68221)[0m f1_macro: 0.36999073285622763
[2m[36m(func pid=68221)[0m f1_weighted: 0.3725004573936525
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.469, 0.615, 0.447, 0.129, 0.217, 0.317, 0.336, 0.246, 0.386]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.19309701492537312
[2m[36m(func pid=74157)[0m top5: 0.8339552238805971
[2m[36m(func pid=74157)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=74157)[0m f1_macro: 0.1647277358222632
[2m[36m(func pid=74157)[0m f1_weighted: 0.15327360652208555
[2m[36m(func pid=74157)[0m f1_per_class: [0.087, 0.047, 0.186, 0.121, 0.073, 0.355, 0.12, 0.516, 0.0, 0.142]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8659 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.0894 | Steps: 2 | Val loss: 6.0438 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 04:06:42 (running for 00:47:16.99)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.37  |                   88 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.947 |      0.165 |                   61 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36427238805970147
[2m[36m(func pid=68221)[0m top5: 0.9169776119402985
[2m[36m(func pid=68221)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=68221)[0m f1_macro: 0.3726593735062319
[2m[36m(func pid=68221)[0m f1_weighted: 0.3756432522064095
[2m[36m(func pid=68221)[0m f1_per_class: [0.537, 0.469, 0.632, 0.445, 0.128, 0.214, 0.33, 0.335, 0.246, 0.391]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.25326492537313433
[2m[36m(func pid=74157)[0m top5: 0.8129664179104478
[2m[36m(func pid=74157)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=74157)[0m f1_macro: 0.1570827834031477
[2m[36m(func pid=74157)[0m f1_weighted: 0.2586152069568508
[2m[36m(func pid=74157)[0m f1_per_class: [0.086, 0.016, 0.214, 0.23, 0.105, 0.355, 0.492, 0.0, 0.0, 0.074]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0012 | Steps: 2 | Val loss: 1.8530 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1459 | Steps: 2 | Val loss: 5.1373 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 04:06:47 (running for 00:47:22.25)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.375 |                   90 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.089 |      0.157 |                   62 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36847014925373134
[2m[36m(func pid=68221)[0m top5: 0.9174440298507462
[2m[36m(func pid=68221)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=68221)[0m f1_macro: 0.3747644805400243
[2m[36m(func pid=68221)[0m f1_weighted: 0.37985564322491294
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.47, 0.632, 0.468, 0.138, 0.214, 0.324, 0.325, 0.242, 0.391]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.23787313432835822
[2m[36m(func pid=74157)[0m top5: 0.8143656716417911
[2m[36m(func pid=74157)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=74157)[0m f1_macro: 0.14006725035170278
[2m[36m(func pid=74157)[0m f1_weighted: 0.2532737863426079
[2m[36m(func pid=74157)[0m f1_per_class: [0.09, 0.037, 0.216, 0.161, 0.0, 0.303, 0.55, 0.0, 0.0, 0.045]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.8563 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.9673 | Steps: 2 | Val loss: 3.9184 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 04:06:52 (running for 00:47:27.63)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.375 |                   91 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   2.146 |      0.14  |                   63 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.36847014925373134
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=68221)[0m f1_macro: 0.3754353942305465
[2m[36m(func pid=68221)[0m f1_weighted: 0.3794647494905488
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.472, 0.649, 0.472, 0.136, 0.203, 0.323, 0.323, 0.243, 0.391]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.1814365671641791
[2m[36m(func pid=74157)[0m top5: 0.7985074626865671
[2m[36m(func pid=74157)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=74157)[0m f1_macro: 0.1696768185903696
[2m[36m(func pid=74157)[0m f1_weighted: 0.16605217274644835
[2m[36m(func pid=74157)[0m f1_per_class: [0.123, 0.0, 0.19, 0.09, 0.196, 0.35, 0.228, 0.469, 0.0, 0.049]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8630 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.4747 | Steps: 2 | Val loss: 3.7085 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=68221)[0m top1: 0.3675373134328358
[2m[36m(func pid=68221)[0m top5: 0.9155783582089553
[2m[36m(func pid=68221)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=68221)[0m f1_macro: 0.3744930408641767
[2m[36m(func pid=68221)[0m f1_weighted: 0.3793518940033063
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.468, 0.632, 0.455, 0.135, 0.204, 0.34, 0.319, 0.244, 0.404]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:06:58 (running for 00:47:33.02)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.374 |                   92 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.967 |      0.17  |                   64 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.12779850746268656
[2m[36m(func pid=74157)[0m top5: 0.7262126865671642
[2m[36m(func pid=74157)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=74157)[0m f1_macro: 0.1342521710290427
[2m[36m(func pid=74157)[0m f1_weighted: 0.0883631644218147
[2m[36m(func pid=74157)[0m f1_per_class: [0.068, 0.0, 0.144, 0.094, 0.256, 0.276, 0.0, 0.443, 0.0, 0.062]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8430 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9742 | Steps: 2 | Val loss: 3.3204 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 04:07:03 (running for 00:47:38.33)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.375 |                   93 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.475 |      0.134 |                   65 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.37220149253731344
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=68221)[0m f1_macro: 0.37478813934408156
[2m[36m(func pid=68221)[0m f1_weighted: 0.38465639692295156
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.471, 0.615, 0.465, 0.133, 0.206, 0.345, 0.326, 0.247, 0.396]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.13432835820895522
[2m[36m(func pid=74157)[0m top5: 0.7532649253731343
[2m[36m(func pid=74157)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=74157)[0m f1_macro: 0.13568952225045433
[2m[36m(func pid=74157)[0m f1_weighted: 0.10817417317441591
[2m[36m(func pid=74157)[0m f1_per_class: [0.061, 0.016, 0.145, 0.145, 0.112, 0.267, 0.0, 0.526, 0.0, 0.086]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.8350 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.5815 | Steps: 2 | Val loss: 3.1048 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=68221)[0m top1: 0.37173507462686567
[2m[36m(func pid=68221)[0m top5: 0.9188432835820896
[2m[36m(func pid=68221)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=68221)[0m f1_macro: 0.3740498320878587
[2m[36m(func pid=68221)[0m f1_weighted: 0.3831857676533712
[2m[36m(func pid=68221)[0m f1_per_class: [0.53, 0.471, 0.615, 0.461, 0.133, 0.213, 0.34, 0.339, 0.247, 0.391]
[2m[36m(func pid=68221)[0m 
== Status ==
Current time: 2024-01-07 04:07:08 (running for 00:47:43.48)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.374 |                   94 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.974 |      0.136 |                   66 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.1515858208955224
[2m[36m(func pid=74157)[0m top5: 0.8176305970149254
[2m[36m(func pid=74157)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=74157)[0m f1_macro: 0.12645619241420455
[2m[36m(func pid=74157)[0m f1_weighted: 0.18026859049104255
[2m[36m(func pid=74157)[0m f1_per_class: [0.069, 0.173, 0.22, 0.173, 0.049, 0.121, 0.269, 0.062, 0.0, 0.129]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.8422 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7433 | Steps: 2 | Val loss: 2.8754 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 04:07:13 (running for 00:47:48.75)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.377 |                   95 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.582 |      0.126 |                   67 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.37406716417910446
[2m[36m(func pid=68221)[0m top5: 0.9197761194029851
[2m[36m(func pid=68221)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=68221)[0m f1_macro: 0.37721497534863124
[2m[36m(func pid=68221)[0m f1_weighted: 0.3862966281667389
[2m[36m(func pid=68221)[0m f1_per_class: [0.539, 0.473, 0.632, 0.468, 0.132, 0.209, 0.343, 0.337, 0.248, 0.391]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.21828358208955223
[2m[36m(func pid=74157)[0m top5: 0.8484141791044776
[2m[36m(func pid=74157)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=74157)[0m f1_macro: 0.15846037900734347
[2m[36m(func pid=74157)[0m f1_weighted: 0.2418687586000952
[2m[36m(func pid=74157)[0m f1_per_class: [0.078, 0.224, 0.458, 0.198, 0.058, 0.0, 0.477, 0.0, 0.0, 0.091]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.8439 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.6241 | Steps: 2 | Val loss: 2.7068 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 04:07:19 (running for 00:47:54.11)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.377 |                   96 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.743 |      0.158 |                   68 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.37546641791044777
[2m[36m(func pid=68221)[0m top5: 0.917910447761194
[2m[36m(func pid=68221)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=68221)[0m f1_macro: 0.37743091527740635
[2m[36m(func pid=68221)[0m f1_weighted: 0.38693168285767987
[2m[36m(func pid=68221)[0m f1_per_class: [0.538, 0.473, 0.615, 0.468, 0.134, 0.21, 0.344, 0.337, 0.25, 0.404]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.23694029850746268
[2m[36m(func pid=74157)[0m top5: 0.8456156716417911
[2m[36m(func pid=74157)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=74157)[0m f1_macro: 0.22993515638712664
[2m[36m(func pid=74157)[0m f1_weighted: 0.2821310775480688
[2m[36m(func pid=74157)[0m f1_per_class: [0.081, 0.297, 0.476, 0.198, 0.091, 0.082, 0.433, 0.498, 0.069, 0.074]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0003 | Steps: 2 | Val loss: 1.8356 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.5760 | Steps: 2 | Val loss: 2.6425 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 04:07:24 (running for 00:47:59.68)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.382 |                   97 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.624 |      0.23  |                   69 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.37453358208955223
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=68221)[0m f1_macro: 0.38198218693600877
[2m[36m(func pid=68221)[0m f1_weighted: 0.3852751117236699
[2m[36m(func pid=68221)[0m f1_per_class: [0.545, 0.473, 0.667, 0.462, 0.135, 0.214, 0.342, 0.341, 0.255, 0.387]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.15671641791044777
[2m[36m(func pid=74157)[0m top5: 0.8544776119402985
[2m[36m(func pid=74157)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=74157)[0m f1_macro: 0.17446471852177195
[2m[36m(func pid=74157)[0m f1_weighted: 0.12510176842253157
[2m[36m(func pid=74157)[0m f1_per_class: [0.087, 0.359, 0.407, 0.042, 0.104, 0.174, 0.006, 0.356, 0.082, 0.126]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0006 | Steps: 2 | Val loss: 1.8341 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7189 | Steps: 2 | Val loss: 2.6106 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 04:07:30 (running for 00:48:05.14)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.377 |                   98 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.576 |      0.174 |                   70 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.37406716417910446
[2m[36m(func pid=68221)[0m top5: 0.9193097014925373
[2m[36m(func pid=68221)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=68221)[0m f1_macro: 0.37722713062045465
[2m[36m(func pid=68221)[0m f1_weighted: 0.3843690991809298
[2m[36m(func pid=68221)[0m f1_per_class: [0.544, 0.475, 0.615, 0.468, 0.135, 0.211, 0.333, 0.341, 0.254, 0.396]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.13432835820895522
[2m[36m(func pid=74157)[0m top5: 0.8558768656716418
[2m[36m(func pid=74157)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=74157)[0m f1_macro: 0.1546420609211504
[2m[36m(func pid=74157)[0m f1_weighted: 0.07325101105597874
[2m[36m(func pid=74157)[0m f1_per_class: [0.092, 0.041, 0.31, 0.007, 0.087, 0.292, 0.0, 0.339, 0.119, 0.259]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0004 | Steps: 2 | Val loss: 1.8339 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.5261 | Steps: 2 | Val loss: 2.7011 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 04:07:35 (running for 00:48:10.59)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3715
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00022 | RUNNING    | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0     |      0.378 |                   99 |
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.719 |      0.155 |                   71 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.3763992537313433
[2m[36m(func pid=68221)[0m top5: 0.9183768656716418
[2m[36m(func pid=68221)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=68221)[0m f1_macro: 0.3779831935893621
[2m[36m(func pid=68221)[0m f1_weighted: 0.38685051310589935
[2m[36m(func pid=68221)[0m f1_per_class: [0.532, 0.473, 0.615, 0.469, 0.136, 0.229, 0.335, 0.343, 0.255, 0.391]
[2m[36m(func pid=68221)[0m 
[2m[36m(func pid=74157)[0m top1: 0.2537313432835821
[2m[36m(func pid=74157)[0m top5: 0.8498134328358209
[2m[36m(func pid=74157)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=74157)[0m f1_macro: 0.21674361587432678
[2m[36m(func pid=74157)[0m f1_weighted: 0.2417343347891363
[2m[36m(func pid=74157)[0m f1_per_class: [0.076, 0.0, 0.289, 0.013, 0.133, 0.316, 0.547, 0.514, 0.075, 0.202]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=68221)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0005 | Steps: 2 | Val loss: 1.8402 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.9233 | Steps: 2 | Val loss: 2.8439 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 04:07:41 (running for 00:48:15.90)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3715
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.526 |      0.217 |                   72 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
| train_2d480_00018 | TERMINATED | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   8.198 |      0.112 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=68221)[0m top1: 0.3736007462686567
[2m[36m(func pid=68221)[0m top5: 0.9197761194029851
[2m[36m(func pid=68221)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=68221)[0m f1_macro: 0.37872627825073496
[2m[36m(func pid=68221)[0m f1_weighted: 0.381273427416355
[2m[36m(func pid=68221)[0m f1_per_class: [0.541, 0.474, 0.632, 0.467, 0.14, 0.234, 0.314, 0.347, 0.26, 0.379]
[2m[36m(func pid=74157)[0m top1: 0.2728544776119403
[2m[36m(func pid=74157)[0m top5: 0.8236940298507462
[2m[36m(func pid=74157)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=74157)[0m f1_macro: 0.21905626162114672
[2m[36m(func pid=74157)[0m f1_weighted: 0.2643017962280686
[2m[36m(func pid=74157)[0m f1_per_class: [0.078, 0.0, 0.286, 0.066, 0.278, 0.288, 0.603, 0.443, 0.032, 0.118]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.3945 | Steps: 2 | Val loss: 2.9519 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 04:07:47 (running for 00:48:21.86)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3715
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.923 |      0.219 |                   73 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
| train_2d480_00018 | TERMINATED | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   8.198 |      0.112 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.197294776119403
[2m[36m(func pid=74157)[0m top5: 0.8138992537313433
[2m[36m(func pid=74157)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=74157)[0m f1_macro: 0.18631751877431738
[2m[36m(func pid=74157)[0m f1_weighted: 0.2204666924053138
[2m[36m(func pid=74157)[0m f1_per_class: [0.09, 0.015, 0.316, 0.085, 0.179, 0.125, 0.487, 0.495, 0.008, 0.064]
[2m[36m(func pid=74157)[0m 
[2m[36m(func pid=74157)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 9.1489 | Steps: 2 | Val loss: 2.2504 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 04:07:52 (running for 00:48:27.37)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3715
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00023 | RUNNING    | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   1.394 |      0.186 |                   74 |
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
| train_2d480_00018 | TERMINATED | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   8.198 |      0.112 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74157)[0m top1: 0.240205223880597
[2m[36m(func pid=74157)[0m top5: 0.871268656716418
[2m[36m(func pid=74157)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=74157)[0m f1_macro: 0.22966251094209394
[2m[36m(func pid=74157)[0m f1_weighted: 0.25794144349893106
[2m[36m(func pid=74157)[0m f1_per_class: [0.082, 0.207, 0.378, 0.094, 0.076, 0.243, 0.432, 0.512, 0.107, 0.166]
== Status ==
Current time: 2024-01-07 04:07:53 (running for 00:48:27.97)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.37124999999999997
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.17 GiB heap, 0.0/55.49 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_2d480_00000 | TERMINATED | 192.168.7.53:157245 | 0.0001 |       0.99 |         0      |   0.008 |      0.381 |                  100 |
| train_2d480_00001 | TERMINATED | 192.168.7.53:157619 | 0.001  |       0.99 |         0      |   0     |      0.416 |                  100 |
| train_2d480_00002 | TERMINATED | 192.168.7.53:158041 | 0.01   |       0.99 |         0      |   0.94  |      0.107 |                  100 |
| train_2d480_00003 | TERMINATED | 192.168.7.53:158460 | 0.1    |       0.99 |         0      |   6.146 |      0.098 |                   75 |
| train_2d480_00004 | TERMINATED | 192.168.7.53:174613 | 0.0001 |       0.9  |         0      |   0.612 |      0.215 |                   75 |
| train_2d480_00005 | TERMINATED | 192.168.7.53:180330 | 0.001  |       0.9  |         0      |   0.02  |      0.308 |                   75 |
| train_2d480_00006 | TERMINATED | 192.168.7.53:180846 | 0.01   |       0.9  |         0      |   0     |      0.384 |                  100 |
| train_2d480_00007 | TERMINATED | 192.168.7.53:181391 | 0.1    |       0.9  |         0      |   1.592 |      0.189 |                   75 |
| train_2d480_00008 | TERMINATED | 192.168.7.53:4220   | 0.0001 |       0.99 |         0.0001 |   0.007 |      0.393 |                  100 |
| train_2d480_00009 | TERMINATED | 192.168.7.53:10089  | 0.001  |       0.99 |         0.0001 |   0     |      0.424 |                  100 |
| train_2d480_00010 | TERMINATED | 192.168.7.53:10638  | 0.01   |       0.99 |         0.0001 |   1.37  |      0.048 |                   75 |
| train_2d480_00011 | TERMINATED | 192.168.7.53:16299  | 0.1    |       0.99 |         0.0001 | 203.976 |      0.009 |                   75 |
| train_2d480_00012 | TERMINATED | 192.168.7.53:27371  | 0.0001 |       0.9  |         0.0001 |   0.66  |      0.217 |                   75 |
| train_2d480_00013 | TERMINATED | 192.168.7.53:27838  | 0.001  |       0.9  |         0.0001 |   0.031 |      0.306 |                   75 |
| train_2d480_00014 | TERMINATED | 192.168.7.53:33107  | 0.01   |       0.9  |         0.0001 |   0     |      0.365 |                  100 |
| train_2d480_00015 | TERMINATED | 192.168.7.53:33680  | 0.1    |       0.9  |         0.0001 |   2.035 |      0.148 |                   75 |
| train_2d480_00016 | TERMINATED | 192.168.7.53:45033  | 0.0001 |       0.99 |         1e-05  |   0.028 |      0.349 |                   75 |
| train_2d480_00017 | TERMINATED | 192.168.7.53:45559  | 0.001  |       0.99 |         1e-05  |   0.001 |      0.392 |                  100 |
| train_2d480_00018 | TERMINATED | 192.168.7.53:50583  | 0.01   |       0.99 |         1e-05  |   8.198 |      0.112 |                   75 |
| train_2d480_00019 | TERMINATED | 192.168.7.53:56084  | 0.1    |       0.99 |         1e-05  |   2.594 |      0.097 |                   75 |
| train_2d480_00020 | TERMINATED | 192.168.7.53:62698  | 0.0001 |       0.9  |         1e-05  |   0.584 |      0.217 |                   75 |
| train_2d480_00021 | TERMINATED | 192.168.7.53:67665  | 0.001  |       0.9  |         1e-05  |   0.022 |      0.306 |                   75 |
| train_2d480_00022 | TERMINATED | 192.168.7.53:68221  | 0.01   |       0.9  |         1e-05  |   0.001 |      0.379 |                  100 |
| train_2d480_00023 | TERMINATED | 192.168.7.53:74157  | 0.1    |       0.9  |         1e-05  |   9.149 |      0.23  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+


2024-01-07 04:07:53,149	INFO tune.py:798 -- Total run time: 2909.04 seconds (2907.95 seconds for the tuning loop).
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341337.1 ON aap04 CANCELLED AT 2024-01-07T04:07:59 ***
srun: error: aap04: task 0: Exited with exit code 1
